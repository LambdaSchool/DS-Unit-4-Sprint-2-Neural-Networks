{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rethinking Backprop\n",
    "### and make your own neural net framework.\n",
    "\n",
    "## Why another guide?\n",
    "- Most guides are bad. Take it from me, a guy who has tried them all. Most of the code is very unhelpful. Usually its a combination of poor math explanations and code that just barely works. \n",
    "- \"example\" neural nets are the only time anyone tries to use functional programming paradigms, despite trying to make everyone conceptualize the layers as objects.\n",
    "- This guide does a better job of connecting the math to the code. \n",
    "- This code should more closely resemble the code found in top libraries and scales better than most examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward propagation of errors is easy to understand in the basic form of \"it's just passing the error rate through all the layers.\" But a deeper understanding is much more difficult.\n",
    "$$ \\delta l = \\sum ' (z^l)(w^{l+1})...\\sum ' (z^{L-1})(w^{L})\\sum ' (z^{L})\\nabla_a C $$ and $$ \\delta_j^L = \\frac{\\partial C}{\\partial a_k^L} \\frac{\\partial a_k^L}{\\partial z_J^L}$$\n",
    "The above is not as difficult as it looks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1],[0,1,1],[1,0,1],[0,1,0],[1,0,0],[1,1,1],[0,0,0]])\n",
    "y = np.array([[0],[1],[1],[1],[1],[0],[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 1, 1],\n",
       "       [1, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 1, 1],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defintions\n",
    "- Cost: this often means the sum of all the error for each prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward\n",
    "Individual Layer\n",
    "### Definition\n",
    "This is for one layer of the neural net. $$a^l = \\sigma (w^l a^{l-1} +b^l) $$\n",
    "\n",
    "### Math Components\n",
    "- $a^l$ is the activation vector of the current layer\n",
    "- z is the sum of multiplying the weights of the nodes times the input, and adding the bias \n",
    "- $ z = \\sum w^l \\cdot a^{l-1} + b^l $\n",
    "- $a^{l-1}$ is the activation vector from the previous layer\n",
    "- $w^l$ is the weight vector of the current layer. \n",
    "- $b^l$ is the bias vector of the layer\n",
    "- $\\sigma(z) = \\frac{1}{1+e^-z}$ is the sigmoid function\n",
    "    - The sigma function is used to compress the values into between 0 and 1. \n",
    "    - If the values passed to the next layer are too large(positive or negative) the net stops working properly.\n",
    "\n",
    "### Multiple Layers\n",
    "This function can be thought of as recursive. For a net with one hidden neuron it looks like this. X is the input data.\n",
    "$$ \\begin{align} a^l & = \\sigma (w^l a^{l-1} +b^l) \\\\\n",
    "a^{l-1} & = \\sigma (w^{l-1} a^{l-2} + b^{l-1}) \\\\\n",
    "a^{l-2} & = \\sigma (w^{l-2} X + b^{l-2}) \\end{align} \\\\ $$\n",
    "\n",
    "Putting this all together it would like.\n",
    "$$ a^l = \\sigma (w^l \\sigma (w^{l-1} \\sigma (w^{l-2} X + b^{l-2}) + b^{l-1}) +b^l) $$\n",
    "\n",
    "### Code components\n",
    "The code simplified\n",
    "```\n",
    "class InputLayer:\n",
    "    self.z = input_data\n",
    "class HiddenLayer:\n",
    "    def feedforward(self):\n",
    "        self.S = np.dot(self.upper_layer.z, self.weights) + self.bias\n",
    "        self.z = 1/(1+np.exp(-self.S))\n",
    "class OutputLayer:\n",
    "     def feedforward(self):\n",
    "         self.S = np.dot(self.upper_layer.z,self.weights) + self.bias\n",
    "         self.z = 1/(1+np.exp(-self.S))\n",
    "```\n",
    "---                                                  \n",
    "```\n",
    "i = InputLayer(X)\n",
    "l = HiddenLayer(upper_layer = i)\n",
    "l2 = OutputLayer(upper_layer=l)\n",
    "l.feedforward()\n",
    "l2.feedforward()\n",
    "```\n",
    "### Matching the code with the math\n",
    "```\n",
    "self.S = np.dot(self.upper_layer.z, self.weights) + self.bias\n",
    "self.z = 1/(1+np.exp(-self.S))\n",
    "```\n",
    "$ z = w^l \\cdot a^{l-1} + b^l $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation of Errors\n",
    "Backprop is just using the sum of all the error from the function to updatethe weights of the different layers.\n",
    "\n",
    "### Components\n",
    "- $\\delta^L $ is the output error, the true values minus the predicted values.\n",
    "- $\\partial $ designates the partial derivate of a function\n",
    "- $z$ & $a$ are the outputs and the activations of those outputs.\n",
    " \n",
    "\n",
    "### Starting with the last layer and some simplified math. \n",
    "$$\\begin{align}\\delta^L &= \\nabla_a C ⊙ \\sigma ' (z^L) \\\\\n",
    "  &= \\frac{\\partial C}{\\partial z^L} \\end{align}$$\n",
    "Using the chain rule, this equation is reshaped as\n",
    "$$ \\delta^L =\\frac{\\partial C}{\\partial a^L} \\frac{\\partial a^L}{\\partial z^L} $$\n",
    "\n",
    "We know what $a^L$ is, $\\sigma (z^L)$, that is $a^L$ is the activation of $z^L$. So the derivative of the activation function function applied to the layer, with respect to the layer is: $$ \\delta^L = \\frac{\\partial C}{\\delta a^L} \\sigma '(z^L) $$\n",
    "\n",
    "### Connecting for all the layers.\n",
    "The error for a hidden layer is:\n",
    "$$ \\delta^{L-1} = \\frac {\\partial C}{\\partial z^{L-1}} $$\n",
    "_(there is more going on in here than even I can follow right now. But I take the mathematicians word.)_  \n",
    "Chain ruling it leaves:\n",
    "$$\\begin{align} \\delta^{L-1} &= \\sum \\frac{\\partial C}{\\partial z^L} \\frac{\\partial z^L}{z^{L-1}} \\\\\n",
    " &= \\sum \\frac{\\partial z^L}{\\partial z^{L-1}} \\delta^L  \\\\\n",
    " &= \\sum w^L \\delta^L \\sigma ' (z^{L-1}) \n",
    " \\end{align} $$\n",
    "\n",
    "So each layer is connected to it's upper layer and the error rate is passed back that way.\n",
    "\n",
    "### Gradient Descent\n",
    "### Components\n",
    "- $\\eta $ is the learning rate\n",
    "- $\\mu $ is the number of samples going through the aloritm at the time, i.e. the size of the minibatch or the size of the of the data in the NAND Gate example.\n",
    "\n",
    "### Math\n",
    "Starting with $\\delta^{L-1} = \\sum w^L \\delta^L \\sigma ' (z^{L-1}) $ as an example layer. The weights of each layer are updated by the learning rate times the error of the layer.\n",
    "$$ w^L = w^L - \\frac{\\eta}{\\mu} \\sum w^L \\delta^L \\sigma ' (z^{L-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeing Backprop with code\n",
    "\n",
    "### Notes\n",
    "In code I use `harp` where $\\nabla$ would be used in math\n",
    "```\n",
    "class OutputLayer:\n",
    "    def backprop(self,y):\n",
    "        self.delta = (self.z - y) * self.activation_function_derivative(self.z)\n",
    "        #uses harp becuase the word nabla weirds me out. nabla is the ∇\n",
    "        self.harp_b = self.delta\n",
    "        self.harp_w = self.upper_layer.z.T @ self.delta\n",
    "        self.weights = self.weights + (self.learning_rate/self.delta.shape[0]) * self.harp_w\n",
    "        self.bias = self.bias + (self.learning_rate/self.delta.shape[0]) * self.harp_b\n",
    "```\n",
    "### Output layer error\n",
    "$ \\delta^L = \\nabla_a C ⊙ \\sigma ' (z^L) $  \n",
    "`self.delta = (self.z - y) * self.activation_function_derivative(self.z)`\n",
    "### gradient of weights and biases\n",
    "$ \\sum w^L \\delta^L \\sigma ' (z^{L-1}) $  \n",
    "`self.harp_w = self.upper_layer.z.T @ self.delta`  \n",
    "`self.harp_b = self.delta`\n",
    "\n",
    "### Updating the weights\n",
    "$ w^L = w^L - \\frac{\\eta}{\\mu} \\sum w^L \\delta^L \\sigma ' (z^{L-1})$  \n",
    "`self.weights = self.weights + (self.learning_rate/self.delta.shape[0]) * self.harp_w`  \n",
    "`self.delta.shape[0]` is hacky way of counting the number of samples that goes through the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- derivative of the sigmoid $\\sigma'(z) = \\sigma_x \\cdot (1 - \\sigma_x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(s):\n",
    "    return 1/1+np.exp(-s)\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "def quadratic_cost(true_values, predictions):\n",
    "    return predictions - true_values\n",
    "# def relu(s):\n",
    "#     return np.maximum(s,0)\n",
    "\n",
    "# def relu_prime(s):\n",
    "#     s[s<=0] = 0\n",
    "#     s[s>0] = 1\n",
    "#     return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer:\n",
    "    def __init__(self,input_data):\n",
    "        self.shape=input_data.shape\n",
    "        self.z = input_data\n",
    "        \n",
    "class HiddenLayer:\n",
    "    def __init__(self,nodes,activation_function,activation_function_derivative, upper_layer,learning_rate):\n",
    "        self.activation_function =  activation_function\n",
    "        self.activation_function_derivative = activation_function_derivative\n",
    "        self.upper_layer = upper_layer\n",
    "        self.weights = 2 * np.random.random(  (self.upper_layer.shape[1] ,nodes)) - 1\n",
    "        self.bias = np.random.random( (self.upper_layer.shape[0],1) )\n",
    "        self.learning_rate = learning_rate\n",
    "        self.shape = (self.upper_layer.shape[0], self.weights.shape[1])\n",
    "    \n",
    "    def add_lower_layer(self,l):\n",
    "        self.lower_layer=l\n",
    "        \n",
    "    def feedforward(self):\n",
    "        self.S = np.dot(self.upper_layer.z, self.weights) + self.bias\n",
    "        self.z = 1/(1+np.exp(-self.S))\n",
    "        \n",
    "    def backprop(self):\n",
    "        #Tricky part. This is Backprop.\n",
    "        self.delta = np.dot(self.lower_layer.delta, self.lower_layer.weights.transpose()) * self.activation_function_derivative(self.z)\n",
    "        self.harp_b = self.delta\n",
    "        self.harp_w = np.dot(self.upper_layer.z.transpose(), self.delta)\n",
    "        self.weights = self.weights + (self.learning_rate/self.delta.shape[0]) * self.harp_w\n",
    "        self.bias = self.bias + (self.learning_rate/self.delta.shape[0]) * self.harp_b\n",
    "        \n",
    "\n",
    "class OutputLayer:\n",
    "    def __init__(self,nodes,activation_function,activation_function_derivative, upper_layer,learning_rate,cost_function):\n",
    "        self.activation_function =  activation_function\n",
    "        self.upper_layer= upper_layer\n",
    "        self.cost_function = cost_function\n",
    "        self.activation_function_derivative = activation_function_derivative\n",
    "        self.weights= 2 * np.random.random(( self.upper_layer.shape[1], 1)) - 1\n",
    "        self.bias = np.random.random((self.upper_layer.shape[0],1))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.shape = (self.upper_layer.shape[0],self.weights.shape[1])\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.S = np.dot(self.upper_layer.z,self.weights) + self.bias\n",
    "        self.z = 1/(1+np.exp(-self.S))\n",
    "        \n",
    "    def backprop(self,y):\n",
    "        self.delta = self.cost_function(y, self.z) * self.activation_function_derivative(self.z)\n",
    "        #uses harp becuase the word nabla weirds me out. nabla is the ∇\n",
    "        self.harp_b = self.delta\n",
    "        self.harp_w = self.upper_layer.z.T @ self.delta\n",
    "        self.weights = self.weights + (self.learning_rate/self.delta.shape[0]) * self.harp_w\n",
    "        self.bias = self.bias + (self.learning_rate/self.delta.shape[0]) * self.harp_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = InputLayer(X)\n",
    "l = HiddenLayer(4,sigmoid,sigmoid_prime,i,0.1)\n",
    "l2 = OutputLayer(4,sigmoid,sigmoid_prime, l, 0.1,quadratic_cost)\n",
    "l.add_lower_layer(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,1000):\n",
    "    l.feedforward()\n",
    "    l2.feedforward()\n",
    "    l2.backprop(y)\n",
    "    l.backprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05170223],\n",
       "       [0.90307504],\n",
       "       [0.90649884],\n",
       "       [0.89662885],\n",
       "       [0.90343907],\n",
       "       [0.05962344],\n",
       "       [0.04555622]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2.z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
