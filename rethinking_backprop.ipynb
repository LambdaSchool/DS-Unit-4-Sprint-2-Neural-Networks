{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rethinking Backprop\n",
    "### and make your own neural net framework.\n",
    "\n",
    "## Why another guide?\n",
    "- Most guides are bad. Take it from me, a guy who has tried them all. Most of the code is very unhelpful. Usually its a combination of poor math explanations and code that just barely works. \n",
    "- \"example\" neural nets are the only time anyone tries to use functional programming paradigms, despite trying to make everyone conceptualize the layers as objects.\n",
    "- This guide does a better job of connecting the math to the code. \n",
    "- This code should more closely resemble the code found in top libraries and scales better than most examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward propagation of errors is easy to understand in the basic form of \"it's just passing the error rate through all the layers.\" But a deeper understanding is much more difficult.\n",
    "$$ \\delta l = \\sum ' (z^l)(w^{l+1})...\\sum ' (z^{L-1})(w^{L})\\sum ' (z^{L})\\nabla_a C $$ and $$ \\delta_j^L = \\frac{\\partial C}{\\partial a_k^L} \\frac{\\partial a_k^L}{\\partial z_J^L}$$\n",
    "The above is not as difficult as it looks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1],[0,1,1],[1,0,1],[0,1,0],[1,0,0],[1,1,1],[0,0,0]])\n",
    "y = np.array([[0],[1],[1],[1],[1],[0],[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 1, 1],\n",
       "       [1, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 1, 1],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defintions\n",
    "- Cost: this often means the sum of all the error for each prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward\n",
    "Individual Layer\n",
    "### Definition\n",
    "This is for one layer of the neural net. $$a^l = \\sigma (w^l a^{l-1} +b^l) $$\n",
    "\n",
    "### Math Components\n",
    "- $a^l$ is the activation vector of the current layer\n",
    "- z is the sum of multiplying the weights of the nodes times the input, and adding the bias \n",
    "- $ z = \\sum w^l \\cdot a^{l-1} + b^l $\n",
    "- $a^{l-1}$ is the activation vector from the previous layer\n",
    "- $w^l$ is the weight vector of the current layer. \n",
    "- $b^l$ is the bias vector of the layer\n",
    "- $\\sigma(z) = \\frac{1}{1+e^-z}$ is the sigmoid function\n",
    "    - The sigma function is used to compress the values into between 0 and 1. \n",
    "    - If the values passed to the next layer are too large(positive or negative) the net stops working properly.\n",
    "\n",
    "### Multiple Layers\n",
    "This function can be thought of as recursive. For a net with one hidden neuron it looks like this. X is the input data.\n",
    "$$ \\begin{align} a^l & = \\sigma (w^l a^{l-1} +b^l) \\\\\n",
    "a^{l-1} & = \\sigma (w^{l-1} a^{l-2} + b^{l-1}) \\\\\n",
    "a^{l-2} & = \\sigma (w^{l-2} X + b^{l-2}) \\end{align} \\\\ $$\n",
    "\n",
    "Putting this all together it would like.\n",
    "$$ a^l = \\sigma (w^l \\sigma (w^{l-1} \\sigma (w^{l-2} X + b^{l-2}) + b^{l-1}) +b^l) $$\n",
    "\n",
    "### Code components\n",
    "The code simplified\n",
    "```\n",
    "class InputLayer:\n",
    "    self.z = input_data\n",
    "class HiddenLayer:\n",
    "    def feedforward(self):\n",
    "        self.S = np.dot(self.upper_layer.z, self.weights) + self.bias\n",
    "        self.z = 1/(1+np.exp(-self.S))\n",
    "class OutputLayer:\n",
    "     def feedforward(self):\n",
    "         self.S = np.dot(self.upper_layer.z,self.weights) + self.bias\n",
    "         self.z = 1/(1+np.exp(-self.S))\n",
    "```\n",
    "---                                                  \n",
    "```\n",
    "i = InputLayer(X)\n",
    "l = HiddenLayer(upper_layer = i)\n",
    "l2 = OutputLayer(upper_layer=l)\n",
    "l.feedforward()\n",
    "l2.feedforward()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- derivative of the sigmoid $\\sigma'(z) = \\sigma_x \\cdot (1 - \\sigma_x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- derivative of the sigmoid $\\sigma'(z) = \\sigma_x \\cdot (1 - \\sigma_x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(s):\n",
    "    return 1/1+np.exp(-s)\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "# def relu(s):\n",
    "#     return np.maximum(s,0)\n",
    "\n",
    "# def relu_prime(s):\n",
    "#     s[s<=0] = 0\n",
    "#     s[s>0] = 1\n",
    "#     return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer:\n",
    "    def __init__(self,input_data):\n",
    "        self.shape=input_data.shape\n",
    "        self.z = input_data\n",
    "        \n",
    "class HiddenLayer:\n",
    "    def __init__(self,nodes,activation_function,activation_function_derivative, upper_layer,learning_rate):\n",
    "        self.activation_function =  activation_function\n",
    "        self.activation_function_derivative = activation_function_derivative\n",
    "        self.upper_layer = upper_layer\n",
    "        self.weights = 2 * np.random.random(  (self.upper_layer.shape[1] ,nodes)) - 1\n",
    "        self.bias = np.random.random( (self.upper_layer.shape[0],1) )\n",
    "        self.learning_rate = learning_rate\n",
    "        self.shape = (self.upper_layer.shape[0], self.weights.shape[1])\n",
    "    \n",
    "    def add_lower_layer(self,l):\n",
    "        self.lower_layer=l\n",
    "        \n",
    "    def feedforward(self):\n",
    "        self.S = np.dot(self.upper_layer.z, self.weights) + self.bias\n",
    "        self.z = 1/(1+np.exp(-self.S))\n",
    "        \n",
    "    def backprop(self):\n",
    "        #Tricky part. This is Backprop.\n",
    "        self.delta = np.dot(self.lower_layer.delta, self.lower_layer.weights.transpose()) * self.activation_function_derivative(self.z)\n",
    "        self.harp_b = self.delta\n",
    "        self.harp_w = np.dot(self.upper_layer.z.transpose(), self.delta)\n",
    "        self.weights = self.weights + (self.learning_rate/self.delta.shape[0]) * self.harp_w\n",
    "        self.bias = self.bias + (self.learning_rate/self.delta.shape[0]) * self.harp_b\n",
    "        \n",
    "\n",
    "class OutputLayer:\n",
    "    def __init__(self,nodes,activation_function,activation_function_derivative, upper_layer,learning_rate):\n",
    "        self.activation_function =  activation_function\n",
    "        self.upper_layer= upper_layer\n",
    "        self.activation_function_derivative = activation_function_derivative\n",
    "        self.weights= 2 * np.random.random(( self.upper_layer.shape[1], 1)) - 1\n",
    "        self.bias = np.random.random((self.upper_layer.shape[0],1))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.shape = (self.upper_layer.shape[0],self.weights.shape[1])\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.S = np.dot(self.upper_layer.z,self.weights) + self.bias\n",
    "        self.z = 1/(1+np.exp(-self.S))\n",
    "        \n",
    "    def backprop(self,y):\n",
    "        self.delta = (self.z - y) * self.activation_function_derivative(self.z)\n",
    "        #uses harp becuase the word nabla weirds me out. nabla is the âˆ‡\n",
    "        self.harp_b = self.delta\n",
    "        self.harp_w = self.upper_layer.z.T @ self.delta\n",
    "        self.weights = self.weights + (self.learning_rate/self.delta.shape[0]) * self.harp_w\n",
    "        self.bias = self.bias + (self.learning_rate/self.delta.shape[0]) * self.harp_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = InputLayer(X)\n",
    "l = HiddenLayer(4,sigmoid,sigmoid_prime,i,0.1)\n",
    "l2 = OutputLayer(4,sigmoid,sigmoid_prime, l, 0.1)\n",
    "l.add_lower_layer(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,1000):\n",
    "    l.feedforward()\n",
    "    l2.feedforward()\n",
    "    l2.backprop(y)\n",
    "    l.backprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04933674],\n",
       "       [0.90870435],\n",
       "       [0.88985425],\n",
       "       [0.90565332],\n",
       "       [0.90071043],\n",
       "       [0.04823712],\n",
       "       [0.06070833]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2.z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
