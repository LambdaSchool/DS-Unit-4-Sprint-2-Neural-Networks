{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "# Backpropagation Practice\n",
    "\n",
    "Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 0  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 1 |\n",
    "| 0  | 1  | 0  | 1 |\n",
    "| 1  | 0  | 0  | 1 |\n",
    "| 1  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 0  | 0 |\n",
    "\n",
    "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEREYT-3wI1f"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "X = np.array(([0,0,1],\n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [0,1,0],\n",
    "              [1,0,0],\n",
    "              [1,1,1],\n",
    "              [0,0,0]),dtype=float)\n",
    "y = np.array(([0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [1],\n",
    "              [1], \n",
    "              [0],\n",
    "              [0]),dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "  def __init__(self):\n",
    "    self.inputs = 3\n",
    "    self.hiddenNodes = 7\n",
    "    self.outputNodes = 1\n",
    "\n",
    "    # Initlize Weights\n",
    "    self.L1_weights = np.random.randn(self.inputs, self.hiddenNodes) # (3x2)\n",
    "    self.L2_weights = np.random.randn(self.hiddenNodes, self.outputNodes) # (3x1)\n",
    "\n",
    "  def feed_forward(self, X):\n",
    "    # Weighted sum between inputs and hidden layer\n",
    "    self.hidden_sum = np.dot(X, self.L1_weights)\n",
    "    # Activations of weighted sum\n",
    "    self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "    # Weighted sum between hidden and output\n",
    "    self.output_sum = np.dot(self.activated_hidden, self.L2_weights)\n",
    "    # final activation of output\n",
    "    self.activated_output = self.sigmoid(self.output_sum)\n",
    "    return self.activated_output\n",
    "    \n",
    "  def sigmoid(self, s):\n",
    "    return 1/(1+np.exp(-s))\n",
    "  \n",
    "  def sigmoidPrime(self, s):\n",
    "    return s * (1 - s)\n",
    "  \n",
    "  def backward(self, X, y, o):\n",
    "    # backward propgate through the network\n",
    "    self.o_error = y - o # error in output\n",
    "    self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to error\n",
    "\n",
    "    self.z2_error = self.o_delta.dot(self.L2_weights.T) # z2 error: how much our hidden layer weights contributed to output error\n",
    "    self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden) # applying derivative of sigmoid to z2 error\n",
    "\n",
    "    self.L1_weights += X.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights\n",
    "    self.L2_weights += self.activated_hidden.T.dot(self.o_delta) # adjusting second set (hidden --> output) weights\n",
    "    \n",
    "  def train (self, X, y):\n",
    "    o = self.feed_forward(X)\n",
    "    self.backward(X, y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------- EPOCH 1 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.54127005]\n",
      " [0.52945822]\n",
      " [0.60089716]\n",
      " [0.57606074]\n",
      " [0.65081152]\n",
      " [0.58037674]\n",
      " [0.5917784 ]]\n",
      "Loss: \n",
      "0.23748026209087486\n",
      "\n",
      "\n",
      "+---------- EPOCH 2 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.52899301]\n",
      " [0.53104114]\n",
      " [0.59431162]\n",
      " [0.58561424]\n",
      " [0.65354514]\n",
      " [0.58734501]\n",
      " [0.58853133]]\n",
      "Loss: \n",
      "0.23534698640508278\n",
      "\n",
      "\n",
      "+---------- EPOCH 3 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.51710461]\n",
      " [0.53245109]\n",
      " [0.58795283]\n",
      " [0.59452122]\n",
      " [0.65603849]\n",
      " [0.5938524 ]\n",
      " [0.58525523]]\n",
      "Loss: \n",
      "0.23338413741537536\n",
      "\n",
      "\n",
      "+---------- EPOCH 4 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.50579726]\n",
      " [0.53392417]\n",
      " [0.58208055]\n",
      " [0.60302934]\n",
      " [0.6585539 ]\n",
      " [0.60019879]\n",
      " [0.58214683]]\n",
      "Loss: \n",
      "0.23157412428901306\n",
      "\n",
      "\n",
      "+---------- EPOCH 5 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.49505235]\n",
      " [0.53546876]\n",
      " [0.57668563]\n",
      " [0.61115708]\n",
      " [0.66109009]\n",
      " [0.60639915]\n",
      " [0.57919234]]\n",
      "Loss: \n",
      "0.22990051439277395\n",
      "\n",
      "\n",
      "+---------- EPOCH 50 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.26421803]\n",
      " [0.63641383]\n",
      " [0.58828375]\n",
      " [0.77006379]\n",
      " [0.75671555]\n",
      " [0.7680747 ]\n",
      " [0.45465402]]\n",
      "Loss: \n",
      "0.182889050639168\n",
      "\n",
      "\n",
      "+---------- EPOCH 100 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.1601601 ]\n",
      " [0.66569902]\n",
      " [0.66384113]\n",
      " [0.7872495 ]\n",
      " [0.81352247]\n",
      " [0.71920995]\n",
      " [0.29196794]]\n",
      "Loss: \n",
      "0.13327943678506265\n",
      "\n",
      "\n",
      "+---------- EPOCH 150 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.12364282]\n",
      " [0.69488515]\n",
      " [0.69259728]\n",
      " [0.79618689]\n",
      " [0.84766634]\n",
      " [0.57028738]\n",
      " [0.19412577]]\n",
      "Loss: \n",
      "0.09007669968264839\n",
      "\n",
      "\n",
      "+---------- EPOCH 200 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.08950193]\n",
      " [0.78848065]\n",
      " [0.7758282 ]\n",
      " [0.83464296]\n",
      " [0.87848591]\n",
      " [0.36015789]\n",
      " [0.13525304]]\n",
      "Loss: \n",
      "0.041874248628945156\n",
      "\n",
      "\n",
      "+---------- EPOCH 250 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.06090168]\n",
      " [0.85767976]\n",
      " [0.84622052]\n",
      " [0.88060289]\n",
      " [0.90973044]\n",
      " [0.23101661]\n",
      " [0.10779076]]\n",
      "Loss: \n",
      "0.019286282768059344\n",
      "\n",
      "\n",
      "+---------- EPOCH 300 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.04512359]\n",
      " [0.89259192]\n",
      " [0.8829502 ]\n",
      " [0.90675432]\n",
      " [0.92843381]\n",
      " [0.17029611]\n",
      " [0.0922735 ]]\n",
      "Loss: \n",
      "0.0112292758640698\n",
      "\n",
      "\n",
      "+---------- EPOCH 350 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.03602026]\n",
      " [0.91234668]\n",
      " [0.90394805]\n",
      " [0.92224298]\n",
      " [0.93973503]\n",
      " [0.13704074]\n",
      " [0.08192439]]\n",
      "Loss: \n",
      "0.00762519029822695\n",
      "\n",
      "\n",
      "+---------- EPOCH 400 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.03020541]\n",
      " [0.92498997]\n",
      " [0.91745452]\n",
      " [0.93244921]\n",
      " [0.94724025]\n",
      " [0.11615141]\n",
      " [0.07439656]]\n",
      "Loss: \n",
      "0.005675046822598512\n",
      "\n",
      "\n",
      "+---------- EPOCH 450 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.02618166]\n",
      " [0.93381196]\n",
      " [0.92691368]\n",
      " [0.93972957]\n",
      " [0.9526173 ]\n",
      " [0.10175269]\n",
      " [0.06861072]]\n",
      "Loss: \n",
      "0.004478090223972894\n",
      "\n",
      "\n",
      "+---------- EPOCH 500 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.02322886]\n",
      " [0.94035176]\n",
      " [0.93394848]\n",
      " [0.94522292]\n",
      " [0.95668817]\n",
      " [0.09117127]\n",
      " [0.06398812]]\n",
      "Loss: \n",
      "0.0036776314553350355\n",
      "\n",
      "\n",
      "+---------- EPOCH 550 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.02096502]\n",
      " [0.94541794]\n",
      " [0.93941429]\n",
      " [0.94954112]\n",
      " [0.95989794]\n",
      " [0.083028  ]\n",
      " [0.06018687]]\n",
      "Loss: \n",
      "0.0031085346938208047\n",
      "\n",
      "\n",
      "+---------- EPOCH 600 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.01917021]\n",
      " [0.94947503]\n",
      " [0.94380346]\n",
      " [0.95304206]\n",
      " [0.96250784]\n",
      " [0.07654067]\n",
      " [0.05699048]]\n",
      "Loss: \n",
      "0.0026850599914780083\n",
      "\n",
      "\n",
      "+---------- EPOCH 650 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.01770929]\n",
      " [0.95280903]\n",
      " [0.94741959]\n",
      " [0.95594945]\n",
      " [0.96468142]\n",
      " [0.07123239]\n",
      " [0.05425451]]\n",
      "Loss: \n",
      "0.002358680613889689\n",
      "\n",
      "\n",
      "+---------- EPOCH 700 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.0164947 ]\n",
      " [0.95560586]\n",
      " [0.95046034]\n",
      " [0.95841069]\n",
      " [0.96652657]\n",
      " [0.06679548]\n",
      " [0.05187842]]\n",
      "Loss: \n",
      "0.002100034365519356\n",
      "\n",
      "\n",
      "+---------- EPOCH 750 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.01546721]\n",
      " [0.95799184]\n",
      " [0.95306014]\n",
      " [0.9605271 ]\n",
      " [0.96811748]\n",
      " [0.06302219]\n",
      " [0.04978989]]\n",
      "Loss: \n",
      "0.0018903862514679252\n",
      "\n",
      "\n",
      "+---------- EPOCH 800 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.01458534]\n",
      " [0.96005589]\n",
      " [0.95531381]\n",
      " [0.96237081]\n",
      " [0.96950702]\n",
      " [0.05976697]\n",
      " [0.04793536]]\n",
      "Loss: \n",
      "0.0017172551865732503\n",
      "\n",
      "\n",
      "+---------- EPOCH 850 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.01381914]\n",
      " [0.96186257]\n",
      " [0.95729024]\n",
      " [0.96399464]\n",
      " [0.97073395]\n",
      " [0.05692468]\n",
      " [0.04627425]]\n",
      "Loss: \n",
      "0.0015720241735163974\n",
      "\n",
      "\n",
      "+---------- EPOCH 900 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.01314644]\n",
      " [0.96345989]\n",
      " [0.95904078]\n",
      " [0.96543828]\n",
      " [0.97182738]\n",
      " [0.05441734]\n",
      " [0.04477517]]\n",
      "Loss: \n",
      "0.0014485627306003218\n",
      "\n",
      "\n",
      "+---------- EPOCH 950 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.01255045]\n",
      " [0.96488439]\n",
      " [0.96060452]\n",
      " [0.96673213]\n",
      " [0.97280968]\n",
      " [0.05218583]\n",
      " [0.04341343]]\n",
      "Loss: \n",
      "0.0013423964105862339\n",
      "\n",
      "\n",
      "+---------- EPOCH 1000 -----------+\n",
      "Input: \n",
      " [[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.01201824]\n",
      " [0.96616438]\n",
      " [0.96201183]\n",
      " [0.96789995]\n",
      " [0.9736983 ]\n",
      " [0.05018445]\n",
      " [0.04216927]]\n",
      "Loss: \n",
      "0.0012501866739572762\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "for i in range(1000): # trains the NN 1,000 times\n",
    "  if i+1 in [1,2,3,4,5] or (i+1) % 50 == 0:\n",
    "    print('+---------- EPOCH', i+1, '-----------+')\n",
    "    print(\"Input: \\n\", X) \n",
    "    print(\"Actual Output: \\n\", y)  \n",
    "    print(\"Predicted Output: \\n\" + str(NN.feed_forward(X))) \n",
    "    print(\"Loss: \\n\" + str(np.mean(np.square(y - NN.feed_forward(X))))) # mean sum squared loss\n",
    "    print(\"\\n\")\n",
    "  NN.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 3\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 7\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #Make empty list to store costs:\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000005\n",
      "         Iterations: 36\n",
      "         Function evaluations: 41\n",
      "         Gradient evaluations: 41\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "T = trainer(NN)\n",
    "T.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: \n",
      "[[6.30416178e-07]\n",
      " [9.99349120e-01]\n",
      " [9.97169964e-01]\n",
      " [9.99943879e-01]\n",
      " [1.00000000e+00]\n",
      " [7.62124269e-04]\n",
      " [1.29253985e-09]]\n",
      "Loss: \n",
      "1.2881041996458532e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Output: \\n\" + str(NN.forward(X))) \n",
    "print(\"Loss: \\n\" + str(np.mean(np.square(y - NN.forward(X))))) # mean sum squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(T.J)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show();     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8b-r70o8p2Dm"
   },
   "source": [
    "## Try building/training a more complex MLP on a bigger dataset.\n",
    "\n",
    "Use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the cannonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
    "\n",
    "If you need inspiration, the internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5MOPtYdk1HgA"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwlRJSfBlCvy"
   },
   "source": [
    "## Stretch Goals: \n",
    "\n",
    "- Implement Cross Validation model evaluation on your MNIST implementation \n",
    "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
    " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
    "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_432_Backprop_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
