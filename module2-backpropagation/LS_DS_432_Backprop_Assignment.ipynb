{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "# Backpropagation Practice\n",
    "\n",
    "Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 0  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 1 |\n",
    "| 0  | 1  | 0  | 1 |\n",
    "| 1  | 0  | 0  | 1 |\n",
    "| 1  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 0  | 0 |\n",
    "\n",
    "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEREYT-3wI1f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data for this example\n",
    "\n",
    "x1 = [0, 0, 1, 0, 1, 1, 0]\n",
    "x2 = [0, 1, 0, 1, 0, 1, 0]\n",
    "x3 = [1, 1, 1, 0, 0, 1, 0]\n",
    "y = [0, 1, 1, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 1.],\n",
       "       [0., 1., 1., 1.],\n",
       "       [1., 0., 1., 1.],\n",
       "       [0., 1., 0., 1.],\n",
       "       [1., 0., 0., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine x lists and one column vector of zeroes to make inputs matrix\n",
    "# The vector of zeroes represents bias\n",
    "\n",
    "inputs = np.array(list(zip(x1, x2, x3, np.ones(len(x1)))))\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The y list represents correct output; create a correct output vector\n",
    "\n",
    "correct_output = np.array([[val] for val in y])\n",
    "correct_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create specified perceptron; credit to Welch Labs and RA/LSDS\n",
    "# Note that this is a supervised learning problem, given the available input data\n",
    "\n",
    "\n",
    "class MultilayerPerceptron():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Define node size of input, hidden layer (one hidden layer only here), and output layer;\n",
    "        plus weights (two). These values are all fixed\n",
    "        '''\n",
    "        self.input_size = 3\n",
    "        self.hidden_layer_size = 4\n",
    "        self.output_layer_size = 1\n",
    "        \n",
    "        # Weights (parameters)\n",
    "        self.L1_weights = np.random.randn(self.input_size, self.hidden_layer_size)  # WL calls self.W1\n",
    "        self.L2_weights = np.random.randn(self.hidden_layer_size, self.output_layer_size)  # WL calls self.W2\n",
    "    \n",
    "    def forward(self):\n",
    "        '''Propagate inputs forward through network'''\n",
    "        # Weighted sum between inputs and hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.L1_weights)  # WL calls this self.z2; summation is the idea\n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)  # WL calls this self.a2\n",
    "        # Weighted sum between hidden layer and output layer\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.L2_weights)  # WL calls this self.z3\n",
    "        y_hat = self.sigmoid(self.output_sum)  # called y_hat because is an estimate of output data \n",
    "        return y_hat\n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "        '''Apply sigmoid activation function to scalar, vector, or matrix'''\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def sigmoid_prime(self, s):\n",
    "        '''Calculate gradient of sigmoid'''\n",
    "        return np.exp(-s) / ((1 + np.exp(-s))**2)\n",
    "            \n",
    "    def cost_function(self, X, y):\n",
    "        '''\n",
    "        Compute cost for given X, y, using weights already stored in class. Cost is a\n",
    "        measure of how incorrect model is after at least one complete forward propagation\n",
    "        '''\n",
    "        self.y_hat = self.forward(X)\n",
    "        J = 0.5 * sum((y - self.y_hat)**2)  # J is term for cost output unit\n",
    "        return J\n",
    "        \n",
    "    def cost_function_prime(self, X, y):\n",
    "        '''Compute derivative with respect to L1_weights and L2_weights for a given X and y'''\n",
    "        self.y_hat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y - self.y_hat), self.sigmoid_prime(self.output_sum))\n",
    "        dJdL2 = np.dot(self.activated_hidden.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.L2_weights.T) * self.sigmoid_prime(self.hidden_sum)\n",
    "        dJdL1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The gradient that comes from our gradient descent calculation will be the vector\n",
    "that takes us in the most downward direction along some function in hyperspace that\n",
    "has as many dimensions as we have weights--2, in this case\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MP = MultilayerPerceptron()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8b-r70o8p2Dm"
   },
   "source": [
    "## Try building/training a more complex MLP on a bigger dataset.\n",
    "\n",
    "Use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the cannonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
    "\n",
    "If you need inspiration, the internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5MOPtYdk1HgA"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwlRJSfBlCvy"
   },
   "source": [
    "## Stretch Goals: \n",
    "\n",
    "- Implement Cross Validation model evaluation on your MNIST implementation \n",
    "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
    " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
    "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_432_Backprop_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
