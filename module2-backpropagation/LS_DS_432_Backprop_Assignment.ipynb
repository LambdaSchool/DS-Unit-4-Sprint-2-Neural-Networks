{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "# Backpropagation Practice\n",
    "\n",
    "Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 0  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 1 |\n",
    "| 0  | 1  | 0  | 1 |\n",
    "| 1  | 0  | 0  | 1 |\n",
    "| 1  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 0  | 0 |\n",
    "\n",
    "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEREYT-3wI1f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data for this example\n",
    "\n",
    "x1 = [0, 0, 1, 0, 1, 1, 0]\n",
    "x2 = [0, 1, 0, 1, 0, 1, 0]\n",
    "x3 = [1, 1, 1, 0, 0, 1, 0]\n",
    "y = [0, 1, 1, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 1.],\n",
       "       [0., 1., 1., 1.],\n",
       "       [1., 0., 1., 1.],\n",
       "       [0., 1., 0., 1.],\n",
       "       [1., 0., 0., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine x lists and one column vector of zeroes to make inputs matrix\n",
    "# The vector of zeroes represents bias\n",
    "\n",
    "inputs = np.array(list(zip(x1, x2, x3, np.ones(len(x1)))))\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The y list represents correct output; create a correct output vector\n",
    "\n",
    "correct_output = np.array([[val] for val in y])\n",
    "correct_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create specified perceptron; credit to Welch Labs and RA/LSDS\n",
    "# Note that this is a supervised learning problem, given the available input data\n",
    "\n",
    "\n",
    "class MultilayerPerceptron():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Define node size of input, hidden layer (one hidden layer only here), and output layer;\n",
    "        plus weights (two). These values are all fixed\n",
    "        '''\n",
    "        self.input_size = 3 + 1  # includes column of ones\n",
    "        self.hidden_layer_size = 4\n",
    "        self.output_layer_size = 1\n",
    "        \n",
    "        # Weights (parameters)\n",
    "        self.L1_weights = np.random.randn(self.input_size, self.hidden_layer_size)  # WL calls self.W1\n",
    "        self.L2_weights = np.random.randn(self.hidden_layer_size, self.output_layer_size)  # WL calls self.W2\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''Propagate inputs forward through network'''\n",
    "        # Weighted sum between inputs and hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.L1_weights)  # WL calls this self.z2; summation is the idea\n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)  # WL calls this self.a2\n",
    "        # Weighted sum between hidden layer and output layer\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.L2_weights)  # WL calls this self.z3\n",
    "        y_hat = self.sigmoid(self.output_sum)  # called y_hat because is an estimate of output data \n",
    "        return y_hat\n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "        '''Apply sigmoid activation function to scalar, vector, or matrix'''\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def sigmoid_prime(self, s):\n",
    "        '''Calculate gradient of sigmoid'''\n",
    "        return np.exp(-s) / ((1 + np.exp(-s))**2)\n",
    "            \n",
    "    def cost_function(self, X, y):\n",
    "        '''\n",
    "        Compute cost for given X, y, using weights already stored in class. Cost is a\n",
    "        measure of how incorrect model is after at least one complete forward propagation\n",
    "        '''\n",
    "        self.y_hat = self.forward(X)\n",
    "        J = 0.5 * sum((y - self.y_hat)**2)  # J is term for cost output unit\n",
    "        return J\n",
    "        \n",
    "    def cost_function_prime(self, X, y):\n",
    "        '''Compute derivative with respect to L1_weights and L2_weights for a given X and y'''\n",
    "        self.y_hat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y - self.y_hat), self.sigmoid_prime(self.output_sum))\n",
    "        dJdL2 = np.dot(self.activated_hidden.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.L2_weights.T) * self.sigmoid_prime(self.hidden_sum)\n",
    "        dJdL1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdL1, dJdL2\n",
    "    \n",
    "    # Helper Functions for interacting with other classes\n",
    "    def get_params(self):\n",
    "        '''Get L1_weights and L2_weights unrolled into vector'''\n",
    "        params = np.concatenate((self.L1_weights.ravel(), self.L2_weights.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        '''Set L1 and L2 using single parameter vector'''\n",
    "        L1_start = 0\n",
    "        L1_end = self.hidden_layer_size * self.input_size\n",
    "        self.L1_weights = np.reshape(params[L1_start: L1_end], (self.input_size, self.hidden_layer_size))\n",
    "        L2_end = L1_end + self.hidden_layer_size * self.output_layer_size\n",
    "        self.L2_weights = np.reshape(params[L1_end: L2_end], (self.hidden_layer_size, self.output_layer_size))\n",
    "    \n",
    "    def compute_gradients(self, X, y):\n",
    "        '''\n",
    "        Returns the vector that takes us in the most downward direction along some function\n",
    "        in hyperspace that has as many dimensions as we have weights--2, in this case\n",
    "        '''\n",
    "        dJdL1, dJdL2 = self.cost_function_prime(X, y)\n",
    "        return np.concatenate((dJdL1.ravel(), dJdL2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer():\n",
    "    def __init__(self, N):\n",
    "        # Make Local reference to network\n",
    "        self.N = N\n",
    "    \n",
    "    def callback_func(self, params):\n",
    "        self.N.set_params(params)\n",
    "        self.J.append(self.N.cost_function(self.X, self.y))   \n",
    "        \n",
    "    def cost_function_wrapper(self, params, X, y):\n",
    "        self.N.set_params(params)\n",
    "        cost = self.N.cost_function(X, y)\n",
    "        grad = self.N.compute_gradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        # Make an internal variable for the callback function\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        # Make empty list to store costs\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.get_params()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.cost_function_wrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callback_func)\n",
    "\n",
    "        self.N.set_params(_res.x)\n",
    "        self.optimization_results = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MP = MultilayerPerceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = trainer(MP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 47\n",
      "         Function evaluations: 53\n",
      "         Gradient evaluations: 53\n"
     ]
    }
   ],
   "source": [
    "T.train(inputs, correct_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[1.06679013e-16]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.85108641e-06]\n",
      " [7.10327890e-04]]\n",
      "Loss: \n",
      "0.4896797163300973\n"
     ]
    }
   ],
   "source": [
    "print(\"Correct output: \\n\", correct_output)\n",
    "print(\"Predicted Output: \\n\" + str(MP.forward(inputs))) \n",
    "print(\"Loss: \\n\" + str(np.mean(np.square(y - MP.forward(inputs)))))  # mean sum squared loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8b-r70o8p2Dm"
   },
   "source": [
    "## Try building/training a more complex MLP on a bigger dataset.\n",
    "\n",
    "Use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the cannonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
    "\n",
    "If you need inspiration, the internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5MOPtYdk1HgA"
   },
   "outputs": [],
   "source": [
    "# See http://rasbt.github.io/mlxtend/user_guide/data/mnist_data/ \n",
    "from mlxtend.data import mnist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X, y matrices\n",
    "\n",
    "X, y = mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize values in X for rgb(0, 255)\n",
    "\n",
    "X = X / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 784)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect data\n",
    "\n",
    "# print('Dimensions: {} x {}'.format(X.shape[0], X.shape[1]), '\\n')\n",
    "# print('First 140 values in 0th row of X: \\n\\n', X[0][:140])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace y with a vector of probabilities that the digit is what it is - credit to DMA/LSDS01\n",
    "y_expanded = []\n",
    "for idx, num in enumerate(y):\n",
    "    row = np.zeros(10)\n",
    "    row[num] = 1\n",
    "    y_expanded.append(row)\n",
    "\n",
    "y = np.array(y_expanded)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build MLP - based, in part, on 3blue1brown [Sanderson] description at \n",
    "# https://www.youtube.com/watch?v=aircAruvnKk\n",
    "\n",
    "\n",
    "class MNistMultilayerPerceptron():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Define node count (size) for input data, each hidden layer, and output layer;\n",
    "        plus weights (three). These values are all fixed\n",
    "        '''\n",
    "        self.input_size = 784\n",
    "        self.hidden_layer1_size = 16\n",
    "        self.hidden_layer2_size = 16\n",
    "        self.output_layer_size = 10\n",
    "        \n",
    "        # Weights (parameters)\n",
    "        self.L1_weights = np.random.randn(self.input_size, self.hidden_layer1_size)\n",
    "        self.L2_weights = np.random.randn(self.hidden_layer1_size, self.hidden_layer2_size)\n",
    "        self.L3_weights = np.random.randn(self.hidden_layer2_size, self.output_layer_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''Propagate inputs forward through network'''\n",
    "        # Weighted sum between inputs and hidden layer\n",
    "        self.hidden_sum1 = np.dot(X, self.L1_weights)\n",
    "        # Activations of weighted sum 1\n",
    "        self.activated_hidden1 = self.sigmoid(self.hidden_sum1)\n",
    "        # Weighted sum between hidden layer 1 and hidden layer 2\n",
    "        self.hidden_sum2 = np.dot(self.activated_hidden1, self.L2_weights)\n",
    "        # Activations of weighted sum 2\n",
    "        self.activated_hidden2 = self.sigmoid(self.hidden_sum2)\n",
    "        # Weighted sum between hidden layer 2 and output layer\n",
    "        self.output_sum = np.dot(self.activated_hidden2, self.L3_weights)\n",
    "        y_hat = self.sigmoid(self.output_sum)\n",
    "        # print('When forward(self, X) is run, self.output_sum is:', self.output_sum)\n",
    "        return y_hat\n",
    "    \n",
    "    def relu(self, s):\n",
    "        '''Apply ReLU activation function'''\n",
    "        return np.maximum(s, 0)\n",
    "    \n",
    "    def relu_prime(self, s):\n",
    "        '''The derivative of ReLU is equivalent to the slope of the ReLU graph'''\n",
    "        # see https://kawahara.ca/what-is-the-derivative-of-relu/\n",
    "        if int(s) > 0: return 1\n",
    "        else: return 0\n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "        '''Apply sigmoid activation function to scalar, vector, or matrix'''\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def sigmoid_prime(self, s):\n",
    "        '''Calculate gradient of sigmoid'''\n",
    "        return np.exp(-s) / ((1 + np.exp(-s))**2)\n",
    "    \n",
    "    def cost_function(self, X, y):\n",
    "        '''\n",
    "        Compute cost for given X, y, using weights already stored in class. Cost is a\n",
    "        measure of how incorrect model is after at least one complete forward propagation\n",
    "        '''\n",
    "        self.y_hat = self.forward(X)\n",
    "        J = 0.5 * sum((y - self.y_hat)**2)  # J is term for cost output unit\n",
    "        return J\n",
    "    \n",
    "    def cost_function_prime(self, X, y):\n",
    "        '''Compute derivative with respect to L1_weights, L2_weights, and L3_weights for a given X and y'''\n",
    "        self.y_hat = self.forward(X)\n",
    "        \n",
    "        delta4 = np.multiply(-(y - self.y_hat), self.sigmoid_prime(self.output_sum))\n",
    "        dJdL3 = np.dot(self.activated_hidden2.T, delta4)\n",
    "        \n",
    "        delta3 = np.dot(delta4, self.L3_weights.T) * self.sigmoid_prime(self.hidden_sum2)\n",
    "        dJdL2 = np.dot(self.activated_hidden1.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.L2_weights.T) * self.sigmoid_prime(self.hidden_sum1)\n",
    "        dJdL1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdL1, dJdL2, dJdL3\n",
    "    \n",
    "    # Helper Functions for interacting with other classes\n",
    "    def get_params(self):\n",
    "        '''Get L1_weights, L2_weights, and L3_weights unrolled into vector'''\n",
    "        params = np.concatenate((self.L1_weights.ravel(), self.L2_weights.ravel(), self.L3_weights.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        '''Set L1, L2, and L3 using single parameter vector'''\n",
    "        L1_start = 0\n",
    "        L1_end = self.hidden_layer1_size * self.input_size\n",
    "        self.L1_weights = np.reshape(params[L1_start: L1_end], (self.input_size, self.hidden_layer1_size))\n",
    "        L2_end = L1_end + self.hidden_layer1_size * self.hidden_layer2_size\n",
    "        self.L2_weights = np.reshape(params[L1_end: L2_end], (self.hidden_layer1_size, self.hidden_layer2_size))\n",
    "        L3_end = L2_end + self.hidden_layer2_size * self.output_layer_size\n",
    "        self.L3_weights = np.reshape(params[L2_end: L3_end], (self.hidden_layer2_size, self.output_layer_size))\n",
    "    \n",
    "    def compute_gradients(self, X, y):\n",
    "        '''\n",
    "        Returns the vector that takes us in the most downward direction along some function\n",
    "        in hyperspace that has as many dimensions as we have weights--2, in this case\n",
    "        '''\n",
    "        dJdL1, dJdL2, dJdL3 = self.cost_function_prime(X, y)\n",
    "        return np.concatenate((dJdL1.ravel(), dJdL2.ravel(), dJdL3.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNMP = MNistMultilayerPerceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = trainer(MNMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-b642452af73c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-96e8a78c6d2f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'maxiter'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'disp'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         _res = optimize.minimize(self.cost_function_wrapper, params0, jac=True, method='BFGS', \\\n\u001b[1;32m---> 29\u001b[1;33m                                  args=(X, y), options=options, callback=self.callback_func)\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'bfgs'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'newton-cg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[0;32m    988\u001b[0m             \u001b[0malpha_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgfkp1\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m                      _line_search_wolfe12(f, myfprime, xk, pk, gfk,\n\u001b[1;32m--> 990\u001b[1;33m                                           old_fval, old_old_fval, amin=1e-100, amax=1e100)\n\u001b[0m\u001b[0;32m    991\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0m_LineSearchError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m             \u001b[1;31m# Line search failed to find a better solution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_line_search_wolfe12\u001b[1;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)\u001b[0m\n\u001b[0;32m    808\u001b[0m     ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n\u001b[0;32m    809\u001b[0m                              \u001b[0mold_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 810\u001b[1;33m                              **kwargs)\n\u001b[0m\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mextra_condition\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py\u001b[0m in \u001b[0;36mline_search_wolfe1\u001b[1;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, args, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[0;32m     99\u001b[0m     stp, fval, old_fval = scalar_search_wolfe1(\n\u001b[0;32m    100\u001b[0m             \u001b[0mphi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mderphi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mderphi0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m             c1=c1, c2=c2, amax=amax, amin=amin, xtol=xtol)\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py\u001b[0m in \u001b[0;36mscalar_search_wolfe1\u001b[1;34m(phi, derphi, phi0, old_phi0, derphi0, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mold_phi0\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mderphi0\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[0malpha1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.01\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphi0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mold_phi0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mderphi0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0malpha1\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[0malpha1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "T.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct output: \n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "Predicted Output: \n",
      "[[0.12428342 0.15411933 0.22286017 ... 0.832137   0.14956555 0.86184676]\n",
      " [0.11826002 0.08835938 0.18060253 ... 0.80371118 0.12031481 0.8609318 ]\n",
      " [0.21700312 0.11230913 0.28483903 ... 0.63891409 0.07989932 0.77786567]\n",
      " ...\n",
      " [0.18607539 0.07714101 0.75373593 ... 0.40616769 0.1026493  0.79189352]\n",
      " [0.14717859 0.06608888 0.4617302  ... 0.70407657 0.10759739 0.755218  ]\n",
      " [0.24524085 0.07316109 0.29127684 ... 0.64464306 0.14675023 0.8156784 ]]\n",
      "Loss: \n",
      "0.30263469146267535\n"
     ]
    }
   ],
   "source": [
    "print(\"Correct output: \\n\", y)\n",
    "print(\"Predicted Output: \\n\" + str(MNMP.forward(X))) \n",
    "print(\"Loss: \\n\" + str(np.mean(np.square(y - MNMP.forward(X)))))  # mean sum squared loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwlRJSfBlCvy"
   },
   "source": [
    "## Stretch Goals: \n",
    "\n",
    "- Implement Cross Validation model evaluation on your MNIST implementation \n",
    "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
    " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
    "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_432_Backprop_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
