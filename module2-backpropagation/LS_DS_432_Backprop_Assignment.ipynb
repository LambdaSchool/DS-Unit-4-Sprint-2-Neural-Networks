{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "# Backpropagation Practice\n",
    "\n",
    "Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 0  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 1 |\n",
    "| 0  | 1  | 0  | 1 |\n",
    "| 1  | 0  | 0  | 1 |\n",
    "| 1  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 0  | 0 |\n",
    "\n",
    "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEREYT-3wI1f"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8b-r70o8p2Dm"
   },
   "source": [
    "## Try building/training a more complex MLP on a bigger dataset.\n",
    "\n",
    "Use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the cannonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
    "\n",
    "If you need inspiration, the internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5MOPtYdk1HgA"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetMLP(object):\n",
    "    \"\"\"\n",
    "    Feed forward nueral network / multi-layer perceptron classifier\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_hidden : int (default: 30)\n",
    "        Number of hidden units\n",
    "    l2 : float (default 0.)\n",
    "        Lambda value for l2 normalization\n",
    "    epochs : int (default: 100)\n",
    "        Number of training epochs\n",
    "    eta : float (default = 0.001)\n",
    "        Learning rate\n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles the training data every epoch if True\n",
    "    minibatch_size : int (default : 1)\n",
    "        Number of training samples per minibatch\n",
    "    seed : int (default: None)\n",
    "        Random seed for initalizing weights and shuffling\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    eval_ : dict\n",
    "        Dictionary collecting the cost, training accuracy,\n",
    "        and validation accuracy for each epoch during training\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_hidden=30, l2=0.,\n",
    "                epochs=100, eta=0.001,\n",
    "                shuffle=True, minibatch_size=1, seed=None):\n",
    "        self.random = np.random.RandomState(seed)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatch_size = minibatch_size\n",
    "        \n",
    "    def _onehot(self, y, n_classes):\n",
    "        \"\"\"\n",
    "        Encode labels into one hot representation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array, shape = [n_samples]\n",
    "            Target values\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        onehot : array, shape = (n_samples, n_labels)\n",
    "        \"\"\"\n",
    "        \n",
    "        onehot = np.zeros((n_classes, y.shape[0]))\n",
    "        for idx, val in enumerate(y.astype(int)):\n",
    "            onehot[val, idx] = 1.\n",
    "        return onehot.T\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Compute logistic function (sigmoid)\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        \"\"\"\n",
    "        Compute forward propogation step\n",
    "        \"\"\"\n",
    "        # step 1: net input of hidden layer\n",
    "        z_h = np.dot(X, self.w_h) + self.b_h\n",
    "        \n",
    "        # step 2: activation of hidden layer\n",
    "        a_h = self._sigmoid(z_h)\n",
    "        \n",
    "        # step 3: net input of output layer\n",
    "        z_out = np.dot(a_h, self.w_out) + self.b_out\n",
    "        \n",
    "        # step 4: activation layer output\n",
    "        a_out = self._sigmoid(z_out)\n",
    "        \n",
    "        return z_h, a_h, z_out, a_out\n",
    "    \n",
    "    def _compute_cost(self, y_enc, output):\n",
    "        \"\"\"\n",
    "        Compute cost function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, shape = (n_samples, n_labels)\n",
    "            one-hot encoded class labels\n",
    "        output : array, shape = [n_samples, n_output_units]\n",
    "            Activation of the output layer (forward propoagation)\n",
    "        Returns\n",
    "        -------\n",
    "        cost : float\n",
    "            Regularized cost\n",
    "        \"\"\"\n",
    "        e = 0.000000001\n",
    "        L2_term = (self.l2 *\n",
    "                  (np.sum(self.w_h ** 2.) + \n",
    "                  np.sum(self.w_out ** 2.)))\n",
    "        term1 = -y_enc * (np.log(output + e))\n",
    "        term2 = (1. - y_enc) * np.log(1. - output + e)\n",
    "        cost = np.sum(term1 - term2) + L2_term\n",
    "        return cost\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels\n",
    "        \n",
    "        Paramters\n",
    "        ---------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array, shape = [n_samples]\n",
    "            Predicted class labels\n",
    "        \"\"\"\n",
    "        z_h, a_h, z_out, a_out = self._forward(X)\n",
    "        y_pred = np.argmax(z_out, axis=1)\n",
    "        return y_pred\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\"\n",
    "        Learn weights from training data\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features\n",
    "        y_train : array, shape = [n_samples]\n",
    "            Target class labels\n",
    "        X_test : array, shape = [n_samples, n_features]\n",
    "            Sample features for validation during training\n",
    "        y_test : array, shape = [n_samples]\n",
    "            Samples test class labels for validation during training\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        n_output = np.unique(y_train).shape[0]\n",
    "        n_features = X_train.shape[1]\n",
    "        \n",
    "        #######################\n",
    "        # Weight Initialization\n",
    "        #######################\n",
    "        \n",
    "        # weights for input -> hidden\n",
    "        self.b_h = np.zeros(self.n_hidden)\n",
    "        self.w_h = self.random.normal(loc=0.0,\n",
    "                                     scale=0.1,\n",
    "                                     size=(n_features,\n",
    "                                          self.n_hidden))\n",
    "        \n",
    "        # weights for hidden -> output\n",
    "        self.b_out = np.zeros(n_output)\n",
    "        self.w_out = self.random.normal(loc=0.0,\n",
    "                                       scale=0.1,\n",
    "                                       size=(self.n_hidden,\n",
    "                                       n_output))\n",
    "        \n",
    "        epoch_strlen = len(str(self.epochs)) # for progr. format\n",
    "        self.eval_ = {'cost' : [],\n",
    "                     'train_acc' : [],\n",
    "                     'valid_acc' : []}\n",
    "        \n",
    "        y_train_enc = self._onehot(y_train, n_output)\n",
    "        \n",
    "        # iterate over training epochs\n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            # iterate over mini batches\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            \n",
    "            if self.shuffle:\n",
    "                self.random.shuffle(indices)\n",
    "            \n",
    "            for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size):\n",
    "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
    "                \n",
    "                # forward propagation\n",
    "                z_h, a_h, z_out, a_out = self._forward(X_train[batch_idx])\n",
    "                \n",
    "                #################\n",
    "                # Backpropagation\n",
    "                #################\n",
    "                \n",
    "                # [n_samples, n_classlabels]\n",
    "                sigma_out = a_out - y_train_enc[batch_idx]\n",
    "                \n",
    "                # [n_samples, n_hidden]\n",
    "                sigmoid_derivative_h = a_h * (1. - a_h)\n",
    "                \n",
    "                # [n_samples, n_classlabels] dot [n_classlabels, n_hidden]\n",
    "                # -> [n_samples, n_hidden]\n",
    "                sigma_h = (np.dot(sigma_out, self.w_out.T) * sigmoid_derivative_h)\n",
    "                \n",
    "                # [n_features, n_samples] dot [n_samples, n_hidden]\n",
    "                # -> [n_features, n_hidden]\n",
    "                grad_w_h = np.dot(X_train[batch_idx].T, sigma_h)\n",
    "                grad_b_h = np.sum(sigma_h, axis=0)\n",
    "                \n",
    "                # [n_hidden, n_samples] dot [n_samples, n_classlabels]\n",
    "                # -> [n_hidden, n_classlabels]\n",
    "                grad_w_out = np.dot(a_h.T, sigma_out)\n",
    "                grad_b_out = np.sum(sigma_out, axis=0)\n",
    "                \n",
    "                # Regularization and weight updates\n",
    "                delta_w_h = (grad_w_h + self.l2 * self.w_h)\n",
    "                delta_b_h = grad_b_h # bias is not regularized\n",
    "                self.w_h -= self.eta * delta_w_h\n",
    "                self.b_h -= self.eta * delta_b_h\n",
    "                \n",
    "                delta_w_out = grad_w_out + self.l2 * self.w_out\n",
    "                delta_b_out = grad_b_out # bias is not regularized\n",
    "                self.w_out -= self.eta * delta_w_out\n",
    "                self.b_out -= self.eta * delta_b_out\n",
    "                \n",
    "            ############\n",
    "            # Evaluation\n",
    "            ############\n",
    "\n",
    "            # evaluation after each epoch during training\n",
    "            z_h, a_h, z_out, a_out = self._forward(X_train)\n",
    "            \n",
    "            cost = self._compute_cost(y_enc=y_train_enc, output=a_out)\n",
    "            \n",
    "            y_train_pred = self.predict(X_train)\n",
    "            y_valid_pred = self.predict(X_valid)\n",
    "            \n",
    "            train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float) / X_train.shape[0])\n",
    "            valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(np.float) / X_valid.shape[0])\n",
    "            \n",
    "            sys.stderr.write('\\r%0*d/%d | Cost: %.2f | Train/Valid Acc: %.2f%%/%.2f%%'\n",
    "                            % (epoch_strlen, i+1, self.epochs, cost, train_acc*100, valid_acc*100))\n",
    "            sys.stderr.flush()\n",
    "            \n",
    "            self.eval_['cost'].append(cost)\n",
    "            self.eval_['train_acc'].append(train_acc)\n",
    "            self.eval_['valid_acc'].append(valid_acc)\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwlRJSfBlCvy"
   },
   "source": [
    "## Stretch Goals: \n",
    "\n",
    "- Implement Cross Validation model evaluation on your MNIST implementation \n",
    "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
    " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
    "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_432_Backprop_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
