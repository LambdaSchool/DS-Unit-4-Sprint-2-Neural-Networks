{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "# Backpropagation Practice\n",
    "\n",
    "Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 0  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 1 |\n",
    "| 0  | 1  | 0  | 1 |\n",
    "| 1  | 0  | 0  | 1 |\n",
    "| 1  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 0  | 0 |\n",
    "\n",
    "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEREYT-3wI1f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data for this example\n",
    "\n",
    "x1 = [0, 0, 1, 0, 1, 1, 0]\n",
    "x2 = [0, 1, 0, 1, 0, 1, 0]\n",
    "x3 = [1, 1, 1, 0, 0, 1, 0]\n",
    "y = [0, 1, 1, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 1.],\n",
       "       [0., 1., 1., 1.],\n",
       "       [1., 0., 1., 1.],\n",
       "       [0., 1., 0., 1.],\n",
       "       [1., 0., 0., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine x lists and one column vector of zeroes to make inputs matrix\n",
    "# The vector of zeroes represents bias\n",
    "\n",
    "inputs = np.array(list(zip(x1, x2, x3, np.ones(len(x1)))))\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The y list represents correct output; create a correct output vector\n",
    "\n",
    "correct_output = np.array([[val] for val in y])\n",
    "correct_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create specified perceptron; credit to Welch Labs and RA/LSDS\n",
    "# Note that this is a supervised learning problem, given the available input data\n",
    "\n",
    "\n",
    "class MultilayerPerceptron():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Define node size of input, hidden layer (one hidden layer only here), and output layer;\n",
    "        plus weights (two). These values are all fixed\n",
    "        '''\n",
    "        self.input_size = 3 + 1  # includes column of ones\n",
    "        self.hidden_layer_size = 4\n",
    "        self.output_layer_size = 1\n",
    "        \n",
    "        # Weights (parameters)\n",
    "        self.L1_weights = np.random.randn(self.input_size, self.hidden_layer_size)  # WL calls self.W1\n",
    "        self.L2_weights = np.random.randn(self.hidden_layer_size, self.output_layer_size)  # WL calls self.W2\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''Propagate inputs forward through network'''\n",
    "        # Weighted sum between inputs and hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.L1_weights)  # WL calls this self.z2; summation is the idea\n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)  # WL calls this self.a2\n",
    "        # Weighted sum between hidden layer and output layer\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.L2_weights)  # WL calls this self.z3\n",
    "        y_hat = self.sigmoid(self.output_sum)  # called y_hat because is an estimate of output data \n",
    "        return y_hat\n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "        '''Apply sigmoid activation function to scalar, vector, or matrix'''\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def sigmoid_prime(self, s):\n",
    "        '''Calculate gradient of sigmoid'''\n",
    "        return np.exp(-s) / ((1 + np.exp(-s))**2)\n",
    "            \n",
    "    def cost_function(self, X, y):\n",
    "        '''\n",
    "        Compute cost for given X, y, using weights already stored in class. Cost is a\n",
    "        measure of how incorrect model is after at least one complete forward propagation\n",
    "        '''\n",
    "        self.y_hat = self.forward(X)\n",
    "        J = 0.5 * sum((y - self.y_hat)**2)  # J is term for cost output unit\n",
    "        return J\n",
    "        \n",
    "    def cost_function_prime(self, X, y):\n",
    "        '''Compute derivative with respect to L1_weights and L2_weights for a given X and y'''\n",
    "        self.y_hat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y - self.y_hat), self.sigmoid_prime(self.output_sum))\n",
    "        dJdL2 = np.dot(self.activated_hidden.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.L2_weights.T) * self.sigmoid_prime(self.hidden_sum)\n",
    "        dJdL1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdL1, dJdL2\n",
    "    \n",
    "    # Helper Functions for interacting with other classes\n",
    "    def get_params(self):\n",
    "        '''Get L1_weights and L2_weights unrolled into vector'''\n",
    "        params = np.concatenate((self.L1_weights.ravel(), self.L2_weights.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        '''Set L1 and L2 using single parameter vector'''\n",
    "        L1_start = 0\n",
    "        L1_end = self.hidden_layer_size * self.input_size\n",
    "        self.L1_weights = np.reshape(params[L1_start: L1_end], (self.input_size, self.hidden_layer_size))\n",
    "        L2_end = L1_end + self.hidden_layer_size * self.output_layer_size\n",
    "        self.L2_weights = np.reshape(params[L1_end: L2_end], (self.hidden_layer_size, self.output_layer_size))\n",
    "    \n",
    "    def compute_gradients(self, X, y):\n",
    "        '''\n",
    "        Returns the vector that takes us in the most downward direction along some function\n",
    "        in hyperspace that has as many dimensions as we have weights--2, in this case\n",
    "        '''\n",
    "        dJdL1, dJdL2 = self.cost_function_prime(X, y)\n",
    "        return np.concatenate((dJdL1.ravel(), dJdL2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer():\n",
    "    def __init__(self, N):\n",
    "        # Make Local reference to network\n",
    "        self.N = N\n",
    "    \n",
    "    def callback_func(self, params):\n",
    "        self.N.set_params(params)\n",
    "        self.J.append(self.N.cost_function(self.X, self.y))   \n",
    "        \n",
    "    def cost_function_wrapper(self, params, X, y):\n",
    "        self.N.set_params(params)\n",
    "        cost = self.N.cost_function(X, y)\n",
    "        grad = self.N.compute_gradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        # Make an internal variable for the callback function\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        # Make empty list to store costs\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.get_params()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.cost_function_wrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callback_func)\n",
    "\n",
    "        self.N.set_params(_res.x)\n",
    "        self.optimization_results = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MP = MultilayerPerceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = trainer(MP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 63\n",
      "         Function evaluations: 74\n",
      "         Gradient evaluations: 74\n"
     ]
    }
   ],
   "source": [
    "T.train(inputs, correct_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[2.98624969e-09]\n",
      " [9.99794173e-01]\n",
      " [9.99104814e-01]\n",
      " [9.99998619e-01]\n",
      " [9.99968123e-01]\n",
      " [2.57418147e-06]\n",
      " [1.57426799e-06]]\n",
      "Loss: \n",
      "0.48965647090597075\n"
     ]
    }
   ],
   "source": [
    "print(\"Correct output: \\n\", correct_output)\n",
    "print(\"Predicted Output: \\n\" + str(MP.forward(inputs))) \n",
    "print(\"Loss: \\n\" + str(np.mean(np.square(y - MP.forward(inputs)))))  # mean sum squared loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8b-r70o8p2Dm"
   },
   "source": [
    "## Try building/training a more complex MLP on a bigger dataset.\n",
    "\n",
    "Use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the cannonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
    "\n",
    "If you need inspiration, the internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5MOPtYdk1HgA"
   },
   "outputs": [],
   "source": [
    "# See http://rasbt.github.io/mlxtend/user_guide/data/mnist_data/ \n",
    "from mlxtend.data import mnist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X, y matrices\n",
    "\n",
    "X, y = mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: 5000 x 784 \n",
      "\n",
      "First 140 values in 0th row of X: \n",
      "\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.  51. 159. 253. 159.  50.   0.   0.   0.   0.   0.   0.   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "# Inspect data\n",
    "\n",
    "print('Dimensions: {} x {}'.format(X.shape[0], X.shape[1]), '\\n')\n",
    "print('First 140 values in 0th row of X: \\n\\n', X[0][:140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    500\n",
       "3    500\n",
       "6    500\n",
       "2    500\n",
       "9    500\n",
       "5    500\n",
       "1    500\n",
       "8    500\n",
       "4    500\n",
       "0    500\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df = pd.DataFrame(y)\n",
    "y_df[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build MLP - based, in part, on 3blue1brown [Sanderson] description at \n",
    "# https://www.youtube.com/watch?v=aircAruvnKk\n",
    "\n",
    "\n",
    "class MNistMultilayerPerceptron():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Define node count (size) for input data, each hidden layer, and output layer;\n",
    "        plus weights (three). These values are all fixed\n",
    "        '''\n",
    "        self.input_size = 784\n",
    "        self.hidden_layer1_size = 16\n",
    "        self.hidden_layer2_size = 16\n",
    "        self.output_layer_size = 9\n",
    "        \n",
    "        # Weights (parameters)\n",
    "        self.L1_weights = np.random.randn(self.input_size, self.hidden_layer1_size)\n",
    "        self.L2_weights = np.random.randn(self.hidden_layer1_size, self.hidden_layer2_size)\n",
    "        self.L3_weights = np.random.randn(self.hidden_layer2_size, self.output_layer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNMP = MNistMultilayerPerceptron()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwlRJSfBlCvy"
   },
   "source": [
    "## Stretch Goals: \n",
    "\n",
    "- Implement Cross Validation model evaluation on your MNIST implementation \n",
    "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
    " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
    "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_432_Backprop_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
