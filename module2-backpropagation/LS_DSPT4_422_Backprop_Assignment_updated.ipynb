{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Backpropagation-Practice\" data-toc-modified-id=\"Backpropagation-Practice-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Backpropagation Practice</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Science-Unit-4-Sprint-2-Assignment-2\" data-toc-modified-id=\"Data-Science-Unit-4-Sprint-2-Assignment-2-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span><em>Data Science Unit 4 Sprint 2 Assignment 2</em></a></span></li><li><span><a href=\"#Try-building/training-a-more-complex-MLP-on-a-bigger-dataset.\" data-toc-modified-id=\"Try-building/training-a-more-complex-MLP-on-a-bigger-dataset.-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Try building/training a more complex MLP on a bigger dataset.</a></span></li><li><span><a href=\"#Stretch-Goals:\" data-toc-modified-id=\"Stretch-Goals:-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Stretch Goals:</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Backpropagation Practice\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 2*\n",
    "\n",
    "Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 0  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 1 |\n",
    "| 0  | 1  | 0  | 1 |\n",
    "| 1  | 0  | 0  | 1 |\n",
    "| 1  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 0  | 0 |\n",
    "\n",
    "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEREYT-3wI1f"
   },
   "outputs": [],
   "source": [
    "# doing some imports\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   x1  x2  x3  y\n0   0   0   1  0\n1   0   1   1  1\n2   1   0   1  1\n3   0   1   0  1\n4   1   0   0  1\n5   1   1   1  0\n6   0   0   0  0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>x3</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# creating the df\n",
    "df = pd.DataFrame({\"x1\": [0,0,1,0,1,1,0], \"x2\": [0,1,0,1,0,1,0], \"x3\": [1,1,1,0,0,1,0], \"y\": [0,1,1,1,1,0,0]})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((7, 3), (7,))"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "\n",
    "X = df[[\"x1\", \"x2\", \"x3\"]]\n",
    "y = df[\"y\"]\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating my class for the multilayered percetron\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, inputs=3, hidden=4, output=1):\n",
    "        # setting up the architecture\n",
    "        self.inputs = inputs\n",
    "        self.hidden = hidden\n",
    "        self.output = output\n",
    "\n",
    "        # Will initialize the weights randomly\n",
    "        self.weights1 = np.random.randn(self.inputs, self.hidden)\n",
    "        self.weights2 = np.random.randn(self.hidden, self.output)\n",
    "\n",
    "    # doing the sigmoid function \n",
    "    def __sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    # doing the feedforward\n",
    "    def feedforward(self, X):\n",
    "\n",
    "        # creating the sums for the specific nodes\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "\n",
    "        self.activated_hidden = self.__sigmoid(self.hidden_sum)\n",
    "        # now sending to the  output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        # do the acitivation for the output layer\n",
    "        self.activated_output = self.__sigmoid(self.output_sum)\n",
    "\n",
    "        return self.activated_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "x1    0\nx2    0\nx3    1\nName: 0, dtype: int64"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "X.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "correct was  0\nguess was  [0.11922027]\n"
    }
   ],
   "source": [
    "# trying a prediction with just the feed forward\n",
    "net = Network()\n",
    "output = net.feedforward(X.iloc[0])\n",
    "print(\"correct was \", y.iloc[0])\n",
    "print(\"guess was \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating my class for the multilayered percetron\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, inputs=3, hidden=4, output=1):\n",
    "        # setting up the architecture\n",
    "        self.inputs = inputs\n",
    "        self.hidden = hidden\n",
    "        self.output = output\n",
    "\n",
    "        # Will initialize the weights randomly\n",
    "        self.weights1 = np.random.randn(self.inputs, self.hidden)\n",
    "        self.weights2 = np.random.randn(self.hidden, self.output)\n",
    "\n",
    "    # doing the sigmoid function \n",
    "    def __sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    # the sigmoid prime or derivitive method\n",
    "    def __sigmoidPrime(self, x):\n",
    "        sig = self.__sigmoid(x)\n",
    "        return sig * (1 - sig)\n",
    "\n",
    "    # making a function that will make the y the same shape as the 0\n",
    "    def __make_right_shape(self, o, y):\n",
    "        if o.shape != y.shape:\n",
    "            # turning the y into a numpy array\n",
    "            numpy_y = np.array(y)\n",
    "            newShape = np.reshape(numpy_y, o.shape)\n",
    "            return newShape\n",
    "        else:\n",
    "            return y\n",
    "        \n",
    "\n",
    "    # function to change the output to a whole number (1 or 0)\n",
    "    def change_to_whole_num(self, output):\n",
    "        #min = a if a < b else b \n",
    "        #output = 1 if output > .5 else 0 \n",
    "        theList = []\n",
    "        for val in output:\n",
    "            if val < .5:\n",
    "                theList.append(0)\n",
    "            else:\n",
    "                theList.append(1)\n",
    "        return theList\n",
    "            \n",
    "\n",
    "    # doing the feedforward\n",
    "    def feedforward(self, X):\n",
    "\n",
    "        # creating the sums for the specific nodes\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "\n",
    "        self.activated_hidden = self.__sigmoid(self.hidden_sum)\n",
    "        # now sending to the  output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        # do the acitivation for the output layer\n",
    "        self.activated_output = self.__sigmoid(self.output_sum)\n",
    "\n",
    "        return self.activated_output\n",
    "\n",
    "    # doing some more methods\n",
    "    def backprop(self, X, y, o):\n",
    "        #breakpoint()\n",
    "\n",
    "        # making sure the shape is the right shape\n",
    "        y = self.__make_right_shape(o, y)\n",
    "\n",
    "        self.o_error = y - o\n",
    "\n",
    "        # Apply derivative of sigmoid to the  error \n",
    "        # to find the amount to adjust\n",
    "        self.o_delta = self.o_error * self.__sigmoidPrime(self.output_sum)\n",
    "\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "\n",
    "        # getting the z2 delta\n",
    "        self.z2_delta = self.z2_error * self.__sigmoidPrime(self.hidden_sum)\n",
    "\n",
    "        self.weights1 = self.weights1 + X.T.dot(self.z2_delta)\n",
    "        self.weights2 = self.weights2 + self.activated_hidden.T.dot(self.o_delta)\n",
    "    \n",
    "    # run Epoch\n",
    "    def __run_epoch_W_back_prop(self, X, y):\n",
    "        output = self.feedforward(X)\n",
    "        self.backprop(X,y, output)\n",
    "    # building the function that will do the training for the neural network\n",
    "\n",
    "    def  train(self, X, y, numEpoch=10000):\n",
    "        for i in range(numEpoch):\n",
    "            self.__run_epoch_W_back_prop(X,y)\n",
    "\n",
    "    def predict(self, x, make_whole=True):\n",
    "        # to do the prediction we wil do feedforward for what is passed \n",
    "        # in using the weghts that are now stored\n",
    "        output = self.feedforward(x)\n",
    "        if make_whole == True:\n",
    "            output = self.change_to_whole_num(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying the class to see how it is working\n",
    "\n",
    "n = Network()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.00167466],\n       [0.98873104],\n       [0.98616994],\n       [0.98907627],\n       [0.98723266],\n       [0.01648135],\n       [0.02080801]])"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Running the predict function showing the \n",
    "# real activated outputs\n",
    "output = n.predict(X, make_whole=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0, 1, 1, 1, 1, 0, 0]"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Running it with the whole number predictions\n",
    "output = n.predict(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'2.0.0'"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import numbers as np\n",
    "# Doing the same thing but now with Tensor Flow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the same data set but this time using the tensorflow \n",
    "# library\n",
    "model = Sequential([\n",
    "    Dense(4, activation=\"relu\", input_dim=3 ),\n",
    "    Dense(1, activation=\"relu\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"sgd\", loss=\"mse\", metrics=[\"mae\", \"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0, 0, 1],\n       [0, 1, 1],\n       [1, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 1, 1],\n       [0, 0, 0]], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# making the data into a format that can be used by the \n",
    "# tensorflow\n",
    "X_numpy = X.to_numpy()\n",
    "y_numpy = y.to_numpy()\n",
    "X_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "och 824/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1197 - mae: 0.2763 - mse: 0.1197\nEpoch 825/1000\n7/7 [==============================] - 0s 572us/sample - loss: 0.1196 - mae: 0.2759 - mse: 0.1196\nEpoch 826/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1196 - mae: 0.2756 - mse: 0.1196\nEpoch 827/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1195 - mae: 0.2753 - mse: 0.1195\nEpoch 828/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1194 - mae: 0.2749 - mse: 0.1194\nEpoch 829/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1193 - mae: 0.2747 - mse: 0.1193\nEpoch 830/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1193 - mae: 0.2745 - mse: 0.1193\nEpoch 831/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1193 - mae: 0.2743 - mse: 0.1193\nEpoch 832/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1193 - mae: 0.2749 - mse: 0.1193\nEpoch 833/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1192 - mae: 0.2746 - mse: 0.1192\nEpoch 834/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1191 - mae: 0.2742 - mse: 0.1191\nEpoch 835/1000\n7/7 [==============================] - 0s 428us/sample - loss: 0.1190 - mae: 0.2739 - mse: 0.1190\nEpoch 836/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1190 - mae: 0.2736 - mse: 0.1190\nEpoch 837/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1189 - mae: 0.2734 - mse: 0.1189\nEpoch 838/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1189 - mae: 0.2731 - mse: 0.1189\nEpoch 839/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1189 - mae: 0.2738 - mse: 0.1189\nEpoch 840/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1188 - mae: 0.2735 - mse: 0.1188\nEpoch 841/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1187 - mae: 0.2731 - mse: 0.1187\nEpoch 842/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1187 - mae: 0.2728 - mse: 0.1187\nEpoch 843/1000\n7/7 [==============================] - 0s 427us/sample - loss: 0.1186 - mae: 0.2725 - mse: 0.1186\nEpoch 844/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1185 - mae: 0.2723 - mse: 0.1185\nEpoch 845/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1185 - mae: 0.2720 - mse: 0.1185\nEpoch 846/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1184 - mae: 0.2718 - mse: 0.1184\nEpoch 847/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1185 - mae: 0.2725 - mse: 0.1185\nEpoch 848/1000\n7/7 [==============================] - 0s 572us/sample - loss: 0.1184 - mae: 0.2722 - mse: 0.1184\nEpoch 849/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1183 - mae: 0.2719 - mse: 0.1183\nEpoch 850/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1183 - mae: 0.2715 - mse: 0.1183\nEpoch 851/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1182 - mae: 0.2712 - mse: 0.1182\nEpoch 852/1000\n7/7 [==============================] - 0s 571us/sample - loss: 0.1181 - mae: 0.2709 - mse: 0.1181\nEpoch 853/1000\n7/7 [==============================] - 0s 428us/sample - loss: 0.1180 - mae: 0.2707 - mse: 0.1180\nEpoch 854/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1180 - mae: 0.2705 - mse: 0.1180\nEpoch 855/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1180 - mae: 0.2703 - mse: 0.1180\nEpoch 856/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1180 - mae: 0.2710 - mse: 0.1180\nEpoch 857/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1179 - mae: 0.2706 - mse: 0.1179\nEpoch 858/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1178 - mae: 0.2703 - mse: 0.1178\nEpoch 859/1000\n7/7 [==============================] - 0s 428us/sample - loss: 0.1178 - mae: 0.2700 - mse: 0.1178\nEpoch 860/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1177 - mae: 0.2697 - mse: 0.1177\nEpoch 861/1000\n7/7 [==============================] - 0s 572us/sample - loss: 0.1176 - mae: 0.2695 - mse: 0.1176\nEpoch 862/1000\n7/7 [==============================] - 0s 572us/sample - loss: 0.1176 - mae: 0.2692 - mse: 0.1176\nEpoch 863/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1176 - mae: 0.2690 - mse: 0.1176\nEpoch 864/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1176 - mae: 0.2698 - mse: 0.1176\nEpoch 865/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1175 - mae: 0.2695 - mse: 0.1175\nEpoch 866/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1175 - mae: 0.2691 - mse: 0.1175\nEpoch 867/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1174 - mae: 0.2688 - mse: 0.1174\nEpoch 868/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1173 - mae: 0.2685 - mse: 0.1173\nEpoch 869/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1172 - mae: 0.2682 - mse: 0.1172\nEpoch 870/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1172 - mae: 0.2680 - mse: 0.1172\nEpoch 871/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1171 - mae: 0.2678 - mse: 0.1171\nEpoch 872/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1172 - mae: 0.2685 - mse: 0.1172\nEpoch 873/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1171 - mae: 0.2682 - mse: 0.1171\nEpoch 874/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1171 - mae: 0.2679 - mse: 0.1171\nEpoch 875/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1170 - mae: 0.2676 - mse: 0.1170\nEpoch 876/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1169 - mae: 0.2673 - mse: 0.1169\nEpoch 877/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1168 - mae: 0.2670 - mse: 0.1168\nEpoch 878/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1168 - mae: 0.2668 - mse: 0.1168\nEpoch 879/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1167 - mae: 0.2665 - mse: 0.1167\nEpoch 880/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1167 - mae: 0.2664 - mse: 0.1167\nEpoch 881/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1168 - mae: 0.2671 - mse: 0.1168\nEpoch 882/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1167 - mae: 0.2668 - mse: 0.1167\nEpoch 883/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1166 - mae: 0.2665 - mse: 0.1166\nEpoch 884/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1165 - mae: 0.2662 - mse: 0.1165\nEpoch 885/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1165 - mae: 0.2659 - mse: 0.1165\nEpoch 886/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1164 - mae: 0.2656 - mse: 0.1164\nEpoch 887/1000\n7/7 [==============================] - 0s 572us/sample - loss: 0.1164 - mae: 0.2654 - mse: 0.1164\nEpoch 888/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1163 - mae: 0.2651 - mse: 0.1163\nEpoch 889/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1163 - mae: 0.2650 - mse: 0.1163\nEpoch 890/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1163 - mae: 0.2656 - mse: 0.1163\nEpoch 891/1000\n7/7 [==============================] - 0s 428us/sample - loss: 0.1162 - mae: 0.2653 - mse: 0.1162\nEpoch 892/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1162 - mae: 0.2650 - mse: 0.1162\nEpoch 893/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1161 - mae: 0.2647 - mse: 0.1161\nEpoch 894/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1160 - mae: 0.2644 - mse: 0.1160\nEpoch 895/1000\n7/7 [==============================] - 0s 285us/sample - loss: 0.1160 - mae: 0.2643 - mse: 0.1160\nEpoch 896/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1159 - mae: 0.2640 - mse: 0.1159\nEpoch 897/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1159 - mae: 0.2638 - mse: 0.1159\nEpoch 898/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1159 - mae: 0.2636 - mse: 0.1159\nEpoch 899/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1159 - mae: 0.2643 - mse: 0.1159\nEpoch 900/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1158 - mae: 0.2640 - mse: 0.1158\nEpoch 901/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1157 - mae: 0.2637 - mse: 0.1157\nEpoch 902/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1157 - mae: 0.2634 - mse: 0.1157\nEpoch 903/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1156 - mae: 0.2631 - mse: 0.1156\nEpoch 904/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1156 - mae: 0.2629 - mse: 0.1156\nEpoch 905/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1155 - mae: 0.2627 - mse: 0.1155\nEpoch 906/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1155 - mae: 0.2624 - mse: 0.1155\nEpoch 907/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1154 - mae: 0.2623 - mse: 0.1154\nEpoch 908/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1155 - mae: 0.2631 - mse: 0.1155\nEpoch 909/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1154 - mae: 0.2628 - mse: 0.1154\nEpoch 910/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1153 - mae: 0.2625 - mse: 0.1153\nEpoch 911/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1153 - mae: 0.2622 - mse: 0.1153\nEpoch 912/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1152 - mae: 0.2619 - mse: 0.1152\nEpoch 913/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1152 - mae: 0.2616 - mse: 0.1152\nEpoch 914/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1151 - mae: 0.2614 - mse: 0.1151\nEpoch 915/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1150 - mae: 0.2612 - mse: 0.1150\nEpoch 916/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1150 - mae: 0.2610 - mse: 0.1150\nEpoch 917/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1151 - mae: 0.2617 - mse: 0.1151\nEpoch 918/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1150 - mae: 0.2614 - mse: 0.1150\nEpoch 919/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1149 - mae: 0.2612 - mse: 0.1149\nEpoch 920/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1149 - mae: 0.2609 - mse: 0.1149\nEpoch 921/1000\n7/7 [==============================] - 0s 430us/sample - loss: 0.1148 - mae: 0.2606 - mse: 0.1148\nEpoch 922/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1147 - mae: 0.2603 - mse: 0.1147\nEpoch 923/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1147 - mae: 0.2601 - mse: 0.1147\nEpoch 924/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1146 - mae: 0.2599 - mse: 0.1146\nEpoch 925/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1146 - mae: 0.2598 - mse: 0.1146\nEpoch 926/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1146 - mae: 0.2595 - mse: 0.1146\nEpoch 927/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1146 - mae: 0.2603 - mse: 0.1146\nEpoch 928/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1146 - mae: 0.2600 - mse: 0.1146\nEpoch 929/1000\n7/7 [==============================] - 0s 572us/sample - loss: 0.1145 - mae: 0.2597 - mse: 0.1145\nEpoch 930/1000\n7/7 [==============================] - 0s 428us/sample - loss: 0.1144 - mae: 0.2595 - mse: 0.1144\nEpoch 931/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1144 - mae: 0.2592 - mse: 0.1144\nEpoch 932/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1143 - mae: 0.2589 - mse: 0.1143\nEpoch 933/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1143 - mae: 0.2587 - mse: 0.1143\nEpoch 934/1000\n7/7 [==============================] - 0s 571us/sample - loss: 0.1142 - mae: 0.2585 - mse: 0.1142\nEpoch 935/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1142 - mae: 0.2583 - mse: 0.1142\nEpoch 936/1000\n7/7 [==============================] - 0s 714us/sample - loss: 0.1141 - mae: 0.2581 - mse: 0.1141\nEpoch 937/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1142 - mae: 0.2580 - mse: 0.1142\nEpoch 938/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1141 - mae: 0.2587 - mse: 0.1141\nEpoch 939/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1141 - mae: 0.2584 - mse: 0.1141\nEpoch 940/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1140 - mae: 0.2581 - mse: 0.1140\nEpoch 941/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1139 - mae: 0.2579 - mse: 0.1139\nEpoch 942/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1139 - mae: 0.2576 - mse: 0.1139\nEpoch 943/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1138 - mae: 0.2573 - mse: 0.1138\nEpoch 944/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1138 - mae: 0.2571 - mse: 0.1138\nEpoch 945/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1137 - mae: 0.2569 - mse: 0.1137\nEpoch 946/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1137 - mae: 0.2567 - mse: 0.1137\nEpoch 947/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1137 - mae: 0.2575 - mse: 0.1137\nEpoch 948/1000\n7/7 [==============================] - 0s 572us/sample - loss: 0.1137 - mae: 0.2572 - mse: 0.1137\nEpoch 949/1000\n7/7 [==============================] - 0s 572us/sample - loss: 0.1136 - mae: 0.2570 - mse: 0.1136\nEpoch 950/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1136 - mae: 0.2567 - mse: 0.1136\nEpoch 951/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1135 - mae: 0.2564 - mse: 0.1135\nEpoch 952/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1135 - mae: 0.2562 - mse: 0.1135\nEpoch 953/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1134 - mae: 0.2559 - mse: 0.1134\nEpoch 954/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1134 - mae: 0.2557 - mse: 0.1134\nEpoch 955/1000\n7/7 [==============================] - 0s 571us/sample - loss: 0.1133 - mae: 0.2555 - mse: 0.1133\nEpoch 956/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1133 - mae: 0.2554 - mse: 0.1133\nEpoch 957/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1133 - mae: 0.2552 - mse: 0.1133\nEpoch 958/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1133 - mae: 0.2560 - mse: 0.1133\nEpoch 959/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1132 - mae: 0.2557 - mse: 0.1132\nEpoch 960/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1132 - mae: 0.2555 - mse: 0.1132\nEpoch 961/1000\n7/7 [==============================] - 0s 571us/sample - loss: 0.1131 - mae: 0.2552 - mse: 0.1131\nEpoch 962/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1131 - mae: 0.2550 - mse: 0.1131\nEpoch 963/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1130 - mae: 0.2547 - mse: 0.1130\nEpoch 964/1000\n7/7 [==============================] - 0s 428us/sample - loss: 0.1129 - mae: 0.2545 - mse: 0.1129\nEpoch 965/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1129 - mae: 0.2542 - mse: 0.1129\nEpoch 966/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1129 - mae: 0.2541 - mse: 0.1129\nEpoch 967/1000\n7/7 [==============================] - 0s 572us/sample - loss: 0.1128 - mae: 0.2539 - mse: 0.1128\nEpoch 968/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1129 - mae: 0.2547 - mse: 0.1129\nEpoch 969/1000\n7/7 [==============================] - 0s 571us/sample - loss: 0.1128 - mae: 0.2544 - mse: 0.1128\nEpoch 970/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1128 - mae: 0.2542 - mse: 0.1128\nEpoch 971/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1127 - mae: 0.2539 - mse: 0.1127\nEpoch 972/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1127 - mae: 0.2537 - mse: 0.1127\nEpoch 973/1000\n7/7 [==============================] - 0s 428us/sample - loss: 0.1126 - mae: 0.2534 - mse: 0.1126\nEpoch 974/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1125 - mae: 0.2532 - mse: 0.1125\nEpoch 975/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1125 - mae: 0.2529 - mse: 0.1125\nEpoch 976/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1125 - mae: 0.2528 - mse: 0.1125\nEpoch 977/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1124 - mae: 0.2526 - mse: 0.1124\nEpoch 978/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1124 - mae: 0.2524 - mse: 0.1124\nEpoch 979/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1124 - mae: 0.2523 - mse: 0.1124\nEpoch 980/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1124 - mae: 0.2531 - mse: 0.1124\nEpoch 981/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1123 - mae: 0.2528 - mse: 0.1123\nEpoch 982/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1123 - mae: 0.2526 - mse: 0.1123\nEpoch 983/1000\n7/7 [==============================] - 0s 572us/sample - loss: 0.1122 - mae: 0.2523 - mse: 0.1122\nEpoch 984/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1122 - mae: 0.2521 - mse: 0.1122\nEpoch 985/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1121 - mae: 0.2518 - mse: 0.1121\nEpoch 986/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1121 - mae: 0.2516 - mse: 0.1121\nEpoch 987/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1120 - mae: 0.2514 - mse: 0.1120\nEpoch 988/1000\n7/7 [==============================] - 0s 428us/sample - loss: 0.1120 - mae: 0.2512 - mse: 0.1120\nEpoch 989/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1119 - mae: 0.2510 - mse: 0.1119\nEpoch 990/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1119 - mae: 0.2509 - mse: 0.1119\nEpoch 991/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1119 - mae: 0.2516 - mse: 0.1119\nEpoch 992/1000\n7/7 [==============================] - 0s 285us/sample - loss: 0.1119 - mae: 0.2514 - mse: 0.1119\nEpoch 993/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1118 - mae: 0.2512 - mse: 0.1118\nEpoch 994/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1118 - mae: 0.2509 - mse: 0.1118\nEpoch 995/1000\n7/7 [==============================] - 0s 572us/sample - loss: 0.1117 - mae: 0.2507 - mse: 0.1117\nEpoch 996/1000\n7/7 [==============================] - 0s 429us/sample - loss: 0.1117 - mae: 0.2504 - mse: 0.1117\nEpoch 997/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1116 - mae: 0.2502 - mse: 0.1116\nEpoch 998/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1116 - mae: 0.2500 - mse: 0.1116\nEpoch 999/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1115 - mae: 0.2499 - mse: 0.1115\nEpoch 1000/1000\n7/7 [==============================] - 0s 286us/sample - loss: 0.1115 - mae: 0.2496 - mse: 0.1115\n"
    }
   ],
   "source": [
    "results = model.fit(X_numpy, y_numpy, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.01372722],\n       [0.9340152 ],\n       [0.63417697],\n       [0.97128713],\n       [0.57565284],\n       [0.6491444 ],\n       [0.19886363]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "pred = model.predict(X_numpy, )\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x2b148327c88>]"
     },
     "metadata": {},
     "execution_count": 41
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 378.465625 248.518125\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 378.465625 248.518125 \r\nL 378.465625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\nL 371.265625 7.2 \r\nL 36.465625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m21e1589b8e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#m21e1589b8e\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(48.502557 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"112.617468\" xlink:href=\"#m21e1589b8e\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 200 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(103.073718 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"173.551129\" xlink:href=\"#m21e1589b8e\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 400 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(164.007379 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"234.48479\" xlink:href=\"#m21e1589b8e\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 600 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(224.94104 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"295.418451\" xlink:href=\"#m21e1589b8e\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 800 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(285.874701 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"356.352111\" xlink:href=\"#m21e1589b8e\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1000 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(343.627111 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mb4f3f9ef20\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mb4f3f9ef20\" y=\"178.155956\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.15 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 181.955175)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mb4f3f9ef20\" y=\"130.616738\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.20 -->\r\n      <g transform=\"translate(7.2 134.415957)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mb4f3f9ef20\" y=\"83.077519\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.25 -->\r\n      <g transform=\"translate(7.2 86.876738)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mb4f3f9ef20\" y=\"35.538301\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.30 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 39.33752)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_11\">\r\n    <path clip-path=\"url(#p731c762079)\" d=\"M 51.683807 17.083636 \r\nL 53.511817 24.890344 \r\nL 55.949163 32.826149 \r\nL 59.909851 44.248846 \r\nL 66.003217 59.17025 \r\nL 67.22189 61.800161 \r\nL 68.440564 64.462203 \r\nL 70.87791 69.261571 \r\nL 72.096583 71.592456 \r\nL 72.70592 72.688278 \r\nL 74.838598 76.493973 \r\nL 76.057271 78.516482 \r\nL 77.580613 81.097967 \r\nL 78.189949 82.063672 \r\nL 80.322627 85.439673 \r\nL 80.931964 86.311559 \r\nL 82.150637 88.165697 \r\nL 83.369311 89.893885 \r\nL 90.986018 100.077551 \r\nL 96.470048 106.59907 \r\nL 97.079384 107.259346 \r\nL 98.602726 108.989389 \r\nL 99.212062 109.621882 \r\nL 100.735404 111.316605 \r\nL 101.34474 111.909017 \r\nL 102.868082 113.556858 \r\nL 103.477419 114.125469 \r\nL 105.00076 115.726259 \r\nL 108.352111 119.029848 \r\nL 109.875453 120.489145 \r\nL 110.48479 121.048191 \r\nL 112.008131 122.513623 \r\nL 112.922136 123.328072 \r\nL 114.445478 124.752686 \r\nL 115.054814 125.237466 \r\nL 116.882824 126.930715 \r\nL 117.492161 127.40127 \r\nL 119.32017 129.051959 \r\nL 119.929507 129.50921 \r\nL 121.757517 131.120187 \r\nL 123.585527 132.574469 \r\nL 125.718205 134.408234 \r\nL 126.327541 134.834175 \r\nL 128.155551 136.350837 \r\nL 142.170293 146.909071 \r\nL 142.77963 147.293911 \r\nL 144.912308 148.856378 \r\nL 145.826313 149.447869 \r\nL 147.958991 150.97374 \r\nL 148.568328 151.317451 \r\nL 150.701006 152.813314 \r\nL 152.529016 154.048817 \r\nL 152.833684 154.108605 \r\nL 153.443021 154.619325 \r\nL 153.747689 154.805986 \r\nL 154.052357 154.845684 \r\nL 154.661694 155.357311 \r\nL 154.966362 155.399432 \r\nL 155.575699 155.897572 \r\nL 155.880367 155.974205 \r\nL 156.489704 156.400755 \r\nL 156.794372 156.517327 \r\nL 157.403709 156.903031 \r\nL 158.622382 157.586627 \r\nL 158.92705 157.836051 \r\nL 159.231718 157.904921 \r\nL 160.75506 158.876548 \r\nL 161.059728 158.907136 \r\nL 161.669065 159.374135 \r\nL 161.973733 159.421739 \r\nL 162.58307 159.871099 \r\nL 162.887738 159.931255 \r\nL 163.497075 160.367467 \r\nL 163.801743 160.435841 \r\nL 164.41108 160.863326 \r\nL 164.715748 160.935624 \r\nL 165.325084 161.358618 \r\nL 165.629753 161.430689 \r\nL 166.239089 161.853456 \r\nL 166.543758 161.921121 \r\nL 167.153094 162.347855 \r\nL 167.457763 162.40702 \r\nL 168.067099 162.841829 \r\nL 168.371768 162.88847 \r\nL 168.981104 163.335449 \r\nL 169.285772 163.365556 \r\nL 169.895109 163.822453 \r\nL 170.199777 163.844555 \r\nL 170.809114 164.290387 \r\nL 171.113782 164.338742 \r\nL 172.637124 165.214169 \r\nL 173.24646 165.448462 \r\nL 173.855797 165.819375 \r\nL 174.160465 165.902016 \r\nL 174.769802 166.312386 \r\nL 175.07447 166.351786 \r\nL 175.683807 166.787984 \r\nL 175.988475 166.814931 \r\nL 176.90248 167.309118 \r\nL 178.425822 168.101565 \r\nL 178.73049 168.296868 \r\nL 179.035158 168.320387 \r\nL 179.644495 168.743125 \r\nL 180.253831 168.958037 \r\nL 180.863168 169.291716 \r\nL 181.167836 169.380378 \r\nL 181.777173 169.78681 \r\nL 182.081841 169.79949 \r\nL 182.691178 170.212084 \r\nL 183.300514 170.419303 \r\nL 183.909851 170.781743 \r\nL 184.214519 170.825876 \r\nL 184.823856 171.228453 \r\nL 185.128524 171.252383 \r\nL 186.042529 171.72638 \r\nL 187.565871 172.465783 \r\nL 187.870539 172.662404 \r\nL 188.175207 172.67191 \r\nL 189.089212 173.173592 \r\nL 189.393881 173.236709 \r\nL 190.003217 173.622357 \r\nL 190.307885 173.64341 \r\nL 190.917222 174.045237 \r\nL 191.22189 174.068146 \r\nL 193.0499 175.000373 \r\nL 193.354568 175.018706 \r\nL 194.268573 175.526125 \r\nL 194.573242 175.539216 \r\nL 195.182578 175.93505 \r\nL 195.791915 176.111935 \r\nL 196.401252 176.478428 \r\nL 196.70592 176.474871 \r\nL 197.619925 176.930224 \r\nL 197.924593 177.014282 \r\nL 198.53393 177.385634 \r\nL 198.838598 177.378819 \r\nL 199.447935 177.757836 \r\nL 200.057271 177.925455 \r\nL 200.971276 178.339112 \r\nL 201.580613 178.632584 \r\nL 201.885281 178.79829 \r\nL 202.189949 178.779008 \r\nL 202.799286 179.156338 \r\nL 203.713291 179.502018 \r\nL 204.017959 179.672655 \r\nL 204.627296 179.828756 \r\nL 205.236632 180.161713 \r\nL 205.541301 180.161217 \r\nL 206.455306 180.613552 \r\nL 206.759974 180.668779 \r\nL 207.369311 181.01715 \r\nL 207.978647 181.167924 \r\nL 208.587984 181.508858 \r\nL 208.892652 181.522685 \r\nL 211.329998 182.499625 \r\nL 211.634667 182.579518 \r\nL 212.244003 182.936489 \r\nL 212.85334 183.048826 \r\nL 213.462677 183.403913 \r\nL 214.072013 183.510653 \r\nL 214.68135 183.865301 \r\nL 215.290686 183.965269 \r\nL 215.900023 184.32081 \r\nL 216.50936 184.412745 \r\nL 217.118696 184.764276 \r\nL 217.423365 184.732809 \r\nL 218.337369 185.224446 \r\nL 218.642038 185.214684 \r\nL 227.477419 188.334588 \r\nL 228.086755 188.594963 \r\nL 228.696092 188.823617 \r\nL 229.00076 188.796869 \r\nL 229.914765 189.242672 \r\nL 230.219433 189.204689 \r\nL 231.133438 189.657038 \r\nL 231.742775 189.742654 \r\nL 232.65678 190.14441 \r\nL 232.961448 190.097968 \r\nL 233.875453 190.54856 \r\nL 234.48479 190.617331 \r\nL 235.094126 190.923355 \r\nL 236.008131 191.162323 \r\nL 236.922136 191.472881 \r\nL 237.226804 191.459634 \r\nL 238.140809 191.901074 \r\nL 238.750146 191.95018 \r\nL 239.664151 192.332795 \r\nL 239.968819 192.305352 \r\nL 240.882824 192.730868 \r\nL 241.492161 192.778117 \r\nL 242.406166 193.195302 \r\nL 243.015502 193.240568 \r\nL 243.929507 193.639646 \r\nL 244.234175 193.574517 \r\nL 245.452849 194.074853 \r\nL 245.757517 194.020972 \r\nL 246.97619 194.514592 \r\nL 247.280858 194.45795 \r\nL 248.499532 194.958668 \r\nL 248.8042 194.885888 \r\nL 249.718205 195.297689 \r\nL 250.63221 195.48187 \r\nL 251.546215 195.784197 \r\nL 251.850883 195.745944 \r\nL 253.069556 196.240102 \r\nL 253.678893 196.253448 \r\nL 254.592898 196.628682 \r\nL 254.897566 196.569871 \r\nL 256.116239 197.046717 \r\nL 257.030244 197.192135 \r\nL 257.944249 197.474542 \r\nL 258.248917 197.460813 \r\nL 259.467591 197.874682 \r\nL 259.772259 197.856109 \r\nL 260.990932 198.30476 \r\nL 262.209605 198.565972 \r\nL 262.818942 198.755296 \r\nL 263.12361 198.687985 \r\nL 264.342283 199.137826 \r\nL 265.256288 199.27446 \r\nL 266.170293 199.578629 \r\nL 266.474962 199.510368 \r\nL 267.693635 199.940276 \r\nL 269.216976 200.28536 \r\nL 269.826313 200.412715 \r\nL 270.130981 200.382877 \r\nL 271.349654 200.802882 \r\nL 272.568328 201.030586 \r\nL 273.482333 201.276553 \r\nL 273.787001 201.212302 \r\nL 275.005674 201.629416 \r\nL 275.919679 201.748057 \r\nL 277.138352 202.087078 \r\nL 277.443021 202.026858 \r\nL 278.966362 202.459698 \r\nL 279.27103 202.43234 \r\nL 280.794372 202.892935 \r\nL 282.013045 203.083435 \r\nL 282.92705 203.30847 \r\nL 283.231718 203.262247 \r\nL 284.75506 203.707364 \r\nL 285.059728 203.636631 \r\nL 286.58307 204.074274 \r\nL 287.801743 204.265582 \r\nL 289.020416 204.51621 \r\nL 289.325084 204.497601 \r\nL 290.848426 204.920941 \r\nL 292.067099 205.092804 \r\nL 293.285772 205.370698 \r\nL 293.590441 205.310513 \r\nL 295.113782 205.729866 \r\nL 296.027787 205.810615 \r\nL 297.551129 206.18021 \r\nL 297.855797 206.101593 \r\nL 299.683807 206.555387 \r\nL 299.988475 206.492249 \r\nL 301.816485 206.942359 \r\nL 302.73049 207.013071 \r\nL 304.253831 207.369157 \r\nL 304.863168 207.357221 \r\nL 306.691178 207.760932 \r\nL 306.995846 207.713661 \r\nL 309.128524 208.189415 \r\nL 309.433193 208.116437 \r\nL 311.565871 208.594805 \r\nL 312.175207 208.579972 \r\nL 314.307885 209.008533 \r\nL 314.612554 208.957699 \r\nL 316.745232 209.391807 \r\nL 317.0499 209.343651 \r\nL 319.487247 209.799188 \r\nL 320.096583 209.835727 \r\nL 322.229261 210.22328 \r\nL 322.838598 210.261929 \r\nL 324.666608 210.6122 \r\nL 325.275944 210.59427 \r\nL 327.713291 211.04317 \r\nL 328.017959 210.981398 \r\nL 330.455306 211.430099 \r\nL 330.759974 211.374016 \r\nL 333.501989 211.850861 \r\nL 334.111325 211.857655 \r\nL 336.548672 212.285338 \r\nL 337.158008 212.266573 \r\nL 339.595355 212.672239 \r\nL 340.204691 212.679776 \r\nL 342.946706 213.087476 \r\nL 343.556043 213.12031 \r\nL 345.993389 213.506587 \r\nL 346.602726 213.509393 \r\nL 349.34474 213.943125 \r\nL 349.954077 213.92504 \r\nL 353.00076 214.387888 \r\nL 353.610097 214.392351 \r\nL 356.047443 214.756364 \r\nL 356.047443 214.756364 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 36.465625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 371.265625 224.64 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.465625 7.2 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p731c762079\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# Doing a plotting of the change in the loss\n",
    "plt.plot(results.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8b-r70o8p2Dm"
   },
   "source": [
    "## Try building/training a more complex MLP on a bigger dataset.\n",
    "\n",
    "Use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the cannonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
    "\n",
    "If you need inspiration, the internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5MOPtYdk1HgA"
   },
   "outputs": [],
   "source": [
    "# Getting the data set\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(60000, 28, 28) (60000,)\n\nThe shapes of the test values\n(10000, 28, 28) (10000,)\n"
    }
   ],
   "source": [
    "# Loading the data\n",
    "(x_train, y_train), (x_test, y_test)  = mnist.load_data()\n",
    "print((X_train.shape), y_train.shape)\n",
    "print(\"\\nThe shapes of the test values\")\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0]], dtype=uint8)"
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing some normalization\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_12\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten_5 (Flatten)          (None, 784)               0         \n_________________________________________________________________\ndense_15 (Dense)             (None, 20)                15700     \n_________________________________________________________________\ndense_16 (Dense)             (None, 10)                210       \n=================================================================\nTotal params: 15,910\nTrainable params: 15,910\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# Will be flattening the array\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28)))\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=\"adam\",\n",
    "                metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "5"
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_test_binary = to_categorical(y_test)\n",
    "y_train_binary = to_categorical(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 60000 samples, validate on 10000 samples\nEpoch 1/10\n60000/60000 [==============================] - 3s 52us/sample - loss: 0.4013 - accuracy: 0.8868 - val_loss: 0.2614 - val_accuracy: 0.9241\nEpoch 2/10\n60000/60000 [==============================] - 3s 53us/sample - loss: 0.2358 - accuracy: 0.9330 - val_loss: 0.2133 - val_accuracy: 0.9357\nEpoch 3/10\n60000/60000 [==============================] - 3s 47us/sample - loss: 0.1951 - accuracy: 0.9440 - val_loss: 0.1923 - val_accuracy: 0.9453\nEpoch 4/10\n60000/60000 [==============================] - 3s 46us/sample - loss: 0.1695 - accuracy: 0.9511 - val_loss: 0.1801 - val_accuracy: 0.9485\nEpoch 5/10\n60000/60000 [==============================] - 3s 47us/sample - loss: 0.1538 - accuracy: 0.9553 - val_loss: 0.1588 - val_accuracy: 0.9535\nEpoch 6/10\n60000/60000 [==============================] - 3s 46us/sample - loss: 0.1418 - accuracy: 0.9580 - val_loss: 0.1582 - val_accuracy: 0.9559\nEpoch 7/10\n60000/60000 [==============================] - 3s 47us/sample - loss: 0.1324 - accuracy: 0.9612 - val_loss: 0.1575 - val_accuracy: 0.9558\nEpoch 8/10\n60000/60000 [==============================] - 3s 47us/sample - loss: 0.1243 - accuracy: 0.9638 - val_loss: 0.1479 - val_accuracy: 0.9579\nEpoch 9/10\n60000/60000 [==============================] - 3s 48us/sample - loss: 0.1180 - accuracy: 0.9651 - val_loss: 0.1450 - val_accuracy: 0.9583\nEpoch 10/10\n60000/60000 [==============================] - 3s 47us/sample - loss: 0.1129 - accuracy: 0.9664 - val_loss: 0.1424 - val_accuracy: 0.9595\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x2b14b09d208>"
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "# fitting the model\n",
    "model.fit(x_train, y_train_binary, epochs=10, \n",
    "                validation_data=(x_test, y_test_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[6.90826801e-06, 8.70155545e-11, 4.45211772e-04, ...,\n        9.98606622e-01, 3.62628498e-06, 1.36997551e-05],\n       [3.17126130e-08, 4.41217935e-06, 9.98472154e-01, ...,\n        3.38671792e-14, 3.03231946e-06, 3.70132341e-10],\n       [1.05897826e-07, 9.97816443e-01, 1.72994361e-04, ...,\n        9.75531002e-04, 8.06573080e-04, 3.51112976e-05],\n       ...,\n       [3.99184782e-11, 5.10512799e-10, 1.20066429e-10, ...,\n        2.77918207e-05, 6.04545530e-05, 5.13284031e-05],\n       [3.00030056e-10, 1.94968333e-10, 4.49252614e-12, ...,\n        1.49644663e-09, 8.87583737e-05, 4.22653113e-09],\n       [9.69370149e-07, 4.08516970e-11, 8.59472564e-08, ...,\n        3.82011615e-13, 2.70109157e-10, 2.62060443e-11]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwlRJSfBlCvy"
   },
   "source": [
    "## Stretch Goals: \n",
    "\n",
    "- Implement Cross Validation model evaluation on your MNIST implementation \n",
    "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
    " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
    "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_432_Backprop_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('lam4-nnf-s2': conda)",
   "language": "python",
   "name": "python37064bitlam4nnfs2condaa223e28e4e5343f49ecebf6cc0bb1d2c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}