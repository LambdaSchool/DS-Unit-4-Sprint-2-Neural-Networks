{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Backpropagation Practice\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 2*\n",
    "\n",
    "Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 0  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 1 |\n",
    "| 0  | 1  | 0  | 1 |\n",
    "| 1  | 0  | 0  | 1 |\n",
    "| 1  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 0  | 0 |\n",
    "\n",
    "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEREYT-3wI1f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  x3  y\n",
       "0   0   0   1  0\n",
       "1   0   1   1  1\n",
       "2   1   0   1  1\n",
       "3   0   1   0  1\n",
       "4   1   0   0  1\n",
       "5   1   1   1  0\n",
       "6   0   0   0  0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {\n",
    "    'x1': [0,0,1,0,1,1,0],\n",
    "    'x2': [0,1,0,1,0,1,0],\n",
    "    'x3': [1,1,1,0,0,1,0],\n",
    "    'y': [0,1,1,1,1,0,0]\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df[['x1','x2']].values)\n",
    "y = np.array(df['y'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 weights: \n",
      " [[ 0.05578761 -0.20502679 -0.55785292]\n",
      " [-0.574949   -1.48010042  0.0244496 ]]\n",
      "Layer 2 weights: \n",
      " [[-0.81524754]\n",
      " [ 0.22031476]\n",
      " [-1.06912962]]\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.inputs = 2 # Set up Architecture of Neural Network\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1 #regression problem\n",
    "        \n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)#initalize weights, 2x3  matrix array for first layer\n",
    "        #nodes from input to hidden layers\n",
    "        \n",
    "        #3x1 Matrix Array for Hidden to Output \n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "        #nodes from hidden nodes to input\n",
    "nn = NeuralNetwork()\n",
    "print(\"Layer 1 weights: \\n\", nn.weights1)\n",
    "print(\"Layer 2 weights: \\n\", nn.weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "output [0.26685878]\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.inputs = 2 # Set up Architecture of Neural Network\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)#initalize weights, 2x3  matrix array for first layer\n",
    "        \n",
    "        #3x1 Matrix Array for Hidden to Output \n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "        #need sigmoid for activation functions\n",
    "        #need bias terms and fixed function error metric\n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward pass.\n",
    "        \"\"\"\n",
    "#input to hidden, hidden to output        \n",
    "        #Weighted sum of inputs and hidden layers, dot product of X and self weights1\n",
    "        self.hidden_sum = np.dot(X, self.weights1)#with hidden sum, we calculate and return for activation function \n",
    "        \n",
    "        #Activation of the weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)#activated hidden to the output layer\n",
    "        \n",
    "        #Weighted sum of the hidden layer to output layer\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        #Final Activation of Output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "    \n",
    "nn = NeuralNetwork()\n",
    "\n",
    "print(X[0])\n",
    "output = nn.feed_forward(X[0])#make a feedforward pass\n",
    "print(\"output\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.26685878])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = y[0] - output #actual activation - ouput\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26685878]\n",
      " [0.35827283]\n",
      " [0.18890694]\n",
      " [0.35827283]\n",
      " [0.18890694]\n",
      " [0.26977302]\n",
      " [0.26685878]]\n",
      "[[-0.26685878  0.73314122  0.73314122  0.73314122  0.73314122 -0.26685878\n",
      "  -0.26685878]\n",
      " [-0.35827283  0.64172717  0.64172717  0.64172717  0.64172717 -0.35827283\n",
      "  -0.35827283]\n",
      " [-0.18890694  0.81109306  0.81109306  0.81109306  0.81109306 -0.18890694\n",
      "  -0.18890694]\n",
      " [-0.35827283  0.64172717  0.64172717  0.64172717  0.64172717 -0.35827283\n",
      "  -0.35827283]\n",
      " [-0.18890694  0.81109306  0.81109306  0.81109306  0.81109306 -0.18890694\n",
      "  -0.18890694]\n",
      " [-0.26977302  0.73022698  0.73022698  0.73022698  0.73022698 -0.26977302\n",
      "  -0.26977302]\n",
      " [-0.26685878  0.73314122  0.73314122  0.73314122  0.73314122 -0.26685878\n",
      "  -0.26685878]]\n"
     ]
    }
   ],
   "source": [
    "output_all = nn.feed_forward(X)\n",
    "error_all = y - output_all\n",
    "print(output_all)\n",
    "print(error_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activated_hidden\n",
      " [[0.5        0.5        0.5       ]\n",
      " [0.32384997 0.7026395  0.2121049 ]\n",
      " [0.06766202 0.55036754 0.74647645]\n",
      " [0.32384997 0.7026395  0.2121049 ]\n",
      " [0.06766202 0.55036754 0.74647645]\n",
      " [0.03359178 0.74308298 0.44216585]\n",
      " [0.5        0.5        0.5       ]] \n",
      "---------\n",
      "activated_output\n",
      " [[0.26685878]\n",
      " [0.35827283]\n",
      " [0.18890694]\n",
      " [0.35827283]\n",
      " [0.18890694]\n",
      " [0.26977302]\n",
      " [0.26685878]] \n",
      "---------\n",
      "feed_forward\n",
      " <bound method NeuralNetwork.feed_forward of <__main__.NeuralNetwork object at 0x00000249524967F0>> \n",
      "---------\n",
      "hiddenNodes\n",
      " 3 \n",
      "---------\n",
      "hidden_sum\n",
      " [[ 0.          0.          0.        ]\n",
      " [-0.73613464  0.85989874 -1.31228399]\n",
      " [-2.62317038  0.20215583  1.07990734]\n",
      " [-0.73613464  0.85989874 -1.31228399]\n",
      " [-2.62317038  0.20215583  1.07990734]\n",
      " [-3.35930501  1.06205458 -0.23237664]\n",
      " [ 0.          0.          0.        ]] \n",
      "---------\n",
      "inputs\n",
      " 2 \n",
      "---------\n",
      "outputNodes\n",
      " 1 \n",
      "---------\n",
      "output_sum\n",
      " [[-1.01061875]\n",
      " [-0.58286845]\n",
      " [-1.45712826]\n",
      " [-0.58286845]\n",
      " [-1.45712826]\n",
      " [-0.99577448]\n",
      " [-1.01061875]] \n",
      "---------\n",
      "sigmoid\n",
      " <bound method NeuralNetwork.sigmoid of <__main__.NeuralNetwork object at 0x00000249524967F0>> \n",
      "---------\n",
      "weights1\n",
      " [[-2.62317038  0.20215583  1.07990734]\n",
      " [-0.73613464  0.85989874 -1.31228399]] \n",
      "---------\n",
      "weights2\n",
      " [[ 0.01569167]\n",
      " [-0.31783189]\n",
      " [-1.71909729]] \n",
      "---------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes = ['weights1', 'hidden_sum', 'activated_hidden', 'weights2', 'output+']\n",
    "[print(i+'\\n', getattr(nn,i), '\\n'+'---'*3) for i in dir(nn) if i[:2]!= '__']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01569167],\n",
       "       [-0.31783189],\n",
       "       [-1.71909729]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes =  3\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        # 2x3 Matrix Array for the First Layer\n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)\n",
    "       \n",
    "        # 3x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "\n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "\n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1-s)\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward pass.\n",
    "        \"\"\"\n",
    "        #Weighted sum of inputs and hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "\n",
    "        #Activation of the weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "\n",
    "        #Weighted sum of hidden layer to output layer\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "\n",
    "        #Final Activation of Output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "\n",
    "        return self.activated_output\n",
    "\n",
    "    #sigmoid needed at this step, will take input matrix, outputs and some other term for error\n",
    "    def backward(self, X, y, o):\n",
    "        #this is our backward pass\n",
    "        #updating weights by fixed amount which we calculated using gradient derivative of our error\n",
    "        self.o_error = y - o#error in output\n",
    "\n",
    "        #delta gives us the adjustment from the output to the hidden\n",
    "        #derivative of error\n",
    "        #Size of Adjustment from hidden =>output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o) #apply derivative of sigmoid to error#basically the gradient\n",
    "        #the z2 error is how much we need to change the weights by\n",
    "        #delta is constant for the backward propagation pass, then gets smaller for every other pass hopefully\n",
    "\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)#How much the hidden layer interprets as the error\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)#this is where you might scale down the learning rate\n",
    "        #we are updating the weights a fixed amount by travelling up the gradient\n",
    "        \n",
    "        #going from right to left\n",
    "        #Adjustments from the hidden to the ouput weights\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "\n",
    "        #Adjustment from input => hidden weights\n",
    "        #taking how much we get wrong from the z2 error and z2 delta\n",
    "        #weights1 is the first hidden error, we are working backwards\n",
    "        self.weights1 += X.T.dot(self.z2_delta)#update the weights of the first layer by taking the dot products \n",
    "        #with self.z2_delta which is representative of a variable value of how much we need to update the weights.\n",
    "\n",
    "        #we are not using gradient descent, we are updating the weights a fixed amount that we calculated using just the \n",
    "        #gradient derivative of our error\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)#calculate a forward path\n",
    "        self.backward(X, y, o )#make a backwards pass\n",
    "        \n",
    "#Since you're never passing back a function, it only matters if you can find the derivative of the activation function\n",
    "#we only care about the derivative of the activation function, the chain rule for the backprop inputs will always be a constant.\n",
    "nn.weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01569167, -0.31783189, -1.71909729]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we are transposing weights so that this and our error is going to be flat\n",
    "nn.weights2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.26685878])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error#flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00404219, -0.0061897 , -0.02716636])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want activated that correspond to negative weights to be lower\n",
    "# And we want more higher activation for positivie weights\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "o_delta = error * nn.sigmoidPrime(output)\n",
    "\n",
    "z2_error = o_delta.dot(nn.weights2.T)\n",
    "z2_error#set this error aside, calculate this error for the next layer, take the error all at once, then update the weights\n",
    "#based on the size of the updates\n",
    "#we want the array this outputs to be the same shape as our weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 0]]\n",
      "Actual Output: \n",
      " [0 1 1 1 1 0 0]\n",
      "Predicted Output: \n",
      " [[0.60113633]\n",
      " [0.61832852]\n",
      " [0.60805514]\n",
      " [0.61832852]\n",
      " [0.60805514]\n",
      " [0.63050647]\n",
      " [0.60113633]]\n",
      "Loss: \n",
      " 0.24666046221404625\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (7,7) and (1,3) not aligned: 7 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-7cfdbba7c2e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Predicted Output: \\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss: \\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#some neurons in the hidden layer are more highlighted than others because with backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-126-86101ec53942>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#calculate a forward path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;31m#make a backwards pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;31m#Since you're never passing back a function, it only matters if you can find the derivative of the activation function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-126-86101ec53942>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, X, y, o)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m#delta is constant for the backward propagation pass, then gets smaller for every other pass hopefully\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz2_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mo_delta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#How much the hidden layer interprets as the error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz2_delta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz2_error\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoidPrime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivated_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#this is where you might scale down the learning rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m#we are updating the weights a fixed amount by travelling up the gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (7,7) and (1,3) not aligned: 7 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "for i in range(1000):#1000 iterations on our model\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 200 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "    nn.train(X,y)\n",
    "    \n",
    "#some neurons in the hidden layer are more highlighted than others because with backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8b-r70o8p2Dm"
   },
   "source": [
    "## Try building/training a more complex MLP on a bigger dataset.\n",
    "\n",
    "Use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the cannonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
    "\n",
    "If you need inspiration, the internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwlRJSfBlCvy"
   },
   "source": [
    "## Stretch Goals: \n",
    "\n",
    "- Implement Cross Validation model evaluation on your MNIST implementation \n",
    "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
    " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
    "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_432_Backprop_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
