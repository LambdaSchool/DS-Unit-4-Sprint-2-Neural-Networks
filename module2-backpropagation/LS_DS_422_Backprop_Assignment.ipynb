{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Backpropagation Practice\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 2*\n",
    "\n",
    "Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 0  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 1 |\n",
    "| 0  | 1  | 0  | 1 |\n",
    "| 1  | 0  | 0  | 1 |\n",
    "| 1  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 0  | 0 |\n",
    "\n",
    "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>x3</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   x1  x2  x3  y\n0   0   0   1  0\n1   0   1   1  1\n2   1   0   1  1\n3   0   1   0  1\n4   1   0   0  1\n5   1   1   1  0\n6   0   0   0  0"
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'x1':[0,0,1,0,1,1,0],\n",
    "    'x2':[0,1,0,1,0,1,0],\n",
    "    'x3':[1,1,1,0,0,1,0],\n",
    "    'y':[0,1,1,1,1,0,0]},\n",
    "    dtype='int8')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = list(df)[:-1]\n",
    "target = 'y'\n",
    "X = df[feats].values\n",
    "y = np.array([[i] for i in df[target]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "((7, 3), (7, 1))"
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NickNetwork:\n",
    "    def __init__(self, inputs=3, hiddenlayers=4, outputnodes=1):\n",
    "        self.inputs = inputs\n",
    "        self.hiddenlayers = hiddenlayers\n",
    "        self.outputnodes = outputnodes\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoidprime(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        return self.activated_output\n",
    "    \n",
    "    def back(self, X, y, o):\n",
    "        self.error = y - o\n",
    "        self.delta = self.error * self.sigmoidprime(o)\n",
    "        self.z2_error = self.delta.dot(self.weights2.T)\n",
    "        self.z2_delta = self.z2_error * self.sigmoidprime(self.activated_hidden)\n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.delta)\n",
    "    \n",
    "    def fit_predict(self, X, y, iter=1000, print_iters=100, verbose=False):\n",
    "        '''\n",
    "        Train network over 'iter' number of iterations\n",
    "        print_iters = Print predicted information every 'print_iters' iterations when\n",
    "        verbose is True\n",
    "        verbose = Print information about model, adjust amount with 'print_iters'\n",
    "        '''\n",
    "        # Initialize weights\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenlayers)\n",
    "        self.weights2 = np.random.rand(self.hiddenlayers, self.outputnodes)\n",
    "\n",
    "        if verbose:\n",
    "            for i in range(iter):\n",
    "                initial = self.forward(X)\n",
    "                self.back(X,y,initial)\n",
    "                if (i == 0) or (i % print_iters ==0):\n",
    "                    print('=' * 9 + f'EPOCH {i}' + '=' * 9 + '\\n')\n",
    "                    print('Input: \\n', X)\n",
    "                    print('Actual Output: \\n', y)\n",
    "                    print('Predicted Output: \\n', str(self.forward(X)))\n",
    "                    print('Loss: \\n', str(np.mean(np.square(y - self.forward(X).round(4)))))\n",
    "        if verbose is False:\n",
    "            for i in range(iter):\n",
    "                initial = self.forward(X)\n",
    "                self.back(X,y,initial)\n",
    "            print('Number of iterations: \\n', str(iter))\n",
    "            print('Predicted Output: \\n', str(self.forward(X).round(4)))\n",
    "\n",
    "    # WAS WORKING THEN BROKE, NEED MORE TIME TO LOOK AT IT!    \n",
    "    # def score(self, X, y, iter=100):\n",
    "    #     '''\n",
    "    #     Same as fit_predict method but returns an accuracy score instead\n",
    "    #     '''\n",
    "    #     for _ in range(iter):\n",
    "    #         init = self.forward(X)\n",
    "    #         self.back(X,y,init)\n",
    "    #     print(f'For {iter} iterations, the accuracy score is: {accuracy_score(self.activated_output, y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NickNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Number of iterations: \n 100\nPredicted Output: \n [[0.2789]\n [0.5612]\n [0.5707]\n [0.7847]\n [0.7982]\n [0.6487]\n [0.496 ]]\n"
    }
   ],
   "source": [
    "nn.fit_predict(X,y, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "=========EPOCH 0=========\n\nInput: \n [[0 0 1]\n [0 1 1]\n [1 0 1]\n [0 1 0]\n [1 0 0]\n [1 1 1]\n [0 0 0]]\nActual Output: \n [[0]\n [1]\n [1]\n [1]\n [1]\n [0]\n [0]]\nPredicted Output: \n [[0.64510151]\n [0.66087059]\n [0.6564824 ]\n [0.62884198]\n [0.6233613 ]\n [0.67006709]\n [0.60895179]]\nLoss: \n 0.2498095828571429\n=========EPOCH 250=========\n\nInput: \n [[0 0 1]\n [0 1 1]\n [1 0 1]\n [0 1 0]\n [1 0 0]\n [1 1 1]\n [0 0 0]]\nActual Output: \n [[0]\n [1]\n [1]\n [1]\n [1]\n [0]\n [0]]\nPredicted Output: \n [[0.07622632]\n [0.80516272]\n [0.80642211]\n [0.87638518]\n [0.87066793]\n [0.24851358]\n [0.25746869]]\nLoss: \n 0.03446977\n=========EPOCH 500=========\n\nInput: \n [[0 0 1]\n [0 1 1]\n [1 0 1]\n [0 1 0]\n [1 0 0]\n [1 1 1]\n [0 0 0]]\nActual Output: \n [[0]\n [1]\n [1]\n [1]\n [1]\n [0]\n [0]]\nPredicted Output: \n [[0.02431481]\n [0.92294411]\n [0.92344103]\n [0.94048162]\n [0.9397346 ]\n [0.07387554]\n [0.14426009]]\nLoss: \n 0.006551785714285714\n=========EPOCH 750=========\n\nInput: \n [[0 0 1]\n [0 1 1]\n [1 0 1]\n [0 1 0]\n [1 0 0]\n [1 1 1]\n [0 0 0]]\nActual Output: \n [[0]\n [1]\n [1]\n [1]\n [1]\n [0]\n [0]]\nPredicted Output: \n [[0.01590686]\n [0.94483946]\n [0.9451368 ]\n [0.95515605]\n [0.95477365]\n [0.04639355]\n [0.11172322]]\nLoss: \n 0.003570541428571428\n"
    }
   ],
   "source": [
    "nn.fit_predict(X,y,iter=1000, print_iters=250, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8b-r70o8p2Dm"
   },
   "source": [
    "## Try building/training a more complex MLP on a bigger dataset.\n",
    "\n",
    "Use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the cannonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
    "\n",
    "If you need inspiration, the internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n",
    "\n",
    "\n",
    "### Parts\n",
    "1. Gathering & Transforming the Data\n",
    "2. Making MNIST a Binary Problem\n",
    "3. Estimating your Neural Network (the part you focus on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'1.17.3'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering the Data \n",
    "\n",
    "`keras` has a handy method to pull the mnist dataset for you. You'll notice that each observation is a 28x28 arrary which represents an image. Although most Neural Network frameworks can handle higher dimensional data, that is more overhead than necessary for us. We need to flatten the image to one long row which will be 784 values (28X28). Basically, you will be appending each row to one another to make on really long row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], img_rows * img_cols)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows * img_cols)\n",
    "\n",
    "# Normalize Our Data\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(60000, 784)"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the data should be in a format you're more familiar with\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making MNIST a Binary Problem \n",
    "MNIST is multiclass classification problem; however we haven't covered all the necessary techniques to handle this yet. You would need to one-hot encode the target, use a different loss metric, and use softmax activations for the last layer. This is all stuff we'll cover later this week, but let us simply the problem for now: Zero or all else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_temp = np.zeros(y_train.shape)\n",
    "y_temp[np.where(y_train == 0.0)[0]] = 1\n",
    "y_train = y_temp\n",
    "\n",
    "y_temp = np.zeros(y_test.shape)\n",
    "y_temp[np.where(y_test == 0.0)[0]] = 1\n",
    "y_test = y_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 1., 0., ..., 0., 0., 0.])"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A Nice Binary target for ya to work with\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Your `net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([50887, 49711, 36515, 46178,  9043, 12626, 50571, 52978,  2257,\n       50613, 56493, 11226, 46460, 52091, 49259, 22470, 44688,  6468,\n       11634, 58967, 50848, 45812, 28436, 34985, 27146, 11359, 48488,\n       46064, 50820, 36073, 40686, 12142, 22905,  8682, 59834, 23419,\n       50042,  8556, 31465, 11402, 55364, 47729, 58815, 40857, 23642,\n       51139, 41259, 53513, 21067, 54177, 24622, 22969, 26495, 11102,\n        9666, 37219, 14740, 29641, 33842, 59693, 16461, 15208, 36093,\n       10416, 47129, 41682, 28045, 14273, 42230, 17677, 49985,  3987,\n       28669, 23974, 52886, 17207, 28689, 57036,  4677, 39360, 25938,\n       52776, 46989, 19562, 50918, 24218, 47225, 23626, 27101, 22975,\n        8012, 36424, 32161, 39354, 27454, 37485, 56229, 26410, 29242,\n       13241, 39813,  7107, 57193, 55418,  1865, 24046, 36736, 53641,\n        6327,  4422, 24674, 39113, 42338, 40491,  8528, 51619, 12038,\n        6910, 45791, 58036, 55466, 12701, 33388, 35792, 14312, 47272,\n       35081, 13406, 54784, 34109, 18565, 33370, 59620, 20360, 53842,\n       55514, 48750,  2037, 21440, 27354, 14685, 16013, 35830, 51422,\n        9893, 44438, 36945, 59799, 15708, 44942, 30410, 37321, 46698,\n        5259, 50217, 22653, 33406, 56109, 27181, 27287, 29972,  1572,\n        4385, 55074, 28421, 45591, 55738, 33529, 35610, 32451, 47924,\n       22947, 40453,  5441, 53890, 48740, 48710,  1699, 30672, 28197,\n       12218,  7878, 20454, 51704, 36644, 20151, 45656, 59724, 41448,\n       23696, 31531, 19307, 47511, 58124, 38398, 38574, 50856, 29559,\n       47507, 41544, 18656, 24472, 32628, 27842, 12892, 47291, 37848,\n        3053,   511, 21418, 51772, 32629, 48636,  1802, 45010, 24519,\n       54354, 41757, 25392,  3968, 41335, 10810, 39868, 47090,  8764,\n       11331, 55632, 29602, 43358, 29029, 32353,  6052, 59910, 13153,\n       44131, 39486, 45992,  9409, 41363, 32715, 51885, 48476, 28610,\n       57124, 56330, 48225, 57318,  2150, 19383, 15008, 27089, 19551,\n       40604, 17279, 13211, 37018, 57514,  2152, 47699, 57726, 23125,\n       50924, 35275, 39184, 37491, 29037,  6585, 33605, 20539,  1898,\n       49385, 30907, 51261, 20062,  9854,  1265, 32507,  9556, 21248,\n       36253, 44235, 20923,  1245, 25191, 48101, 50334,  4757, 13348,\n       18842, 53843, 10375, 40101,  2147, 17437, 11946,  7552, 13897,\n       26767, 47737, 46068, 30083,  7222, 37918, 59709, 12106, 45689,\n        6398, 43543, 16289, 43008, 10076, 50573, 15120, 37935, 20635,\n       41904, 40848, 10467, 50854, 57208, 19817, 41931, 35668, 39388,\n       50946, 46627, 20706, 19584, 36744, 38697, 19761, 19140, 46660,\n       13345, 28828, 37908, 44930, 34270, 28217, 56134, 27480, 56095,\n       57492,  8768, 25511,  9444, 10700,  4927,  8160, 46124, 38793,\n       21305, 20554, 40618,  8084, 17059, 22842, 34331, 51916, 28624,\n       45659, 30299, 54204, 29365,  2899, 54689,  8457, 38580, 48038,\n       21390, 46810,  6762,  6945, 51997, 36938,  3634, 15046, 37662,\n       46249, 59927, 20153, 40310, 53093, 37983, 32091, 20837, 18850,\n       10987, 24935, 10257, 15709,  6035, 44506, 22414, 19753, 28537,\n        1121, 20634, 18619,  3637, 50601, 41849, 43905,  9476, 30714,\n       33205, 36316, 31681, 45469, 37062, 39739, 57456,  9618, 11019,\n       21342, 27406, 13712, 41289, 12325, 20793, 34319, 31244, 23522,\n       52651, 36996, 39018, 27313, 30414, 31892, 13819,  9872, 31867,\n       49909, 16291, 24653, 29237, 32373, 35666, 49269, 20990, 51767,\n       35579, 32110, 50551, 20021, 48448, 56910, 52190, 34936,   545,\n       54427, 59633,  4762, 46624, 14924, 36896,  1359, 16192,   522,\n       37717, 13117,  7921, 30162, 50147, 30825, 47575, 52075, 30604,\n        4414, 47930, 16113,  7602, 46711, 21216, 17541,  6116, 59246,\n       15141,  9299,  8207, 36800, 15872, 33938, 27539, 11292, 57356,\n       46638,  9762, 32895,  4531, 30199, 28516, 43057, 51015, 19119,\n       57838,  7676, 26030, 10733, 41926, 16612, 39407, 51848, 20789,\n       28380,  6618,  1137, 35011, 49072, 57927, 26088, 43847, 53383,\n       45235, 54576, 18285, 52290, 20576, 13936, 11421, 13928, 39946,\n        9554, 58538, 57247, 41201, 38384, 27666, 54017, 19727, 15553,\n       24524, 50393, 29731, 43063, 48027, 17391, 47688, 41142, 43679,\n       49851, 38754, 57756, 28974, 28632, 36503, 17173, 57358, 51150,\n       11349, 55800, 34335, 37104, 17725, 26325, 50223, 55043, 16241,\n       13732, 55100, 15424, 18385, 10554, 15278, 59247,  8934, 31157,\n       11227, 39909,  7746, 57867, 58356, 55257,  5397, 22319, 42211,\n       59456, 12316,   672, 46935, 26764, 36510, 26350, 49255, 52501,\n        3959, 20225, 53170, 51301, 33931, 28770,  3041, 12232, 30964,\n       24987, 25343, 53705, 52947, 55336, 34453, 53608, 54802, 25098,\n        9938, 50021,  7464, 34288, 16994, 29786, 31178, 40812, 21595,\n       32592,  8269, 50685, 19000, 56158, 54973, 12098, 57558, 32081,\n       49072,  3517, 21384, 12625, 21078,  5118, 42748, 46587, 34559,\n       43591, 54719, 29836, 11924, 27039, 33327,   855, 27087, 23697,\n       52075, 26341, 51258, 45001, 18151, 26244, 44577, 52974, 24389,\n       19547, 16148, 16410, 42380, 51887, 20346, 25425, 31108,  4509,\n       55500, 26100, 19783, 21917, 43329,  5092, 58665, 23830,  9253,\n       50410, 16370, 45085, 32512, 15819, 25966, 15495, 30725, 17952,\n       59049, 33819, 13562, 22115, 54449, 16537, 23147, 49268, 13205,\n       30511, 59788, 31585, 57704, 10435, 13205, 50770, 32069, 59001,\n       10580, 43287, 56059, 20730, 50587, 50103, 21673, 30424, 36322,\n       50689, 55251, 19494, 22578, 56231, 33200, 29834, 42622, 40723,\n       40770, 24216, 17896,  9672, 36239, 55377, 48106, 12382, 44011,\n       49199, 12163, 13864, 14124, 59823, 49688,  4364, 34399, 17503,\n       39440,  9086,  3295, 39416, 55804, 11436, 29281, 45626, 11007,\n       38903, 16354, 32382, 12050,  4611, 39864, 30682, 40205, 43824,\n       56830, 21664, 40052, 21085, 27687, 45940, 14284, 28635, 43355,\n        5076, 50755, 56681, 30683, 45056, 33821, 49258,  9548, 21672,\n        9841,  5272, 43177, 59465, 47156, 17887, 41982, 10581, 33450,\n       51844, 28594, 27258, 31078, 22741, 56182, 56532, 30599, 51018,\n       29162, 40483, 51727, 17083, 15925, 40218, 57466, 12003, 41273,\n       12474, 31730, 22795, 44654, 26501, 16908, 54418, 36432, 35537,\n       48481, 23620, 26989,  3001, 12997,  8337,   216, 54297,  9634,\n       24679, 13567,  2355, 59354, 11340, 43791, 11853, 43641, 27784,\n       58667, 51165, 57132, 33399, 10840, 54970, 45945, 28741, 33374,\n       58115, 27787, 42144, 48502,  2687, 26649, 36584, 24369, 30384,\n       17390, 33531, 37807, 16016, 15917, 52222, 42031, 47525, 58547,\n       33502,  9921,  8848, 16608,  3936, 56672, 41303,  8161, 18156,\n       46851, 43183, 14342, 29942, 15603, 26759, 34369, 33173,  8488,\n       51718, 13520, 39894, 19823, 58051, 58708, 52680, 47401,  5293,\n       10457, 11759, 25277, 47605, 37695, 50997, 28698, 51560, 29407,\n       10036, 20432,  2137, 51518, 11952, 52231, 55854, 33804,  4422,\n       15565,   824, 48870,  8717, 25620, 17696, 49977, 32732, 11800,\n       33699, 17077, 20444, 52582, 37819, 45628, 28328,  7797, 52251,\n        4811, 37181,  4777, 28334, 32289, 40544, 16859, 59704, 21783,\n       17277, 17630, 28667, 27669, 43707, 10716, 44602, 25020, 58218,\n       25071,  6066, 42542, 31833, 10520, 18511, 13929,  1169, 43416,\n       42157, 47536, 30310, 57994, 55116,  9826, 27136, 19210, 46365,\n       47268,   957,  8350, 34412, 55393, 37197, 36375, 38054, 55257,\n       44636,  8866, 59075, 47877,  9662, 42081, 31393,  1141, 24963,\n       29277, 10185, 15099, 32658, 29206, 13067, 39529, 34411, 53798,\n       53998, 14403,  8285, 22475, 50197, 10143, 30688, 13847, 49010,\n       36298, 41340, 11020, 24625,  6858, 20081, 27054, 44812, 31595,\n       37154, 23092, 52511,  5950, 58130, 54289,  2362,  9941, 58307,\n       21978])"
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = np.random.randint(60000, size=1000)\n",
    "sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_train[sampler]\n",
    "y = np.array([[i] for i in y_train[sampler]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "((1000, 784), (1000, 1))"
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "nn = NickNetwork(inputs=784, hiddenlayers=3, outputnodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "=========EPOCH 0=========\n\nInput: \n [[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\nActual Output: \n [[0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]]\nPredicted Output: \n [[2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284296e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284320e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284311e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]]\nLoss: \n 0.08\n=========EPOCH 250=========\n\nInput: \n [[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\nActual Output: \n [[0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]]\nPredicted Output: \n [[2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284296e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284320e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284311e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]]\nLoss: \n 0.08\n=========EPOCH 500=========\n\nInput: \n [[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\nActual Output: \n [[0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]]\nPredicted Output: \n [[2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284296e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284320e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284311e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]]\nLoss: \n 0.08\n=========EPOCH 750=========\n\nInput: \n [[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\nActual Output: \n [[0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]]\nPredicted Output: \n [[2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284296e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284320e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284311e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]\n [2.50284290e-15]]\nLoss: \n 0.08\n"
    }
   ],
   "source": [
    "nn.fit_predict(X,y, iter=1000, print_iters=250, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This output is silly, let's create a new method in the class for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NickNetwork(inputs=784, outputnodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-193-122008e471f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-191-c918942679b7>\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, iter)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'For {iter} iterations, the accuracy score is: {accuracy_score(self.activated_output.tolist(), y)}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;31m# return self.activated_output.tolist()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Unit4-Sprint2\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Unit4-Sprint2\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 90\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and binary targets"
     ]
    }
   ],
   "source": [
    "nn.score(X,y, iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-208-e84484582340>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwlRJSfBlCvy"
   },
   "source": [
    "## Stretch Goals: \n",
    "\n",
    "- Make MNIST a multiclass problem using cross entropy & soft-max\n",
    "- Implement Cross Validation model evaluation on your MNIST implementation \n",
    "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
    " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
    "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_432_Backprop_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}