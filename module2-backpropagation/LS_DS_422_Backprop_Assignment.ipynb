{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Backpropagation Practice\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 2*\n",
    "\n",
    "Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 0  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 1 |\n",
    "| 0  | 1  | 0  | 1 |\n",
    "| 1  | 0  | 0  | 1 |\n",
    "| 1  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 0  | 0 |\n",
    "\n",
    "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEREYT-3wI1f"
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self,inlay, outlay, hidlay):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = inlay\n",
    "        self.outputLayerSize = outlay\n",
    "        self.hiddenLayerSize = hidlay\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #Make empty list to store costs:\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = Neural_Network(inlay=3, outlay=1, hidlay=4)\n",
    "T = trainer(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[0,0,1],\n",
    "                  [0,1,1],\n",
    "                  [1,0,1],\n",
    "                  [0,1,0],\n",
    "                  [1,0,0],\n",
    "                  [1,1,1],\n",
    "                  [0,0,0]])\n",
    "\n",
    "target  = np.array([[0], [1], [1], [1], [1], [0], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.500001\n",
      "         Iterations: 30\n",
      "         Function evaluations: 32\n",
      "         Gradient evaluations: 32\n"
     ]
    }
   ],
   "source": [
    "T.train(inputs,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: \n",
      "[[1.35562912e-03]\n",
      " [1.00000000e+00]\n",
      " [9.99515703e-01]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [3.78977397e-04]]\n",
      "Loss: \n",
      "0.1428574594107322\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Output: \\n\" + str(NN.forward(inputs))) \n",
    "print(\"Loss: \\n\" + str(np.mean(np.square(target - NN.forward(inputs))))) # mean sum squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(T.J)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 1 |\n",
    "| 0  | 1  | 0  | 1 |\n",
    "| 1  | 0  | 0  | 1 |\n",
    "| 1  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 0  | 0 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8b-r70o8p2Dm"
   },
   "source": [
    "## Try building/training a more complex MLP on a bigger dataset.\n",
    "\n",
    "Use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the cannonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
    "\n",
    "If you need inspiration, the internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5MOPtYdk1HgA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], img_rows * img_cols)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows * img_cols)\n",
    "\n",
    "\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making MNIST a Binary Problem MNIST is multiclass classification problem; however we haven't covered all the necessary techniques to handle this yet. You would need to one-hot encode the target, use a different loss metric, and use softmax activations for the last layer. This is all stuff we'll cover later this week, but let us simply the problem for now: Zero or all else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_temp = np.zeros(y_train.shape)\n",
    "y_temp[np.where(y_train == 0.0)[0]] = 1\n",
    "y_train = y_temp\n",
    "\n",
    "y_temp = np.zeros(y_test.shape)\n",
    "y_temp[np.where(y_test == 0.0)[0]] = 1\n",
    "y_test = y_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import random\n",
    "\n",
    "\n",
    "bias = 1\n",
    "lear_rate = 0.1\n",
    "alpha = 0.9\n",
    "\n",
    "# No. of hidden inputs\n",
    "n = 100\n",
    "\n",
    "\n",
    "weight_i2h = np.random.uniform(-0.05,0.05,(784,n))\n",
    "# print(weight_i2h.shape)\n",
    "\n",
    "weight_h20 = np.random.uniform(-0.05,0.05,(n+1,10))\n",
    "# print(weight_h20.shape)\n",
    "\n",
    "# store previous delta wt from hidden to output layer\n",
    "prev_wt_h20 = np.zeros((n+1,10))\n",
    "\n",
    "# store previous delta wt from input to hidden layer\n",
    "prev_wt_i2h = np.zeros((784,n))\n",
    "\n",
    "\n",
    "# matrix to store the activation h1...hk \n",
    "hl_input = np.zeros((1,n+1))\n",
    "hl_input[0,0] = 1\n",
    "\n",
    "\n",
    "# print(hl_input.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def multi_perceptron(epoch,input_ds,set_flag):\n",
    "    global weight_i2h,weight_h20,prev_wt_i2h,prev_wt_h20\n",
    "    pred_list = []\n",
    "    actual_list = []\n",
    "    for i in range(input_ds.shape[0]):\n",
    "        target_class = input_ds[i,0].astype('int')\n",
    "        actual_list.append(target_class)    \n",
    "        xi = input_ds[i].astype('float16')/255\n",
    "        xi[0] = bias            ## Set the value of x0 to bias unit = 1\n",
    "        xi = xi.reshape(1,784)\n",
    "\n",
    "        z_hl = np.dot(xi,weight_i2h)\n",
    "        sig_hl = expit(z_hl)\n",
    "        # print(\"sig_hl\",sig_hl.shape)\n",
    "        hl_input[0,1:] = sig_hl\n",
    "        # print(\"hl_input\",hl_input)\n",
    "        # print(hl_input.shape)\n",
    "        z_ol = np.dot(hl_input,weight_h20)\n",
    "        sig_ol = expit(z_ol)\n",
    "        # print(sig_ol)\n",
    "        predict = np.argmax(sig_ol)\n",
    "        # print(predict)\n",
    "        pred_list.append(predict)\n",
    "        # print(type(sig_ol))\n",
    "        # print(sig_ol.shape)\n",
    "\n",
    "\n",
    "        if epoch>0 and set_flag == 1:\n",
    "            # print(\"inside wt updation\",epoch)\n",
    "            ###### Calculating error term #######\n",
    "\n",
    "            ##error term for output unit \n",
    "            tk = np.zeros((1,10))+0.1\n",
    "            tk[0,target_class] = 0.9\n",
    "            # print(tk)\n",
    "            error_ol = sig_ol*(1-sig_ol)* (tk - sig_ol)\n",
    "            # print(\"error_ol shape for \",epoch,\" \",error_ol.shape)\n",
    "            ##error term for hidden unit\n",
    "            error_hl = sig_hl*(1-sig_hl)*np.dot(error_ol,weight_h20[1:,:].T) \n",
    "            # print(delta_hl.shape)\n",
    "            # print(\"error_hl shape for \",epoch,\" \",error_hl.shape)\n",
    "            ####### Update weights ##########\n",
    "\n",
    "            ### Hidden to output layer wt updation\n",
    "\n",
    "            delta_weight_h20 = (lear_rate * error_ol * hl_input.T) + (alpha * prev_wt_h20)\n",
    "            prev_wt_h20 = delta_weight_h20\n",
    "            # print(\"delta_weight_h20.shape after wt updation\", delta_weight_h20.shape)\n",
    "            weight_h20 = weight_h20 + delta_weight_h20\n",
    "\n",
    "            ### Input to output layer wt updation    \n",
    "\n",
    "            delta_weight_i2h = (lear_rate * error_hl * xi.T) + (alpha * prev_wt_i2h) \n",
    "            prev_wt_i2h = delta_weight_i2h\n",
    "            # print(\"delta_weight_i2h.shape after wt updation\", delta_weight_i2h.shape)\n",
    "            weight_i2h = weight_i2h + delta_weight_i2h\n",
    "\n",
    "\n",
    "\n",
    "    accur = (np.array(pred_list) == np.array(actual_list)).sum()/float(len(actual_list))*100\n",
    "\n",
    "    print(\"len of actual_list after \", epoch,\" is \",len(actual_list))\n",
    "    print(\"len of pred_list after \", epoch,\" is \",len(pred_list))\n",
    "\n",
    "    if(set_flag == 0):\n",
    "        print(\"Confusion matrix for epoch \",epoch)\n",
    "        print(confusion_matrix(actual_list,pred_list))  \n",
    "    return accur\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in range(50):\n",
    "    trn_accuracy = multi_perceptron(each,x_train,1)\n",
    "    #tst_accuracy = multi_perceptron(each,y_test,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### different perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weights(data):\n",
    "    a, b = np.shape(data)\n",
    "    weights = np.random.rand(b,1)\n",
    "    return weights\n",
    "\n",
    "weights = create_weights(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_point, weights):\n",
    "    b = np.dot(data_point, weights)\n",
    "    a = b>0\n",
    "    return a*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(weights, data_point, labels, alpha=.1):\n",
    "    predicted = predict(data_point, weights)\n",
    "    weight_temp = np.zeros(np.shape(weights))\n",
    "    weight_temp[:,0] = alpha*(labels-predicted)*data_point\n",
    "    return weight_temp+weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(data, labels, weights, alpha = .001, iterations = 10):\n",
    "    for j in range(0, iterations):\n",
    "        for i in range(0, len(data)):\n",
    "            weights = update(weights, data[i], labels[i], alpha)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.79790068e-01],\n",
       "       [ 1.47280882e-01],\n",
       "       [ 6.45221193e-01],\n",
       "       [ 7.13058562e-01],\n",
       "       [ 2.25713404e-01],\n",
       "       [ 1.77389394e-01],\n",
       "       [ 4.88582152e-01],\n",
       "       [ 9.64955796e-01],\n",
       "       [ 2.22361233e-01],\n",
       "       [ 1.04998056e-01],\n",
       "       [ 3.57841068e-01],\n",
       "       [ 2.00819391e-01],\n",
       "       [ 1.66934493e-01],\n",
       "       [ 8.78911684e-01],\n",
       "       [ 6.62857682e-01],\n",
       "       [ 6.76800810e-01],\n",
       "       [ 5.55685193e-01],\n",
       "       [ 3.52641035e-01],\n",
       "       [ 9.65801233e-01],\n",
       "       [ 9.18248821e-01],\n",
       "       [ 3.60799118e-01],\n",
       "       [ 2.64910275e-01],\n",
       "       [ 2.03230421e-01],\n",
       "       [ 2.51221211e-01],\n",
       "       [ 9.64785226e-01],\n",
       "       [ 1.42263073e-01],\n",
       "       [ 3.91353444e-02],\n",
       "       [ 3.01226621e-01],\n",
       "       [ 3.46306260e-01],\n",
       "       [ 4.26384025e-01],\n",
       "       [ 6.14352935e-01],\n",
       "       [ 5.11649522e-01],\n",
       "       [ 1.93944864e-01],\n",
       "       [ 2.61314248e-01],\n",
       "       [ 3.51310674e-01],\n",
       "       [ 9.40573807e-01],\n",
       "       [ 7.21817532e-01],\n",
       "       [ 6.73920478e-01],\n",
       "       [ 7.40544933e-02],\n",
       "       [ 2.70997869e-01],\n",
       "       [ 9.64829985e-01],\n",
       "       [ 4.06973047e-01],\n",
       "       [ 5.14923348e-01],\n",
       "       [ 1.69168695e-01],\n",
       "       [ 9.46174321e-01],\n",
       "       [ 3.47280086e-01],\n",
       "       [ 3.09712800e-01],\n",
       "       [ 5.75275042e-01],\n",
       "       [ 9.49122496e-02],\n",
       "       [ 5.69310855e-01],\n",
       "       [ 7.77952777e-01],\n",
       "       [ 3.53505284e-01],\n",
       "       [ 6.83868487e-01],\n",
       "       [ 4.96771619e-01],\n",
       "       [ 1.56778115e-01],\n",
       "       [ 5.57681499e-01],\n",
       "       [ 6.12556987e-01],\n",
       "       [ 7.55397067e-01],\n",
       "       [ 3.91421863e-01],\n",
       "       [ 7.79222656e-02],\n",
       "       [ 3.72660843e-01],\n",
       "       [ 2.05742208e-01],\n",
       "       [ 2.58784189e-01],\n",
       "       [ 3.19936690e-01],\n",
       "       [ 6.51257292e-02],\n",
       "       [ 3.10870938e-01],\n",
       "       [ 4.99363955e-01],\n",
       "       [ 6.59182648e-01],\n",
       "       [ 6.24275172e-01],\n",
       "       [-4.47449508e-02],\n",
       "       [ 7.17946807e-01],\n",
       "       [ 7.09765858e-01],\n",
       "       [ 6.27251700e-01],\n",
       "       [-5.63446941e-02],\n",
       "       [-5.03752607e-02],\n",
       "       [ 2.57628480e-01],\n",
       "       [ 4.31057084e-01],\n",
       "       [ 3.01658584e-02],\n",
       "       [ 2.24371661e-02],\n",
       "       [ 7.10231465e-01],\n",
       "       [ 2.76455652e-01],\n",
       "       [ 9.18775553e-02],\n",
       "       [ 6.73042342e-01],\n",
       "       [ 2.07734540e-01],\n",
       "       [ 6.36187970e-01],\n",
       "       [ 3.35642960e-01],\n",
       "       [ 6.22986245e-01],\n",
       "       [ 8.35176830e-01],\n",
       "       [ 8.74908341e-01],\n",
       "       [ 8.97338339e-01],\n",
       "       [-3.38147445e-02],\n",
       "       [ 6.59925271e-01],\n",
       "       [ 5.05965906e-01],\n",
       "       [-2.62933371e-01],\n",
       "       [ 1.90279969e-01],\n",
       "       [-2.98953430e-01],\n",
       "       [ 4.78484897e-01],\n",
       "       [-3.67294651e-01],\n",
       "       [-1.48496367e-01],\n",
       "       [-1.18379239e-02],\n",
       "       [-2.15850386e-01],\n",
       "       [-1.59129219e-01],\n",
       "       [ 6.13217900e-02],\n",
       "       [-4.89146485e-02],\n",
       "       [ 7.81950044e-02],\n",
       "       [ 8.17132551e-02],\n",
       "       [ 1.70022469e-01],\n",
       "       [-2.00841970e-02],\n",
       "       [ 7.75126474e-01],\n",
       "       [ 2.50587280e-01],\n",
       "       [ 8.28954707e-02],\n",
       "       [ 2.14043525e-01],\n",
       "       [ 9.16336330e-01],\n",
       "       [ 2.06444940e-01],\n",
       "       [ 3.09405779e-01],\n",
       "       [ 1.71400541e-01],\n",
       "       [ 5.35822113e-01],\n",
       "       [ 7.72085786e-01],\n",
       "       [ 8.12585852e-02],\n",
       "       [ 1.86244966e-01],\n",
       "       [ 1.71558531e-01],\n",
       "       [ 3.42494972e-01],\n",
       "       [ 1.17723283e-01],\n",
       "       [-3.83633451e-01],\n",
       "       [ 5.39526422e-02],\n",
       "       [ 4.35697032e-02],\n",
       "       [-6.21208988e-02],\n",
       "       [-1.47992070e-01],\n",
       "       [-3.56839163e-02],\n",
       "       [-1.97259114e-01],\n",
       "       [ 3.51246276e-01],\n",
       "       [-3.68257962e-01],\n",
       "       [-1.94120110e-01],\n",
       "       [ 1.73024566e-01],\n",
       "       [ 4.49630973e-01],\n",
       "       [-7.06051545e-03],\n",
       "       [ 5.19486065e-01],\n",
       "       [ 2.67980071e-01],\n",
       "       [ 4.69888751e-01],\n",
       "       [ 2.03354893e-01],\n",
       "       [ 5.52384325e-01],\n",
       "       [ 4.62418377e-01],\n",
       "       [ 3.20112775e-01],\n",
       "       [ 1.37541972e-01],\n",
       "       [ 4.43546711e-01],\n",
       "       [ 7.42492186e-03],\n",
       "       [ 4.54475204e-01],\n",
       "       [ 2.27631286e-01],\n",
       "       [ 1.72448307e-01],\n",
       "       [-5.03132622e-01],\n",
       "       [-3.02223383e-01],\n",
       "       [ 1.02513432e-01],\n",
       "       [-1.93977147e-02],\n",
       "       [-2.98629871e-01],\n",
       "       [ 3.14784659e-01],\n",
       "       [-7.01093343e-02],\n",
       "       [-1.64407545e-01],\n",
       "       [ 4.16830029e-01],\n",
       "       [-1.10831147e-01],\n",
       "       [ 1.33470592e-02],\n",
       "       [ 2.03215922e-01],\n",
       "       [-1.46272664e-01],\n",
       "       [ 2.96435633e-01],\n",
       "       [ 3.29757529e-01],\n",
       "       [-9.20759862e-03],\n",
       "       [ 2.83167905e-01],\n",
       "       [ 7.41312877e-01],\n",
       "       [ 4.26950659e-01],\n",
       "       [ 3.63830908e-01],\n",
       "       [ 3.47824304e-01],\n",
       "       [ 1.66167243e-01],\n",
       "       [ 2.57957456e-01],\n",
       "       [ 5.39303711e-01],\n",
       "       [-1.33779829e-01],\n",
       "       [-2.78585612e-01],\n",
       "       [-6.43035120e-01],\n",
       "       [ 1.81326931e-01],\n",
       "       [ 1.10406646e-01],\n",
       "       [-2.19476696e-01],\n",
       "       [ 7.99840426e-02],\n",
       "       [-8.17533877e-02],\n",
       "       [-1.43476796e-01],\n",
       "       [ 6.03821172e-01],\n",
       "       [ 8.26281735e-02],\n",
       "       [ 1.08418097e-01],\n",
       "       [ 1.72686577e-03],\n",
       "       [ 3.92405314e-01],\n",
       "       [-2.48174153e-01],\n",
       "       [-2.09323689e-01],\n",
       "       [-7.00846631e-02],\n",
       "       [ 1.17605063e-01],\n",
       "       [ 1.68870326e-01],\n",
       "       [ 3.50634524e-01],\n",
       "       [-3.48095971e-02],\n",
       "       [ 4.80261090e-01],\n",
       "       [ 4.20550226e-01],\n",
       "       [ 1.89296831e-01],\n",
       "       [ 8.61278259e-01],\n",
       "       [ 2.30732508e-02],\n",
       "       [ 6.20955028e-01],\n",
       "       [-1.94885146e-01],\n",
       "       [ 3.70595436e-01],\n",
       "       [-1.75084463e-01],\n",
       "       [ 2.63458582e-02],\n",
       "       [-1.12724761e-01],\n",
       "       [-3.14753860e-01],\n",
       "       [ 1.44656148e-01],\n",
       "       [-1.49033157e-02],\n",
       "       [ 1.51243902e-01],\n",
       "       [ 3.69410972e-01],\n",
       "       [-8.53229080e-02],\n",
       "       [-2.08431227e-01],\n",
       "       [-1.25361133e-01],\n",
       "       [ 1.22755204e-01],\n",
       "       [ 3.85245187e-01],\n",
       "       [ 1.51471344e-01],\n",
       "       [-7.38177475e-02],\n",
       "       [-3.28503363e-01],\n",
       "       [ 6.28551082e-02],\n",
       "       [-4.95896878e-01],\n",
       "       [ 1.15384491e-01],\n",
       "       [-1.86022167e-02],\n",
       "       [-1.69603296e-04],\n",
       "       [ 4.75517962e-01],\n",
       "       [ 2.94116722e-01],\n",
       "       [ 5.01116043e-01],\n",
       "       [-5.29868602e-02],\n",
       "       [-1.13217769e-01],\n",
       "       [ 3.32166120e-01],\n",
       "       [ 6.86901578e-02],\n",
       "       [-1.45513584e-01],\n",
       "       [-1.67796104e-02],\n",
       "       [-3.06815790e-01],\n",
       "       [-6.64094744e-02],\n",
       "       [ 4.48525960e-01],\n",
       "       [-5.45037504e-03],\n",
       "       [-3.56611015e-01],\n",
       "       [ 6.45693341e-02],\n",
       "       [ 1.93162408e-01],\n",
       "       [ 1.92493121e-01],\n",
       "       [ 4.52413469e-01],\n",
       "       [ 4.24067012e-01],\n",
       "       [-1.51031767e-01],\n",
       "       [-2.50344047e-01],\n",
       "       [ 5.79765281e-01],\n",
       "       [-3.75069405e-01],\n",
       "       [ 2.08520439e-01],\n",
       "       [-8.28724446e-02],\n",
       "       [ 4.71698817e-02],\n",
       "       [ 5.42697563e-01],\n",
       "       [ 6.07611680e-01],\n",
       "       [ 4.92553324e-01],\n",
       "       [ 6.31608359e-01],\n",
       "       [ 5.65782810e-01],\n",
       "       [ 6.27337279e-01],\n",
       "       [ 1.22517389e-01],\n",
       "       [ 1.68116449e-01],\n",
       "       [-1.25496017e-01],\n",
       "       [-1.18395484e-01],\n",
       "       [-2.80713263e-01],\n",
       "       [ 4.34931953e-01],\n",
       "       [-2.64055258e-01],\n",
       "       [-4.62771393e-01],\n",
       "       [ 1.82319943e-01],\n",
       "       [ 1.65025989e-01],\n",
       "       [-4.69073576e-01],\n",
       "       [ 2.61676236e-01],\n",
       "       [-4.76254543e-01],\n",
       "       [ 1.33796188e-01],\n",
       "       [-1.34979468e-01],\n",
       "       [-2.19173929e-02],\n",
       "       [ 3.39353979e-01],\n",
       "       [-2.08517328e-01],\n",
       "       [ 2.49768357e-01],\n",
       "       [-1.87699802e-01],\n",
       "       [-2.12554564e-01],\n",
       "       [ 2.22531815e-01],\n",
       "       [-7.29676944e-02],\n",
       "       [ 3.59996535e-01],\n",
       "       [ 1.99392902e-01],\n",
       "       [ 4.12923740e-01],\n",
       "       [ 5.24650811e-01],\n",
       "       [ 4.90452181e-01],\n",
       "       [ 5.84083616e-01],\n",
       "       [ 3.07755526e-01],\n",
       "       [-1.13910141e-01],\n",
       "       [-5.99247514e-01],\n",
       "       [ 2.68179858e-01],\n",
       "       [-1.88026787e-01],\n",
       "       [-1.10220625e-01],\n",
       "       [ 4.64172873e-02],\n",
       "       [-3.88770634e-01],\n",
       "       [-2.97613338e-01],\n",
       "       [ 2.80578157e-01],\n",
       "       [ 6.54007528e-02],\n",
       "       [-7.53721786e-02],\n",
       "       [ 3.66519074e-01],\n",
       "       [-8.59879051e-02],\n",
       "       [ 4.93172713e-01],\n",
       "       [-2.56023432e-01],\n",
       "       [ 5.27026030e-02],\n",
       "       [ 1.11703004e-01],\n",
       "       [-8.51322542e-02],\n",
       "       [ 5.18316478e-01],\n",
       "       [ 4.11736634e-01],\n",
       "       [-1.57391988e-01],\n",
       "       [ 2.71481981e-01],\n",
       "       [ 5.50489191e-02],\n",
       "       [ 8.39315152e-01],\n",
       "       [ 1.07523954e-01],\n",
       "       [ 5.40723996e-01],\n",
       "       [ 3.13675141e-01],\n",
       "       [-4.52430054e-02],\n",
       "       [-3.95298377e-01],\n",
       "       [-2.27065368e-01],\n",
       "       [-1.22850100e-01],\n",
       "       [-3.45667490e-01],\n",
       "       [ 1.96264939e-01],\n",
       "       [-1.73871996e-01],\n",
       "       [ 1.36733002e-01],\n",
       "       [ 1.73692360e-01],\n",
       "       [-4.80941608e-01],\n",
       "       [-2.31270358e-02],\n",
       "       [-6.28298349e-01],\n",
       "       [-1.70990537e-01],\n",
       "       [ 7.23419535e-02],\n",
       "       [ 3.58827304e-01],\n",
       "       [-2.38849030e-01],\n",
       "       [ 1.81155016e-01],\n",
       "       [-1.12719719e-01],\n",
       "       [ 1.64776717e-01],\n",
       "       [ 4.80503705e-01],\n",
       "       [ 2.68126011e-02],\n",
       "       [ 5.26475640e-01],\n",
       "       [ 3.84696451e-01],\n",
       "       [ 9.47914424e-01],\n",
       "       [ 9.99690825e-01],\n",
       "       [ 8.85741123e-01],\n",
       "       [-3.89035560e-03],\n",
       "       [ 3.98466597e-01],\n",
       "       [ 3.66955228e-01],\n",
       "       [-1.14306288e-01],\n",
       "       [ 2.83130899e-02],\n",
       "       [ 1.03406507e-01],\n",
       "       [ 3.79615038e-01],\n",
       "       [-5.24950801e-01],\n",
       "       [ 1.95796068e-01],\n",
       "       [-5.56383077e-02],\n",
       "       [-8.78397253e-02],\n",
       "       [-2.44976124e-01],\n",
       "       [-6.24389267e-01],\n",
       "       [-3.41266602e-01],\n",
       "       [-3.90472746e-01],\n",
       "       [-3.23151214e-01],\n",
       "       [-1.21797915e-01],\n",
       "       [ 2.88274908e-01],\n",
       "       [-2.14180635e-02],\n",
       "       [ 2.94456140e-01],\n",
       "       [-2.36962391e-01],\n",
       "       [ 1.85959235e-01],\n",
       "       [-2.04527420e-01],\n",
       "       [ 5.89743248e-01],\n",
       "       [ 4.49479195e-02],\n",
       "       [ 8.95314218e-01],\n",
       "       [ 4.86919962e-01],\n",
       "       [ 3.71789523e-01],\n",
       "       [ 6.74988404e-01],\n",
       "       [ 3.03733161e-01],\n",
       "       [ 1.47539626e-02],\n",
       "       [ 1.51426586e-01],\n",
       "       [ 1.91292322e-01],\n",
       "       [ 6.14011428e-02],\n",
       "       [ 2.15425706e-02],\n",
       "       [ 1.33609771e-01],\n",
       "       [ 3.99113678e-02],\n",
       "       [-3.43114746e-02],\n",
       "       [ 4.37680006e-01],\n",
       "       [-2.83816289e-01],\n",
       "       [-1.41549439e-02],\n",
       "       [-9.64158800e-01],\n",
       "       [-2.03375705e-01],\n",
       "       [ 2.52115142e-01],\n",
       "       [-1.63197165e-01],\n",
       "       [ 1.50815368e-01],\n",
       "       [-4.31984274e-01],\n",
       "       [ 2.89320645e-01],\n",
       "       [ 3.79159607e-01],\n",
       "       [-1.44576128e-01],\n",
       "       [-6.60547523e-03],\n",
       "       [ 7.59928532e-01],\n",
       "       [ 4.25227821e-01],\n",
       "       [ 7.92757955e-01],\n",
       "       [ 9.29885200e-02],\n",
       "       [ 6.69515730e-01],\n",
       "       [ 2.44594702e-01],\n",
       "       [ 5.45527911e-01],\n",
       "       [ 1.80791619e-01],\n",
       "       [-1.82534617e-01],\n",
       "       [ 3.10255846e-01],\n",
       "       [-7.70775297e-02],\n",
       "       [ 2.66794499e-01],\n",
       "       [ 3.45136406e-01],\n",
       "       [-2.72369210e-01],\n",
       "       [-1.98966173e-01],\n",
       "       [ 1.61732252e-01],\n",
       "       [-4.21850917e-01],\n",
       "       [-1.62901660e-01],\n",
       "       [-8.60606115e-01],\n",
       "       [-4.63331293e-01],\n",
       "       [ 1.15272731e-01],\n",
       "       [ 1.30073373e-01],\n",
       "       [-2.43526028e-01],\n",
       "       [ 3.51200941e-01],\n",
       "       [ 1.49632796e-01],\n",
       "       [ 2.33087232e-01],\n",
       "       [ 5.51282087e-01],\n",
       "       [ 5.95074535e-01],\n",
       "       [ 3.21534863e-01],\n",
       "       [ 1.93883042e-01],\n",
       "       [ 6.34572966e-01],\n",
       "       [ 7.89744749e-01],\n",
       "       [ 7.12249007e-01],\n",
       "       [ 1.56495607e-01],\n",
       "       [-1.37424350e-01],\n",
       "       [ 4.42138943e-01],\n",
       "       [ 1.80100249e-01],\n",
       "       [ 3.36263980e-01],\n",
       "       [-2.71837842e-03],\n",
       "       [ 2.05008285e-01],\n",
       "       [-1.72271678e-01],\n",
       "       [ 4.98931395e-01],\n",
       "       [ 2.89792958e-01],\n",
       "       [-2.32232561e-01],\n",
       "       [-1.69403208e-01],\n",
       "       [-7.68349358e-01],\n",
       "       [-5.43495914e-01],\n",
       "       [-1.94298600e-01],\n",
       "       [-3.82141782e-01],\n",
       "       [-3.16067749e-01],\n",
       "       [-2.52896614e-01],\n",
       "       [ 2.74325375e-01],\n",
       "       [-3.16404476e-01],\n",
       "       [ 2.78253551e-01],\n",
       "       [ 4.69571526e-02],\n",
       "       [ 3.45576270e-02],\n",
       "       [ 3.45209543e-01],\n",
       "       [ 4.51063919e-01],\n",
       "       [ 9.42860114e-01],\n",
       "       [ 8.99544506e-01],\n",
       "       [ 2.48509707e-01],\n",
       "       [ 6.36377865e-01],\n",
       "       [ 4.46269756e-01],\n",
       "       [ 2.53918535e-01],\n",
       "       [ 2.10430738e-01],\n",
       "       [-8.12944201e-02],\n",
       "       [ 3.46512327e-01],\n",
       "       [-4.34247149e-02],\n",
       "       [ 5.22600070e-01],\n",
       "       [-1.43852018e-02],\n",
       "       [-5.92376257e-02],\n",
       "       [-1.48557011e-01],\n",
       "       [-7.47153723e-01],\n",
       "       [-6.95545775e-01],\n",
       "       [-4.43068779e-01],\n",
       "       [-2.16915305e-02],\n",
       "       [-4.61966210e-01],\n",
       "       [ 3.87923017e-02],\n",
       "       [-2.86026541e-02],\n",
       "       [-1.71097196e-01],\n",
       "       [-3.08642729e-01],\n",
       "       [ 3.22337681e-01],\n",
       "       [ 2.30080476e-01],\n",
       "       [-1.23437143e-01],\n",
       "       [ 5.94273876e-01],\n",
       "       [ 2.10544161e-01],\n",
       "       [ 6.40315021e-02],\n",
       "       [ 5.86721759e-02],\n",
       "       [ 5.70232005e-01],\n",
       "       [ 9.08245518e-01],\n",
       "       [ 5.15471622e-01],\n",
       "       [-2.03501377e-01],\n",
       "       [ 2.50035824e-01],\n",
       "       [ 4.71047280e-01],\n",
       "       [ 3.62630525e-01],\n",
       "       [ 3.45819808e-01],\n",
       "       [-1.46675233e-01],\n",
       "       [ 6.35605713e-01],\n",
       "       [-4.29310550e-01],\n",
       "       [-5.25198498e-02],\n",
       "       [-5.79179753e-01],\n",
       "       [-4.73426814e-01],\n",
       "       [ 2.18028196e-02],\n",
       "       [-1.46122429e-01],\n",
       "       [ 2.32426935e-01],\n",
       "       [-3.92988688e-02],\n",
       "       [-3.15418102e-01],\n",
       "       [ 3.66777287e-01],\n",
       "       [ 3.72002956e-01],\n",
       "       [-4.07624863e-01],\n",
       "       [-2.50376752e-01],\n",
       "       [ 6.47948210e-03],\n",
       "       [ 3.09764173e-01],\n",
       "       [ 2.32189034e-01],\n",
       "       [ 8.72422602e-01],\n",
       "       [ 7.49384238e-01],\n",
       "       [ 4.26055781e-01],\n",
       "       [ 6.89659260e-01],\n",
       "       [ 3.68244592e-01],\n",
       "       [ 2.49265230e-01],\n",
       "       [-1.64909486e-01],\n",
       "       [-1.48131701e-01],\n",
       "       [-1.44018306e-01],\n",
       "       [ 1.83710509e-01],\n",
       "       [ 1.92172852e-01],\n",
       "       [ 2.87500627e-03],\n",
       "       [ 2.80879471e-01],\n",
       "       [-2.41289466e-01],\n",
       "       [-2.65718455e-01],\n",
       "       [-6.59072983e-01],\n",
       "       [-2.36392914e-01],\n",
       "       [-9.05914902e-02],\n",
       "       [-1.11433184e-01],\n",
       "       [ 2.21502026e-01],\n",
       "       [-4.87090606e-01],\n",
       "       [ 4.54829434e-02],\n",
       "       [-4.01192810e-01],\n",
       "       [-9.83627156e-03],\n",
       "       [-4.95463286e-02],\n",
       "       [-2.67301585e-01],\n",
       "       [ 4.02690927e-02],\n",
       "       [ 5.87836055e-01],\n",
       "       [ 6.92681547e-01],\n",
       "       [ 4.83513233e-01],\n",
       "       [ 8.90465195e-02],\n",
       "       [ 7.41786436e-01],\n",
       "       [ 1.01697589e-01],\n",
       "       [-3.06503829e-01],\n",
       "       [ 1.57360837e-01],\n",
       "       [ 1.33392130e-01],\n",
       "       [ 1.19641667e-02],\n",
       "       [-3.44418017e-03],\n",
       "       [ 2.43140466e-01],\n",
       "       [ 2.07989350e-01],\n",
       "       [ 4.25431691e-01],\n",
       "       [-5.81300107e-01],\n",
       "       [-1.16729733e-01],\n",
       "       [ 6.92813564e-02],\n",
       "       [-4.55017919e-01],\n",
       "       [-1.25008220e-01],\n",
       "       [-1.56044715e-01],\n",
       "       [-4.98345638e-01],\n",
       "       [-1.23515206e-02],\n",
       "       [-2.28449752e-01],\n",
       "       [-3.21067241e-01],\n",
       "       [ 3.09032705e-01],\n",
       "       [-5.14712798e-02],\n",
       "       [-8.34459528e-02],\n",
       "       [ 4.30015475e-01],\n",
       "       [ 2.64009534e-01],\n",
       "       [ 6.72213432e-01],\n",
       "       [ 1.69250595e-01],\n",
       "       [ 1.02911798e-01],\n",
       "       [ 1.73596368e-01],\n",
       "       [ 3.77185241e-01],\n",
       "       [-6.83097900e-02],\n",
       "       [-4.03390489e-02],\n",
       "       [-9.67180055e-03],\n",
       "       [-2.91427453e-01],\n",
       "       [ 3.20522532e-01],\n",
       "       [-1.30899723e-01],\n",
       "       [ 4.90977908e-01],\n",
       "       [ 1.33865557e-01],\n",
       "       [ 3.29217416e-01],\n",
       "       [-3.04535916e-01],\n",
       "       [-4.50484254e-01],\n",
       "       [-1.02255537e-01],\n",
       "       [-3.03168374e-01],\n",
       "       [ 1.32814189e-01],\n",
       "       [-1.53532869e-01],\n",
       "       [-1.83552577e-01],\n",
       "       [-7.51412693e-02],\n",
       "       [-4.58786938e-01],\n",
       "       [-9.92386912e-02],\n",
       "       [-1.70656468e-01],\n",
       "       [ 5.13232991e-01],\n",
       "       [-6.17517849e-02],\n",
       "       [ 1.76467820e-01],\n",
       "       [ 3.46404645e-02],\n",
       "       [ 1.21246346e-01],\n",
       "       [ 8.05742302e-01],\n",
       "       [ 5.99769254e-02],\n",
       "       [-5.44207525e-02],\n",
       "       [ 3.27624318e-01],\n",
       "       [ 2.88171423e-01],\n",
       "       [-1.33381762e-01],\n",
       "       [ 2.63262005e-01],\n",
       "       [ 1.04079445e-01],\n",
       "       [ 2.75897755e-01],\n",
       "       [-3.32376206e-01],\n",
       "       [ 2.48451560e-01],\n",
       "       [ 3.50687938e-01],\n",
       "       [-6.46766769e-02],\n",
       "       [-3.08702414e-01],\n",
       "       [-2.15397894e-01],\n",
       "       [-3.06617393e-01],\n",
       "       [ 2.81351175e-02],\n",
       "       [-4.20452748e-03],\n",
       "       [-3.38498330e-01],\n",
       "       [-4.95324095e-01],\n",
       "       [ 2.46230350e-02],\n",
       "       [-1.94818598e-01],\n",
       "       [ 1.55463440e-01],\n",
       "       [ 3.30935020e-01],\n",
       "       [ 8.81059946e-01],\n",
       "       [ 6.32896214e-01],\n",
       "       [ 5.35469127e-01],\n",
       "       [ 3.79434072e-01],\n",
       "       [ 2.44995553e-01],\n",
       "       [ 8.51793497e-01],\n",
       "       [ 3.50082725e-01],\n",
       "       [ 5.87871241e-01],\n",
       "       [-1.31877504e-01],\n",
       "       [ 3.09241739e-01],\n",
       "       [-3.55580546e-01],\n",
       "       [ 8.72096787e-02],\n",
       "       [ 1.15887969e-01],\n",
       "       [-8.45954948e-02],\n",
       "       [ 1.65974840e-01],\n",
       "       [-1.64322706e-01],\n",
       "       [ 4.69734030e-01],\n",
       "       [ 3.20344639e-01],\n",
       "       [-2.43910693e-01],\n",
       "       [-1.70490504e-02],\n",
       "       [ 5.04721583e-03],\n",
       "       [-2.58694587e-01],\n",
       "       [ 5.34653631e-02],\n",
       "       [-2.59725406e-01],\n",
       "       [-2.71641600e-01],\n",
       "       [ 1.20474130e-01],\n",
       "       [ 1.50608217e-01],\n",
       "       [ 2.08617456e-01],\n",
       "       [ 8.65295086e-01],\n",
       "       [ 1.58911248e-01],\n",
       "       [ 7.03761564e-01],\n",
       "       [ 1.70452131e-01],\n",
       "       [ 4.86493255e-01],\n",
       "       [ 5.43532374e-01],\n",
       "       [ 2.73327710e-01],\n",
       "       [-1.47666558e-02],\n",
       "       [ 1.21983932e-01],\n",
       "       [-6.44417481e-02],\n",
       "       [ 2.72124353e-01],\n",
       "       [-2.48828477e-01],\n",
       "       [-5.64949952e-02],\n",
       "       [ 5.51905915e-01],\n",
       "       [-1.82950114e-01],\n",
       "       [ 3.41964025e-01],\n",
       "       [ 3.38044208e-01],\n",
       "       [ 7.71769805e-03],\n",
       "       [ 3.46195385e-02],\n",
       "       [-8.60202711e-02],\n",
       "       [-3.54507840e-01],\n",
       "       [ 2.00957653e-02],\n",
       "       [ 6.32314432e-02],\n",
       "       [ 8.20089223e-02],\n",
       "       [ 1.67873158e-01],\n",
       "       [ 2.22317439e-01],\n",
       "       [ 4.28822622e-01],\n",
       "       [ 7.20123286e-01],\n",
       "       [ 6.85152459e-01],\n",
       "       [ 4.64226500e-01],\n",
       "       [ 3.55957882e-01],\n",
       "       [ 5.54121609e-01],\n",
       "       [ 9.72171661e-01],\n",
       "       [ 8.22676278e-01],\n",
       "       [ 3.83460965e-01],\n",
       "       [ 2.82865311e-01],\n",
       "       [ 1.63652727e-01],\n",
       "       [-9.82879744e-02],\n",
       "       [ 1.22559918e-01],\n",
       "       [-3.93100178e-01],\n",
       "       [ 3.87231522e-01],\n",
       "       [-1.04053583e-01],\n",
       "       [-2.98317330e-01],\n",
       "       [-4.29322428e-01],\n",
       "       [-2.00085857e-01],\n",
       "       [-1.66795708e-01],\n",
       "       [-1.67657898e-01],\n",
       "       [ 3.98657399e-02],\n",
       "       [-1.96583283e-01],\n",
       "       [ 1.57401186e-02],\n",
       "       [-3.34259690e-01],\n",
       "       [-2.29863395e-01],\n",
       "       [ 5.01936888e-01],\n",
       "       [ 4.49890546e-02],\n",
       "       [ 7.08160184e-01],\n",
       "       [ 2.79032733e-01],\n",
       "       [ 8.91237570e-01],\n",
       "       [ 7.65109287e-01],\n",
       "       [ 2.40403855e-01],\n",
       "       [ 1.14505693e-01],\n",
       "       [ 2.05784177e-01],\n",
       "       [ 2.87418056e-01],\n",
       "       [ 1.80557006e-01],\n",
       "       [ 7.54353060e-01],\n",
       "       [ 4.43195340e-01],\n",
       "       [ 1.66726529e-01],\n",
       "       [ 3.65486494e-01],\n",
       "       [ 5.15274712e-01],\n",
       "       [-9.06961406e-02],\n",
       "       [-1.22890218e-01],\n",
       "       [ 1.43495097e-01],\n",
       "       [ 1.36420423e-01],\n",
       "       [-1.52735437e-01],\n",
       "       [ 2.96130903e-01],\n",
       "       [-4.31031123e-01],\n",
       "       [ 4.47792345e-01],\n",
       "       [-9.17294107e-02],\n",
       "       [ 8.74933194e-02],\n",
       "       [ 4.12816450e-01],\n",
       "       [ 6.61789179e-01],\n",
       "       [-1.45169076e-01],\n",
       "       [ 4.79625547e-02],\n",
       "       [ 5.92611264e-01],\n",
       "       [ 4.21808432e-02],\n",
       "       [ 2.51626778e-02],\n",
       "       [ 7.70183609e-01],\n",
       "       [ 5.12593444e-01],\n",
       "       [ 1.79237315e-01],\n",
       "       [ 9.77943375e-01],\n",
       "       [ 9.38820453e-01],\n",
       "       [ 4.86660017e-01],\n",
       "       [ 1.68428225e-02],\n",
       "       [ 8.15537253e-01],\n",
       "       [ 6.30432103e-01],\n",
       "       [ 6.47739012e-02],\n",
       "       [ 1.05113335e-01],\n",
       "       [ 4.91943286e-04],\n",
       "       [-1.70848829e-01],\n",
       "       [ 5.74165687e-01],\n",
       "       [ 4.11296814e-01],\n",
       "       [ 3.18333572e-01],\n",
       "       [ 2.39976803e-02],\n",
       "       [-2.15657736e-01],\n",
       "       [-2.14965426e-01],\n",
       "       [-5.70871777e-02],\n",
       "       [-1.34113347e-01],\n",
       "       [ 3.12696808e-01],\n",
       "       [ 3.71692188e-02],\n",
       "       [ 1.15993878e-01],\n",
       "       [ 5.53916554e-01],\n",
       "       [ 8.90867513e-02],\n",
       "       [ 7.13049975e-01],\n",
       "       [ 7.62121413e-01],\n",
       "       [ 8.75088654e-01],\n",
       "       [ 3.95157949e-01],\n",
       "       [ 1.28614993e-01],\n",
       "       [ 8.78043142e-01],\n",
       "       [ 2.40437973e-01],\n",
       "       [ 3.52499367e-01],\n",
       "       [ 4.14113102e-01],\n",
       "       [ 5.62201428e-01],\n",
       "       [ 4.82891435e-01],\n",
       "       [ 4.16755532e-01],\n",
       "       [ 3.03389174e-01],\n",
       "       [ 8.05908000e-02],\n",
       "       [ 5.69141651e-01],\n",
       "       [ 2.92499833e-02],\n",
       "       [ 2.35818237e-01],\n",
       "       [ 3.08485656e-01],\n",
       "       [ 6.08930953e-01],\n",
       "       [ 8.11179268e-01],\n",
       "       [ 3.51411947e-01],\n",
       "       [ 4.49322301e-01],\n",
       "       [ 6.80691495e-02],\n",
       "       [ 8.89483221e-01],\n",
       "       [ 6.16042793e-01],\n",
       "       [ 5.33666935e-01],\n",
       "       [ 4.57946662e-01],\n",
       "       [ 9.59433566e-01],\n",
       "       [ 7.66866786e-01],\n",
       "       [ 3.22564656e-02],\n",
       "       [ 9.10596636e-01],\n",
       "       [ 1.26077059e-01]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_perceptron(x_train, y_train, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_perceptron_f(data, labels, weights):\n",
    "    a,b = np.shape(data)\n",
    "    predicted = predict(data, weights)\n",
    "    correct = (predicted==labels)*1\n",
    "    true_pos = np.sum((labels==1)*(correct))\n",
    "    true_neg = np.sum((labels==0)*(correct))\n",
    "    tp_p = true_pos/float(np.sum(labels))\n",
    "    print(np.sum(labels))\n",
    "    tn_p = true_neg/float(a- np.sum(labels))\n",
    "    return true_pos, true_neg, tp_p, tn_p, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9800000, 0, 10000.0, 0.0, 10000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_perceptron_f(x_test, y_test, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_number(labels, number):\n",
    "    return (labels == number)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_numbers(data,labels):\n",
    "    c,d = np.shape(data)\n",
    "    w = create_weights(data)\n",
    "    weights = []\n",
    "    for i in range(0, 10):\n",
    "        z = one_number(labels, i)\n",
    "        a = train_perceptron(data, z, w, .001, 4)\n",
    "        weights.append(a[:,0])\n",
    "    return np.asarray(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_all(data, weights):\n",
    "    a = np.dot(data,np.transpose(weights))\n",
    "    b = len(np.shape(data))\n",
    "    if b == 1:\n",
    "        return np.argmax(a)\n",
    "    return np.argmax(a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all(data, labels, weights):\n",
    "    a, b = np.shape(labels)\n",
    "    predicted = one_all(data, weights)\n",
    "    correct = predicted == labels[:,0]\n",
    "    accuracy = np.sum(correct)/float(a)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7464715 , 0.39716893, 0.371977  , ..., 0.6015871 , 0.20134565,\n",
       "        0.57255632],\n",
       "       [0.7464715 , 0.39716893, 0.371977  , ..., 0.6015871 , 0.20134565,\n",
       "        0.57255632],\n",
       "       [0.7464715 , 0.39716893, 0.371977  , ..., 0.6015871 , 0.20134565,\n",
       "        0.57255632],\n",
       "       ...,\n",
       "       [0.7464715 , 0.39716893, 0.371977  , ..., 0.6015871 , 0.20134565,\n",
       "        0.57255632],\n",
       "       [0.7464715 , 0.39716893, 0.371977  , ..., 0.6015871 , 0.20134565,\n",
       "        0.57255632],\n",
       "       [0.7464715 , 0.39716893, 0.371977  , ..., 0.6015871 , 0.20134565,\n",
       "        0.57255632]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_numbers(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-18cdb490771d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-166292729a44>\u001b[0m in \u001b[0;36mtest_all\u001b[1;34m(data, labels, weights)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mone_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "test_all(x_test, y_test, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwlRJSfBlCvy"
   },
   "source": [
    "## Stretch Goals: \n",
    "\n",
    "- Implement Cross Validation model evaluation on your MNIST implementation \n",
    "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
    " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
    "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-bb9fd5561af5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "y_test[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_432_Backprop_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
