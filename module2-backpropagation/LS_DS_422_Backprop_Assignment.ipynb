{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Backpropagation Practice\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 2*\n",
    "\n",
    "Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 0  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 1 |\n",
    "| 0  | 1  | 0  | 1 |\n",
    "| 1  | 0  | 0  | 1 |\n",
    "| 1  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 0  | 0 |\n",
    "\n",
    "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0, 1],\n",
    "                [0, 1, 1],\n",
    "                [1, 0, 1],\n",
    "                [0, 1, 0],\n",
    "                [1, 0, 0],\n",
    "                [1, 1, 1],\n",
    "                [0, 0, 0]])\n",
    "y = np.array([[0], [1], [1], [1], [1], [0], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEREYT-3wI1f"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, inputs, hiddenNodes, outputNodes):\n",
    "        self.inputs = inputs\n",
    "        self.hiddenNodes = hiddenNodes\n",
    "        self.outputNodes = outputNodes\n",
    "        \n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        self.o_error = y - o\n",
    "        \n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "        \n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "Actual Output: \n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1797,64) and (3,5) not aligned: 64 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e996efd5e848>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input: \\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Actual Output: \\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted Output: \\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss: \\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-1f8396749c2a>\u001b[0m in \u001b[0;36mfeed_forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivated_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1797,64) and (3,5) not aligned: 64 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(3, 5, 1)\n",
    "\n",
    "for i in range(10000):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "        \n",
    "    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8b-r70o8p2Dm"
   },
   "source": [
    "## Try building/training a more complex MLP on a bigger dataset.\n",
    "\n",
    "Use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the cannonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
    "\n",
    "If you need inspiration, the internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5MOPtYdk1HgA"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits[\"data\"]\n",
    "y = digits['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits[\"data\"].reshape(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, inputs, hiddenNodes, outputNodes):\n",
    "        self.inputs = inputs\n",
    "        self.hiddenNodes = hiddenNodes\n",
    "        self.outputNodes = outputNodes\n",
    "        \n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "\n",
    "        return self.activated_output\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        self.o_error = y - o\n",
    "        \n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "        \n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "Actual Output: \n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "Predicted Output: \n",
      " [[1.01918473e-05 7.32344300e-01 1.39428853e-01 ... 9.99990605e-01\n",
      "  2.03140741e-01 4.52615812e-01]\n",
      " [2.19396832e-05 9.99990502e-01 8.27863706e-04 ... 9.99999982e-01\n",
      "  8.16586577e-05 2.42090363e-01]\n",
      " [2.19446384e-06 2.42853903e-02 9.99171217e-01 ... 9.99989797e-01\n",
      "  2.27071704e-04 5.92589770e-04]\n",
      " ...\n",
      " [2.42607741e-06 3.90785039e-01 1.41676437e-02 ... 9.99865992e-01\n",
      "  4.55102134e-04 8.90859584e-02]\n",
      " [4.21983178e-07 8.61223413e-01 8.96542297e-01 ... 9.99940659e-01\n",
      "  5.70308394e-04 1.51383555e-01]\n",
      " [2.11720235e-02 7.32575966e-01 2.03260715e-01 ... 9.99987420e-01\n",
      "  4.17677599e-05 9.57306511e-01]]\n",
      "Loss: \n",
      " 0.3398571213064386\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "Actual Output: \n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "Predicted Output: \n",
      " [[9.77772342e-034 0.00000000e+000 0.00000000e+000 ... 1.48660431e-055\n",
      "  1.59274016e-170 0.00000000e+000]\n",
      " [1.40783399e-035 0.00000000e+000 0.00000000e+000 ... 3.68568988e-056\n",
      "  1.65544554e-175 0.00000000e+000]\n",
      " [4.32026502e-036 0.00000000e+000 0.00000000e+000 ... 1.21374049e-058\n",
      "  5.20476450e-181 0.00000000e+000]\n",
      " ...\n",
      " [4.38853414e-036 0.00000000e+000 0.00000000e+000 ... 1.18771390e-058\n",
      "  5.01092664e-181 0.00000000e+000]\n",
      " [1.40436317e-032 0.00000000e+000 0.00000000e+000 ... 7.68771001e-053\n",
      "  7.79324847e-165 0.00000000e+000]\n",
      " [1.13421403e-035 0.00000000e+000 0.00000000e+000 ... 3.19744499e-059\n",
      "  5.03669410e-182 0.00000000e+000]]\n",
      "Loss: \n",
      " 0.1\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "Actual Output: \n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "Predicted Output: \n",
      " [[9.77772342e-034 0.00000000e+000 0.00000000e+000 ... 1.48660431e-055\n",
      "  1.59274016e-170 0.00000000e+000]\n",
      " [1.40783399e-035 0.00000000e+000 0.00000000e+000 ... 3.68568988e-056\n",
      "  1.65544554e-175 0.00000000e+000]\n",
      " [4.32026502e-036 0.00000000e+000 0.00000000e+000 ... 1.21374049e-058\n",
      "  5.20476450e-181 0.00000000e+000]\n",
      " ...\n",
      " [4.38853414e-036 0.00000000e+000 0.00000000e+000 ... 1.18771390e-058\n",
      "  5.01092664e-181 0.00000000e+000]\n",
      " [1.40436317e-032 0.00000000e+000 0.00000000e+000 ... 7.68771001e-053\n",
      "  7.79324847e-165 0.00000000e+000]\n",
      " [1.13421403e-035 0.00000000e+000 0.00000000e+000 ... 3.19744499e-059\n",
      "  5.03669410e-182 0.00000000e+000]]\n",
      "Loss: \n",
      " 0.1\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "Actual Output: \n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "Predicted Output: \n",
      " [[9.77772342e-034 0.00000000e+000 0.00000000e+000 ... 1.48660431e-055\n",
      "  1.59274016e-170 0.00000000e+000]\n",
      " [1.40783399e-035 0.00000000e+000 0.00000000e+000 ... 3.68568988e-056\n",
      "  1.65544554e-175 0.00000000e+000]\n",
      " [4.32026502e-036 0.00000000e+000 0.00000000e+000 ... 1.21374049e-058\n",
      "  5.20476450e-181 0.00000000e+000]\n",
      " ...\n",
      " [4.38853414e-036 0.00000000e+000 0.00000000e+000 ... 1.18771390e-058\n",
      "  5.01092664e-181 0.00000000e+000]\n",
      " [1.40436317e-032 0.00000000e+000 0.00000000e+000 ... 7.68771001e-053\n",
      "  7.79324847e-165 0.00000000e+000]\n",
      " [1.13421403e-035 0.00000000e+000 0.00000000e+000 ... 3.19744499e-059\n",
      "  5.03669410e-182 0.00000000e+000]]\n",
      "Loss: \n",
      " 0.1\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "Actual Output: \n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "Predicted Output: \n",
      " [[9.77772342e-034 0.00000000e+000 0.00000000e+000 ... 1.48660431e-055\n",
      "  1.59274016e-170 0.00000000e+000]\n",
      " [1.40783399e-035 0.00000000e+000 0.00000000e+000 ... 3.68568988e-056\n",
      "  1.65544554e-175 0.00000000e+000]\n",
      " [4.32026502e-036 0.00000000e+000 0.00000000e+000 ... 1.21374049e-058\n",
      "  5.20476450e-181 0.00000000e+000]\n",
      " ...\n",
      " [4.38853414e-036 0.00000000e+000 0.00000000e+000 ... 1.18771390e-058\n",
      "  5.01092664e-181 0.00000000e+000]\n",
      " [1.40436317e-032 0.00000000e+000 0.00000000e+000 ... 7.68771001e-053\n",
      "  7.79324847e-165 0.00000000e+000]\n",
      " [1.13421403e-035 0.00000000e+000 0.00000000e+000 ... 3.19744499e-059\n",
      "  5.03669410e-182 0.00000000e+000]]\n",
      "Loss: \n",
      " 0.1\n",
      "+---------EPOCH 1000---------+\n",
      "Input: \n",
      " [[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "Actual Output: \n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "Predicted Output: \n",
      " [[9.77772342e-034 0.00000000e+000 0.00000000e+000 ... 1.48660431e-055\n",
      "  1.59274016e-170 0.00000000e+000]\n",
      " [1.40783399e-035 0.00000000e+000 0.00000000e+000 ... 3.68568988e-056\n",
      "  1.65544554e-175 0.00000000e+000]\n",
      " [4.32026502e-036 0.00000000e+000 0.00000000e+000 ... 1.21374049e-058\n",
      "  5.20476450e-181 0.00000000e+000]\n",
      " ...\n",
      " [4.38853414e-036 0.00000000e+000 0.00000000e+000 ... 1.18771390e-058\n",
      "  5.01092664e-181 0.00000000e+000]\n",
      " [1.40436317e-032 0.00000000e+000 0.00000000e+000 ... 7.68771001e-053\n",
      "  7.79324847e-165 0.00000000e+000]\n",
      " [1.13421403e-035 0.00000000e+000 0.00000000e+000 ... 3.19744499e-059\n",
      "  5.03669410e-182 0.00000000e+000]]\n",
      "Loss: \n",
      " 0.1\n",
      "+---------EPOCH 2000---------+\n",
      "Input: \n",
      " [[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "Actual Output: \n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "Predicted Output: \n",
      " [[9.77772342e-034 0.00000000e+000 0.00000000e+000 ... 1.48660431e-055\n",
      "  1.59274016e-170 0.00000000e+000]\n",
      " [1.40783399e-035 0.00000000e+000 0.00000000e+000 ... 3.68568988e-056\n",
      "  1.65544554e-175 0.00000000e+000]\n",
      " [4.32026502e-036 0.00000000e+000 0.00000000e+000 ... 1.21374049e-058\n",
      "  5.20476450e-181 0.00000000e+000]\n",
      " ...\n",
      " [4.38853414e-036 0.00000000e+000 0.00000000e+000 ... 1.18771390e-058\n",
      "  5.01092664e-181 0.00000000e+000]\n",
      " [1.40436317e-032 0.00000000e+000 0.00000000e+000 ... 7.68771001e-053\n",
      "  7.79324847e-165 0.00000000e+000]\n",
      " [1.13421403e-035 0.00000000e+000 0.00000000e+000 ... 3.19744499e-059\n",
      "  5.03669410e-182 0.00000000e+000]]\n",
      "Loss: \n",
      " 0.1\n",
      "+---------EPOCH 3000---------+\n",
      "Input: \n",
      " [[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "Actual Output: \n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "Predicted Output: \n",
      " [[9.77772342e-034 0.00000000e+000 0.00000000e+000 ... 1.48660431e-055\n",
      "  1.59274016e-170 0.00000000e+000]\n",
      " [1.40783399e-035 0.00000000e+000 0.00000000e+000 ... 3.68568988e-056\n",
      "  1.65544554e-175 0.00000000e+000]\n",
      " [4.32026502e-036 0.00000000e+000 0.00000000e+000 ... 1.21374049e-058\n",
      "  5.20476450e-181 0.00000000e+000]\n",
      " ...\n",
      " [4.38853414e-036 0.00000000e+000 0.00000000e+000 ... 1.18771390e-058\n",
      "  5.01092664e-181 0.00000000e+000]\n",
      " [1.40436317e-032 0.00000000e+000 0.00000000e+000 ... 7.68771001e-053\n",
      "  7.79324847e-165 0.00000000e+000]\n",
      " [1.13421403e-035 0.00000000e+000 0.00000000e+000 ... 3.19744499e-059\n",
      "  5.03669410e-182 0.00000000e+000]]\n",
      "Loss: \n",
      " 0.1\n",
      "+---------EPOCH 4000---------+\n",
      "Input: \n",
      " [[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "Actual Output: \n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "Predicted Output: \n",
      " [[9.77772342e-034 0.00000000e+000 0.00000000e+000 ... 1.48660431e-055\n",
      "  1.59274016e-170 0.00000000e+000]\n",
      " [1.40783399e-035 0.00000000e+000 0.00000000e+000 ... 3.68568988e-056\n",
      "  1.65544554e-175 0.00000000e+000]\n",
      " [4.32026502e-036 0.00000000e+000 0.00000000e+000 ... 1.21374049e-058\n",
      "  5.20476450e-181 0.00000000e+000]\n",
      " ...\n",
      " [4.38853414e-036 0.00000000e+000 0.00000000e+000 ... 1.18771390e-058\n",
      "  5.01092664e-181 0.00000000e+000]\n",
      " [1.40436317e-032 0.00000000e+000 0.00000000e+000 ... 7.68771001e-053\n",
      "  7.79324847e-165 0.00000000e+000]\n",
      " [1.13421403e-035 0.00000000e+000 0.00000000e+000 ... 3.19744499e-059\n",
      "  5.03669410e-182 0.00000000e+000]]\n",
      "Loss: \n",
      " 0.1\n",
      "+---------EPOCH 5000---------+\n",
      "Input: \n",
      " [[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "Actual Output: \n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "Predicted Output: \n",
      " [[9.77772342e-034 0.00000000e+000 0.00000000e+000 ... 1.48660431e-055\n",
      "  1.59274016e-170 0.00000000e+000]\n",
      " [1.40783399e-035 0.00000000e+000 0.00000000e+000 ... 3.68568988e-056\n",
      "  1.65544554e-175 0.00000000e+000]\n",
      " [4.32026502e-036 0.00000000e+000 0.00000000e+000 ... 1.21374049e-058\n",
      "  5.20476450e-181 0.00000000e+000]\n",
      " ...\n",
      " [4.38853414e-036 0.00000000e+000 0.00000000e+000 ... 1.18771390e-058\n",
      "  5.01092664e-181 0.00000000e+000]\n",
      " [1.40436317e-032 0.00000000e+000 0.00000000e+000 ... 7.68771001e-053\n",
      "  7.79324847e-165 0.00000000e+000]\n",
      " [1.13421403e-035 0.00000000e+000 0.00000000e+000 ... 3.19744499e-059\n",
      "  5.03669410e-182 0.00000000e+000]]\n",
      "Loss: \n",
      " 0.1\n",
      "+---------EPOCH 6000---------+\n",
      "Input: \n",
      " [[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "Actual Output: \n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "Predicted Output: \n",
      " [[9.77772342e-034 0.00000000e+000 0.00000000e+000 ... 1.48660431e-055\n",
      "  1.59274016e-170 0.00000000e+000]\n",
      " [1.40783399e-035 0.00000000e+000 0.00000000e+000 ... 3.68568988e-056\n",
      "  1.65544554e-175 0.00000000e+000]\n",
      " [4.32026502e-036 0.00000000e+000 0.00000000e+000 ... 1.21374049e-058\n",
      "  5.20476450e-181 0.00000000e+000]\n",
      " ...\n",
      " [4.38853414e-036 0.00000000e+000 0.00000000e+000 ... 1.18771390e-058\n",
      "  5.01092664e-181 0.00000000e+000]\n",
      " [1.40436317e-032 0.00000000e+000 0.00000000e+000 ... 7.68771001e-053\n",
      "  7.79324847e-165 0.00000000e+000]\n",
      " [1.13421403e-035 0.00000000e+000 0.00000000e+000 ... 3.19744499e-059\n",
      "  5.03669410e-182 0.00000000e+000]]\n",
      "Loss: \n",
      " 0.1\n",
      "+---------EPOCH 7000---------+\n",
      "Input: \n",
      " [[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "Actual Output: \n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "Predicted Output: \n",
      " [[9.77772342e-034 0.00000000e+000 0.00000000e+000 ... 1.48660431e-055\n",
      "  1.59274016e-170 0.00000000e+000]\n",
      " [1.40783399e-035 0.00000000e+000 0.00000000e+000 ... 3.68568988e-056\n",
      "  1.65544554e-175 0.00000000e+000]\n",
      " [4.32026502e-036 0.00000000e+000 0.00000000e+000 ... 1.21374049e-058\n",
      "  5.20476450e-181 0.00000000e+000]\n",
      " ...\n",
      " [4.38853414e-036 0.00000000e+000 0.00000000e+000 ... 1.18771390e-058\n",
      "  5.01092664e-181 0.00000000e+000]\n",
      " [1.40436317e-032 0.00000000e+000 0.00000000e+000 ... 7.68771001e-053\n",
      "  7.79324847e-165 0.00000000e+000]\n",
      " [1.13421403e-035 0.00000000e+000 0.00000000e+000 ... 3.19744499e-059\n",
      "  5.03669410e-182 0.00000000e+000]]\n",
      "Loss: \n",
      " 0.1\n",
      "+---------EPOCH 8000---------+\n",
      "Input: \n",
      " [[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "Actual Output: \n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "Predicted Output: \n",
      " [[9.77772342e-034 0.00000000e+000 0.00000000e+000 ... 1.48660431e-055\n",
      "  1.59274016e-170 0.00000000e+000]\n",
      " [1.40783399e-035 0.00000000e+000 0.00000000e+000 ... 3.68568988e-056\n",
      "  1.65544554e-175 0.00000000e+000]\n",
      " [4.32026502e-036 0.00000000e+000 0.00000000e+000 ... 1.21374049e-058\n",
      "  5.20476450e-181 0.00000000e+000]\n",
      " ...\n",
      " [4.38853414e-036 0.00000000e+000 0.00000000e+000 ... 1.18771390e-058\n",
      "  5.01092664e-181 0.00000000e+000]\n",
      " [1.40436317e-032 0.00000000e+000 0.00000000e+000 ... 7.68771001e-053\n",
      "  7.79324847e-165 0.00000000e+000]\n",
      " [1.13421403e-035 0.00000000e+000 0.00000000e+000 ... 3.19744499e-059\n",
      "  5.03669410e-182 0.00000000e+000]]\n",
      "Loss: \n",
      " 0.1\n",
      "+---------EPOCH 9000---------+\n",
      "Input: \n",
      " [[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "Actual Output: \n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "Predicted Output: \n",
      " [[9.77772342e-034 0.00000000e+000 0.00000000e+000 ... 1.48660431e-055\n",
      "  1.59274016e-170 0.00000000e+000]\n",
      " [1.40783399e-035 0.00000000e+000 0.00000000e+000 ... 3.68568988e-056\n",
      "  1.65544554e-175 0.00000000e+000]\n",
      " [4.32026502e-036 0.00000000e+000 0.00000000e+000 ... 1.21374049e-058\n",
      "  5.20476450e-181 0.00000000e+000]\n",
      " ...\n",
      " [4.38853414e-036 0.00000000e+000 0.00000000e+000 ... 1.18771390e-058\n",
      "  5.01092664e-181 0.00000000e+000]\n",
      " [1.40436317e-032 0.00000000e+000 0.00000000e+000 ... 7.68771001e-053\n",
      "  7.79324847e-165 0.00000000e+000]\n",
      " [1.13421403e-035 0.00000000e+000 0.00000000e+000 ... 3.19744499e-059\n",
      "  5.03669410e-182 0.00000000e+000]]\n",
      "Loss: \n",
      " 0.1\n",
      "+---------EPOCH 10000---------+\n",
      "Input: \n",
      " [[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "Actual Output: \n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "Predicted Output: \n",
      " [[9.77772342e-034 0.00000000e+000 0.00000000e+000 ... 1.48660431e-055\n",
      "  1.59274016e-170 0.00000000e+000]\n",
      " [1.40783399e-035 0.00000000e+000 0.00000000e+000 ... 3.68568988e-056\n",
      "  1.65544554e-175 0.00000000e+000]\n",
      " [4.32026502e-036 0.00000000e+000 0.00000000e+000 ... 1.21374049e-058\n",
      "  5.20476450e-181 0.00000000e+000]\n",
      " ...\n",
      " [4.38853414e-036 0.00000000e+000 0.00000000e+000 ... 1.18771390e-058\n",
      "  5.01092664e-181 0.00000000e+000]\n",
      " [1.40436317e-032 0.00000000e+000 0.00000000e+000 ... 7.68771001e-053\n",
      "  7.79324847e-165 0.00000000e+000]\n",
      " [1.13421403e-035 0.00000000e+000 0.00000000e+000 ... 3.19744499e-059\n",
      "  5.03669410e-182 0.00000000e+000]]\n",
      "Loss: \n",
      " 0.1\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(64, 100, 10)\n",
    "\n",
    "for i in range(10000):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "        \n",
    "    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: \n",
      "[[9.77772342e-034 0.00000000e+000 0.00000000e+000 ... 1.48660431e-055\n",
      "  1.59274016e-170 0.00000000e+000]\n",
      " [1.40783399e-035 0.00000000e+000 0.00000000e+000 ... 3.68568988e-056\n",
      "  1.65544554e-175 0.00000000e+000]\n",
      " [4.32026502e-036 0.00000000e+000 0.00000000e+000 ... 1.21374049e-058\n",
      "  5.20476450e-181 0.00000000e+000]\n",
      " ...\n",
      " [4.38853414e-036 0.00000000e+000 0.00000000e+000 ... 1.18771390e-058\n",
      "  5.01092664e-181 0.00000000e+000]\n",
      " [1.40436317e-032 0.00000000e+000 0.00000000e+000 ... 7.68771001e-053\n",
      "  7.79324847e-165 0.00000000e+000]\n",
      " [1.13421403e-035 0.00000000e+000 0.00000000e+000 ... 3.19744499e-059\n",
      "  5.03669410e-182 0.00000000e+000]]\n",
      "Loss: \n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Output: \\n\" + str(nn.feed_forward(X))) \n",
    "print(\"Loss: \\n\" + str(np.mean(np.square(y - nn.feed_forward(X))))) # mean sum squared loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwlRJSfBlCvy"
   },
   "source": [
    "## Stretch Goals: \n",
    "\n",
    "- Implement Cross Validation model evaluation on your MNIST implementation \n",
    "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
    " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
    "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X / 255\n",
    "\n",
    "# digits = 10\n",
    "# examples = y.shape[0]\n",
    "\n",
    "# y = y.reshape(1, examples)\n",
    "\n",
    "# Y_new = np.eye(digits)[y.astype('int32')]\n",
    "# Y_new = Y_new.T.reshape(digits, examples)\n",
    "\n",
    "# m = 60000\n",
    "# m_test = X.shape[0] - m\n",
    "\n",
    "# X_train, X_test = X[:m].T, X[m:].T\n",
    "# Y_train, Y_test = Y_new[:, :m], Y_new[:, m:]\n",
    "\n",
    "# shuffle_index = np.random.permutation(m)\n",
    "# X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]\n",
    "# X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = X.reshape(X.shape[0],8,8,1)\n",
    "# Split the data\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1203, 8, 8, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_valid = to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_mnist_model():\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(8,8,1)))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "#     model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'],\n",
    "             optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1203/1203 [==============================] - 0s 379us/sample - loss: 0.0467 - acc: 0.9859\n",
      "Epoch 2/500\n",
      "1203/1203 [==============================] - 0s 380us/sample - loss: 0.0297 - acc: 0.9925\n",
      "Epoch 3/500\n",
      "1203/1203 [==============================] - 0s 376us/sample - loss: 0.0383 - acc: 0.9875\n",
      "Epoch 4/500\n",
      "1203/1203 [==============================] - 0s 380us/sample - loss: 0.0388 - acc: 0.9900\n",
      "Epoch 5/500\n",
      "1203/1203 [==============================] - 1s 532us/sample - loss: 0.0623 - acc: 0.9859\n",
      "Epoch 6/500\n",
      "1203/1203 [==============================] - 0s 383us/sample - loss: 0.0437 - acc: 0.9875\n",
      "Epoch 7/500\n",
      "1203/1203 [==============================] - 0s 384us/sample - loss: 0.0529 - acc: 0.9867\n",
      "Epoch 8/500\n",
      "1203/1203 [==============================] - 0s 392us/sample - loss: 0.0522 - acc: 0.9867\n",
      "Epoch 9/500\n",
      "1203/1203 [==============================] - 1s 547us/sample - loss: 0.0349 - acc: 0.9892\n",
      "Epoch 10/500\n",
      "1203/1203 [==============================] - 0s 396us/sample - loss: 0.0174 - acc: 0.9933\n",
      "Epoch 11/500\n",
      "1203/1203 [==============================] - 0s 390us/sample - loss: 0.0426 - acc: 0.9900\n",
      "Epoch 12/500\n",
      "1203/1203 [==============================] - 0s 407us/sample - loss: 0.0357 - acc: 0.9917\n",
      "Epoch 13/500\n",
      "1203/1203 [==============================] - 1s 559us/sample - loss: 0.0427 - acc: 0.9884\n",
      "Epoch 14/500\n",
      "1203/1203 [==============================] - 0s 399us/sample - loss: 0.0582 - acc: 0.9809\n",
      "Epoch 15/500\n",
      "1203/1203 [==============================] - 0s 399us/sample - loss: 0.0478 - acc: 0.9900\n",
      "Epoch 16/500\n",
      "1203/1203 [==============================] - 0s 397us/sample - loss: 0.0313 - acc: 0.9917\n",
      "Epoch 17/500\n",
      "1203/1203 [==============================] - 1s 541us/sample - loss: 0.0623 - acc: 0.9850\n",
      "Epoch 18/500\n",
      "1203/1203 [==============================] - 0s 396us/sample - loss: 0.0501 - acc: 0.9875\n",
      "Epoch 19/500\n",
      "1203/1203 [==============================] - 0s 396us/sample - loss: 0.0315 - acc: 0.9942\n",
      "Epoch 20/500\n",
      "1203/1203 [==============================] - 0s 393us/sample - loss: 0.0283 - acc: 0.9917\n",
      "Epoch 21/500\n",
      "1203/1203 [==============================] - 1s 551us/sample - loss: 0.0224 - acc: 0.9892\n",
      "Epoch 22/500\n",
      "1203/1203 [==============================] - 0s 393us/sample - loss: 0.0347 - acc: 0.9892\n",
      "Epoch 23/500\n",
      "1203/1203 [==============================] - 0s 400us/sample - loss: 0.0155 - acc: 0.9942\n",
      "Epoch 24/500\n",
      "1203/1203 [==============================] - 0s 412us/sample - loss: 0.0217 - acc: 0.9925\n",
      "Epoch 25/500\n",
      "1203/1203 [==============================] - 1s 574us/sample - loss: 0.0300 - acc: 0.9925\n",
      "Epoch 26/500\n",
      "1203/1203 [==============================] - 1s 423us/sample - loss: 0.0499 - acc: 0.9892\n",
      "Epoch 27/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0361 - acc: 0.9909\n",
      "Epoch 28/500\n",
      "1203/1203 [==============================] - 1s 583us/sample - loss: 0.0358 - acc: 0.9900\n",
      "Epoch 29/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0423 - acc: 0.9909\n",
      "Epoch 30/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0238 - acc: 0.9925\n",
      "Epoch 31/500\n",
      "1203/1203 [==============================] - 0s 410us/sample - loss: 0.0245 - acc: 0.9925\n",
      "Epoch 32/500\n",
      "1203/1203 [==============================] - 1s 565us/sample - loss: 0.0280 - acc: 0.9900\n",
      "Epoch 33/500\n",
      "1203/1203 [==============================] - 0s 402us/sample - loss: 0.0338 - acc: 0.9892\n",
      "Epoch 34/500\n",
      "1203/1203 [==============================] - 1s 424us/sample - loss: 0.0348 - acc: 0.9859\n",
      "Epoch 35/500\n",
      "1203/1203 [==============================] - 0s 406us/sample - loss: 0.0387 - acc: 0.9900\n",
      "Epoch 36/500\n",
      "1203/1203 [==============================] - 1s 582us/sample - loss: 0.0396 - acc: 0.9884\n",
      "Epoch 37/500\n",
      "1203/1203 [==============================] - 0s 415us/sample - loss: 0.0306 - acc: 0.9942\n",
      "Epoch 38/500\n",
      "1203/1203 [==============================] - 1s 422us/sample - loss: 0.0360 - acc: 0.9925\n",
      "Epoch 39/500\n",
      "1203/1203 [==============================] - 0s 410us/sample - loss: 0.0388 - acc: 0.9884\n",
      "Epoch 40/500\n",
      "1203/1203 [==============================] - 1s 567us/sample - loss: 0.0486 - acc: 0.9859\n",
      "Epoch 41/500\n",
      "1203/1203 [==============================] - 0s 408us/sample - loss: 0.0409 - acc: 0.9867\n",
      "Epoch 42/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0291 - acc: 0.9900\n",
      "Epoch 43/500\n",
      "1203/1203 [==============================] - 1s 579us/sample - loss: 0.0431 - acc: 0.9884\n",
      "Epoch 44/500\n",
      "1203/1203 [==============================] - 1s 424us/sample - loss: 0.0247 - acc: 0.9909\n",
      "Epoch 45/500\n",
      "1203/1203 [==============================] - 1s 435us/sample - loss: 0.0269 - acc: 0.9900\n",
      "Epoch 46/500\n",
      "1203/1203 [==============================] - 0s 414us/sample - loss: 0.0331 - acc: 0.9867\n",
      "Epoch 47/500\n",
      "1203/1203 [==============================] - 1s 560us/sample - loss: 0.0566 - acc: 0.9884\n",
      "Epoch 48/500\n",
      "1203/1203 [==============================] - 1s 422us/sample - loss: 0.0529 - acc: 0.9842\n",
      "Epoch 49/500\n",
      "1203/1203 [==============================] - 1s 443us/sample - loss: 0.0259 - acc: 0.9917\n",
      "Epoch 50/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0498 - acc: 0.9842\n",
      "Epoch 51/500\n",
      "1203/1203 [==============================] - 1s 584us/sample - loss: 0.0310 - acc: 0.9925\n",
      "Epoch 52/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0434 - acc: 0.9900\n",
      "Epoch 53/500\n",
      "1203/1203 [==============================] - 1s 429us/sample - loss: 0.0326 - acc: 0.9867\n",
      "Epoch 54/500\n",
      "1203/1203 [==============================] - 1s 584us/sample - loss: 0.0246 - acc: 0.9925\n",
      "Epoch 55/500\n",
      "1203/1203 [==============================] - 1s 423us/sample - loss: 0.0470 - acc: 0.9892\n",
      "Epoch 56/500\n",
      "1203/1203 [==============================] - 1s 423us/sample - loss: 0.0092 - acc: 0.9967\n",
      "Epoch 57/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0369 - acc: 0.9892\n",
      "Epoch 58/500\n",
      "1203/1203 [==============================] - 1s 565us/sample - loss: 0.0297 - acc: 0.9909\n",
      "Epoch 59/500\n",
      "1203/1203 [==============================] - 1s 424us/sample - loss: 0.0339 - acc: 0.9917\n",
      "Epoch 60/500\n",
      "1203/1203 [==============================] - 1s 423us/sample - loss: 0.0449 - acc: 0.9850\n",
      "Epoch 61/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0302 - acc: 0.9933\n",
      "Epoch 62/500\n",
      "1203/1203 [==============================] - 1s 571us/sample - loss: 0.0313 - acc: 0.9942\n",
      "Epoch 63/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0315 - acc: 0.9933\n",
      "Epoch 64/500\n",
      "1203/1203 [==============================] - 1s 420us/sample - loss: 0.0405 - acc: 0.9875\n",
      "Epoch 65/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0452 - acc: 0.9859\n",
      "Epoch 66/500\n",
      "1203/1203 [==============================] - 1s 571us/sample - loss: 0.0372 - acc: 0.9875\n",
      "Epoch 67/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0359 - acc: 0.9867\n",
      "Epoch 68/500\n",
      "1203/1203 [==============================] - 1s 422us/sample - loss: 0.0489 - acc: 0.9884\n",
      "Epoch 69/500\n",
      "1203/1203 [==============================] - 1s 580us/sample - loss: 0.0368 - acc: 0.9917\n",
      "Epoch 70/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0329 - acc: 0.9909\n",
      "Epoch 71/500\n",
      "1203/1203 [==============================] - 1s 421us/sample - loss: 0.0452 - acc: 0.9909\n",
      "Epoch 72/500\n",
      "1203/1203 [==============================] - 1s 423us/sample - loss: 0.0322 - acc: 0.9925\n",
      "Epoch 73/500\n",
      "1203/1203 [==============================] - 1s 570us/sample - loss: 0.0158 - acc: 0.9925\n",
      "Epoch 74/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0241 - acc: 0.9933\n",
      "Epoch 75/500\n",
      "1203/1203 [==============================] - 1s 421us/sample - loss: 0.0182 - acc: 0.9925\n",
      "Epoch 76/500\n",
      "1203/1203 [==============================] - 1s 423us/sample - loss: 0.0445 - acc: 0.9900\n",
      "Epoch 77/500\n",
      "1203/1203 [==============================] - 1s 568us/sample - loss: 0.0404 - acc: 0.9900\n",
      "Epoch 78/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0460 - acc: 0.9875\n",
      "Epoch 79/500\n",
      "1203/1203 [==============================] - 1s 420us/sample - loss: 0.0741 - acc: 0.9850\n",
      "Epoch 80/500\n",
      "1203/1203 [==============================] - 1s 424us/sample - loss: 0.0608 - acc: 0.9884\n",
      "Epoch 81/500\n",
      "1203/1203 [==============================] - 1s 578us/sample - loss: 0.0160 - acc: 0.9942\n",
      "Epoch 82/500\n",
      "1203/1203 [==============================] - 1s 422us/sample - loss: 0.0184 - acc: 0.9933\n",
      "Epoch 83/500\n",
      "1203/1203 [==============================] - 1s 420us/sample - loss: 0.0333 - acc: 0.9892\n",
      "Epoch 84/500\n",
      "1203/1203 [==============================] - 1s 575us/sample - loss: 0.0450 - acc: 0.9909\n",
      "Epoch 85/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0241 - acc: 0.9925\n",
      "Epoch 86/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0183 - acc: 0.9942\n",
      "Epoch 87/500\n",
      "1203/1203 [==============================] - 1s 421us/sample - loss: 0.0534 - acc: 0.9867\n",
      "Epoch 88/500\n",
      "1203/1203 [==============================] - 1s 573us/sample - loss: 0.0359 - acc: 0.9900\n",
      "Epoch 89/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0459 - acc: 0.9909\n",
      "Epoch 90/500\n",
      "1203/1203 [==============================] - 1s 422us/sample - loss: 0.0315 - acc: 0.9909\n",
      "Epoch 91/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0568 - acc: 0.9850\n",
      "Epoch 92/500\n",
      "1203/1203 [==============================] - 1s 580us/sample - loss: 0.0551 - acc: 0.9867\n",
      "Epoch 93/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0389 - acc: 0.9884\n",
      "Epoch 94/500\n",
      "1203/1203 [==============================] - 1s 423us/sample - loss: 0.0475 - acc: 0.9900\n",
      "Epoch 95/500\n",
      "1203/1203 [==============================] - 1s 580us/sample - loss: 0.0417 - acc: 0.9859\n",
      "Epoch 96/500\n",
      "1203/1203 [==============================] - 1s 435us/sample - loss: 0.0352 - acc: 0.9875\n",
      "Epoch 97/500\n",
      "1203/1203 [==============================] - 1s 434us/sample - loss: 0.0302 - acc: 0.9909\n",
      "Epoch 98/500\n",
      "1203/1203 [==============================] - 1s 433us/sample - loss: 0.0234 - acc: 0.9917\n",
      "Epoch 99/500\n",
      "1203/1203 [==============================] - 1s 585us/sample - loss: 0.0244 - acc: 0.9925\n",
      "Epoch 100/500\n",
      "1203/1203 [==============================] - 1s 422us/sample - loss: 0.0555 - acc: 0.9875\n",
      "Epoch 101/500\n",
      "1203/1203 [==============================] - 1s 424us/sample - loss: 0.0288 - acc: 0.9909\n",
      "Epoch 102/500\n",
      "1203/1203 [==============================] - 1s 447us/sample - loss: 0.0139 - acc: 0.9950\n",
      "Epoch 103/500\n",
      "1203/1203 [==============================] - 1s 585us/sample - loss: 0.0231 - acc: 0.9892\n",
      "Epoch 104/500\n",
      "1203/1203 [==============================] - 1s 453us/sample - loss: 0.0356 - acc: 0.9900\n",
      "Epoch 105/500\n",
      "1203/1203 [==============================] - 1s 446us/sample - loss: 0.0345 - acc: 0.9917\n",
      "Epoch 106/500\n",
      "1203/1203 [==============================] - 1s 597us/sample - loss: 0.0209 - acc: 0.9933\n",
      "Epoch 107/500\n",
      "1203/1203 [==============================] - 1s 444us/sample - loss: 0.0269 - acc: 0.9909\n",
      "Epoch 108/500\n",
      "1203/1203 [==============================] - 1s 447us/sample - loss: 0.0297 - acc: 0.9900\n",
      "Epoch 109/500\n",
      "1203/1203 [==============================] - 1s 454us/sample - loss: 0.0350 - acc: 0.9859\n",
      "Epoch 110/500\n",
      "1203/1203 [==============================] - 1s 601us/sample - loss: 0.0386 - acc: 0.9884\n",
      "Epoch 111/500\n",
      "1203/1203 [==============================] - 1s 444us/sample - loss: 0.0292 - acc: 0.9900\n",
      "Epoch 112/500\n",
      "1203/1203 [==============================] - 1s 440us/sample - loss: 0.0484 - acc: 0.9909\n",
      "Epoch 113/500\n",
      "1203/1203 [==============================] - 1s 587us/sample - loss: 0.0129 - acc: 0.9942\n",
      "Epoch 114/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0275 - acc: 0.9942\n",
      "Epoch 115/500\n",
      "1203/1203 [==============================] - 1s 432us/sample - loss: 0.0248 - acc: 0.9942\n",
      "Epoch 116/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0303 - acc: 0.9925\n",
      "Epoch 117/500\n",
      "1203/1203 [==============================] - 1s 576us/sample - loss: 0.0384 - acc: 0.9884\n",
      "Epoch 118/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0157 - acc: 0.9933\n",
      "Epoch 119/500\n",
      "1203/1203 [==============================] - 1s 433us/sample - loss: 0.0345 - acc: 0.9909\n",
      "Epoch 120/500\n",
      "1203/1203 [==============================] - 1s 434us/sample - loss: 0.0415 - acc: 0.9917\n",
      "Epoch 121/500\n",
      "1203/1203 [==============================] - 1s 577us/sample - loss: 0.0352 - acc: 0.9884\n",
      "Epoch 122/500\n",
      "1203/1203 [==============================] - 1s 433us/sample - loss: 0.0370 - acc: 0.9909\n",
      "Epoch 123/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0303 - acc: 0.9900\n",
      "Epoch 124/500\n",
      "1203/1203 [==============================] - 1s 576us/sample - loss: 0.0352 - acc: 0.9909\n",
      "Epoch 125/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0549 - acc: 0.9900\n",
      "Epoch 126/500\n",
      "1203/1203 [==============================] - 1s 434us/sample - loss: 0.0213 - acc: 0.9933\n",
      "Epoch 127/500\n",
      "1203/1203 [==============================] - 1s 432us/sample - loss: 0.0307 - acc: 0.9917\n",
      "Epoch 128/500\n",
      "1203/1203 [==============================] - 1s 582us/sample - loss: 0.0452 - acc: 0.9875\n",
      "Epoch 129/500\n",
      "1203/1203 [==============================] - 1s 443us/sample - loss: 0.0203 - acc: 0.9958\n",
      "Epoch 130/500\n",
      "1203/1203 [==============================] - 1s 462us/sample - loss: 0.0566 - acc: 0.9842\n",
      "Epoch 131/500\n",
      "1203/1203 [==============================] - 1s 446us/sample - loss: 0.0464 - acc: 0.9884\n",
      "Epoch 132/500\n",
      "1203/1203 [==============================] - 1s 613us/sample - loss: 0.0298 - acc: 0.9892\n",
      "Epoch 133/500\n",
      "1203/1203 [==============================] - 1s 470us/sample - loss: 0.0216 - acc: 0.9925\n",
      "Epoch 134/500\n",
      "1203/1203 [==============================] - 1s 445us/sample - loss: 0.0632 - acc: 0.9867\n",
      "Epoch 135/500\n",
      "1203/1203 [==============================] - 1s 583us/sample - loss: 0.0438 - acc: 0.9884\n",
      "Epoch 136/500\n",
      "1203/1203 [==============================] - 1s 424us/sample - loss: 0.0473 - acc: 0.9884\n",
      "Epoch 137/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0583 - acc: 0.9825\n",
      "Epoch 138/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0447 - acc: 0.9884\n",
      "Epoch 139/500\n",
      "1203/1203 [==============================] - 1s 584us/sample - loss: 0.0395 - acc: 0.9892\n",
      "Epoch 140/500\n",
      "1203/1203 [==============================] - 1s 429us/sample - loss: 0.0405 - acc: 0.9892\n",
      "Epoch 141/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0317 - acc: 0.9925\n",
      "Epoch 142/500\n",
      "1203/1203 [==============================] - 1s 586us/sample - loss: 0.0302 - acc: 0.9884\n",
      "Epoch 143/500\n",
      "1203/1203 [==============================] - 1s 437us/sample - loss: 0.0361 - acc: 0.9917\n",
      "Epoch 144/500\n",
      "1203/1203 [==============================] - 1s 424us/sample - loss: 0.0298 - acc: 0.9909\n",
      "Epoch 145/500\n",
      "1203/1203 [==============================] - 1s 421us/sample - loss: 0.0332 - acc: 0.9892\n",
      "Epoch 146/500\n",
      "1203/1203 [==============================] - 1s 570us/sample - loss: 0.0471 - acc: 0.9884\n",
      "Epoch 147/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0475 - acc: 0.9917\n",
      "Epoch 148/500\n",
      "1203/1203 [==============================] - 1s 421us/sample - loss: 0.0134 - acc: 0.9950\n",
      "Epoch 149/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0320 - acc: 0.9900\n",
      "Epoch 150/500\n",
      "1203/1203 [==============================] - 1s 576us/sample - loss: 0.0400 - acc: 0.9892\n",
      "Epoch 151/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0267 - acc: 0.9892\n",
      "Epoch 152/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0361 - acc: 0.9875\n",
      "Epoch 153/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0336 - acc: 0.9909\n",
      "Epoch 154/500\n",
      "1203/1203 [==============================] - 1s 581us/sample - loss: 0.0252 - acc: 0.9933\n",
      "Epoch 155/500\n",
      "1203/1203 [==============================] - 1s 433us/sample - loss: 0.0250 - acc: 0.9900\n",
      "Epoch 156/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0308 - acc: 0.9900\n",
      "Epoch 157/500\n",
      "1203/1203 [==============================] - 1s 579us/sample - loss: 0.0403 - acc: 0.9875\n",
      "Epoch 158/500\n",
      "1203/1203 [==============================] - 1s 424us/sample - loss: 0.0283 - acc: 0.9909\n",
      "Epoch 159/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0306 - acc: 0.9917\n",
      "Epoch 160/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0612 - acc: 0.9825\n",
      "Epoch 161/500\n",
      "1203/1203 [==============================] - 1s 577us/sample - loss: 0.0327 - acc: 0.9917\n",
      "Epoch 162/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0557 - acc: 0.9859\n",
      "Epoch 163/500\n",
      "1203/1203 [==============================] - 1s 423us/sample - loss: 0.0182 - acc: 0.9942\n",
      "Epoch 164/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0362 - acc: 0.9875\n",
      "Epoch 165/500\n",
      "1203/1203 [==============================] - 1s 574us/sample - loss: 0.0287 - acc: 0.9909\n",
      "Epoch 166/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0385 - acc: 0.9859\n",
      "Epoch 167/500\n",
      "1203/1203 [==============================] - 1s 424us/sample - loss: 0.0436 - acc: 0.9867\n",
      "Epoch 168/500\n",
      "1203/1203 [==============================] - 1s 578us/sample - loss: 0.0403 - acc: 0.9850\n",
      "Epoch 169/500\n",
      "1203/1203 [==============================] - 1s 423us/sample - loss: 0.0455 - acc: 0.9909\n",
      "Epoch 170/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0372 - acc: 0.9859\n",
      "Epoch 171/500\n",
      "1203/1203 [==============================] - 1s 423us/sample - loss: 0.0257 - acc: 0.9958\n",
      "Epoch 172/500\n",
      "1203/1203 [==============================] - 1s 576us/sample - loss: 0.0337 - acc: 0.9909\n",
      "Epoch 173/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0299 - acc: 0.9909\n",
      "Epoch 174/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0508 - acc: 0.9884\n",
      "Epoch 175/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0288 - acc: 0.9892\n",
      "Epoch 176/500\n",
      "1203/1203 [==============================] - 1s 573us/sample - loss: 0.0264 - acc: 0.9925\n",
      "Epoch 177/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0214 - acc: 0.9925\n",
      "Epoch 178/500\n",
      "1203/1203 [==============================] - 1s 424us/sample - loss: 0.0363 - acc: 0.9917\n",
      "Epoch 179/500\n",
      "1203/1203 [==============================] - 1s 577us/sample - loss: 0.0448 - acc: 0.9867\n",
      "Epoch 180/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0656 - acc: 0.9867\n",
      "Epoch 181/500\n",
      "1203/1203 [==============================] - 1s 424us/sample - loss: 0.0156 - acc: 0.9967\n",
      "Epoch 182/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0413 - acc: 0.9917\n",
      "Epoch 183/500\n",
      "1203/1203 [==============================] - 1s 579us/sample - loss: 0.0471 - acc: 0.9867\n",
      "Epoch 184/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0491 - acc: 0.9850\n",
      "Epoch 185/500\n",
      "1203/1203 [==============================] - 1s 424us/sample - loss: 0.0322 - acc: 0.9892\n",
      "Epoch 186/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0223 - acc: 0.9917\n",
      "Epoch 187/500\n",
      "1203/1203 [==============================] - 1s 574us/sample - loss: 0.0138 - acc: 0.9958\n",
      "Epoch 188/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0442 - acc: 0.9875\n",
      "Epoch 189/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0482 - acc: 0.9884\n",
      "Epoch 190/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0330 - acc: 0.9900\n",
      "Epoch 191/500\n",
      "1203/1203 [==============================] - 1s 578us/sample - loss: 0.0207 - acc: 0.9933\n",
      "Epoch 192/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0523 - acc: 0.9875\n",
      "Epoch 193/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0245 - acc: 0.9900\n",
      "Epoch 194/500\n",
      "1203/1203 [==============================] - 1s 575us/sample - loss: 0.0469 - acc: 0.9867\n",
      "Epoch 195/500\n",
      "1203/1203 [==============================] - 1s 436us/sample - loss: 0.0193 - acc: 0.9950\n",
      "Epoch 196/500\n",
      "1203/1203 [==============================] - 1s 423us/sample - loss: 0.0397 - acc: 0.9917\n",
      "Epoch 197/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0564 - acc: 0.9850\n",
      "Epoch 198/500\n",
      "1203/1203 [==============================] - 1s 583us/sample - loss: 0.0575 - acc: 0.9850\n",
      "Epoch 199/500\n",
      "1203/1203 [==============================] - 1s 423us/sample - loss: 0.0204 - acc: 0.9958\n",
      "Epoch 200/500\n",
      "1203/1203 [==============================] - 1s 423us/sample - loss: 0.0315 - acc: 0.9884\n",
      "Epoch 201/500\n",
      "1203/1203 [==============================] - 1s 432us/sample - loss: 0.0496 - acc: 0.9867\n",
      "Epoch 202/500\n",
      "1203/1203 [==============================] - 1s 586us/sample - loss: 0.0337 - acc: 0.9909\n",
      "Epoch 203/500\n",
      "1203/1203 [==============================] - 1s 436us/sample - loss: 0.0254 - acc: 0.9909\n",
      "Epoch 204/500\n",
      "1203/1203 [==============================] - 1s 433us/sample - loss: 0.0274 - acc: 0.9909\n",
      "Epoch 205/500\n",
      "1203/1203 [==============================] - 1s 585us/sample - loss: 0.0363 - acc: 0.9909\n",
      "Epoch 206/500\n",
      "1203/1203 [==============================] - 1s 435us/sample - loss: 0.0363 - acc: 0.9884\n",
      "Epoch 207/500\n",
      "1203/1203 [==============================] - 1s 438us/sample - loss: 0.0614 - acc: 0.9867\n",
      "Epoch 208/500\n",
      "1203/1203 [==============================] - 1s 463us/sample - loss: 0.0585 - acc: 0.9859\n",
      "Epoch 209/500\n",
      "1203/1203 [==============================] - 1s 584us/sample - loss: 0.0439 - acc: 0.9900\n",
      "Epoch 210/500\n",
      "1203/1203 [==============================] - 1s 439us/sample - loss: 0.0220 - acc: 0.9925\n",
      "Epoch 211/500\n",
      "1203/1203 [==============================] - 1s 434us/sample - loss: 0.0672 - acc: 0.9842\n",
      "Epoch 212/500\n",
      "1203/1203 [==============================] - 1s 584us/sample - loss: 0.0351 - acc: 0.9859\n",
      "Epoch 213/500\n",
      "1203/1203 [==============================] - 1s 433us/sample - loss: 0.0209 - acc: 0.9909\n",
      "Epoch 214/500\n",
      "1203/1203 [==============================] - 1s 432us/sample - loss: 0.0720 - acc: 0.9817\n",
      "Epoch 215/500\n",
      "1203/1203 [==============================] - 1s 436us/sample - loss: 0.0375 - acc: 0.9867\n",
      "Epoch 216/500\n",
      "1203/1203 [==============================] - 1s 582us/sample - loss: 0.0309 - acc: 0.9900\n",
      "Epoch 217/500\n",
      "1203/1203 [==============================] - 1s 433us/sample - loss: 0.0328 - acc: 0.9942\n",
      "Epoch 218/500\n",
      "1203/1203 [==============================] - 1s 437us/sample - loss: 0.0277 - acc: 0.9917\n",
      "Epoch 219/500\n",
      "1203/1203 [==============================] - 1s 432us/sample - loss: 0.0161 - acc: 0.9925\n",
      "Epoch 220/500\n",
      "1203/1203 [==============================] - 1s 586us/sample - loss: 0.0435 - acc: 0.9884\n",
      "Epoch 221/500\n",
      "1203/1203 [==============================] - 1s 440us/sample - loss: 0.0394 - acc: 0.9875\n",
      "Epoch 222/500\n",
      "1203/1203 [==============================] - 1s 435us/sample - loss: 0.0361 - acc: 0.9917\n",
      "Epoch 223/500\n",
      "1203/1203 [==============================] - 1s 588us/sample - loss: 0.0204 - acc: 0.9925\n",
      "Epoch 224/500\n",
      "1203/1203 [==============================] - 1s 435us/sample - loss: 0.0218 - acc: 0.9925\n",
      "Epoch 225/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0267 - acc: 0.9909\n",
      "Epoch 226/500\n",
      "1203/1203 [==============================] - 1s 434us/sample - loss: 0.0202 - acc: 0.9933\n",
      "Epoch 227/500\n",
      "1203/1203 [==============================] - 1s 576us/sample - loss: 0.0179 - acc: 0.9967\n",
      "Epoch 228/500\n",
      "1203/1203 [==============================] - 1s 418us/sample - loss: 0.0505 - acc: 0.9875\n",
      "Epoch 229/500\n",
      "1203/1203 [==============================] - 1s 419us/sample - loss: 0.0433 - acc: 0.9909\n",
      "Epoch 230/500\n",
      "1203/1203 [==============================] - 1s 429us/sample - loss: 0.0393 - acc: 0.9875\n",
      "Epoch 231/500\n",
      "1203/1203 [==============================] - 1s 579us/sample - loss: 0.0417 - acc: 0.9917\n",
      "Epoch 232/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0483 - acc: 0.9867\n",
      "Epoch 233/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0237 - acc: 0.9933\n",
      "Epoch 234/500\n",
      "1203/1203 [==============================] - 1s 581us/sample - loss: 0.0323 - acc: 0.9892\n",
      "Epoch 235/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0311 - acc: 0.9925\n",
      "Epoch 236/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0148 - acc: 0.9967\n",
      "Epoch 237/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0256 - acc: 0.9942\n",
      "Epoch 238/500\n",
      "1203/1203 [==============================] - 1s 578us/sample - loss: 0.0319 - acc: 0.9884\n",
      "Epoch 239/500\n",
      "1203/1203 [==============================] - 1s 434us/sample - loss: 0.0207 - acc: 0.9950\n",
      "Epoch 240/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0308 - acc: 0.9917\n",
      "Epoch 241/500\n",
      "1203/1203 [==============================] - 1s 429us/sample - loss: 0.0272 - acc: 0.9884\n",
      "Epoch 242/500\n",
      "1203/1203 [==============================] - 1s 590us/sample - loss: 0.0177 - acc: 0.9958\n",
      "Epoch 243/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0361 - acc: 0.9859\n",
      "Epoch 244/500\n",
      "1203/1203 [==============================] - 1s 444us/sample - loss: 0.0329 - acc: 0.9925\n",
      "Epoch 245/500\n",
      "1203/1203 [==============================] - 1s 606us/sample - loss: 0.0180 - acc: 0.9942\n",
      "Epoch 246/500\n",
      "1203/1203 [==============================] - 1s 454us/sample - loss: 0.0170 - acc: 0.9950\n",
      "Epoch 247/500\n",
      "1203/1203 [==============================] - 1s 460us/sample - loss: 0.0379 - acc: 0.9884\n",
      "Epoch 248/500\n",
      "1203/1203 [==============================] - 1s 461us/sample - loss: 0.0151 - acc: 0.9942\n",
      "Epoch 249/500\n",
      "1203/1203 [==============================] - 1s 604us/sample - loss: 0.0693 - acc: 0.9842\n",
      "Epoch 250/500\n",
      "1203/1203 [==============================] - 1s 446us/sample - loss: 0.0274 - acc: 0.9900\n",
      "Epoch 251/500\n",
      "1203/1203 [==============================] - 1s 454us/sample - loss: 0.0566 - acc: 0.9850\n",
      "Epoch 252/500\n",
      "1203/1203 [==============================] - 1s 590us/sample - loss: 0.0290 - acc: 0.9942\n",
      "Epoch 253/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0358 - acc: 0.9933\n",
      "Epoch 254/500\n",
      "1203/1203 [==============================] - 1s 429us/sample - loss: 0.0275 - acc: 0.9942\n",
      "Epoch 255/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0341 - acc: 0.9909\n",
      "Epoch 256/500\n",
      "1203/1203 [==============================] - 1s 586us/sample - loss: 0.0343 - acc: 0.9900\n",
      "Epoch 257/500\n",
      "1203/1203 [==============================] - 1s 429us/sample - loss: 0.0803 - acc: 0.9834\n",
      "Epoch 258/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0393 - acc: 0.9867\n",
      "Epoch 259/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0255 - acc: 0.9917\n",
      "Epoch 260/500\n",
      "1203/1203 [==============================] - 1s 579us/sample - loss: 0.0281 - acc: 0.9917\n",
      "Epoch 261/500\n",
      "1203/1203 [==============================] - 1s 429us/sample - loss: 0.0391 - acc: 0.9875\n",
      "Epoch 262/500\n",
      "1203/1203 [==============================] - 1s 438us/sample - loss: 0.0298 - acc: 0.9892\n",
      "Epoch 263/500\n",
      "1203/1203 [==============================] - 1s 584us/sample - loss: 0.0391 - acc: 0.9884\n",
      "Epoch 264/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0307 - acc: 0.9909\n",
      "Epoch 265/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0487 - acc: 0.9850\n",
      "Epoch 266/500\n",
      "1203/1203 [==============================] - 1s 432us/sample - loss: 0.0211 - acc: 0.9950\n",
      "Epoch 267/500\n",
      "1203/1203 [==============================] - 1s 584us/sample - loss: 0.0198 - acc: 0.9942\n",
      "Epoch 268/500\n",
      "1203/1203 [==============================] - 1s 429us/sample - loss: 0.0445 - acc: 0.9875\n",
      "Epoch 269/500\n",
      "1203/1203 [==============================] - 1s 433us/sample - loss: 0.0496 - acc: 0.9900\n",
      "Epoch 270/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0263 - acc: 0.9942\n",
      "Epoch 271/500\n",
      "1203/1203 [==============================] - 1s 600us/sample - loss: 0.0308 - acc: 0.9933\n",
      "Epoch 272/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0427 - acc: 0.9892\n",
      "Epoch 273/500\n",
      "1203/1203 [==============================] - 1s 434us/sample - loss: 0.0166 - acc: 0.9942\n",
      "Epoch 274/500\n",
      "1203/1203 [==============================] - 1s 577us/sample - loss: 0.0246 - acc: 0.9925\n",
      "Epoch 275/500\n",
      "1203/1203 [==============================] - 1s 434us/sample - loss: 0.0401 - acc: 0.9892\n",
      "Epoch 276/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0219 - acc: 0.9933\n",
      "Epoch 277/500\n",
      "1203/1203 [==============================] - 1s 436us/sample - loss: 0.0643 - acc: 0.9825\n",
      "Epoch 278/500\n",
      "1203/1203 [==============================] - 1s 575us/sample - loss: 0.0220 - acc: 0.9917\n",
      "Epoch 279/500\n",
      "1203/1203 [==============================] - 1s 434us/sample - loss: 0.0461 - acc: 0.9875\n",
      "Epoch 280/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0620 - acc: 0.9875\n",
      "Epoch 281/500\n",
      "1203/1203 [==============================] - 1s 434us/sample - loss: 0.0453 - acc: 0.9825\n",
      "Epoch 282/500\n",
      "1203/1203 [==============================] - 1s 586us/sample - loss: 0.0350 - acc: 0.9875\n",
      "Epoch 283/500\n",
      "1203/1203 [==============================] - 1s 437us/sample - loss: 0.0361 - acc: 0.9909\n",
      "Epoch 284/500\n",
      "1203/1203 [==============================] - 1s 429us/sample - loss: 0.0541 - acc: 0.9875\n",
      "Epoch 285/500\n",
      "1203/1203 [==============================] - 1s 585us/sample - loss: 0.0613 - acc: 0.9875\n",
      "Epoch 286/500\n",
      "1203/1203 [==============================] - 1s 439us/sample - loss: 0.0325 - acc: 0.9900\n",
      "Epoch 287/500\n",
      "1203/1203 [==============================] - 1s 436us/sample - loss: 0.0435 - acc: 0.9925\n",
      "Epoch 288/500\n",
      "1203/1203 [==============================] - 1s 440us/sample - loss: 0.0245 - acc: 0.9942\n",
      "Epoch 289/500\n",
      "1203/1203 [==============================] - 1s 589us/sample - loss: 0.0315 - acc: 0.9917\n",
      "Epoch 290/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0404 - acc: 0.9917\n",
      "Epoch 291/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0291 - acc: 0.9900\n",
      "Epoch 292/500\n",
      "1203/1203 [==============================] - 1s 601us/sample - loss: 0.0437 - acc: 0.9859\n",
      "Epoch 293/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0371 - acc: 0.9917\n",
      "Epoch 294/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0477 - acc: 0.9884\n",
      "Epoch 295/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0408 - acc: 0.9892\n",
      "Epoch 296/500\n",
      "1203/1203 [==============================] - 1s 576us/sample - loss: 0.0267 - acc: 0.9958\n",
      "Epoch 297/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0287 - acc: 0.9892\n",
      "Epoch 298/500\n",
      "1203/1203 [==============================] - 1s 434us/sample - loss: 0.0412 - acc: 0.9875\n",
      "Epoch 299/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0388 - acc: 0.9884\n",
      "Epoch 300/500\n",
      "1203/1203 [==============================] - 1s 586us/sample - loss: 0.0313 - acc: 0.9900\n",
      "Epoch 301/500\n",
      "1203/1203 [==============================] - 1s 432us/sample - loss: 0.0318 - acc: 0.9892\n",
      "Epoch 302/500\n",
      "1203/1203 [==============================] - 1s 432us/sample - loss: 0.0235 - acc: 0.9950\n",
      "Epoch 303/500\n",
      "1203/1203 [==============================] - 1s 579us/sample - loss: 0.0192 - acc: 0.9950\n",
      "Epoch 304/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0338 - acc: 0.9909\n",
      "Epoch 305/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0214 - acc: 0.9950\n",
      "Epoch 306/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0154 - acc: 0.9942\n",
      "Epoch 307/500\n",
      "1203/1203 [==============================] - 1s 576us/sample - loss: 0.0461 - acc: 0.9942\n",
      "Epoch 308/500\n",
      "1203/1203 [==============================] - 1s 433us/sample - loss: 0.0164 - acc: 0.9942\n",
      "Epoch 309/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0208 - acc: 0.9933\n",
      "Epoch 310/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0389 - acc: 0.9892\n",
      "Epoch 311/500\n",
      "1203/1203 [==============================] - 1s 579us/sample - loss: 0.0353 - acc: 0.9892\n",
      "Epoch 312/500\n",
      "1203/1203 [==============================] - 1s 434us/sample - loss: 0.0287 - acc: 0.9933\n",
      "Epoch 313/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0368 - acc: 0.9884\n",
      "Epoch 314/500\n",
      "1203/1203 [==============================] - 1s 591us/sample - loss: 0.0205 - acc: 0.9950\n",
      "Epoch 315/500\n",
      "1203/1203 [==============================] - 1s 441us/sample - loss: 0.0238 - acc: 0.9933\n",
      "Epoch 316/500\n",
      "1203/1203 [==============================] - 1s 444us/sample - loss: 0.0234 - acc: 0.9917\n",
      "Epoch 317/500\n",
      "1203/1203 [==============================] - 1s 444us/sample - loss: 0.0242 - acc: 0.9925\n",
      "Epoch 318/500\n",
      "1203/1203 [==============================] - 1s 580us/sample - loss: 0.0388 - acc: 0.9933\n",
      "Epoch 319/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0476 - acc: 0.9867\n",
      "Epoch 320/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0523 - acc: 0.9917\n",
      "Epoch 321/500\n",
      "1203/1203 [==============================] - 1s 429us/sample - loss: 0.0661 - acc: 0.9800\n",
      "Epoch 322/500\n",
      "1203/1203 [==============================] - 1s 579us/sample - loss: 0.0299 - acc: 0.9933\n",
      "Epoch 323/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0393 - acc: 0.9900\n",
      "Epoch 324/500\n",
      "1203/1203 [==============================] - 1s 424us/sample - loss: 0.0557 - acc: 0.9850\n",
      "Epoch 325/500\n",
      "1203/1203 [==============================] - 1s 583us/sample - loss: 0.0432 - acc: 0.9884\n",
      "Epoch 326/500\n",
      "1203/1203 [==============================] - 1s 429us/sample - loss: 0.0458 - acc: 0.9900\n",
      "Epoch 327/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0738 - acc: 0.9867\n",
      "Epoch 328/500\n",
      "1203/1203 [==============================] - 1s 444us/sample - loss: 0.0603 - acc: 0.9842\n",
      "Epoch 329/500\n",
      "1203/1203 [==============================] - 1s 589us/sample - loss: 0.0362 - acc: 0.9900\n",
      "Epoch 330/500\n",
      "1203/1203 [==============================] - 1s 436us/sample - loss: 0.0218 - acc: 0.9917\n",
      "Epoch 331/500\n",
      "1203/1203 [==============================] - 1s 451us/sample - loss: 0.0191 - acc: 0.9925\n",
      "Epoch 332/500\n",
      "1203/1203 [==============================] - 1s 446us/sample - loss: 0.0431 - acc: 0.9867\n",
      "Epoch 333/500\n",
      "1203/1203 [==============================] - 1s 586us/sample - loss: 0.0294 - acc: 0.9892\n",
      "Epoch 334/500\n",
      "1203/1203 [==============================] - 1s 437us/sample - loss: 0.0516 - acc: 0.9892\n",
      "Epoch 335/500\n",
      "1203/1203 [==============================] - 1s 444us/sample - loss: 0.0483 - acc: 0.9884\n",
      "Epoch 336/500\n",
      "1203/1203 [==============================] - 1s 596us/sample - loss: 0.0430 - acc: 0.9909\n",
      "Epoch 337/500\n",
      "1203/1203 [==============================] - 1s 444us/sample - loss: 0.0470 - acc: 0.9867\n",
      "Epoch 338/500\n",
      "1203/1203 [==============================] - 1s 446us/sample - loss: 0.0350 - acc: 0.9917\n",
      "Epoch 339/500\n",
      "1203/1203 [==============================] - 1s 445us/sample - loss: 0.0303 - acc: 0.9892\n",
      "Epoch 340/500\n",
      "1203/1203 [==============================] - 1s 594us/sample - loss: 0.0347 - acc: 0.9900\n",
      "Epoch 341/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0556 - acc: 0.9892\n",
      "Epoch 342/500\n",
      "1203/1203 [==============================] - 1s 432us/sample - loss: 0.0559 - acc: 0.9867\n",
      "Epoch 343/500\n",
      "1203/1203 [==============================] - 1s 586us/sample - loss: 0.0250 - acc: 0.9925\n",
      "Epoch 344/500\n",
      "1203/1203 [==============================] - 1s 432us/sample - loss: 0.0355 - acc: 0.9900\n",
      "Epoch 345/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0190 - acc: 0.9917\n",
      "Epoch 346/500\n",
      "1203/1203 [==============================] - 1s 441us/sample - loss: 0.0392 - acc: 0.9909\n",
      "Epoch 347/500\n",
      "1203/1203 [==============================] - 1s 584us/sample - loss: 0.0145 - acc: 0.9958\n",
      "Epoch 348/500\n",
      "1203/1203 [==============================] - 1s 432us/sample - loss: 0.0419 - acc: 0.9917\n",
      "Epoch 349/500\n",
      "1203/1203 [==============================] - 1s 432us/sample - loss: 0.0205 - acc: 0.9892\n",
      "Epoch 350/500\n",
      "1203/1203 [==============================] - 1s 432us/sample - loss: 0.0243 - acc: 0.9925\n",
      "Epoch 351/500\n",
      "1203/1203 [==============================] - 1s 584us/sample - loss: 0.0435 - acc: 0.9875\n",
      "Epoch 352/500\n",
      "1203/1203 [==============================] - 1s 440us/sample - loss: 0.0448 - acc: 0.9867\n",
      "Epoch 353/500\n",
      "1203/1203 [==============================] - 1s 447us/sample - loss: 0.0450 - acc: 0.9900\n",
      "Epoch 354/500\n",
      "1203/1203 [==============================] - 1s 604us/sample - loss: 0.0218 - acc: 0.9942\n",
      "Epoch 355/500\n",
      "1203/1203 [==============================] - 1s 457us/sample - loss: 0.0378 - acc: 0.9900\n",
      "Epoch 356/500\n",
      "1203/1203 [==============================] - 1s 468us/sample - loss: 0.0235 - acc: 0.9950\n",
      "Epoch 357/500\n",
      "1203/1203 [==============================] - 1s 470us/sample - loss: 0.0237 - acc: 0.9917\n",
      "Epoch 358/500\n",
      "1203/1203 [==============================] - 1s 589us/sample - loss: 0.0225 - acc: 0.9950\n",
      "Epoch 359/500\n",
      "1203/1203 [==============================] - 1s 450us/sample - loss: 0.0165 - acc: 0.9950\n",
      "Epoch 360/500\n",
      "1203/1203 [==============================] - 1s 456us/sample - loss: 0.0419 - acc: 0.9884\n",
      "Epoch 361/500\n",
      "1203/1203 [==============================] - 1s 589us/sample - loss: 0.0297 - acc: 0.9942\n",
      "Epoch 362/500\n",
      "1203/1203 [==============================] - 1s 452us/sample - loss: 0.0403 - acc: 0.9917\n",
      "Epoch 363/500\n",
      "1203/1203 [==============================] - 1s 450us/sample - loss: 0.0140 - acc: 0.9958\n",
      "Epoch 364/500\n",
      "1203/1203 [==============================] - 1s 453us/sample - loss: 0.0708 - acc: 0.9875\n",
      "Epoch 365/500\n",
      "1203/1203 [==============================] - 1s 596us/sample - loss: 0.0313 - acc: 0.9892\n",
      "Epoch 366/500\n",
      "1203/1203 [==============================] - 1s 444us/sample - loss: 0.0351 - acc: 0.9900\n",
      "Epoch 367/500\n",
      "1203/1203 [==============================] - 1s 437us/sample - loss: 0.0540 - acc: 0.9892\n",
      "Epoch 368/500\n",
      "1203/1203 [==============================] - 1s 598us/sample - loss: 0.0108 - acc: 0.9967\n",
      "Epoch 369/500\n",
      "1203/1203 [==============================] - 1s 435us/sample - loss: 0.0389 - acc: 0.9867\n",
      "Epoch 370/500\n",
      "1203/1203 [==============================] - 1s 438us/sample - loss: 0.0204 - acc: 0.9917\n",
      "Epoch 371/500\n",
      "1203/1203 [==============================] - 1s 437us/sample - loss: 0.0132 - acc: 0.9958\n",
      "Epoch 372/500\n",
      "1203/1203 [==============================] - 1s 596us/sample - loss: 0.0367 - acc: 0.9884\n",
      "Epoch 373/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0235 - acc: 0.9909\n",
      "Epoch 374/500\n",
      "1203/1203 [==============================] - 1s 432us/sample - loss: 0.0377 - acc: 0.9884\n",
      "Epoch 375/500\n",
      "1203/1203 [==============================] - 1s 424us/sample - loss: 0.0347 - acc: 0.9942\n",
      "Epoch 376/500\n",
      "1203/1203 [==============================] - 1s 581us/sample - loss: 0.0299 - acc: 0.9892\n",
      "Epoch 377/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0360 - acc: 0.9909\n",
      "Epoch 378/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0394 - acc: 0.9875\n",
      "Epoch 379/500\n",
      "1203/1203 [==============================] - 1s 583us/sample - loss: 0.0242 - acc: 0.9925\n",
      "Epoch 380/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0324 - acc: 0.9917\n",
      "Epoch 381/500\n",
      "1203/1203 [==============================] - 1s 437us/sample - loss: 0.0137 - acc: 0.9925\n",
      "Epoch 382/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0230 - acc: 0.9942\n",
      "Epoch 383/500\n",
      "1203/1203 [==============================] - 1s 579us/sample - loss: 0.0393 - acc: 0.9925\n",
      "Epoch 384/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0506 - acc: 0.9867\n",
      "Epoch 385/500\n",
      "1203/1203 [==============================] - 1s 435us/sample - loss: 0.0371 - acc: 0.9917\n",
      "Epoch 386/500\n",
      "1203/1203 [==============================] - 1s 439us/sample - loss: 0.0337 - acc: 0.9900\n",
      "Epoch 387/500\n",
      "1203/1203 [==============================] - 1s 586us/sample - loss: 0.0114 - acc: 0.9967\n",
      "Epoch 388/500\n",
      "1203/1203 [==============================] - 1s 449us/sample - loss: 0.0404 - acc: 0.9884\n",
      "Epoch 389/500\n",
      "1203/1203 [==============================] - 1s 445us/sample - loss: 0.0493 - acc: 0.9884\n",
      "Epoch 390/500\n",
      "1203/1203 [==============================] - 1s 594us/sample - loss: 0.0091 - acc: 0.9967\n",
      "Epoch 391/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0425 - acc: 0.9900\n",
      "Epoch 392/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0322 - acc: 0.9909\n",
      "Epoch 393/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0249 - acc: 0.9933\n",
      "Epoch 394/500\n",
      "1203/1203 [==============================] - 1s 580us/sample - loss: 0.0166 - acc: 0.9933\n",
      "Epoch 395/500\n",
      "1203/1203 [==============================] - 1s 429us/sample - loss: 0.0309 - acc: 0.9917\n",
      "Epoch 396/500\n",
      "1203/1203 [==============================] - 1s 429us/sample - loss: 0.0332 - acc: 0.9933\n",
      "Epoch 397/500\n",
      "1203/1203 [==============================] - 1s 576us/sample - loss: 0.0325 - acc: 0.9917\n",
      "Epoch 398/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0331 - acc: 0.9900\n",
      "Epoch 399/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0459 - acc: 0.9909\n",
      "Epoch 400/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0339 - acc: 0.9867\n",
      "Epoch 401/500\n",
      "1203/1203 [==============================] - 1s 577us/sample - loss: 0.0260 - acc: 0.9900\n",
      "Epoch 402/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0372 - acc: 0.9909\n",
      "Epoch 403/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0284 - acc: 0.9909\n",
      "Epoch 404/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0451 - acc: 0.9859\n",
      "Epoch 405/500\n",
      "1203/1203 [==============================] - 1s 578us/sample - loss: 0.0191 - acc: 0.9942\n",
      "Epoch 406/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0420 - acc: 0.9909\n",
      "Epoch 407/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0379 - acc: 0.9933\n",
      "Epoch 408/500\n",
      "1203/1203 [==============================] - 1s 591us/sample - loss: 0.0407 - acc: 0.9900\n",
      "Epoch 409/500\n",
      "1203/1203 [==============================] - 1s 438us/sample - loss: 0.0250 - acc: 0.9909\n",
      "Epoch 410/500\n",
      "1203/1203 [==============================] - 1s 440us/sample - loss: 0.0223 - acc: 0.9933\n",
      "Epoch 411/500\n",
      "1203/1203 [==============================] - 1s 438us/sample - loss: 0.0187 - acc: 0.9933\n",
      "Epoch 412/500\n",
      "1203/1203 [==============================] - 1s 587us/sample - loss: 0.0320 - acc: 0.9942\n",
      "Epoch 413/500\n",
      "1203/1203 [==============================] - 1s 424us/sample - loss: 0.0593 - acc: 0.9859\n",
      "Epoch 414/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0282 - acc: 0.9933\n",
      "Epoch 415/500\n",
      "1203/1203 [==============================] - 1s 424us/sample - loss: 0.0416 - acc: 0.9917\n",
      "Epoch 416/500\n",
      "1203/1203 [==============================] - 1s 579us/sample - loss: 0.0335 - acc: 0.9925\n",
      "Epoch 417/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0282 - acc: 0.9900\n",
      "Epoch 418/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0382 - acc: 0.9892\n",
      "Epoch 419/500\n",
      "1203/1203 [==============================] - 1s 581us/sample - loss: 0.0413 - acc: 0.9884\n",
      "Epoch 420/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0549 - acc: 0.9859\n",
      "Epoch 421/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0438 - acc: 0.9875\n",
      "Epoch 422/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0530 - acc: 0.9875\n",
      "Epoch 423/500\n",
      "1203/1203 [==============================] - 1s 579us/sample - loss: 0.0204 - acc: 0.9950\n",
      "Epoch 424/500\n",
      "1203/1203 [==============================] - 1s 445us/sample - loss: 0.0265 - acc: 0.9958\n",
      "Epoch 425/500\n",
      "1203/1203 [==============================] - 1s 440us/sample - loss: 0.0213 - acc: 0.9942\n",
      "Epoch 426/500\n",
      "1203/1203 [==============================] - 1s 445us/sample - loss: 0.0668 - acc: 0.9884\n",
      "Epoch 427/500\n",
      "1203/1203 [==============================] - 1s 588us/sample - loss: 0.0390 - acc: 0.9900\n",
      "Epoch 428/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0270 - acc: 0.9909\n",
      "Epoch 429/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0406 - acc: 0.9884\n",
      "Epoch 430/500\n",
      "1203/1203 [==============================] - 1s 576us/sample - loss: 0.0303 - acc: 0.9942\n",
      "Epoch 431/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0573 - acc: 0.9917\n",
      "Epoch 432/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0307 - acc: 0.9925\n",
      "Epoch 433/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0308 - acc: 0.9900\n",
      "Epoch 434/500\n",
      "1203/1203 [==============================] - 1s 582us/sample - loss: 0.0368 - acc: 0.9884\n",
      "Epoch 435/500\n",
      "1203/1203 [==============================] - 1s 432us/sample - loss: 0.0314 - acc: 0.9942\n",
      "Epoch 436/500\n",
      "1203/1203 [==============================] - 1s 424us/sample - loss: 0.0518 - acc: 0.9875\n",
      "Epoch 437/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0177 - acc: 0.9950\n",
      "Epoch 438/500\n",
      "1203/1203 [==============================] - 1s 575us/sample - loss: 0.0257 - acc: 0.9917\n",
      "Epoch 439/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0133 - acc: 0.9950\n",
      "Epoch 440/500\n",
      "1203/1203 [==============================] - 1s 424us/sample - loss: 0.0331 - acc: 0.9909\n",
      "Epoch 441/500\n",
      "1203/1203 [==============================] - 1s 580us/sample - loss: 0.0216 - acc: 0.9933\n",
      "Epoch 442/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0147 - acc: 0.9950\n",
      "Epoch 443/500\n",
      "1203/1203 [==============================] - 1s 437us/sample - loss: 0.0452 - acc: 0.9917\n",
      "Epoch 444/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0216 - acc: 0.9950\n",
      "Epoch 445/500\n",
      "1203/1203 [==============================] - 1s 571us/sample - loss: 0.0202 - acc: 0.9925\n",
      "Epoch 446/500\n",
      "1203/1203 [==============================] - 1s 429us/sample - loss: 0.0364 - acc: 0.9892\n",
      "Epoch 447/500\n",
      "1203/1203 [==============================] - 1s 434us/sample - loss: 0.0163 - acc: 0.9950\n",
      "Epoch 448/500\n",
      "1203/1203 [==============================] - 1s 433us/sample - loss: 0.0204 - acc: 0.9925\n",
      "Epoch 449/500\n",
      "1203/1203 [==============================] - 1s 588us/sample - loss: 0.0344 - acc: 0.9909\n",
      "Epoch 450/500\n",
      "1203/1203 [==============================] - 1s 444us/sample - loss: 0.0470 - acc: 0.9909\n",
      "Epoch 451/500\n",
      "1203/1203 [==============================] - 1s 442us/sample - loss: 0.0221 - acc: 0.9942\n",
      "Epoch 452/500\n",
      "1203/1203 [==============================] - 1s 587us/sample - loss: 0.0504 - acc: 0.9884\n",
      "Epoch 453/500\n",
      "1203/1203 [==============================] - 1s 421us/sample - loss: 0.0853 - acc: 0.9867\n",
      "Epoch 454/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0448 - acc: 0.9850\n",
      "Epoch 455/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0693 - acc: 0.9834\n",
      "Epoch 456/500\n",
      "1203/1203 [==============================] - 1s 579us/sample - loss: 0.0328 - acc: 0.9909\n",
      "Epoch 457/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0277 - acc: 0.9917\n",
      "Epoch 458/500\n",
      "1203/1203 [==============================] - 1s 431us/sample - loss: 0.0439 - acc: 0.9884\n",
      "Epoch 459/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0260 - acc: 0.9917\n",
      "Epoch 460/500\n",
      "1203/1203 [==============================] - 1s 581us/sample - loss: 0.0393 - acc: 0.9909\n",
      "Epoch 461/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0375 - acc: 0.9933\n",
      "Epoch 462/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0192 - acc: 0.9933\n",
      "Epoch 463/500\n",
      "1203/1203 [==============================] - 1s 557us/sample - loss: 0.0417 - acc: 0.9925\n",
      "Epoch 464/500\n",
      "1203/1203 [==============================] - 1s 438us/sample - loss: 0.0578 - acc: 0.9875\n",
      "Epoch 465/500\n",
      "1203/1203 [==============================] - 1s 424us/sample - loss: 0.0396 - acc: 0.9850\n",
      "Epoch 466/500\n",
      "1203/1203 [==============================] - 1s 421us/sample - loss: 0.0334 - acc: 0.9909\n",
      "Epoch 467/500\n",
      "1203/1203 [==============================] - 1s 577us/sample - loss: 0.0196 - acc: 0.9909\n",
      "Epoch 468/500\n",
      "1203/1203 [==============================] - 1s 437us/sample - loss: 0.0063 - acc: 0.9992\n",
      "Epoch 469/500\n",
      "1203/1203 [==============================] - 1s 428us/sample - loss: 0.0517 - acc: 0.9867\n",
      "Epoch 470/500\n",
      "1203/1203 [==============================] - 1s 437us/sample - loss: 0.0221 - acc: 0.9942\n",
      "Epoch 471/500\n",
      "1203/1203 [==============================] - 1s 594us/sample - loss: 0.0369 - acc: 0.9909\n",
      "Epoch 472/500\n",
      "1203/1203 [==============================] - 1s 437us/sample - loss: 0.0242 - acc: 0.9917\n",
      "Epoch 473/500\n",
      "1203/1203 [==============================] - 1s 437us/sample - loss: 0.0825 - acc: 0.9825\n",
      "Epoch 474/500\n",
      "1203/1203 [==============================] - 1s 589us/sample - loss: 0.0227 - acc: 0.9942\n",
      "Epoch 475/500\n",
      "1203/1203 [==============================] - 1s 432us/sample - loss: 0.0789 - acc: 0.9850\n",
      "Epoch 476/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0737 - acc: 0.9850\n",
      "Epoch 477/500\n",
      "1203/1203 [==============================] - 1s 430us/sample - loss: 0.0440 - acc: 0.9859\n",
      "Epoch 478/500\n",
      "1203/1203 [==============================] - 1s 573us/sample - loss: 0.0233 - acc: 0.9950\n",
      "Epoch 479/500\n",
      "1203/1203 [==============================] - 1s 426us/sample - loss: 0.0459 - acc: 0.9892\n",
      "Epoch 480/500\n",
      "1203/1203 [==============================] - 1s 420us/sample - loss: 0.0392 - acc: 0.9933\n",
      "Epoch 481/500\n",
      "1203/1203 [==============================] - 1s 427us/sample - loss: 0.0256 - acc: 0.9942\n",
      "Epoch 482/500\n",
      "1203/1203 [==============================] - 1s 569us/sample - loss: 0.0383 - acc: 0.9917\n",
      "Epoch 483/500\n",
      "1203/1203 [==============================] - 1s 433us/sample - loss: 0.0398 - acc: 0.9917\n",
      "Epoch 484/500\n",
      "1203/1203 [==============================] - 1s 420us/sample - loss: 0.0147 - acc: 0.9950\n",
      "Epoch 485/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0193 - acc: 0.9933\n",
      "Epoch 486/500\n",
      "1203/1203 [==============================] - 1s 589us/sample - loss: 0.0137 - acc: 0.9958\n",
      "Epoch 487/500\n",
      "1203/1203 [==============================] - 1s 429us/sample - loss: 0.0193 - acc: 0.9933\n",
      "Epoch 488/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0538 - acc: 0.9859\n",
      "Epoch 489/500\n",
      "1203/1203 [==============================] - 1s 575us/sample - loss: 0.0454 - acc: 0.9933\n",
      "Epoch 490/500\n",
      "1203/1203 [==============================] - 1s 435us/sample - loss: 0.0262 - acc: 0.9917\n",
      "Epoch 491/500\n",
      "1203/1203 [==============================] - 1s 429us/sample - loss: 0.0083 - acc: 0.9975\n",
      "Epoch 492/500\n",
      "1203/1203 [==============================] - 1s 448us/sample - loss: 0.0151 - acc: 0.9958\n",
      "Epoch 493/500\n",
      "1203/1203 [==============================] - 1s 583us/sample - loss: 0.0246 - acc: 0.9933\n",
      "Epoch 494/500\n",
      "1203/1203 [==============================] - 1s 425us/sample - loss: 0.0252 - acc: 0.9958\n",
      "Epoch 495/500\n",
      "1203/1203 [==============================] - 1s 433us/sample - loss: 0.0172 - acc: 0.9942\n",
      "Epoch 496/500\n",
      "1203/1203 [==============================] - 1s 439us/sample - loss: 0.0131 - acc: 0.9958\n",
      "Epoch 497/500\n",
      "1203/1203 [==============================] - 1s 607us/sample - loss: 0.0302 - acc: 0.9892\n",
      "Epoch 498/500\n",
      "1203/1203 [==============================] - 1s 442us/sample - loss: 0.0673 - acc: 0.9800\n",
      "Epoch 499/500\n",
      "1203/1203 [==============================] - 1s 442us/sample - loss: 0.0469 - acc: 0.9917\n",
      "Epoch 500/500\n",
      "1203/1203 [==============================] - 1s 593us/sample - loss: 0.0349 - acc: 0.9909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcf68015710>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = cnn_mnist_model()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=20, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594/594 [==============================] - 0s 69us/sample - loss: 0.0845 - acc: 0.9865\n",
      "acc: 98.65319728851318\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_valid, y_valid)\n",
    "print(f'{model.metrics_names[1]}: {scores[1]*100}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_432_Backprop_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
