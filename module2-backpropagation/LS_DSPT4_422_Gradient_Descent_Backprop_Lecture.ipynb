{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 2, Module 2*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Backpropagation & Gradient Descent (Prepare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* <a href=\"#p1\">Part 1</a>: Explain the intutition behind backproprogation\n",
    "* <a href=\"#p2\">Part 2</a>: Implement gradient descent + backpropagation on a feedforward neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Yesterday\n",
    "\n",
    "Yesterday, we learned about some of the principal components of Neural Networks: Neurons, Weights, Activation Functions, and layers (input, output, & hidden). Today, we will reinforce our understanding of those components and introduce the mechanics of training a neural network. Feed-forward neural networks, such as multi-layer perceptrons (MLPs), are almost always trained using some variation of gradient descent where the gradient has been calculated by backpropagation.\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1600/1*_M4bZyuwaGby6KMiYVYXvg.jpeg\" width=\"400\"></center>\n",
    "\n",
    "- There are three kinds of layers: input, hidden, and output layers.\n",
    "- Each layer is made up of **n** individual neurons (aka activation units) which have a corresponding weight and bias.\n",
    "- Signal is passed from layer to layer through a network by:\n",
    " - Taking in inputs from the training data (or previous layer)\n",
    " - Multiplying each input by its corresponding weight (think arrow/connecting line)\n",
    " - Adding a bias to this weighted some of inputs and weights\n",
    " - Activating this weighted sum + bias by squishifying it with sigmoid or some other activation function. With a single perceptron with three inputs, calculating the output from the node is done like so:\n",
    "\\begin{align}\n",
    " y = sigmoid(\\sum(weight_{1}input_{1} + weight_{2}input_{2} + weight_{3}input_{3}) + bias)\n",
    "\\end{align}\n",
    " - this final activated value is the signal that gets passed onto the next layer of the network.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Network: *Formal Summary*\n",
    "\n",
    "1. Pick a network architecture\n",
    "   - No. of input units = No. of features\n",
    "   - No. of output units = Number of Classes (or expected targets)\n",
    "   - Select the number of hidden layers and number of neurons within each hidden layer\n",
    "2. Randomly initialize weights\n",
    "3. Implement forward propagation to get $h_{\\theta}(x^{(i)})$ for any $x^{(i)}$\n",
    "4. Implement code to compute a cost function $J(\\theta)$\n",
    "5. Implement backpropagation to compute partial derivatives $\\frac{\\delta}{\\delta\\theta_{jk}^{l}}{J(\\theta)}$\n",
    "6. Use gradient descent (or other advanced optimizer) with backpropagation to minimize $J(\\theta)$ as a function of parameters $\\theta\\$\n",
    "7. Repeat steps 2 - 5 until cost function is 'minimized' or some other stopping criteria is met. One pass over steps 2 - 5 is called an iteration or epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Calculating *\"cost\"*, *\"loss\"* or *\"error\"*\n",
    "\n",
    "We've talked about how in order to evaluate a network's performance, the data is \"fed forward\" until predictions are obtained and then the \"loss\" or \"error\" for a given observation is ascertained by looking at what the network predicted for that observation and comparing it to what it *should* have predicted. \n",
    "\n",
    "The error for a given observation is calculated by taking the square of the difference between the predicted value and the actual value. \n",
    "\n",
    "We can summarize the overall quality of a network's predictions by finding the average error across all observations. This gives us the \"Mean Squared Error.\" which hopefully is a fairly familiar model evaluation metric by now. Graphing the MSE over each epoch (training cycle) is a common practice with Neural Networks. This is what you're seeing in the top right corner of the Tensorflow Playground website as the number of \"epochs\" climbs higher and higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an \"Epoch\"?\n",
    "\n",
    "An \"Epoch\" is one cycle of passing our data forward through the network, measuring error given our specified cost function, and then -via gradient descent- updating weights within our network to hopefully improve the quality of our predictions on the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch vs Minibatch vs Stochastic Gradient Descent Epochs\n",
    "\n",
    "You may have heard these variations on the training process referenced in the 3Blue1Brown videos about backpropagation. \"Minibatch\" Gradient Descent means that instead of passing all of our data through the network for a given epoch (Batch GD), we just pass a randomized portion of our data through the network for each epoch. \n",
    "\n",
    "Stochastic Gradient Descent is when we make updates to our weights after forward propagating each individual training observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about Hyperparameters\n",
    "\n",
    "Neural Networks have many more hyperparameters than other machine learning algorithms which is part of what makes them a beast to train.\n",
    "\n",
    "1. You need more data to train them on. \n",
    "2. They're complex so they take longer to train. \n",
    "3. They have lots and lots of hyperparameters which we need to find the most optimal combination of, so we might end up training our model dozens or hundreds of times with different combinations of hyperparameters in order to try and squeeze out a few more tenths of a percent of accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aM4CK1IarId4",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Backpropagation (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aM4CK1IarId4",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Overview\n",
    "\n",
    "Backpropagation is short for [\"Backwards Propagation of errors\"](https://en.wikipedia.org/wiki/Backpropagation) and refers to a specific (rather calculus intensive) algorithm for how weights in a neural network are updated in reverse order at the end of each training epoch. Our purpose today is to demonstrate the backpropagation algorithm on a simple Feedforward Neural Network and in so doing help you get a grasp on the main process. If you want to understand all of the underlying calculus of how the gradients are calculated then you'll need to dive into it yourself, [3Blue1Brown's video is a great starting place](https://www.youtube.com/watch?v=tIeHLnjs5U8). I also highly recommend this Welch Labs series [Neural Networks Demystified](https://www.youtube.com/watch?v=bxe2T-V8XRs) if you want a rapid yet orderly walkthrough of the main intuitions and math behind the backpropagation algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Gradient?\n",
    "\n",
    "> In vector calculus, the gradient is a multi-variable generalization of the derivative. \n",
    "\n",
    "The gradients that we will deal with today will be vector representations of the derivative of the activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "In this section, we will again implement a multi-layer perceptron using numpy. We'll focus on using a __Feed Forward Neural Network__ to predict test scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dm2HPETcrgy6",
    "toc-hr-collapsed": true
   },
   "source": [
    "![231 Neural Network](https://cdn-images-1.medium.com/max/1600/1*IjY3wFF24sK9UhiOlf36Bw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4d4tzpwO6B47"
   },
   "source": [
    "### Generate some Fake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ERyVgeO_IWyV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(812)\n",
    "\n",
    "# hours studying, hours sleep\n",
    "X = np.array(([2,9],\n",
    "              [1,5],\n",
    "              [3,6]), dtype=float)\n",
    "\n",
    "# Exam Scores\n",
    "y = np.array(([90],\n",
    "              [72],\n",
    "              [80]), dtype=float)\n",
    "\n",
    "# y = 2*hours_studying + 4*hours_sleeping + 50\n",
    "# ^ what the network is trying to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDeUBW6k4Ri4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studying, Sleeping \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Test Score \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n"
     ]
    }
   ],
   "source": [
    "# Normalizing Data on feature \n",
    "# Neural Network would probably do this on its own, but it will help us converge on a solution faster\n",
    "X = X / np.amax(X, axis=0)\n",
    "y = y / 100\n",
    "\n",
    "print(\"Studying, Sleeping \\n\", X)\n",
    "print(\"Test Score \\n\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgTf6vTS69Sw"
   },
   "source": [
    "### Neural Network Architecture\n",
    "Lets create a Neural_Network class to contain this functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUI8VSR5zyBv"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork: \n",
    "    \n",
    "    def __init__(self):\n",
    "        # Setup Arch\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3 \n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        # Initialize Weights\n",
    "        # 2x3\n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)\n",
    "        \n",
    "        # 3x1\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gbyT_FJ88IlK"
   },
   "source": [
    "### Randomly Initialize Weights\n",
    "How many random weights do we need to initialize? \"Fully-connected Layers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IreIDe6P8H0H"
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 weights: \n",
      " [[ 2.48783189  0.11697987 -1.97118428]\n",
      " [-0.48325593 -1.50361209  0.57515126]]\n",
      "Layer 2 weights: \n",
      " [[-0.20672583]\n",
      " [ 0.41271104]\n",
      " [-0.57757999]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer 1 weights: \\n\", nn.weights1)\n",
    "print(\"Layer 2 weights: \\n\", nn.weights2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbxDhyjQ-RwS"
   },
   "source": [
    "### Implement Feedforward Functionality\n",
    "\n",
    "After this step our neural network should be able to generate an output even though it has not been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gGivpEk-VdP"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork: \n",
    "    \n",
    "    def __init__(self):\n",
    "        # Setup Arch\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3 \n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        # Initialize Weights\n",
    "        # 2x3\n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)\n",
    "        \n",
    "        # 3x1\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weight Sum\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activation\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weighted Sum 2\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final Output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66666667, 1.        ],\n",
       "       [0.33333333, 0.55555556],\n",
       "       [1.        , 0.66666667]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a1pxdfmDAaJg"
   },
   "source": [
    "### Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intput [0.66666667 1.        ]\n",
      "output [0.25814933]\n"
     ]
    }
   ],
   "source": [
    "# Try to make a prediction with our updated 'net\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "output = nn.feed_forward(X[0])\n",
    "print(\"intput\", X[0])\n",
    "print(\"output\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3V61yNmAB2T5"
   },
   "source": [
    "### Calculate Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64185067]\n"
     ]
    }
   ],
   "source": [
    "error = y[0] - output\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25814933]\n",
      " [0.33067192]\n",
      " [0.22642076]]\n",
      "[[0.64185067]\n",
      " [0.38932808]\n",
      " [0.57357924]]\n"
     ]
    }
   ],
   "source": [
    "output_all = nn.feed_forward(X)\n",
    "error_all = y - output_all\n",
    "print(output_all)\n",
    "print(error_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26wgCLU0TLvy"
   },
   "source": [
    "Why is my error so big?\n",
    "\n",
    "My error is so big because my prediction is low.\n",
    "\n",
    "Why are my prediction low?\n",
    "\n",
    "Because either:\n",
    "\n",
    "  1) Second layer **weights** are low\n",
    "  \n",
    "  (or)\n",
    "  \n",
    "  2) Activations coming from the first layer are low\n",
    "  \n",
    "How are activations from the first layer determined? \n",
    "\n",
    "  1) By inputs - fixed\n",
    "  \n",
    "  2) by **weights** - variable\n",
    "  \n",
    "The only thing that I have control over throughout this process in order to increase the value of my final predictions is to either increase weights in layer 2 or increase weights in layer 1. \n",
    "\n",
    "Imagine that you could only change your weights by a fixed amount. Say you have .3 and you have to split that up and disperse it over your weights so as to increase your predictions as much as possible. (This isn't actually what happens, but it will help us identify which weights we would benefit the most from moving.)\n",
    "\n",
    "I need to increase weights of my model somewhere, I'll get the biggest bang for my buck if I increase weights in places where I'm already seeing high activation values -because they end up getting multiplied together before being passed to the sigmoid function. \n",
    "\n",
    "> \"Neurons that fire together, wire together\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_eyzItYIxgm"
   },
   "source": [
    "### Implement Backpropagation \n",
    "\n",
    "> *Assigning blame for bad predictions and delivering justice - repeatedly and a little bit at a time*\n",
    "\n",
    "What in our model could be causing our predictions to suck so bad? \n",
    "\n",
    "Well, we know that our inputs (X) and outputs (y) are correct, if they weren't then we would have bigger problems than understanding backpropagation.\n",
    "\n",
    "We also know that our activation function (sigmoid) is working correctly. It can't be blamed because it just does whatever we tell it to and transforms the data in a known way.\n",
    "\n",
    "So what are the potential culprits for these terrible predictions? The **weights** of our model. Here's the problem though. I have weights that exist in both layers of my model. How do I know if the weights in the first layer are to blame, or the second layer, or both? \n",
    "\n",
    "Lets investigate. And see if we can just eyeball what should be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights1\n",
      " [[-1.75351135  1.23279898  0.24464757]\n",
      " [-0.06568225  0.30190098  0.79723428]] \n",
      "---------\n",
      "hidden_sum\n",
      " [[-1.23468981  1.12376697  0.96033266]\n",
      " [-0.62099392  0.57865576  0.52445712]\n",
      " [-1.79729952  1.4340663   0.77613709]] \n",
      "---------\n",
      "activated_hidden\n",
      " [[0.22536165 0.75468678 0.7231884 ]\n",
      " [0.34955543 0.64075804 0.6281894 ]\n",
      " [0.14218011 0.8075341  0.68484697]] \n",
      "---------\n",
      "weights2\n",
      " [[ 1.23073545]\n",
      " [-1.52187331]\n",
      " [-0.25502715]] \n",
      "---------\n",
      "activated_output\n",
      " [[0.25814933]\n",
      " [0.33067192]\n",
      " [0.22642076]] \n",
      "---------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes = ['weights1', 'hidden_sum', 'activated_hidden', 'weights2', 'activated_output']\n",
    "[print(i+'\\n', getattr(nn,i), '\\n'+'---'*3) for i in attributes if i[:2]!= '__'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16Ujj6vNYQyX"
   },
   "source": [
    "### Backpropagation (Simple Overview)\n",
    "\n",
    "Our model has 9 total weights (6 in the first layer, 3 in the last layer) that could be off.\n",
    "\n",
    "1) Calculate Error for a given each observation\n",
    "\n",
    "2) Does the error indicate that I'm overestimating or underestimating in my prediction?\n",
    "\n",
    "3) Look at final layer weights to get an idea for which weights are helping pass desirable signals and which are stifling desirable signals\n",
    "\n",
    "4) Also go to the previous layer and see what can be done to boost activations that are associated with helpful weights, and limit activations that are associated with unhelpful weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16Ujj6vNYQyX",
    "toc-hr-collapsed": true
   },
   "source": [
    "### Update Weights Based on Gradient\n",
    "\n",
    "Repeat steps 1-4 for every observation in a given batch, and then given the network's cost function, calculate its gradient using calculus and update weights associated with the (negative) gradient of the cost function. \n",
    "\n",
    "Remember that we have 9 weights in our network therefore the gradient that comes from our gradient descent calculation will be the vector that takes us in the most downward direction along some function in 9-dimensional hyperspace.\n",
    "\n",
    "\\begin{align}\n",
    "C(w1, w2, w3, w4, w5, w6, w7, w8, w9)\n",
    "\\end{align}\n",
    "\n",
    "You should also know that with neural networks it is common to have gradients that are not convex (like what we saw when we applied gradient descent to linear regression). Due to the high complexity of these models and their nonlinearity, it is common for gradient descent to get stuck in a local minimum, but there are ways to combat this:\n",
    "\n",
    "1) Stochastic Gradient Descent\n",
    "\n",
    "2) More advanced Gradient-Descent-based \"Optimizers\" - See Stretch Goals on assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want activations that correspond to negative weights to be lower\n",
    "# and activations that correspond to positive weights to be higher\n",
    "\n",
    "class NeuralNetwork: \n",
    "    \n",
    "    def __init__(self):\n",
    "        # Setup Arch\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes =3 \n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        # Initialize Weights\n",
    "        # 2x3\n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)\n",
    "        \n",
    "        # 3x1\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        sx = self.sigmoid(s)\n",
    "        return sx * (1-sx)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weight Sum\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activation\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weighted Sum 2\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final Output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "    \n",
    "    def backward(self, X,y,o):\n",
    "        \"\"\"\n",
    "        Back prop thru the network\n",
    "        \"\"\"\n",
    "        \n",
    "        self.o_error = y - o\n",
    "        \n",
    "        # Apply derivative of sigmoid to error\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(self.output_sum)\n",
    "        \n",
    "        # z2 error: how much were our output layer weights off\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        \n",
    "        # z2 delta: how much were the weights off?\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.hidden_sum)\n",
    "\n",
    "        self.weights1 += X.T.dot(self.z2_delta) #Adjust first set (input => hidden) weights\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta) #adjust second set (hidden => output) weights\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X,y,o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Let's look at the shape of the Gradient Componets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from reference import NeuralNetwork\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "nn.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our Error Associated with Each Observation \n",
    "aka how wrong were we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.60746115],\n",
       "       [0.46613403],\n",
       "       [0.53667427]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.o_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1st Gradient \n",
    "Simple interpretation - how much more sigmoid activation would have pushed us towards the right answer?\n",
    "\n",
    "`self.o_delta = self.o_error * self.sigmoidPrime(self.output_sum)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.88308525],\n",
       "       [-1.07809882],\n",
       "       [-1.02875405]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.output_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29253885],\n",
       "       [0.25386597],\n",
       "       [0.26332573]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.sigmoid(nn.output_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20695987],\n",
       "       [0.18941804],\n",
       "       [0.19398529]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.sigmoidPrime(nn.output_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12572008],\n",
       "       [0.08829419],\n",
       "       [0.10410691]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.o_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2nd Error\n",
    "Justice hasn't been served yet - tho. We still have neurons to blame. Let's go back another layer. \n",
    "\n",
    "`self.z2_error = self.o_delta.dot(self.weights2.T)`\n",
    "\n",
    "__Discussion:__ Why is this shape different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.22405998, -0.1801229 ,  0.07127711],\n",
       "       [-0.15735907, -0.12650171,  0.05005847],\n",
       "       [-0.18554071, -0.14915707,  0.0590235 ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.z2_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2nd Gradient\n",
    "For each observation, how much more sigmoid activation from this layer would have pushed us towards the right answer?\n",
    "\n",
    "`self.z2_delta = self.z2_error * self.sigmoidPrime(self.hidden_sum)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05147205, -0.04168453,  0.01780458],\n",
       "       [-0.03808351, -0.03115659,  0.01251457],\n",
       "       [-0.04613442, -0.02419762,  0.01401073]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.z2_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.shape == nn.weights1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Descent\n",
    "\n",
    "*Discussion:* Input to Hidden Weight Update\n",
    "- We multiply the gradient by the inputs. Why?\n",
    "- Why do we need to transpose the inputs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66666667, 0.33333333, 1.        ],\n",
       "       [1.        , 0.55555556, 0.66666667]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09314363, -0.06237284,  0.03005197],\n",
       "       [-0.10338584, -0.07512549,  0.0340976 ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.dot(nn.z2_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discussion:* Hidden to Output Weight Update\n",
    "- Why is output the shape 3x1? \n",
    "- We multiply the gradient by the inputs. Why?\n",
    "- Why do we need to transpose the inputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13709717],\n",
       "       [0.10570706],\n",
       "       [0.14546975]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.activated_hidden.T.dot(nn.o_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Network (fo real this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.37771334]\n",
      " [0.41477855]\n",
      " [0.40348484]]\n",
      "Loss: \n",
      " 0.17438925217089843\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.46600841]\n",
      " [0.49415696]\n",
      " [0.48831231]]\n",
      "Loss: \n",
      " 0.11216766504960145\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.54158907]\n",
      " [0.56049596]\n",
      " [0.55974774]]\n",
      "Loss: \n",
      " 0.07054036236129774\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.59995339]\n",
      " [0.61107483]\n",
      " [0.61445312]]\n",
      "Loss: \n",
      " 0.04544010213424358\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.6433419 ]\n",
      " [0.6484747 ]\n",
      " [0.65498557]]\n",
      "Loss: \n",
      " 0.030672811712652893\n",
      "+---------EPOCH 1000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.82268242]\n",
      " [0.78629293]\n",
      " [0.81241377]]\n",
      "Loss: \n",
      " 0.00350895417812754\n",
      "+---------EPOCH 2000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.82439478]\n",
      " [0.78394768]\n",
      " [0.81150952]]\n",
      "Loss: \n",
      " 0.003312641348226292\n",
      "+---------EPOCH 3000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.83101491]\n",
      " [0.77214288]\n",
      " [0.81268188]]\n",
      "Loss: \n",
      " 0.0025462174943048596\n",
      "+---------EPOCH 4000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.84599519]\n",
      " [0.75320748]\n",
      " [0.81206729]]\n",
      "Loss: \n",
      " 0.0013882919781529242\n",
      "+---------EPOCH 5000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.85837718]\n",
      " [0.74410449]\n",
      " [0.8074435 ]]\n",
      "Loss: \n",
      " 0.000789630452383025\n",
      "+---------EPOCH 6000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.86762851]\n",
      " [0.73843862]\n",
      " [0.80423041]]\n",
      "Loss: \n",
      " 0.00046859751461036215\n",
      "+---------EPOCH 7000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.87444825]\n",
      " [0.73416474]\n",
      " [0.80269207]]\n",
      "Loss: \n",
      " 0.0002869262837669205\n",
      "+---------EPOCH 8000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.87959742]\n",
      " [0.7309803 ]\n",
      " [0.80187561]]\n",
      "Loss: \n",
      " 0.0001801167224290982\n",
      "+---------EPOCH 9000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.88356771]\n",
      " [0.72861137]\n",
      " [0.80137658]]\n",
      "Loss: \n",
      " 0.0001153569336709512\n",
      "+---------EPOCH 10000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.88667507]\n",
      " [0.72682676]\n",
      " [0.80104389]]\n",
      "Loss: \n",
      " 7.508276293462717e-05\n"
     ]
    }
   ],
   "source": [
    "# Train my 'net\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "# Number of Epochs / Iterations\n",
    "for i in range(10000):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "In the module project, you will implement backpropagation inside a multi-layer perceptron (aka a feedforward neural network). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The What - Stochastic Gradient Descent calculates an approximation of the gradient over the entire dataset by reviewing the predictions of a random sample. \n",
    "\n",
    "The Why - *Speed*. Calculating the gradient over the entire dataset is extremely expensive computationally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZF7UE-KluPsX"
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "A true Stochastic GD-based implementation from [Welch Labs](https://www.youtube.com/watch?v=bxe2T-V8XRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uA9LaTgKr6rP"
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #Make empty list to store costs:\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_kHb6Se1u9y"
   },
   "outputs": [],
   "source": [
    "NN = Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYYVhFf4rn3q"
   },
   "outputs": [],
   "source": [
    "T = trainer(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "L-gYdVfgrysE",
    "outputId": "ae371bf9-692c-49b4-b165-8562dab9c06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 67\n",
      "         Function evaluations: 83\n",
      "         Gradient evaluations: 83\n"
     ]
    }
   ],
   "source": [
    "T.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "Jyv_L8Z2sKOA",
    "outputId": "08725651-6d21-401b-85c0-3487370b8bc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: \n",
      "[[0.89997748]\n",
      " [0.71999704]\n",
      " [0.80001258]]\n",
      "Loss: \n",
      "2.2470751910666767e-10\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Output: \\n\" + str(NN.forward(X))) \n",
    "print(\"Loss: \\n\" + str(np.mean(np.square(y - NN.forward(X))))) # mean sum squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "Gtf9WI9FtGPk",
    "outputId": "d062b2a3-5a92-403e-8ce0-c070aa79907b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de3xdZZ3v8c9v7yQ7aZImbZJeSEJTaWhNkbYQK8hFLgJF0YriUBwUlRlmRkAZdByYOTPO4eiMjheQEZzhAIoMWjioQwUGFMoIKLQN95bSNvRC0kKTNk3TJs1lZ//OH3ulhJDm0nZn7d18369XXl3rWc9a+W1em367nmddzN0REREZqUjYBYiISGZRcIiIyKgoOEREZFQUHCIiMioKDhERGZWssAsYC6WlpV5VVRV2GSIiGeW5557b4e5lA9vHRXBUVVVRV1cXdhkiIhnFzLYM1q6hKhERGZWUBoeZLTKzdWZWb2bXDbI9Zmb3BttXmFlV0F5iZk+Y2V4z+1G//hPM7CEze83M1pjZt1NZv4iIvFvKgsPMosAtwPlADXCJmdUM6HY5sMvdZwE3At8J2juBfwC+Nsihv+fuc4AFwClmdn4q6hcRkcGl8oxjIVDv7hvdvRtYCiwe0GcxcFewfD9wtpmZu7e7+9MkA2Q/d+9w9yeC5W7geaAihZ9BREQGSGVwlAMN/dYbg7ZB+7h7HNgNlIzk4GZWDHwMePwA268wszozq2tubh5l6SIiciAZOTluZlnAL4Cb3X3jYH3c/TZ3r3X32rKyd11NJiIiBymVwbEVqOy3XhG0DdonCIMiYOcIjn0bsMHdbzoMdYqIyCikMjhWAdVmNtPMcoAlwLIBfZYBlwXLFwHLfZjnvJvZN0kGzDWHud53ueuPm/nNS9tS/WtERDJKym4AdPe4mV0FPApEgTvdfY2Z3QDUufsy4A7gbjOrB1pIhgsAZrYZmAjkmNkngHOBNuDvgdeA580M4EfufnsqPsO9qxqYVpTLx+YdlYrDi4hkpJTeOe7uDwMPD2j7x37LncCnD7Bv1QEOa4ervuFUTMpj8872sfp1IiIZISMnx8dKxaQJNO7ah96SKCLyNgXHEMon5dHR3UtrR0/YpYiIpA0FxxAqJuUB0LhrX8iViIikDwXHEN4Ojo6QKxERSR8KjiFUFE8AdMYhItKfgmMIE/OyKIxlsbVVwSEi0kfBMQQzo3xSnoaqRET6UXAMo++SXBERSVJwDKNiUh5bdS+HiMh+Co5hVEzKY09XnLZ98bBLERFJCwqOYfRdktugeQ4REUDBMaxyXZIrIvIOCo5h9J1x6JJcEZEkBccwiidkk58T1SW5IiIBBccwzEyX5IqI9KPgGIHy4JJcERFRcIxIhe4eFxHZT8ExAhWT8mjrjLN7n97LISKi4BiBvktyNVwlIqLgGBFdkisi8jYFxwjohU4iIm9TcIzA5Pwc8rKjuiRXRAQFx4j0vZdDcxwiIgqOEauYlEdjq4aqREQUHCOUvJdDZxwiIikNDjNbZGbrzKzezK4bZHvMzO4Ntq8ws6qgvcTMnjCzvWb2owH7nGhmrwT73GxmlsrP0Ke8eAKtHT3s7dJ7OURkfEtZcJhZFLgFOB+oAS4xs5oB3S4Hdrn7LOBG4DtBeyfwD8DXBjn0j4E/B6qDn0WHv/p3239Jrs46RGScS+UZx0Kg3t03uns3sBRYPKDPYuCuYPl+4GwzM3dvd/enSQbIfmY2HZjo7s968l2uPwM+kcLPsJ8uyRURSUplcJQDDf3WG4O2Qfu4exzYDZQMc8zGYY4JgJldYWZ1ZlbX3Nw8ytLfrWKSXugkIgJH8OS4u9/m7rXuXltWVnbIxystyCGWFdHd4yIy7qUyOLYClf3WK4K2QfuYWRZQBOwc5pgVwxwzJfru5dBQlYiMd6kMjlVAtZnNNLMcYAmwbECfZcBlwfJFwPJg7mJQ7v4m0GZmJwVXU30OeODwlz44vdBJRASyUnVgd4+b2VXAo0AUuNPd15jZDUCduy8D7gDuNrN6oIVkuABgZpuBiUCOmX0CONfdXwW+BPwUyAP+O/gZE+XFeazeunusfp2ISFpKWXAAuPvDwMMD2v6x33In8OkD7Ft1gPY64LjDV+XIVUzKo6W9m47uOBNyUvqfTkQkbR2xk+Op0HdJ7qrNu+iK94ZcjYhIOPTP5lGYNaUAgMvuXEnE4KjiPKpK8ikvzmNyQQ4l+TlMmpDD5PwcKiblUTl5ArnZ0ZCrFhE5vBQcozD3qCIevPpU1m/fw+adHWzZ2c7mnR0sX9fErvZu4ol3zuubwVFFecwszWfWlAJOek8JJx9TQlFedkifQETk0NkQFzEdMWpra72uri6lv8PdaeuMs6u9m53tXTS07GPTjnY272xn8452NjTtpaO7l2jEmFdRxKnVZXz0fdOZPa0wpXWJiBwsM3vO3Wvf1a7gGBvd8QQvNrTy1IZmntqwg5cbW0k4fOR90/jK2ccqQEQk7Sg4Qg6OgVrau/nJHzbxkz9spr07zkffN51rPlzNrCkKEBFJDwcKDl1VFZLJ+Tl89dzZPPX1M/mrDx3D8teaOOfGJ7nz6U1hlyYiMiQFR8gm5efw9UVzeOrrZ3JuzVRuePBVbnpsPePhTFBEMpOCI02UFMS45TMn8KkTKrjpsQ1886G1Cg8RSUu6HDeNZEUjfPei4ynMzeKOpzextzPOP3/yfUQjY/KSQxGREVFwpJlIxPjGx2qYmJvFzcvr6Yz3ctPF8xmjN+SKiAxLQ1VpyMy49tzZXPPhah54cRu/e3V72CWJiOyn4EhjV545i1lTCvjWw2v1bCwRSRsKjjSWHY3wvz76Xrbs7OCuP24OuxwREUDBkfbOmD2FM2eX8W+P17Njb1fY5YiIKDgywd9/tIZ9Pb18/7frwy5FRETBkQlmTSngsyfP4N5Vb/DqtrawyxGRcU7BkSGuOftYJuZl838efFU3BopIqBQcGaJoQjbXnnMsz2zcqctzRSRUCo4M8pmFR1OUl80T65rCLkVExjEFRwbJikaYXpRL857usEsRkXFMwZFhygpjNOuyXBEJkYIjw5QVxNixR8EhIuFRcGSYvjMOXVklImFJaXCY2SIzW2dm9WZ23SDbY2Z2b7B9hZlV9dt2fdC+zszO69f+12a2xsxWm9kvzCw3lZ8h3ZQWxOiOJ2jrjIddioiMUykLDjOLArcA5wM1wCVmVjOg2+XALnefBdwIfCfYtwZYAswFFgG3mlnUzMqBLwO17n4cEA36jRtlhTEAPX5EREKTyjOOhUC9u290925gKbB4QJ/FwF3B8v3A2ZZ88cRiYKm7d7n7JqA+OB4k3yGSZ2ZZwARgWwo/Q9rpC45mzXOISEhSGRzlQEO/9cagbdA+7h4HdgMlB9rX3bcC3wPeAN4Edrv7b1NSfZpScIhI2DJqctzMJpE8G5kJHAXkm9mlB+h7hZnVmVldc3PzWJaZUqUFCg4RCVcqg2MrUNlvvSJoG7RPMPRUBOwcYt8PA5vcvdnde4BfAR8c7Je7+23uXuvutWVlZYfh46SH4rxssiKmOQ4RCU0qg2MVUG1mM80sh+Qk9rIBfZYBlwXLFwHLPXmd6TJgSXDV1UygGlhJcojqJDObEMyFnA2sTeFnSDuRiFFaENMZh4iEJitVB3b3uJldBTxK8uqnO919jZndANS5+zLgDuBuM6sHWgiukAr63Qe8CsSBK929F1hhZvcDzwftLwC3peozpKvSwhzdPS4iobHxcCNZbW2t19XVhV3GYfOFn6ykeW8XD159WtiliMgRzMyec/fage0ZNTkuSWWFGqoSkfAoODJQWWGMHXu7SSSO/LNFEUk/Co4MVFoQozfh7OrQ49VFZOwpODLQ248dUXCIyNhTcGSgMt0EKCIhUnBkoP2PHdnbGXIlIjIeKTgyUGnfUJVeISsiIVBwZKDCWBaxrIhuAhSRUCg4MpCZ6V4OEQmNgiNDKThEJCwKjgxVWhDTE3JFJBQKjgylMw4RCYuCI0OVFcRo6egm3psIuxQRGWcUHBmqtDCGO7S065JcERlbCo4M1Xf3eJOGq0RkjCk4MtTbd48rOERkbCk4MtSUQj2vSkTCoeDIUKUFfU/IVXCIyNhScGSovJwoBbEsnXGIyJhTcGQw3cshImFQcGSwsgIFh4iMPQVHBistzNEch4iMuREFh5ndPZI2GVs64xCRMIz0jGNu/xUziwInHv5yZDTKCmO0dcbp7OkNuxQRGUeGDA4zu97M9gDHm1lb8LMHaAIeGJMK5YD6LsndqceOiMgYGjI43P1f3L0Q+K67Twx+Ct29xN2vH+7gZrbIzNaZWb2ZXTfI9piZ3RtsX2FmVf22XR+0rzOz8/q1F5vZ/Wb2mpmtNbOTR/WJjyBluglQREIw0qGqB80sH8DMLjWzH5jZjKF2CIazbgHOB2qAS8ysZkC3y4Fd7j4LuBH4TrBvDbCE5BDZIuDW4HgAPwQecfc5wDxg7Qg/wxFHwSEiYRhpcPwY6DCzecBXgdeBnw2zz0Kg3t03uns3sBRYPKDPYuCuYPl+4Gwzs6B9qbt3ufsmoB5YaGZFwOnAHQDu3u3urSP8DEccBYeIhGGkwRF3dyf5F/qP3P0WoHCYfcqBhn7rjUHboH3cPQ7sBkqG2Hcm0Az8xMxeMLPb+86EBjKzK8yszszqmpubR/IZM05Jvh47IiJjb6TBscfMrgc+CzxkZhEgO3VlHVAWcALwY3dfALQD75o7AXD329y91t1ry8rKxrLGMZOTFaF4QrbOOERkTI00OC4GuoAvuvtbQAXw3WH22QpU9luvCNoG7WNmWUARsHOIfRuBRndfEbTfTzJIxi3dyyEiY21EwRGExT1AkZldAHS6+3BzHKuAajObaWY5JCe7lw3oswy4LFi+CFgeDIktA5YEV13NBKqBlUEdDWY2O9jnbODVkXyGI1VZYUxDVSIypkZ65/ifACuBTwN/Aqwws4uG2ieYs7gKeJTklU/3ufsaM7vBzD4edLsDKDGzeuBagmEnd18D3EcyFB4BrnT3vrvcrgbuMbOXgfnAP4/0wx6JSgtiepmTiIyprBH2+3vg/e7eBGBmZcBjJIeKDsjdHwYeHtD2j/2WO0mG0WD7fgv41iDtLwK1I6z7iKcn5IrIWBvpHEekLzQCO0exr6RQWWGMju5e2rviYZciIuPESP/yf8TMHjWzz5vZ54GHGHAmIeEoCx478uT6I/OSYxFJP8M9q2qWmZ3i7n8D/AdwfPDzDHDbGNQnwzhzzhSqpxTwV/c8zzceWK0HHopIyg13xnET0Abg7r9y92vd/Vrg18E2Cdnk/Bx+c/WpfOGUKu56ZgsX/NvTrN66O+yyROQINlxwTHX3VwY2Bm1VKalIRi03O8o3PjaXuy9fyJ7OHi689Q9844HVPPTymzTu6iB5hbOIyOEx3FVVxUNsyzuchcihO626jEe+cjr/9Js1/GJVA3c9swVIXrI7v7KYeRVFHF9ZzPHlRUzKzwm5WhHJVMMFR52Z/bm7/9/+jWb2Z8BzqStLDtak/Bx+uGQB340neO2tNl5qaOWFhlZeamjlsbXb9/ernJxH9ZRCcrMjZEcjZEUi5GQZEbP9ffoWDRv4a/pt61t/536GEbFg2QwziJgRtWR7JJJczopGyIoYWdHkck7UyM2OEsuKEMuOkpsVJT8WpTA3m4JYFoW5WcSyIu/4fSIytmyoYQwzm0pyPqObt4OiFsgBLgzu5E57tbW1XldXF3YZodvT2cMrW3fzcuNuXm5sZdOODnp6E8R7E/T0Ot29if3DWn1fi8G+Hfv77F8f0ObJP92dhIMT/Bms9yYObegsJxphcn4OJQU5TM7PobQgxpTCGNOKcplelMu0ojymF+VSWhAjGlHAiBwsM3vO3d9139yQZxzuvh34oJmdCRwXND/k7stTUKOkWGFuNh88ppQPHlMadikkEk6vO/FeJ55IEO91ehLJAOvs6aWrJ0FnvJfOnl7au3rZ29XD3s44e7ritO2L09Lexc693exo72bTjnaa9nTRHU+843dEDEoKYkydGGNKYS4l+Tnkx7LIj0WZkJNFfk6UWHaUnGiEnKzkmVdOliXPsIY4o3pne/82e8eZWCRiZEeNrEiErKiRE40Qy4qSmx2cTWVHyInq7Ekyz4juHHf3J4AnUlyLjCORiBHByI4CRIfrPix3p6W9mzd3d/LW7k7ebOukqa2TprYumvZ0sr2tk7VvttHeFae9u/eQz3oOl9OPLeNnX1wYdhkiozLSR46IpDUzo6QgRklBjOPKi4bs6+50xRO0d8Xp7k3QHU/Q05ugK5484xlsKG7AEd7V7v3W3R0neVbVk/D9Q4F9v6OzJ3km9dSGHTxdv4OO7jgTcvS/omQOfVtl3DFLTsDnZh/6mc6hmFmaz+/XN7NmWxvvr5ocai0io6HnTYmE5PiK5NXuLzWM27cfS4ZScIiEpKwwRnlxHi816k5/ySwKDpEQzass0hmHZBwFh0iI5lUU80ZLBy3t3WGXIjJiCg6REM2rDOY5GnXWIZlDwSESoveVFxExTZBLZlFwiIQoP5ZF9ZRCBYdkFAWHSMjmVRbxUuNuPf5eMoaCQyRk8yqLaWnvpnHXvrBLERkRBYdIyOYFNwK+qOEqyRAKDpGQzZ5WSCwronkOyRgKDpGQZUcjHFdepEtyJWMoOETSwPEVRbyydTfx3sTwnUVCltLgMLNFZrbOzOrN7LpBtsfM7N5g+wozq+q37fqgfZ2ZnTdgv6iZvWBmD6ayfpGxMr+ymM6eBOu37w27FJFhpSw4zCwK3AKcD9QAl5hZzYBulwO73H0WcCPwnWDfGmAJMBdYBNwaHK/PV4C1qapdZKz1TZBruEoyQSrPOBYC9e6+0d27gaXA4gF9FgN3Bcv3A2db8j2ai4Gl7t7l7puA+uB4mFkF8FHg9hTWLjKmZpRMoCgvm5cVHJIBUhkc5UBDv/XGoG3QPu4eB3YDJcPsexPwdWDIwWAzu8LM6sysrrm5+WA/g8iYMDPmVRbzYoMesS7pL6Mmx83sAqDJ3Z8brq+73+bute5eW1ZWNgbViRya+RVFrN++h47ueNiliAwplcGxFajst14RtA3ax8yygCJg5xD7ngJ83Mw2kxz6OsvM/jMVxYuMtXmVxfQmnKc37KCprZMde7vY1d5NV7w37NJE3iGV7xxfBVSb2UySf+kvAT4zoM8y4DLgGeAiYLm7u5ktA35uZj8AjgKqgZXu/gxwPYCZnQF8zd0vTeFnEBkz8yqLMYMr7n7nCXXxhGzu+4uTOXZqYUiVibxTyoLD3eNmdhXwKBAF7nT3NWZ2A1Dn7suAO4C7zaweaCEZLgT97gNeBeLAle6uf3bJEa20IMZdX1jI1tZ99CachDvxXufW/6nnynue54GrTmFCTir/rScyMjYenshZW1vrdXV1YZchclD+UL+DS+9YwYULyvn+p+eRvPBQJPXM7Dl3rx3YnlGT4yLj0SmzSvnyWdX86vmt/L+6xrDLEVFwiGSCL59dzSmzSviHB1bz2lttYZcj45yCQyQDRCPGTRcvYGJeNl+653n2dumSXQmPgkMkQ5QVxrh5yQI272jniz9ZxcZmPddKwqHgEMkgJx9Twvc+PY+1b7ax6Kan+MHv1tPZowsOZWwpOEQyzCdPqODxr32I8983jZsf38B5Nz3J79frsToydhQcIhloSmEuP1yygHv+7ANEzbjszpVcdudKVm/Vs64k9RQcIhnslFml/Pc1p3H9+XN4qbGVC/7taa6853le1/yHpJBuABQ5QrR19nD7kxu5/elNdPb08qkTKrj6rGqOLpkQdmmSoQ50A6CCQ+QIs2NvF7c8Uc89K96gN+FcuKCcq86cRVVpftilSYZRcCg4ZJzZ3tbJv//+dX6+4g3iCWfx/KP4yw8do4clyogpOBQcMk41tXXyH09u5J4VW+jsSfCBmZP53MlVnDt3KtlRTXPKgSk4FBwyzrW0d3Pvqgb+89ktbG3dx9SJMT570gz+/PT3EMuKhl2epCEFh4JDBIDehPPEa0387NktPLm+mXkVRdx66YmUF+eFXZqkGT0dV0SA5HOvPlwzlZ99cSH/fumJbGxu54Kbn9JNhDJiCg6RcWzRcdNYdvWpTJ2Yy+d/spKbHltPInHkj0LIoVFwiIxzM0vz+fWXTuHCBeXc9NgGvrz0BcbDELYcPAWHiJCXE+X7n57H35w3mwdffpPbn9oUdkmSxhQcIgKAmfGlM45h0dxpfPuR16jb3BJ2SZKmFBwisp+Z8a+fPp6KSXlc9fMX2Lm3K+ySJA0pOETkHSbmZnPrn55AS0c319z7Ir2aLJcBFBwi8i5zjyriho/P5akNO7j58Q1hlyNpRsEhIoO6+P2VfOqECm5evoEVG3eGXY6kEQWHiAzKzPjmJ45j+sRcvvXwWt3fIfulNDjMbJGZrTOzejO7bpDtMTO7N9i+wsyq+m27PmhfZ2bnBW2VZvaEmb1qZmvM7CuprF9kvMvLiXLtubN5uXE3D73yZtjlSJpIWXCYWRS4BTgfqAEuMbOaAd0uB3a5+yzgRuA7wb41wBJgLrAIuDU4Xhz4qrvXACcBVw5yTBE5jC5cUM6caYV899F1dMcTYZcjaSCVZxwLgXp33+ju3cBSYPGAPouBu4Ll+4GzzcyC9qXu3uXum4B6YKG7v+nuzwO4+x5gLVCews8gMu5FI8bfnj+HN1o6+PmKLWGXI2kglcFRDjT0W2/k3X/J7+/j7nFgN1Aykn2DYa0FwIrBfrmZXWFmdWZW19ysh7eJHIozji3j5PeUcPPyevZ09oRdjoQsIyfHzawA+CVwjbu3DdbH3W9z91p3ry0rKxvbAkWOMGbG9R+ZQ0t7N7c9uTHsciRkqQyOrUBlv/WKoG3QPmaWBRQBO4fa18yySYbGPe7+q5RULiLvcnxFMRccP53bn9pEU1tn2OVIiFIZHKuAajObaWY5JCe7lw3oswy4LFi+CFjuycdyLgOWBFddzQSqgZXB/McdwFp3/0EKaxeRQfzNebOJJxLc+JhuChzPUhYcwZzFVcCjJCex73P3NWZ2g5l9POh2B1BiZvXAtcB1wb5rgPuAV4FHgCvdvRc4BfgscJaZvRj8fCRVn0FE3mlGST6XnjSDpave4Il1TWGXIyHRq2NFZFT2dffyqR//kcZdHfzm6lOZUZIfdkmSInp1rIgcFnk5Uf7jsydiZvzF3c/R0R0PuyQZYwoOERm1yskTuPmSBazbvoe//eUremPgOKPgEJGD8qFjy/jaubP5zUvbuONpvTFwPMkKuwARyVxfOuMYXm5s5V/++zXc4YzZZcyaUkDyAkg5UmlyXEQOyd6uOJfevoIXG1oBKC3I4QPvKeGUY0r56PHTKcrLDrlCOVgHmhxXcIjIIXN3Glr28czGHTy7sYVnXt/JW22d5GVHWTz/KC49aQbHlReFXaaMkoJDwSEyZtyd1VvbuGfFFh54cRv7enqZX1nMn502k4++b7qGsjKEgkPBIRKK3ft6+NXzjfzns1t4vbmdhTMnc8PiucyZNjHs0mQYCg4Fh0ioehPOvasa+NdHX2NPZ5zPnTyDaz58rOZA0tiBgkNXVYnImIhGjM984GjOP24a3//dOn76x8385qVtnDd3GvMqiplXWcysKQVEIxrGSnc64xCRUKzeupsf/G49qza3sKczeff5hJwo8yuLOWvOFM6pmarHmYRMQ1UKDpG0lEg4m3a281JDKy837uaZ13eybvseAKqnFHBOzVQ+Pv8ozYmEQMGh4BDJGG/s7OCxtdt5bO12VmxqoTfhfGDmZC77YBXn1kwlK6qHXowFBYeCQyQj7Wrv5r66Bu5+dguNu/YxvSiXSxYeTe2MScyaWkBZQUyX96aIgkPBIZLRehPO42u387NntvB0/Y797RNzs6ieWsicaYWcOGMSJxw9iRklExQmh4GCQ8EhcsRoautk/fa91DftYUPTXjY07WXttjb2dCUn2UvyczhhxiROnVXKWXOmUDl5QsgVZyYFh4JD5IjWm3A2NO3h+S2tPLdlF6s2t/BGSwcAs6cWctZ7p3BadSlVJflMnZiry35HQMGh4BAZdzY272X5a008vraJlZuTk+yQvKdk2sRcyiflUVUygdnTJjJ7aiGzpxVSWpCjYa6AgkPBITKu7d7Xwwtv7GJr6z62te5jW2snW3ft4/Xmvexs797fryQ/h5PeU8Lpx5Zy+rFlTC/KC7HqcOnOcREZ14rysjlj9pRBt+3Y28W6t/aw7q09rN62m6c37OChV94EkveSnDKrlIUzJ1M7YxJTJuaOZdlpSWccIiIDuDvrtu/hyfXNPLl+B6s2t9AVTwBQOTmP98+YzGnHlnLWnKlH9LO2NFSl4BCRg9QdT7Bm226e27KLus27qNvSwo693WRFjJOPKeHcudM4r2bqEXc2ouBQcIjIYZJIOC82tvLbNdv57Zq32LijnYjBh987lc9/sIqTjyk5IibYFRwKDhFJAXenvmkvv35hK0tXNdDS3s2xUwv43MlVfGJBOQWxzJ1KVnAoOEQkxTp7evnNS9u465nNrN7aRjRizD1qIrUzJvP+qknUVk2mrDAWdpkjFkpwmNki4IdAFLjd3b89YHsM+BlwIrATuNjdNwfbrgcuB3qBL7v7oyM55mAUHCIyltydFxpaeeK1JlZuauHFhtb9k+vHlU/knPdO45yaqbx3emFaD2mNeXCYWRRYD5wDNAKrgEvc/dV+fb4EHO/uf2lmS4AL3f1iM6sBfgEsBI4CHgOODXYb8piDUXCISJi64wle2bqbZzfu5PG123mhoRV3KC/O47TqUionT6C8OI+jivOYXpRL8YRs8rKjoT8FOIz7OBYC9e6+MShgKbAY6P+X/GLgn4Ll+4EfWTJ+FwNL3b0L2GRm9cHxGMExRUTSSk5WhBNnTOLEGZO48sxZNO/p4vG12/ndq9t5ZM1btHb0DLpfdtTIzY4Sy4oSjYBhRAzMDDOIWHI9EqwPdvby0JdPJZYVPayfJ5XBUQ409FtvBD5woD7uHjez3UBJ0P7sgH3Lg+XhjgmAmV0BXAFw9NFHH9wnEBFJgbLCGEsWHs2Shcm/mzq6484G2rUAAAdESURBVGxr7WRb6z7e3L2Ptn1x9vX00tnTG/yZwN1JuOMOCU8OhzmQcCfhySu9BmMc/qGwzJ3uH4a73wbcBsmhqpDLERE5oAk5WcyaUsCsKQVhlzIiqRxA2wpU9luvCNoG7WNmWUARyUnyA+07kmOKiEgKpTI4VgHVZjbTzHKAJcCyAX2WAZcFyxcByz05W78MWGJmMTObCVQDK0d4TBERSaGUDVUFcxZXAY+SvHT2TndfY2Y3AHXuvgy4A7g7mPxuIRkEBP3uIznpHQeudPdegMGOmarPICIi76YbAEVEZFAHuhw33IuERUQk4yg4RERkVBQcIiIyKgoOEREZlXExOW5mzcCWg9y9FNhxGMsZK6p7bKnusZWpdUNm1T7D3csGNo6L4DgUZlY32FUF6U51jy3VPbYytW7I7Nr7aKhKRERGRcEhIiKjouAY3m1hF3CQVPfYUt1jK1PrhsyuHdAch4iIjJLOOEREZFQUHCIiMioKjgMws0Vmts7M6s3surDrGYqZ3WlmTWa2ul/bZDP7nZltCP6cFGaNgzGzSjN7wsxeNbM1ZvaVoD2tazezXDNbaWYvBXX/76B9ppmtCL4z9waP/k87ZhY1sxfM7MFgPe3rNrPNZvaKmb1oZnVBW1p/TwDMrNjM7jez18xsrZmdnAl1D0fBMQgziwK3AOcDNcAlZlYTblVD+imwaEDbdcDj7l4NPB6sp5s48FV3rwFOAq4M/june+1dwFnuPg+YDywys5OA7wA3uvssYBdweYg1DuUrwNp+65lS95nuPr/fPRDp/j0B+CHwiLvPAeaR/O+eCXUPzd31M+AHOBl4tN/69cD1Ydc1TM1VwOp+6+uA6cHydGBd2DWO4DM8AJyTSbUDE4DngQ+QvBs4a7DvULr8kHxr5uPAWcCDgGVI3ZuB0gFtaf09IflG000EFyFlSt0j+dEZx+DKgYZ+641BWyaZ6u5vBstvAVPDLGY4ZlYFLABWkAG1B8M9LwJNwO+A14FWd48HXdL1O3MT8HUgEayXkBl1O/BbM3vOzK4I2tL9ezITaAZ+EgwN3m5m+aR/3cNScIwDnvynTdped21mBcAvgWvcva3/tnSt3d173X0+yX/BLwTmhFzSsMzsAqDJ3Z8Lu5aDcKq7n0By+PhKMzu9/8Y0/Z5kAScAP3b3BUA7A4al0rTuYSk4BrcVqOy3XhG0ZZLtZjYdIPizKeR6BmVm2SRD4x53/1XQnBG1A7h7K/AEySGeYjPrex1zOn5nTgE+bmabgaUkh6t+SPrXjbtvDf5sAn5NMqzT/XvSCDS6+4pg/X6SQZLudQ9LwTG4VUB1cLVJDsl3oS8LuabRWgZcFixfRnL+IK2YmZF87/xad/9Bv01pXbuZlZlZcbCcR3JeZi3JALko6JZ2dbv79e5e4e5VJL/Ty939T0nzus0s38wK+5aBc4HVpPn3xN3fAhrMbHbQdDbwKmle90jozvEDMLOPkBwPjgJ3uvu3Qi7pgMzsF8AZJB/XvB34BvBfwH3A0SQfKf8n7t4SVo2DMbNTgaeAV3h7zP3vSM5zpG3tZnY8cBfJ70YEuM/dbzCz95D8l/xk4AXgUnfvCq/SAzOzM4CvufsF6V53UN+vg9Us4Ofu/i0zKyGNvycAZjYfuB3IATYCXyD4zpDGdQ9HwSEiIqOioSoRERkVBYeIiIyKgkNEREZFwSEiIqOi4BARkVFRcIgMw8z2Bn9WmdlnDvOx/27A+h8P5/FFUkHBITJyVcCogqPfHdkH8o7gcPcPjrImkTGn4BAZuW8DpwXvhPjr4EGH3zWzVWb2spn9BSRvrjOzp8xsGck7hTGz/woe0Lem7yF9ZvZtIC843j1BW9/ZjQXHXh28h+Lifsf+n37veLgnuAMfM/u2Jd9t8rKZfW/M/+vIuDHcv4ZE5G3XEdxtDRAEwG53f7+ZxYA/mNlvg74nAMe5+6Zg/Yvu3hI8omSVmf3S3a8zs6uChyUO9EmS7/qYR/KJAKvM7Mlg2wJgLrAN+ANwipmtBS4E5ri79z0SRSQVdMYhcvDOBT4XPF59BclHlFcH21b2Cw2AL5vZS8CzJB+gWc3QTgV+ETyFdzvwe+D9/Y7d6O4J4EWSQ2i7gU7gDjP7JNBxyJ9O5AAUHCIHz4CrPflWuvnuPtPd+8442vd3Sj4X6sPAyZ58a+ALQO4h/N7+z5HqJfkSpjjJJ8beD1wAPHIIxxcZkoJDZOT2AIX91h8F/ip4NDxmdmzw9NaBioBd7t5hZnNIvia3T0/f/gM8BVwczKOUAacDKw9UWPBOkyJ3fxj4a5JDXCIpoTkOkZF7GegNhpx+SvJdFlXA88EEdTPwiUH2ewT4y2AeYh3J4ao+twEvm9nzwSPO+/ya5Ds+XiL5op+vu/tbQfAMphB4wMxySZ4JXXtwH1FkeHo6roiIjIqGqkREZFQUHCIiMioKDhERGRUFh4iIjIqCQ0RERkXBISIio6LgEBGRUfn/PTfZRXGzF/8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(T.J)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "This is a reference implementation for you to explore. You will not be expected to apply it to today's module project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = 2*hours_studying + 4*hours_sleeping + 50\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_samples(n=1000):\n",
    "    \n",
    "    study = np.random.uniform(1,5,(n,1))\n",
    "    sleep = np.random.uniform(1,6,(n,1))\n",
    "    \n",
    "    y = 2*study + 4*sleep + 50\n",
    "    \n",
    "    X = np.append(study, sleep, axis=1)\n",
    "    \n",
    "    X = X / np.amax(X, axis=0)\n",
    "    y = y / 100\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70063648, 0.4498957 ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Dense(....))\n",
    "# model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0060 - mae: 0.0628 - mse: 0.0060\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0058 - mae: 0.0614 - mse: 0.0058\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0055 - mae: 0.0602 - mse: 0.0055\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0053 - mae: 0.0590 - mse: 0.0053\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0051 - mae: 0.0579 - mse: 0.0051\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0049 - mae: 0.0570 - mse: 0.0049\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0047 - mae: 0.0561 - mse: 0.0047\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0046 - mae: 0.0553 - mse: 0.0046\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0044 - mae: 0.0545 - mse: 0.0044\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0043 - mae: 0.0539 - mse: 0.0043\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0042 - mae: 0.0533 - mse: 0.0042\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0041 - mae: 0.0527 - mse: 0.0041\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0040 - mae: 0.0522 - mse: 0.0040\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0039 - mae: 0.0517 - mse: 0.0039\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0038 - mae: 0.0513 - mse: 0.0038\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0038 - mae: 0.0509 - mse: 0.0038\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0037 - mae: 0.0506 - mse: 0.0037\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0036 - mae: 0.0503 - mse: 0.0036\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0036 - mae: 0.0500 - mse: 0.0036\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0035 - mae: 0.0498 - mse: 0.0035\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0035 - mae: 0.0495 - mse: 0.0035\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0035 - mae: 0.0494 - mse: 0.0035\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0492 - mse: 0.0034\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0490 - mse: 0.0034\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0489 - mse: 0.0034\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0033 - mae: 0.0488 - mse: 0.0033\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0033 - mae: 0.0486 - mse: 0.0033\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0033 - mae: 0.0485 - mse: 0.0033\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0033 - mae: 0.0484 - mse: 0.0033\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0033 - mae: 0.0484 - mse: 0.0033\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0033 - mae: 0.0483 - mse: 0.0033\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0032 - mae: 0.0482 - mse: 0.0032\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0482 - mse: 0.0032\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0481 - mse: 0.0032\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0032 - mae: 0.0481 - mse: 0.0032\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0032 - mae: 0.0480 - mse: 0.0032\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0032 - mae: 0.0480 - mse: 0.0032\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0032 - mae: 0.0480 - mse: 0.0032\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0479 - mse: 0.0032\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0479 - mse: 0.0032\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0479 - mse: 0.0032\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0478 - mse: 0.0032\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0478 - mse: 0.0032\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0478 - mse: 0.0032\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0032 - mae: 0.0478 - mse: 0.0032\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0032 - mae: 0.0478 - mse: 0.0032\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0478 - mse: 0.0032\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0477 - mse: 0.0031\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0477 - mse: 0.0031\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0477 - mse: 0.0031\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0477 - mse: 0.0031\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0477 - mse: 0.0031\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0477 - mse: 0.0031\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0477 - mse: 0.0031\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0477 - mse: 0.0031\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0477 - mse: 0.0031\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0477 - mse: 0.0031\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0477 - mse: 0.0031\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0477 - mse: 0.0031\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0477 - mse: 0.0031\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0477 - mse: 0.0031\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0477 - mse: 0.0031\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0477 - mse: 0.0031\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0477 - mse: 0.0031\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0031 - mae: 0.0477 - mse: 0.0031\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0476 - mse: 0.0031\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0475 - mse: 0.0031\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0475 - mse: 0.0031\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0475 - mse: 0.0031\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0475 - mse: 0.0031\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0475 - mse: 0.0031\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0475 - mse: 0.0031\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0475 - mse: 0.0031\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(3, activation='sigmoid', input_dim=2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mae', 'mse'])\n",
    "\n",
    "results = model.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'mae', 'mse'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [i for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13839e198>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST Example with Tensorflow\n",
    "\n",
    "In this section, we will experiment with a very simple fashion classifier. \n",
    "\n",
    "It uses a data set called \"fashion mnist\", a set of small b&w images of apparel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 2s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "(60000, 28, 28) (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize inputs to be between 0 and 1\n",
    "X_train = X_train / 255.\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(9):\n",
    "    # Define subplot\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    \n",
    "    # Plot raw pixel data\n",
    "    plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n",
    "    plt.axis('off')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many unique classes do we have?\n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.53787005e-05, 0.00000000e+00, 0.00000000e+00, 1.99923106e-04,\n",
       "        1.12264514e-03, 0.00000000e+00, 0.00000000e+00, 1.53787005e-05,\n",
       "        6.15148020e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.53787005e-05, 1.53787005e-05, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.61361015e-05, 0.00000000e+00, 5.53633218e-04, 2.09150327e-03,\n",
       "        1.95309496e-03, 9.53479431e-04, 8.30449827e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.53787005e-05, 4.61361015e-05,\n",
       "        6.15148020e-05, 0.00000000e+00, 0.00000000e+00, 4.61361015e-05],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        9.22722030e-05, 0.00000000e+00, 1.56862745e-03, 3.13725490e-03,\n",
       "        2.70665129e-03, 2.06074587e-03, 2.21453287e-03, 1.89158016e-03,\n",
       "        3.53710111e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.84544406e-04, 1.53787005e-04, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.38369858e-03, 3.62937332e-03,\n",
       "        3.18339100e-03, 2.73740869e-03, 1.64552095e-03, 2.39907728e-03,\n",
       "        2.47597078e-03, 1.67627835e-03, 9.84236832e-04, 3.53710111e-04,\n",
       "        1.18415994e-03, 1.99923106e-03, 1.10726644e-03, 2.30680507e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.53787005e-05,\n",
       "        0.00000000e+00, 1.06113033e-03, 3.18339100e-03, 3.42945021e-03,\n",
       "        3.35255671e-03, 3.32179931e-03, 3.32179931e-03, 2.50672818e-03,\n",
       "        1.95309496e-03, 1.86082276e-03, 1.87620146e-03, 2.24529027e-03,\n",
       "        2.16839677e-03, 1.35332564e-03, 2.64513649e-03, 1.01499423e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.53787005e-05, 1.53787005e-05, 1.53787005e-05,\n",
       "        0.00000000e+00, 3.07574010e-03, 3.56785852e-03, 3.56785852e-03,\n",
       "        3.58323722e-03, 3.52172241e-03, 3.42945021e-03, 3.42945021e-03,\n",
       "        3.30642061e-03, 3.27566321e-03, 2.52210688e-03, 1.95309496e-03,\n",
       "        1.89158016e-03, 3.01422530e-03, 3.52172241e-03, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.81430219e-03, 3.46020761e-03, 3.32179931e-03,\n",
       "        3.42945021e-03, 3.50634371e-03, 3.61399462e-03, 3.49096501e-03,\n",
       "        3.44482891e-03, 3.41407151e-03, 3.44482891e-03, 3.39869281e-03,\n",
       "        3.42945021e-03, 3.76778162e-03, 2.66051519e-03, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.96808920e-03, 3.50634371e-03, 3.35255671e-03,\n",
       "        3.27566321e-03, 3.04498270e-03, 2.76816609e-03, 3.26028451e-03,\n",
       "        3.22952710e-03, 3.24490581e-03, 3.27566321e-03, 3.42945021e-03,\n",
       "        3.38331411e-03, 3.73702422e-03, 3.10649750e-03, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.53787005e-05, 4.61361015e-05, 0.00000000e+00,\n",
       "        1.84544406e-04, 3.36793541e-03, 3.38331411e-03, 3.26028451e-03,\n",
       "        3.35255671e-03, 2.95271050e-03, 2.59900038e-03, 3.49096501e-03,\n",
       "        3.19876970e-03, 3.35255671e-03, 3.44482891e-03, 3.26028451e-03,\n",
       "        3.47558631e-03, 3.02960400e-03, 3.21414840e-03, 7.99692426e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 9.22722030e-05, 0.00000000e+00,\n",
       "        1.52249135e-03, 3.75240292e-03, 3.41407151e-03, 3.38331411e-03,\n",
       "        3.35255671e-03, 3.12187620e-03, 3.04498270e-03, 3.39869281e-03,\n",
       "        3.30642061e-03, 3.27566321e-03, 3.41407151e-03, 3.38331411e-03,\n",
       "        3.76778162e-03, 1.83006536e-03, 2.56824298e-03, 8.61207228e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.15148020e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "        8.45828527e-04, 3.62937332e-03, 3.50634371e-03, 3.53710111e-03,\n",
       "        3.50634371e-03, 3.69088812e-03, 3.56785852e-03, 3.27566321e-03,\n",
       "        3.35255671e-03, 3.42945021e-03, 3.59861592e-03, 3.33717801e-03,\n",
       "        3.33717801e-03, 3.21414840e-03, 1.41484045e-03, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.53787005e-05, 6.15148020e-05,\n",
       "        9.22722030e-05, 1.07650903e-04, 3.07574010e-05, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.64475202e-03, 3.47558631e-03, 3.33717801e-03, 3.42945021e-03,\n",
       "        3.41407151e-03, 3.36793541e-03, 3.41407151e-03, 3.39869281e-03,\n",
       "        3.32179931e-03, 3.42945021e-03, 3.52172241e-03, 3.30642061e-03,\n",
       "        3.35255671e-03, 3.92156863e-03, 1.18415994e-03, 0.00000000e+00],\n",
       "       [0.00000000e+00, 4.61361015e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 9.53479431e-04, 2.22991157e-03, 3.13725490e-03,\n",
       "        3.50634371e-03, 3.18339100e-03, 3.27566321e-03, 3.39869281e-03,\n",
       "        3.35255671e-03, 3.19876970e-03, 3.24490581e-03, 3.35255671e-03,\n",
       "        3.44482891e-03, 3.42945021e-03, 3.36793541e-03, 3.30642061e-03,\n",
       "        3.44482891e-03, 3.75240292e-03, 2.44521338e-03, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.76816609e-04, 6.76662822e-04, 1.26105344e-03, 1.64552095e-03,\n",
       "        2.90657439e-03, 3.50634371e-03, 3.38331411e-03, 3.41407151e-03,\n",
       "        3.33717801e-03, 3.47558631e-03, 3.07574010e-03, 3.15263360e-03,\n",
       "        3.24490581e-03, 3.53710111e-03, 3.44482891e-03, 3.59861592e-03,\n",
       "        2.70665129e-03, 2.89119569e-03, 3.84467512e-03, 3.81391772e-03,\n",
       "        3.58323722e-03, 3.66013072e-03, 3.30642061e-03, 0.00000000e+00],\n",
       "       [0.00000000e+00, 8.76585928e-04, 2.87581699e-03, 3.19876970e-03,\n",
       "        3.44482891e-03, 3.39869281e-03, 3.44482891e-03, 3.19876970e-03,\n",
       "        3.13725490e-03, 3.29104191e-03, 3.19876970e-03, 3.21414840e-03,\n",
       "        3.07574010e-03, 2.44521338e-03, 3.76778162e-03, 2.96808920e-03,\n",
       "        3.16801230e-03, 3.42945021e-03, 3.92156863e-03, 3.92156863e-03,\n",
       "        3.39869281e-03, 3.59861592e-03, 3.39869281e-03, 3.24490581e-03,\n",
       "        3.38331411e-03, 3.56785852e-03, 3.78316032e-03, 0.00000000e+00],\n",
       "       [4.61361015e-05, 3.10649750e-03, 3.50634371e-03, 3.44482891e-03,\n",
       "        3.39869281e-03, 3.24490581e-03, 3.24490581e-03, 3.29104191e-03,\n",
       "        3.15263360e-03, 3.15263360e-03, 3.15263360e-03, 3.38331411e-03,\n",
       "        3.69088812e-03, 1.23029604e-03, 2.30680507e-03, 3.92156863e-03,\n",
       "        3.52172241e-03, 3.39869281e-03, 2.89119569e-03, 2.36831988e-03,\n",
       "        2.93733180e-03, 3.22952710e-03, 3.13725490e-03, 3.21414840e-03,\n",
       "        3.41407151e-03, 3.50634371e-03, 3.46020761e-03, 0.00000000e+00],\n",
       "       [1.50711265e-03, 3.58323722e-03, 3.04498270e-03, 3.22952710e-03,\n",
       "        3.41407151e-03, 3.52172241e-03, 3.52172241e-03, 3.59861592e-03,\n",
       "        3.82929642e-03, 3.38331411e-03, 2.98346790e-03, 3.30642061e-03,\n",
       "        3.33717801e-03, 3.70626682e-03, 9.99615532e-04, 1.12264514e-03,\n",
       "        1.63014225e-03, 1.79930796e-03, 2.58362168e-03, 3.36793541e-03,\n",
       "        3.39869281e-03, 3.30642061e-03, 3.33717801e-03, 3.42945021e-03,\n",
       "        3.42945021e-03, 3.44482891e-03, 3.52172241e-03, 4.45982314e-04],\n",
       "       [1.15340254e-03, 3.13725490e-03, 3.26028451e-03, 3.13725490e-03,\n",
       "        2.96808920e-03, 3.15263360e-03, 3.24490581e-03, 3.46020761e-03,\n",
       "        3.32179931e-03, 2.84505959e-03, 3.02960400e-03, 3.16801230e-03,\n",
       "        3.04498270e-03, 3.27566321e-03, 3.69088812e-03, 2.99884660e-03,\n",
       "        3.49096501e-03, 3.76778162e-03, 3.67550942e-03, 3.42945021e-03,\n",
       "        3.35255671e-03, 3.26028451e-03, 3.21414840e-03, 3.41407151e-03,\n",
       "        3.38331411e-03, 3.39869281e-03, 3.53710111e-03, 1.03037293e-03],\n",
       "       [7.38177624e-04, 3.12187620e-03, 2.81430219e-03, 2.98346790e-03,\n",
       "        3.27566321e-03, 3.02960400e-03, 2.84505959e-03, 2.92195309e-03,\n",
       "        2.98346790e-03, 2.95271050e-03, 3.10649750e-03, 3.29104191e-03,\n",
       "        3.36793541e-03, 3.39869281e-03, 3.38331411e-03, 3.62937332e-03,\n",
       "        3.46020761e-03, 3.32179931e-03, 3.06036140e-03, 3.16801230e-03,\n",
       "        2.86043829e-03, 2.78354479e-03, 2.72202999e-03, 2.64513649e-03,\n",
       "        2.78354479e-03, 3.15263360e-03, 3.16801230e-03, 1.76855056e-03],\n",
       "       [0.00000000e+00, 1.87620146e-03, 3.36793541e-03, 2.96808920e-03,\n",
       "        2.75278739e-03, 2.62975779e-03, 2.81430219e-03, 3.01422530e-03,\n",
       "        3.13725490e-03, 3.22952710e-03, 3.27566321e-03, 3.18339100e-03,\n",
       "        3.24490581e-03, 3.22952710e-03, 3.07574010e-03, 3.01422530e-03,\n",
       "        2.98346790e-03, 2.93733180e-03, 2.99884660e-03, 2.93733180e-03,\n",
       "        3.04498270e-03, 2.95271050e-03, 2.70665129e-03, 2.39907728e-03,\n",
       "        2.56824298e-03, 2.72202999e-03, 3.22952710e-03, 1.41484045e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.13802384e-03, 2.90657439e-03,\n",
       "        3.26028451e-03, 2.93733180e-03, 2.69127259e-03, 2.64513649e-03,\n",
       "        2.69127259e-03, 2.78354479e-03, 2.84505959e-03, 2.89119569e-03,\n",
       "        2.90657439e-03, 2.89119569e-03, 2.96808920e-03, 3.04498270e-03,\n",
       "        3.13725490e-03, 3.21414840e-03, 3.22952710e-03, 3.22952710e-03,\n",
       "        3.24490581e-03, 2.89119569e-03, 2.89119569e-03, 2.98346790e-03,\n",
       "        2.95271050e-03, 3.32179931e-03, 2.61437908e-03, 0.00000000e+00],\n",
       "       [3.07574010e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.01499423e-03, 3.07574010e-03, 3.41407151e-03, 3.64475202e-03,\n",
       "        3.67550942e-03, 3.72164552e-03, 3.78316032e-03, 3.73702422e-03,\n",
       "        3.75240292e-03, 3.39869281e-03, 3.38331411e-03, 2.96808920e-03,\n",
       "        2.93733180e-03, 2.75278739e-03, 2.79892349e-03, 2.79892349e-03,\n",
       "        2.78354479e-03, 2.70665129e-03, 2.55286428e-03, 2.58362168e-03,\n",
       "        1.52249135e-03, 8.91964629e-04, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.15148020e-04,\n",
       "        9.38100730e-04, 6.76662822e-04, 1.10726644e-03, 6.30526720e-04,\n",
       "        5.38254517e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# One hot encode outputs\n",
    "y_binary = to_categorical(y_train)\n",
    "y_binary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28)))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.0642 - accuracy: 0.5969 - val_loss: 1.8568 - val_accuracy: 0.6642\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.6912 - accuracy: 0.6829 - val_loss: 1.5505 - val_accuracy: 0.6858\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.4341 - accuracy: 0.6943 - val_loss: 1.3388 - val_accuracy: 0.7010\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.2544 - accuracy: 0.7080 - val_loss: 1.1891 - val_accuracy: 0.7086\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.1261 - accuracy: 0.7175 - val_loss: 1.0810 - val_accuracy: 0.7137\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0328 - accuracy: 0.7239 - val_loss: 1.0016 - val_accuracy: 0.7204\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.9632 - accuracy: 0.7306 - val_loss: 0.9417 - val_accuracy: 0.7285\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.9100 - accuracy: 0.7358 - val_loss: 0.8952 - val_accuracy: 0.7330\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.8681 - accuracy: 0.7411 - val_loss: 0.8583 - val_accuracy: 0.7379\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.8343 - accuracy: 0.7449 - val_loss: 0.8282 - val_accuracy: 0.7424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1405819b0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, \n",
    "          epochs=10, \n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.78420902e-03, 2.72019720e-03, 1.04633989e-02, ...,\n",
       "        2.31584638e-01, 7.59387389e-02, 3.09993207e-01],\n",
       "       [8.02122056e-03, 1.91451298e-04, 6.58077598e-01, ...,\n",
       "        2.89059535e-05, 1.10142948e-02, 1.29874016e-03],\n",
       "       [1.60926078e-02, 8.98868382e-01, 5.28522767e-03, ...,\n",
       "        2.08269295e-04, 8.04545940e-04, 3.54485528e-04],\n",
       "       ...,\n",
       "       [1.96171775e-01, 3.99938002e-02, 2.21234728e-02, ...,\n",
       "        8.19119252e-03, 1.76841587e-01, 1.95414517e-02],\n",
       "       [2.76763402e-02, 6.25570595e-01, 1.23657668e-02, ...,\n",
       "        2.41766404e-03, 2.70733377e-03, 3.40003450e-03],\n",
       "       [1.18861515e-02, 1.34067042e-02, 2.41314229e-02, ...,\n",
       "        3.14870358e-01, 6.16550893e-02, 6.58785626e-02]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S2-NNF (Python3)",
   "language": "python",
   "name": "u4-s2-nnf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
