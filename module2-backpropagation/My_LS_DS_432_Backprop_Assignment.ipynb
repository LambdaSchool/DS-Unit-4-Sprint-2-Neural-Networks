{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "My LS_DS_432_Backprop_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wel51x/DS-Unit-4-Sprint-3-Neural-Networks/blob/master/My_LS_DS_432_Backprop_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4XC6lnhikuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGGrt9EYlCqY",
        "colab_type": "text"
      },
      "source": [
        "# Backpropagation Practice\n",
        "\n",
        "Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
        "\n",
        "| x1 | x2 | x3 | y |\n",
        "|----|----|----|---|\n",
        "| 0  | 0  | 1  | 0 |\n",
        "| 0  | 1  | 1  | 1 |\n",
        "| 1  | 0  | 1  | 1 |\n",
        "| 0  | 1  | 0  | 1 |\n",
        "| 1  | 0  | 0  | 1 |\n",
        "| 1  | 1  | 1  | 0 |\n",
        "| 0  | 0  | 0  | 0 |\n",
        "\n",
        "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEREYT-3wI1f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "79c5760c-81b3-4d5a-94a9-759e6eabe3dc"
      },
      "source": [
        "X = np.array([[0,0,1],[0,1,1],[1,0,1],[0,1,0],[1,0,0],[1,1,1],[0,0,0]])\n",
        "y = np.array([[0],[1],[1],[1],[1],[0],[0]])\n",
        "print(\"X:\\n\", X)\n",
        "print(\"y:\\n\", y)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X:\n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "y:\n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk_YKEyQlpP_",
        "colab_type": "text"
      },
      "source": [
        "###use Lecture nn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8w4LNIYjCW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Neural_Network(object):\n",
        "  def __init__(self):\n",
        "    self.inputs = 3\n",
        "    self.hiddenNodes = 4\n",
        "    self.outputNodes = 1\n",
        "\n",
        "    # Initlize Weights\n",
        "    self.L1_weights = np.random.randn(self.inputs, self.hiddenNodes) # (3x2)\n",
        "    self.L2_weights = np.random.randn(self.hiddenNodes, self.outputNodes) # (3x1)\n",
        "\n",
        "  def feed_forward(self, X):\n",
        "    # Weighted sum between inputs and hidden layer\n",
        "    self.hidden_sum = np.dot(X, self.L1_weights)\n",
        "    # Activations of weighted sum\n",
        "    self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "    # Weighted sum between hidden and output\n",
        "    self.output_sum = np.dot(self.activated_hidden, self.L2_weights)\n",
        "    # final activation of output\n",
        "    self.activated_output = self.sigmoid(self.output_sum)\n",
        "    return self.activated_output\n",
        "    \n",
        "  def sigmoid(self, s):\n",
        "    return 1/(1+np.exp(-s))\n",
        "  \n",
        "  def sigmoid_derivative(self, s):\n",
        "    return s * (1 - s)\n",
        "  \n",
        "  def backward(self, X, y, o):\n",
        "    # backward propgate through the network\n",
        "    self.o_error = y - o # error in output\n",
        "    # applying derivative of sigmoid to error\n",
        "    self.o_delta = self.o_error*self.sigmoid_derivative(o)\n",
        "    \n",
        "    # z2 error: how much our hidden layer weights contributed to output error\n",
        "    self.z2_error = self.o_delta.dot(self.L2_weights.T)\n",
        "    # applying derivative of sigmoid to z2 error\n",
        "    self.z2_delta = self.z2_error*self.sigmoid_derivative(self.activated_hidden)\n",
        "\n",
        "    # adjusting first set (input --> hidden) weights\n",
        "    self.L1_weights += X.T.dot(self.z2_delta)\n",
        "    # adjusting second set (hidden --> output) weights\n",
        "    self.L2_weights += self.activated_hidden.T.dot(self.o_delta)\n",
        "    \n",
        "  def train (self, X, y):\n",
        "    o = self.feed_forward(X)\n",
        "    self.backward(X, y, o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUnTRk6SkYKI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7847
        },
        "outputId": "f58e9bc3-fa26-4f13-c5eb-93f0b146c35f"
      },
      "source": [
        "nn = Neural_Network()\n",
        "for i in range(1000): # trains the NN 1,000 times\n",
        "  if i+1 in [1,2,3,4,5] or (i+1) % 100 == 0:\n",
        "    print('+---------- EPOCH', i+1, '-----------+')\n",
        "    print(\"Input: \\n\", X) \n",
        "    print(\"Actual Output: \\n\", y)  \n",
        "    print(\"Predicted Output: \\n\" + str(nn.feed_forward(X))) \n",
        "    print(\"Loss: \\n\" + str(np.mean(np.square(y - nn.feed_forward(X))))) # mean sum squared loss\n",
        "    print(\"\\n\")\n",
        "  nn.train(X, y)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 1 -----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            "[[0.79420304]\n",
            " [0.78843249]\n",
            " [0.78174599]\n",
            " [0.74055806]\n",
            " [0.73882551]\n",
            " [0.77422521]\n",
            " [0.75438558]]\n",
            "Loss: \n",
            "0.2895998019148148\n",
            "\n",
            "\n",
            "+---------- EPOCH 2 -----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            "[[0.72250153]\n",
            " [0.71509539]\n",
            " [0.69906853]\n",
            " [0.6850166 ]\n",
            " [0.67265595]\n",
            " [0.68979132]\n",
            " [0.70015795]]\n",
            "Loss: \n",
            "0.2665915359495494\n",
            "\n",
            "\n",
            "+---------- EPOCH 3 -----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            "[[0.6564472 ]\n",
            " [0.64859483]\n",
            " [0.62435428]\n",
            " [0.63964917]\n",
            " [0.61819845]\n",
            " [0.61483203]\n",
            " [0.65442613]]\n",
            "Loss: \n",
            "0.2539193368200126\n",
            "\n",
            "\n",
            "+---------- EPOCH 4 -----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            "[[0.61388525]\n",
            " [0.60701974]\n",
            " [0.57837805]\n",
            " [0.61488327]\n",
            " [0.58852187]\n",
            " [0.57001912]\n",
            " [0.627733  ]]\n",
            "Loss: \n",
            "0.24937904654852044\n",
            "\n",
            "\n",
            "+---------- EPOCH 5 -----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            "[[0.59181928]\n",
            " [0.58676123]\n",
            " [0.55657593]\n",
            " [0.6061522 ]\n",
            " [0.57814357]\n",
            " [0.5500317 ]\n",
            " [0.6162607 ]]\n",
            "Loss: \n",
            "0.2475760438317148\n",
            "\n",
            "\n",
            "+---------- EPOCH 100 -----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            "[[0.38744673]\n",
            " [0.51424249]\n",
            " [0.49008279]\n",
            " [0.72580257]\n",
            " [0.70401741]\n",
            " [0.59164022]\n",
            " [0.61238293]]\n",
            "Loss: \n",
            "0.21913311611344763\n",
            "\n",
            "\n",
            "+---------- EPOCH 200 -----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            "[[0.21861621]\n",
            " [0.7255573 ]\n",
            " [0.6435995 ]\n",
            " [0.83970711]\n",
            " [0.74669308]\n",
            " [0.33253252]\n",
            " [0.66742096]]\n",
            "Loss: \n",
            "0.12800285547788565\n",
            "\n",
            "\n",
            "+---------- EPOCH 300 -----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            "[[0.10234306]\n",
            " [0.86279293]\n",
            " [0.74278471]\n",
            " [0.91369678]\n",
            " [0.77648851]\n",
            " [0.15415786]\n",
            " [0.67251377]]\n",
            "Loss: \n",
            "0.08984352054827441\n",
            "\n",
            "\n",
            "+---------- EPOCH 400 -----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            "[[0.06995367]\n",
            " [0.90077232]\n",
            " [0.75944256]\n",
            " [0.93754643]\n",
            " [0.78162835]\n",
            " [0.10941406]\n",
            " [0.66591168]]\n",
            "Loss: \n",
            "0.08280056530553967\n",
            "\n",
            "\n",
            "+---------- EPOCH 500 -----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            "[[0.05479991]\n",
            " [0.91789026]\n",
            " [0.76438286]\n",
            " [0.94958041]\n",
            " [0.78440844]\n",
            " [0.09019994]\n",
            " [0.66134593]]\n",
            "Loss: \n",
            "0.07997097126258924\n",
            "\n",
            "\n",
            "+---------- EPOCH 600 -----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            "[[0.04553324]\n",
            " [0.92725727]\n",
            " [0.76582234]\n",
            " [0.95711586]\n",
            " [0.7868297 ]\n",
            " [0.08003527]\n",
            " [0.65787938]]\n",
            "Loss: \n",
            "0.07838507192119148\n",
            "\n",
            "\n",
            "+---------- EPOCH 700 -----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            "[[0.03890315]\n",
            " [0.93240964]\n",
            " [0.76533052]\n",
            " [0.96243635]\n",
            " [0.78961118]\n",
            " [0.07469433]\n",
            " [0.6546674 ]]\n",
            "Loss: \n",
            "0.0772849724420346\n",
            "\n",
            "\n",
            "+---------- EPOCH 800 -----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            "[[0.03354986]\n",
            " [0.93428417]\n",
            " [0.76293419]\n",
            " [0.96651074]\n",
            " [0.79355944]\n",
            " [0.0730143 ]\n",
            " [0.65095842]]\n",
            "Loss: \n",
            "0.0763516494680118\n",
            "\n",
            "\n",
            "+---------- EPOCH 900 -----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            "[[0.02871781]\n",
            " [0.93233691]\n",
            " [0.75763875]\n",
            " [0.96984708]\n",
            " [0.80023921]\n",
            " [0.07530557]\n",
            " [0.64566402]]\n",
            "Loss: \n",
            "0.07535835841703978\n",
            "\n",
            "\n",
            "+---------- EPOCH 1000 -----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            "[[0.02400572]\n",
            " [0.92449985]\n",
            " [0.74719691]\n",
            " [0.97280986]\n",
            " [0.81296582]\n",
            " [0.08297704]\n",
            " [0.63690909]]\n",
            "Loss: \n",
            "0.07406363115881842\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz0KzGnSlUD5",
        "colab_type": "text"
      },
      "source": [
        "###now try with Welch Labs GD\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdKsBFFMlgvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Neural_Network(object):\n",
        "    def __init__(self):        \n",
        "        #Define Hyperparameters\n",
        "        self.inputLayerSize = 3\n",
        "        self.outputLayerSize = 1\n",
        "        self.hiddenLayerSize = 4\n",
        "        \n",
        "        #Weights (parameters)\n",
        "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
        "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        #Propogate inputs though network\n",
        "        self.z2 = np.dot(X, self.W1)\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "        self.z3 = np.dot(self.a2, self.W2)\n",
        "        yHat = self.sigmoid(self.z3) \n",
        "        return yHat\n",
        "        \n",
        "    def sigmoid(self, z):\n",
        "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
        "        return 1/(1+np.exp(-z))\n",
        "    \n",
        "    def sigmoidPrime(self,z):  # derivative\n",
        "        #Gradient of sigmoid\n",
        "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
        "    \n",
        "    def costFunction(self, X, y):\n",
        "        #Compute cost for given X,y, use weights already stored in class.\n",
        "        self.yHat = self.forward(X)\n",
        "        J = 0.5*sum((y-self.yHat)**2)\n",
        "        return J\n",
        "        \n",
        "    def costFunctionPrime(self, X, y):  # derivative\n",
        "        #Compute derivative with respect to W and W2 for a given X and y:\n",
        "        self.yHat = self.forward(X)\n",
        "        \n",
        "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
        "        dJdW2 = np.dot(self.a2.T, delta3)\n",
        "        \n",
        "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
        "        dJdW1 = np.dot(X.T, delta2)  \n",
        "        \n",
        "        return dJdW1, dJdW2\n",
        "    \n",
        "    #Helper Functions for interacting with other classes:\n",
        "    def getParams(self):\n",
        "        #Get W1 and W2 unrolled into vector:\n",
        "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
        "        return params\n",
        "    \n",
        "    def setParams(self, params):\n",
        "        #Set W1 and W2 using single paramater vector.\n",
        "        W1_start = 0\n",
        "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
        "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize,\n",
        "                                                       self.hiddenLayerSize))\n",
        "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
        "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize,\n",
        "                                                     self.outputLayerSize))\n",
        "        \n",
        "    def computeGradients(self, X, y):\n",
        "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
        "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlOS-_AsmD6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import optimize\n",
        "class trainer(object):\n",
        "    def __init__(self, N):\n",
        "        #Make Local reference to network:\n",
        "        self.N = N\n",
        "        \n",
        "    def callbackF(self, params):\n",
        "        self.N.setParams(params)\n",
        "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
        "        \n",
        "    def costFunctionWrapper(self, params, X, y):\n",
        "        self.N.setParams(params)\n",
        "        cost = self.N.costFunction(X, y)\n",
        "        grad = self.N.computeGradients(X,y)\n",
        "        \n",
        "        return cost, grad\n",
        "        \n",
        "    def train(self, X, y):\n",
        "        #Make an internal variable for the callback function:\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        #Make empty list to store costs:\n",
        "        self.J = []\n",
        "        \n",
        "        params0 = self.N.getParams()\n",
        "\n",
        "        options = {'maxiter': 200, 'disp' : True}\n",
        "        _res = optimize.minimize(self.costFunctionWrapper, params0,\n",
        "                                 jac=True, method='BFGS',\n",
        "                                 args=(X, y), options=options,\n",
        "                                 callback=self.callbackF)\n",
        "\n",
        "        self.N.setParams(_res.x)\n",
        "        self.optimizationResults = _res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wart7079mdMi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "outputId": "5a1694f9-45a4-4808-ea03-d85ecdfe5d15"
      },
      "source": [
        "NN = Neural_Network()\n",
        "T = trainer(NN)\n",
        "T.train(X,y)\n",
        "print(\"Predicted Output: \\n\" + str(NN.forward(X))) \n",
        "# mean sum squared loss\n",
        "print(\"Loss: \\n\" + str(np.mean(np.square(y - NN.forward(X)))))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Desired error not necessarily achieved due to precision loss.\n",
            "         Current function value: 0.252220\n",
            "         Iterations: 46\n",
            "         Function evaluations: 69\n",
            "         Gradient evaluations: 57\n",
            "Predicted Output: \n",
            "[[1.42032451e-20]\n",
            " [9.85933474e-01]\n",
            " [5.00000000e-01]\n",
            " [1.00000000e+00]\n",
            " [9.67774501e-01]\n",
            " [5.03192485e-01]\n",
            " [2.93073740e-19]]\n",
            "Loss: \n",
            "0.0720627181218564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in square\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in square\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CUu-5-doJK7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "303975c7-3bc5-4f34-9dcb-68bfee84dfc5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(T.J)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAbQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b-r70o8p2Dm",
        "colab_type": "text"
      },
      "source": [
        "## Try building/training a more complex MLP on a bigger dataset.\n",
        "\n",
        "Use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the cannonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
        "\n",
        "If you need inspiration, the internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MOPtYdk1HgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwlRJSfBlCvy",
        "colab_type": "text"
      },
      "source": [
        "## Stretch Goals: \n",
        "\n",
        "- Implement Cross Validation model evaluation on your MNIST implementation \n",
        "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
        " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
        "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
      ]
    }
  ]
}
