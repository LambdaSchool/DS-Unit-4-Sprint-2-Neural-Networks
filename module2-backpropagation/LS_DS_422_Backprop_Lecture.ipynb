{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mrl19RIeXgTW"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Backpropagation\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Lesson 2*\n",
    "\n",
    "\n",
    "### A Review of Yesterday's Content:\n",
    "\n",
    "- Neural Networks are made up of layers of nodes or neurons. \n",
    "- There are three kinds of layers: input, hidden, and output layers.\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1600/1*_M4bZyuwaGby6KMiYVYXvg.jpeg\" width=\"400\"></center>\n",
    "- Each layer is made up of individual neurons which have a corresponding weight and bias.\n",
    "<center><img src=\"https://i.stack.imgur.com/6S6Bz.png\" width=\"400\"></center>\n",
    "- Signal is passed from layer to layer through a network by:\n",
    " - Taking in inputs from the training data (or previous layer)\n",
    " - Multiplying each input by its corresponding weight (think arrow/connecting line)\n",
    " - Adding a bias to this weighted some of inputs and weights\n",
    " - Activating this weighted sum + bias by squishifying it with sigmoid or some other activation function. With a single perceptron with three inputs, calculating the output from the node is done like so:\n",
    "\\begin{align}\n",
    " y = sigmoid(\\sum(weight_{1}input_{1} + weight_{2}input_{2} + weight_{3}input_{3}) + bias)\n",
    "\\end{align}\n",
    " - this final activated value is the signal that gets passed onto the next layer of the network.\n",
    " \n",
    " \n",
    "## Learning Objectives\n",
    "* <a href=\"#p1\">Part 1</a>: Explain the intutition behind backproprogation\n",
    "* <a href=\"#p2\">Part 2</a>: Implement backpropagation on a feedforward neural network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PzDoNQeFb9nV"
   },
   "source": [
    "## Multilayer Perceptrons AKA Feedforward Neural Networks\n",
    "\n",
    "Yesterday we talked about the capabilities and limitations of individual perceptrons.\n",
    "\n",
    "- Perceptrons are like a single neuron in a neural network\n",
    "- Perceptrons can only fit linear boundaries between classes\n",
    "\n",
    "What is a Feedforward Neural Network? Well, it's a Neural Network made up of multiple perceptrons that has at least 1 hidden layer (does not include input and output layers). When we use the [Tensorflow Playground](https://playground.tensorflow.org) website to explore different neural network architectures, we're really just composing different architectures (topologies) of feedforward neural networks. However, when we give the network multiple hidden layers it should be designated as a \"deep\" feedforward neural network.\n",
    "\n",
    "### Perceptron\n",
    "\n",
    "<center><img src=\"http://www.ryanleeallred.com/wp-content/uploads/2019/04/Perceptron.png\" width=\"300\"></center>\n",
    "\n",
    "### Feedforward Neural Network\n",
    "\n",
    "<center><img src=\"http://www.ryanleeallred.com/wp-content/uploads/2019/04/Feedforward-NN.png\" width=\"300\"></center>\n",
    "\n",
    "### Deep Feedforward Neural Network\n",
    "<center><img src=\"http://www.ryanleeallred.com/wp-content/uploads/2019/04/Deep-Feedforward-NN.png\" width=\"400\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "99UvbTjRIoEy"
   },
   "source": [
    "# How do Neural Networks Learn?\n",
    "\n",
    "## Calculating *\"loss\"* or *\"error\"*\n",
    "\n",
    "We've talked about how in order to evaluate a network's performance, the data is \"fed forward\" until predictions are obtained and then the \"loss\" or \"error\" for a given observation is ascertained by looking at what the network predicted for that observation and comparing it to what it *should* have predicted. \n",
    "\n",
    "The error for a given observation is calculated by taking the square of the difference between the predicted value and the actual value. \n",
    "\n",
    "We can summarize the overal quality of a network's predictions by finding the average error across all observations. This gives us the \"Mean Squared Error.\" which hopefully is a fairly familiar model evaluation metric by now. Graphing the MSE over each epoch (training cycle) is a common practice with Neural Networks. This is what you're seeing in the top right corner of the Tensorflow Playground website as the number of \"epochs\" climbs higher and higher.\n",
    "\n",
    "We also graphed loss over a number of iterations when working with gradient descent. Neural Networks use gradient descent to find their optimal parameters.\n",
    "\n",
    "<center><img src=\"http://cs231n.github.io/assets/nn3/learningrates.jpeg\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkJHpgNLlMRe"
   },
   "source": [
    "## What is an \"Epoch\"?\n",
    "\n",
    "An \"Epoch\" is one cycle of passing our data forward through the network, measuring error given our specified cost function, and then -via gradient descent- updating weights within our network to hopefully improve the quality of our predictions on the next iteration.\n",
    "\n",
    "### Batch vs Minibatch vs Stochastic Gradient Descent Epochs\n",
    "\n",
    "You may have heard these variations on the training process referenced in the 3Blue1Brown videos about backpropagation. \"Minibatch\" Gradient Descent means that instead of passing all of our data through the network for a given epoch (Batch GD), we just pass a randomized portion of our data through the network for each epoch. \n",
    "\n",
    "Stochastic Gradient Descent is when we make updates to our weights after forward propagating each individual training observation.\n",
    "\n",
    "### Why might we use something like Minibatch GD? \n",
    "\n",
    "Neural Networks often require high quantities of training data. Consider the [MNIST Dataset](http://yann.lecun.com/exdb/mnist/):\n",
    "\n",
    "![MNIST](https://corochann.com/wp-content/uploads/2017/02/mnist_plot-800x600.png)\n",
    "\n",
    "This dataset is what has been used in the 3Blue1Brown videos for demonstrating important neural network concepts. The dataset has 60,000 training observations and 10,000 test observations. SInce there are 10 numeric digits that works out to be 6,000 training images per digit. Do you think our neural network needs to look at all 6000 number 3s in order to get an idea of how accurate or innacurate it is at recognizing the number 3? No, it doesn't. It's actually pretty computationally expensive to run the entire dataset through the network for each epoch, so we'll just randomly select a portion of the dataset for each training epoch.\n",
    "\n",
    "How much of the data should you pass through for each epoch? Well, you'll probably get sick of hearing this response to questions like this, but `batch_size` is another hyperparameter for our model that we'll need to tune to try and get the best results. We'll try different amounts out and try to find an optimal \"batch size.\"\n",
    "\n",
    "### A note about Hyperparameters\n",
    "\n",
    "Neural Networks have many more hyperparameters than other machine learning algorithms which is part of what makes them a beast to train. 1) You need more data to train them on. 2) They're complex so they take longer to train. 3) They have lots and lots of hyperparameters which we need to find the most optimal combination of, so we might end up training our model dozens or hundreds of times with different combinations of hyperparameters in order to try and squeeze out a few more tenths of a percent of accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aM4CK1IarId4"
   },
   "source": [
    "# Backpropagation\n",
    "\n",
    "Backpropagation is short for [\"Backwards Propagation of errors\"](https://en.wikipedia.org/wiki/Backpropagation) and refers to a specific (rather calculus intensive) algorithm for how weights in a neural network are updated in reverse order at the end of each training epoch. Our purpose today is to demonstrate the backpropagation algorithm on a simple Feedforward Neural Network and in so doing help you get a grasp on the main process. If you want to understand all of the underlying calculus of how the gradients are calculated then you'll need to dive into it yourself, [3Blue1Brown's video is a great starting place](https://www.youtube.com/watch?v=tIeHLnjs5U8). I also highly recommend this Welch Labs series [Neural Networks Demystified](https://www.youtube.com/watch?v=bxe2T-V8XRs) if you want a rapid yet orderly walkthrough of the main intuitions and math behind the backpropagation algorithm. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dm2HPETcrgy6"
   },
   "source": [
    "## FeedForward NN for predicting Test Scores\n",
    "\n",
    "![231 Neural Network](https://cdn-images-1.medium.com/max/1600/1*IjY3wFF24sK9UhiOlf36Bw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4d4tzpwO6B47"
   },
   "source": [
    "### Generate some Fake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data is fed forward through the network, and then we obtain whether there is a cost, loss or error from each observation\n",
    "#error for every observation is often calculated with cross-entropy, binary cross-entropy for binary problems. \n",
    "\n",
    "#batch size or stochastic gradient descent epoches - instead of passing all our data, we pass through a random selection\n",
    "# we don't need to look at all the data to figure out whether the model needs to be tuned a certain way\n",
    "\n",
    "#tuning - gridsearch and randomsearch\n",
    "\n",
    "#backpropagation - done in reverse order backwards in our network. - computes the gradient descent from weights, compared to loss\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ERyVgeO_IWyV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(812)\n",
    "\n",
    "X = np.array(([2,9],\n",
    "             [1,5],\n",
    "             [3,6]), dtype=float)\n",
    "\n",
    "y = np.array(([92],\n",
    "             [86],\n",
    "             [89]), dtype=float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDeUBW6k4Ri4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studying, Sleeping \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Test Score \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n"
     ]
    }
   ],
   "source": [
    "# Normalizing Data on feature \n",
    "# Neural Network would probably do this on its own, but it will help us converge on a solution faster\n",
    "\n",
    "X = X / np.amax(X, axis=0)#normalize to the maximum value\n",
    "y = y / 100\n",
    "\n",
    "print(\"Studying, Sleeping \\n\", X)\n",
    "print(\"Test Score \\n\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgTf6vTS69Sw"
   },
   "source": [
    "### Lets create a Neural_Network class to contain this functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUI8VSR5zyBv"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.inputs = 2 # Set up Architecture of Neural Network\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1 #regression problem\n",
    "        \n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)#initalize weights, 2x3  matrix array for first layer\n",
    "        #nodes from input to hidden layers\n",
    "        \n",
    "        #3x1 Matrix Array for Hidden to Output \n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "        #nodes from hidden nodes to input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gbyT_FJ88IlK"
   },
   "source": [
    "### How many random weights do we need to initialize? \"Fully-connected Layers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IreIDe6P8H0H"
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 weights: \n",
      " [[ 2.48783189  0.11697987 -1.97118428]\n",
      " [-0.48325593 -1.50361209  0.57515126]]\n",
      "Layer 2 weights: \n",
      " [[-0.20672583]\n",
      " [ 0.41271104]\n",
      " [-0.57757999]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer 1 weights: \\n\", nn.weights1)\n",
    "print(\"Layer 2 weights: \\n\", nn.weights2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbxDhyjQ-RwS"
   },
   "source": [
    "### Implement Feedforward Functionality\n",
    "\n",
    "After this step our neural network should be able to generate an output even though it has not been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gGivpEk-VdP"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.inputs = 2 # Set up Architecture of Neural Network\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)#initalize weights, 2x3  matrix array for first layer\n",
    "        \n",
    "        #3x1 Matrix Array for Hidden to Output \n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "        #need sigmoid for activation functions\n",
    "        #need bias terms and fixed function error metric\n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward pass.\n",
    "        \"\"\"\n",
    "#input to hidden, hidden to output        \n",
    "        #Weighted sum of inputs and hidden layers, dot product of X and self weights1\n",
    "        self.hidden_sum = np.dot(X, self.weights1)#with hidden sum, we calculate and return for activation function \n",
    "        \n",
    "        #Activation of the weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)#activated hidden to the output layer\n",
    "        \n",
    "        #Weighted sum of the hidden layer to output layer\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        #Final Activation of Output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a1pxdfmDAaJg"
   },
   "source": [
    "### Can we generate an output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eknilPKeBNBN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66666667 1.        ]\n",
      "output [0.25814933]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "\n",
    "print(X[0])\n",
    "output = nn.feed_forward(X[0])#make a feedforward pass\n",
    "print(\"output\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3V61yNmAB2T5"
   },
   "source": [
    "### Calculating Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4jh9v8eXB0Mj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66185067])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = y[0] - output #actual activation - ouput\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25814933]\n",
      " [0.33067192]\n",
      " [0.22642076]]\n",
      "[[0.66185067]\n",
      " [0.52932808]\n",
      " [0.66357924]]\n"
     ]
    }
   ],
   "source": [
    "output_all = nn.feed_forward(X)\n",
    "error_all = y - output_all\n",
    "print(output_all)\n",
    "print(error_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_eyzItYIxgm"
   },
   "source": [
    "## Backpropagation (Assigning blame for bad predictions and delivering justice - repeatedly and a little bit at a time)\n",
    "\n",
    "What in our model could be causing our predictions to suck so bad? \n",
    "\n",
    "Well, we know that our inputs (X) and outputs (y) are correct, if they weren't then we would have bigger problems than understanding backpropagation.\n",
    "\n",
    "We also know that our activation function (sigmoid) is working correctly. It can't be blamed because it just does whatever we tell it to and transforms the data in a known way.\n",
    "\n",
    "So what are the potential culprits for these terrible predictions? The **weights** of our model. Here's the problem though. I have weights that exist in both layers of my model. How do I know if the weights in the first layer are to blame, or the second layer, or both? \n",
    "\n",
    "Lets investigate. And see if we can just eyeball what should be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activated_hidden\n",
      " [[0.91532593 0.10241457 0.53738066]\n",
      " [0.78757799 0.23338436 0.52894836]\n",
      " [0.85426171 0.15490573 0.40356642]] \n",
      "---------\n",
      "activated_output\n",
      " [[0.91686002]\n",
      " [0.86735248]\n",
      " [0.88454551]] \n",
      "---------\n",
      "backward\n",
      " <bound method NeuralNetwork.backward of <__main__.NeuralNetwork object at 0x00000130F033BD30>> \n",
      "---------\n",
      "feed_forward\n",
      " <bound method NeuralNetwork.feed_forward of <__main__.NeuralNetwork object at 0x00000130F033BD30>> \n",
      "---------\n",
      "hiddenNodes\n",
      " 3 \n",
      "---------\n",
      "hidden_sum\n",
      " [[ 2.38047083 -2.17067935  0.14980217]\n",
      " [ 1.31038749 -1.18929885  0.11592309]\n",
      " [ 1.76842513 -1.69663146 -0.39062678]] \n",
      "---------\n",
      "inputs\n",
      " 2 \n",
      "---------\n",
      "o_delta\n",
      " [[ 0.00023935]\n",
      " [-0.00084592]\n",
      " [ 0.00055704]] \n",
      "---------\n",
      "o_error\n",
      " [[ 0.00313998]\n",
      " [-0.00735248]\n",
      " [ 0.00545449]] \n",
      "---------\n",
      "outputNodes\n",
      " 1 \n",
      "---------\n",
      "output_sum\n",
      " [[2.40042913]\n",
      " [1.87775009]\n",
      " [2.03619754]] \n",
      "---------\n",
      "sigmoid\n",
      " <bound method NeuralNetwork.sigmoid of <__main__.NeuralNetwork object at 0x00000130F033BD30>> \n",
      "---------\n",
      "sigmoidPrime\n",
      " <bound method NeuralNetwork.sigmoidPrime of <__main__.NeuralNetwork object at 0x00000130F033BD30>> \n",
      "---------\n",
      "train\n",
      " <bound method NeuralNetwork.train of <__main__.NeuralNetwork object at 0x00000130F033BD30>> \n",
      "---------\n",
      "weights1\n",
      " [[ 0.32667698 -0.44918672 -0.88278542]\n",
      " [ 2.16270659 -1.8712414   0.73842842]] \n",
      "---------\n",
      "weights2\n",
      " [[ 2.22137792]\n",
      " [-1.75865032]\n",
      " [ 1.01831468]] \n",
      "---------\n",
      "z2_delta\n",
      " [[ 4.12081093e-05 -3.86932576e-05  6.05992962e-05]\n",
      " [-3.14368264e-04  2.66156026e-04 -2.14650878e-04]\n",
      " [ 1.54051739e-04 -1.28237754e-04  1.36547695e-04]] \n",
      "---------\n",
      "z2_error\n",
      " [[ 0.00053169 -0.00042092  0.00024376]\n",
      " [-0.00187908  0.0014876  -0.00086149]\n",
      " [ 0.00123738 -0.00097959  0.00056729]] \n",
      "---------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes = ['weights1', 'hidden_sum', 'activated_hidden', 'weights2', 'output+']\n",
    "[print(i+'\\n', getattr(nn,i), '\\n'+'---'*3) for i in dir(nn) if i[:2]!= '__']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_7Qrqr7fSGS_"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes =  3\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        # 2x3 Matrix Array for the First Layer\n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)\n",
    "       \n",
    "        # 3x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "\n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "\n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1-s)\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward pass.\n",
    "        \"\"\"\n",
    "        #Weighted sum of inputs and hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "\n",
    "        #Activation of the weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "\n",
    "        #Weighted sum of hidden layer to output layer\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "\n",
    "        #Final Activation of Output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "\n",
    "        return self.activated_output\n",
    "\n",
    "    #sigmoid needed at this step, will take input matrix, outputs and some other term for error\n",
    "    def backward(self, X, y, o):\n",
    "        #this is our backward pass\n",
    "        #updating weights by fixed amount which we calculated using gradient derivative of our error\n",
    "        self.o_error = y - o#error in output\n",
    "\n",
    "        #delta gives us the adjustment from the output to the hidden\n",
    "        #derivative of error\n",
    "        #Size of Adjustment from hidden =>output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o) #apply derivative of sigmoid to error#basically the gradient\n",
    "        #the z2 error is how much we need to change the weights by\n",
    "        #delta is constant for the backward propagation pass, then gets smaller for every other pass hopefully\n",
    "\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)#How much the hidden layer interprets as the error\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)#this is where you might scale down the learning rate\n",
    "        #we are updating the weights a fixed amount by travelling up the gradient\n",
    "        \n",
    "        #going from right to left\n",
    "        #Adjustments from the hidden to the ouput weights\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "\n",
    "        #Adjustment from input => hidden weights\n",
    "        #taking how much we get wrong from the z2 error and z2 delta\n",
    "        #weights1 is the first hidden error, we are working backwards\n",
    "        self.weights1 += X.T.dot(self.z2_delta)#update the weights of the first layer by taking the dot products \n",
    "        #with self.z2_delta which is representative of a variable value of how much we need to update the weights.\n",
    "\n",
    "        #we are not using gradient descent, we are updating the weights a fixed amount that we calculated using just the \n",
    "        #gradient derivative of our error\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)#calculate a forward path\n",
    "        self.backward(X, y, o )#make a backwards pass\n",
    "        \n",
    "#Since you're never passing back a function, it only matters if you can find the derivative of the activation function\n",
    "#we only care about the derivative of the activation function, the chain rule for the backprop inputs will always be a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.09563085],\n",
       "       [-2.07968346],\n",
       "       [ 0.05764744]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.09563085, -2.07968346,  0.05764744]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we are transposing weights so that this and our error is going to be flat\n",
    "nn.weights2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19150825])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error#flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rPkfRI-iMvoV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04762951, -0.21536764, -0.01285294])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want activated that correspond to negative weights to be lower\n",
    "# And we want more higher activation for positivie weights\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "o_delta = error * nn.sigmoidPrime(output)\n",
    "\n",
    "z2_error = o_delta.dot(nn.weights2.T)\n",
    "z2_error#set this error aside, calculate this error for the next layer, take the error all at once, then update the weights\n",
    "#based on the size of the updates\n",
    "#we want the array this outputs to be the same shape as our weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.35238723]\n",
      " [0.36996977]\n",
      " [0.36033211]]\n",
      "Loss: \n",
      " 0.280953986448507\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.40850442]\n",
      " [0.4246174 ]\n",
      " [0.42172238]]\n",
      "Loss: \n",
      " 0.22348988717812576\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.4601898 ]\n",
      " [0.47558857]\n",
      " [0.47935124]]\n",
      "Loss: \n",
      " 0.1759433242704489\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.50655462]\n",
      " [0.52146417]\n",
      " [0.53149146]]\n",
      "Loss: \n",
      " 0.13802398897889118\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.54770074]\n",
      " [0.56188907]\n",
      " [0.57768253]]\n",
      "Loss: \n",
      " 0.10833968631600992\n",
      "+---------EPOCH 200---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.89174229]\n",
      " [0.86911713]\n",
      " [0.91319078]]\n",
      "Loss: \n",
      " 0.0004731442141253799\n",
      "+---------EPOCH 400---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.89337207]\n",
      " [0.86849252]\n",
      " [0.91136479]]\n",
      "Loss: \n",
      " 0.0004125412928038415\n",
      "+---------EPOCH 600---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.89470579]\n",
      " [0.86796728]\n",
      " [0.90969871]]\n",
      "Loss: \n",
      " 0.0003637713498223312\n",
      "+---------EPOCH 800---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.89587752]\n",
      " [0.8675833 ]\n",
      " [0.90823149]]\n",
      "Loss: \n",
      " 0.00032392917819870984\n",
      "+---------EPOCH 1000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.89691592]\n",
      " [0.8673024 ]\n",
      " [0.90692861]]\n",
      "Loss: \n",
      " 0.0002909258342468941\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "for i in range(1000):#1000 iterations on our model\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 200 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "    nn.train(X,y)\n",
    "    \n",
    "#some neurons in the hidden layer are more highlighted than others because with backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26wgCLU0TLvy"
   },
   "source": [
    "Why is my error so big?\n",
    "\n",
    "My error is so big because my prediction is low.\n",
    "\n",
    "Why are my prediction low?\n",
    "\n",
    "Because either:\n",
    "\n",
    "  1) Second layer **weights** are low\n",
    "  \n",
    "  (or)\n",
    "  \n",
    "  2) Activations coming from the first layer are low\n",
    "  \n",
    "How are activations from the first layer determined? \n",
    "\n",
    "  1) By inputs - fixed\n",
    "  \n",
    "  2) by **weights** - variable\n",
    "  \n",
    "The only thing that I have control over throughout this process in order to increase the value of my final predictions is to either increase weights in layer 2 or increase weights in layer 1. \n",
    "\n",
    "Imagine that you could only change your weights by a fixed amount. Say you have .3 and you have to split that up and disperse it over your weights so as to increase your predictions as much as possible. (This isn't actually what happens, but it will help us identify which weights we would benefit the most from moving.)\n",
    "\n",
    "I need to increase weights of my model somewhere, I'll get the biggest bang for my buck if I increase weights in places where I'm already seeing high activation values -because they end up getting multiplied together before being passed to the sigmoid function. \n",
    "\n",
    "> \"Neurons that fire together, wire together\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ur3oFEEV6r7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16Ujj6vNYQyX"
   },
   "source": [
    "## Diagnosing Backpropagation issues by hand. (not recommended)\n",
    "\n",
    "Our model has 9 total weights (6 in the first layer, 3 in the last layer) that could be off.\n",
    "\n",
    "1) Calculate Error for a given each observation\n",
    "\n",
    "2) Does the error indicate that I'm overestimating or underestimating in my prediction?\n",
    "\n",
    "3) Look at final layer weights to get an idea for which weights are helping pass desireable signals and which are stifling desireable signals\n",
    "\n",
    "4) Also go to the previous layer and see what can be done to boost activations that are associated with helpful weights, and limit activations that are associated with unhelpful weights.\n",
    "\n",
    "### However, we would prefer to do all this in a fancy automated, controlled calculus way.\n",
    "\n",
    "5) Repeat steps 1-4 for every observation in a given batch, and then given the network's cost function, calculate its gradient using calculus and update weights associated with the (negative) gradient of the cost function. \n",
    "\n",
    "Remember that we have 9 weights in our network therefore the gradient that comes from our gradient descent calculation will be the vector that takes us in the most downward direction along some function in 9-dimensional hyperspace.\n",
    "\n",
    "\\begin{align}\n",
    "C(w1, w2, w3, w4, w5, w6, w7, w8, w9)\n",
    "\\end{align}\n",
    "\n",
    "You should also know that with neural networks it is common to have gradients that are not convex (like what we saw when we applied gradient descent to linear regression). Due to the high complexity of these models and their nonlinearity, it is common for gradient descent to get stuck in a local minimum, but there are ways to combat this:\n",
    "\n",
    "1) Stochastic Gradient Descent\n",
    "\n",
    "2) More advanced Gradient-Descent-based \"Optimizers\" - See Stretch Goals on assignment.\n",
    "\n",
    "Adam is the most popular right now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPCxvGBay6rO"
   },
   "source": [
    "## Adding BackPropogation to our implementation (non Gradient-Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVc6gRalIwy6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZF7UE-KluPsX"
   },
   "source": [
    "## A true GD-based implementation from [Welch Labs](https://www.youtube.com/watch?v=bxe2T-V8XRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SPlNkC91o6OU"
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1 \n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uA9LaTgKr6rP"
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #Make empty list to store costs:\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callbackF)#optimize or minimize my cost function\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_kHb6Se1u9y"
   },
   "outputs": [],
   "source": [
    "NN = Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYYVhFf4rn3q"
   },
   "outputs": [],
   "source": [
    "T = trainer(NN)#pass our neural network to our trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "L-gYdVfgrysE",
    "outputId": "ae371bf9-692c-49b4-b165-8562dab9c06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 65\n",
      "         Function evaluations: 70\n",
      "         Gradient evaluations: 70\n"
     ]
    }
   ],
   "source": [
    "T.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "Jyv_L8Z2sKOA",
    "outputId": "08725651-6d21-401b-85c0-3487370b8bc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: \n",
      "[[0.91994142]\n",
      " [0.86007942]\n",
      " [0.89004417]]\n",
      "Loss: \n",
      "3.896447758768781e-09\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Output: \\n\" + str(NN.forward(X))) \n",
    "print(\"Loss: \\n\" + str(np.mean(np.square(y - NN.forward(X))))) # mean sum squared loss\n",
    "\n",
    "#predicts accuratly after 65 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "Gtf9WI9FtGPk",
    "outputId": "d062b2a3-5a92-403e-8ce0-c070aa79907b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(T.J)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizers keras adam, stochastic gradient descent and adagrade "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_432_Backprop_Lecture.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
