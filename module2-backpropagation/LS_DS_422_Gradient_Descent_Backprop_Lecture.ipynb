{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 2, Module 2*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Backpropagation & Gradient Descent (Prepare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* <a href=\"#p1\">Part 1</a>: Explain the intutition behind backproprogation\n",
    "* <a href=\"#p2\">Part 2</a>: Implement gradient descent + backpropagation on a feedforward neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Two Days Ago\n",
    "\n",
    "Yesterday, we learned about some of the principal components of Neural Networks: Neurons, Weights, Activation Functions, and layers (input, output, & hidden). Today, we will reinfornce our understanding of those components and introduce the mechanics of training a neural network. Feedforward neural networks, such as multi-layer perceptrons (MLPs), are almost always trained using some variation of gradient descent where the gradient has been calculated by backpropagation.\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1600/1*_M4bZyuwaGby6KMiYVYXvg.jpeg\" width=\"400\"></center>\n",
    "\n",
    "- There are three kinds of layers: input, hidden, and output layers.\n",
    "- Each layer is made up of **n** individual neurons (aka activation units) which have a corresponding weight and bias.\n",
    "- Signal is passed from layer to layer through a network by:\n",
    " - Taking in inputs from the training data (or previous layer)\n",
    " - Multiplying each input by its corresponding weight (think arrow/connecting line)\n",
    " - Adding a bias to this weighted some of inputs and weights\n",
    " - Activating this weighted sum + bias by squishifying it with sigmoid or some other activation function. With a single perceptron with three inputs, calculating the output from the node is done like so:\n",
    "\\begin{align}\n",
    " y = sigmoid(\\sum(weight_{1}input_{1} + weight_{2}input_{2} + weight_{3}input_{3}) + bias)\n",
    "\\end{align}\n",
    " - this final activated value is the signal that gets passed onto the next layer of the network.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names for activation functions: sigmoid (squishification from 0 to 1), \n",
    "# tanh (similar to sigmoid but range -1 to 1), relu (rectified linear unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Network: *Formal Summary*\n",
    "\n",
    "0. Pick a network architecture\n",
    "   - No. of input units = No. of features\n",
    "   - No. of output units = Number of Classes (or expected targets)\n",
    "   - Select the number of hidden layers and number of neurons within each hidden layer\n",
    "1. Randomly initialize weights\n",
    "2. Implement forward propagation to get $h_{\\theta}(x^{(i)})$ for any $x^{(i)}$\n",
    "3. Implement code to compute a cost function $J(\\theta)$\n",
    "4. Implement backpropagation to compute partial derivatives $\\frac{\\delta}{\\delta\\theta_{jk}^{l}}{J(\\theta)}$\n",
    "5. Use gradient descent (or other advanced optimizer) with backpropagation to minimize $J(\\theta)$ as a function of parameters $\\theta\\$\n",
    "6. Repeat steps 2 - 5 until cost function is 'minimized' or some other stopping criteria is met. One pass over steps 2 - 5 is called an iteration or epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Calculating *\"cost\"*, *\"loss\"* or *\"error\"*\n",
    "\n",
    "We've talked about how in order to evaluate a network's performance, the data is \"fed forward\" until predictions are obtained and then the \"loss\" or \"error\" for a given observation is ascertained by looking at what the network predicted for that observation and comparing it to what it *should* have predicted. \n",
    "\n",
    "The error for a given observation is calculated by taking the square of the difference between the predicted value and the actual value. \n",
    "\n",
    "We can summarize the overal quality of a network's predictions by finding the average error across all observations. This gives us the \"Mean Squared Error.\" which hopefully is a fairly familiar model evaluation metric by now. Graphing the MSE over each epoch (training cycle) is a common practice with Neural Networks. This is what you're seeing in the top right corner of the Tensorflow Playground website as the number of \"epochs\" climbs higher and higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an \"Epoch\"?\n",
    "\n",
    "An \"Epoch\" is one cycle of passing our data forward through the network, measuring error given our specified cost function, and then -via gradient descent- updating weights within our network to hopefully improve the quality of our predictions on the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch vs Minibatch vs Stochastic Gradient Descent Epochs\n",
    "\n",
    "You may have heard these variations on the training process referenced in the 3Blue1Brown videos about backpropagation. \"Minibatch\" Gradient Descent means that instead of passing all of our data through the network for a given epoch (Batch GD), we just pass a randomized portion of our data through the network for each epoch. \n",
    "\n",
    "Stochastic Gradient Descent is when we make updates to our weights after forward propagating each individual training observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about Hyperparameters\n",
    "\n",
    "Neural Networks have many more hyperparameters than other machine learning algorithms which is part of what makes them a beast to train.\n",
    "\n",
    "1. You need more data to train them on. \n",
    "2. They're complex so they take longer to train. \n",
    "3. They have lots and lots of hyperparameters which we need to find the most optimal combination of, so we might end up training our model dozens or hundreds of times with different combinations of hyperparameters in order to try and squeeze out a few more tenths of a percent of accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aM4CK1IarId4",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Backpropagation (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aM4CK1IarId4",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Overview\n",
    "\n",
    "Backpropagation is short for [\"Backwards Propagation of errors\"](https://en.wikipedia.org/wiki/Backpropagation) and refers to a specific (rather calculus intensive) algorithm for how weights in a neural network are updated in reverse order at the end of each training epoch. Our purpose today is to demonstrate the backpropagation algorithm on a simple Feedforward Neural Network and in so doing help you get a grasp on the main process. If you want to understand all of the underlying calculus of how the gradients are calculated then you'll need to dive into it yourself, [3Blue1Brown's video is a great starting place](https://www.youtube.com/watch?v=tIeHLnjs5U8). I also highly recommend this Welch Labs series [Neural Networks Demystified](https://www.youtube.com/watch?v=bxe2T-V8XRs) if you want a rapid yet orderly walkthrough of the main intuitions and math behind the backpropagation algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Gradient?\n",
    "\n",
    "> In vector calculus, the gradient is a multi-variable generalization of the derivative. \n",
    "\n",
    "The gradients that we will deal with today will be vector representations of the derivative of the activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "In this section, we will again implement a multi-layer perceptron using numpy. We'll focus on using a __Feed Forward Neural Network__ to predict test scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dm2HPETcrgy6",
    "toc-hr-collapsed": true
   },
   "source": [
    "![231 Neural Network](https://cdn-images-1.medium.com/max/1600/1*IjY3wFF24sK9UhiOlf36Bw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4d4tzpwO6B47"
   },
   "source": [
    "### Generate some Fake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(812)\n",
    "\n",
    "# hours studying, hours sleep\n",
    "X = np.array(([2,9],\n",
    "              [1,5],\n",
    "              [3,6]), dtype=float)\n",
    "\n",
    "# Exam Scores\n",
    "y = np.array(([92],\n",
    "              [86],\n",
    "              [89]), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Studying, Sleeping \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nTest Score \n [[0.92]\n [0.86]\n [0.89]]\n"
    }
   ],
   "source": [
    "# Normalizing Data on feature \n",
    "# Neural Network would probably do this on its own, but it will help us converge on a solution faster\n",
    "# Normalizes values based on max value in array\n",
    "# np.amax gives max value in each column instead of entire array\n",
    "X = X / np.amax(X, axis=0)\n",
    "y = y / 100\n",
    "\n",
    "print(\"Studying, Sleeping \\n\", X)\n",
    "print(\"Test Score \\n\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgTf6vTS69Sw"
   },
   "source": [
    "### Neural Network Architecture\n",
    "Lets create a Neural_Network class to contain this functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        # 2x3 Matrix Array for the First Layer\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
    "       \n",
    "        # 3x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gbyT_FJ88IlK"
   },
   "source": [
    "### Randomly Initialize Weights\n",
    "How many random weights do we need to initialize? \"Fully-connected Layers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Layer 1 weights: \n [[0.5049808  0.60592761 0.45748719]\n [0.32659171 0.59345002 0.25569456]]\nLayer 2 weights: \n [[0.23870931]\n [0.95553049]\n [0.95889787]]\n"
    }
   ],
   "source": [
    "print(\"Layer 1 weights: \\n\", nn.weights1)\n",
    "print(\"Layer 2 weights: \\n\", nn.weights2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbxDhyjQ-RwS"
   },
   "source": [
    "### Implement Feedforward Functionality\n",
    "\n",
    "After this step our neural network should be able to generate an output even though it has not been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        # 2x3 Matrix Array for the First Layer\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
    "       \n",
    "        # 3x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weighted sum of inputs => hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a1pxdfmDAaJg"
   },
   "source": [
    "### Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "intput [0.66666667 1.        ]\noutput [0.79105842]\n"
    }
   ],
   "source": [
    "# Try to make a prediction with our updated 'net\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "output = nn.feed_forward(X[0])\n",
    "print(\"intput\", X[0])\n",
    "print(\"output\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3V61yNmAB2T5"
   },
   "source": [
    "### Calculate Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.12894158])"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = y[0] - output\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[0.79105842]\n [0.7590318 ]\n [0.79893077]]\n[[0.12894158]\n [0.1009682 ]\n [0.09106923]]\n"
    }
   ],
   "source": [
    "output_all = nn.feed_forward(X)\n",
    "error_all = y - output_all\n",
    "print(output_all)\n",
    "print(error_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26wgCLU0TLvy"
   },
   "source": [
    "Why is my error so big?\n",
    "\n",
    "My error is so big because my prediction is low.\n",
    "\n",
    "Why are my prediction low?\n",
    "\n",
    "Because either:\n",
    "\n",
    "  1) Second layer **weights** are low\n",
    "  \n",
    "  (or)\n",
    "  \n",
    "  2) Activations coming from the first layer are low\n",
    "  \n",
    "How are activations from the first layer determined? \n",
    "\n",
    "  1) By inputs - fixed\n",
    "  \n",
    "  2) by **weights** - variable\n",
    "  \n",
    "The only thing that I have control over throughout this process in order to increase the value of my final predictions is to either increase weights in layer 2 or increase weights in layer 1. \n",
    "\n",
    "Imagine that you could only change your weights by a fixed amount. Say you have .3 and you have to split that up and disperse it over your weights so as to increase your predictions as much as possible. (This isn't actually what happens, but it will help us identify which weights we would benefit the most from moving.)\n",
    "\n",
    "I need to increase weights of my model somewhere, I'll get the biggest bang for my buck if I increase weights in places where I'm already seeing high activation values -because they end up getting multiplied together before being passed to the sigmoid function. \n",
    "\n",
    "> \"Neurons that fire together, wire together\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_eyzItYIxgm"
   },
   "source": [
    "### Implement Backpropagation \n",
    "\n",
    "> *Assigning blame for bad predictions and delivering justice - repeatedly and a little bit at a time*\n",
    "\n",
    "What in our model could be causing our predictions to suck so bad? \n",
    "\n",
    "Well, we know that our inputs (X) and outputs (y) are correct, if they weren't then we would have bigger problems than understanding backpropagation.\n",
    "\n",
    "We also know that our activation function (sigmoid) is working correctly. It can't be blamed because it just does whatever we tell it to and transforms the data in a known way.\n",
    "\n",
    "So what are the potential culprits for these terrible predictions? The **weights** of our model. Here's the problem though. I have weights that exist in both layers of my model. How do I know if the weights in the first layer are to blame, or the second layer, or both? \n",
    "\n",
    "Lets investigate. And see if we can just eyeball what should be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "weights1\n [[0.73665387 0.81926386 0.88654768]\n [0.92386319 0.28768809 0.29744598]] \n---------\nhidden_sum\n [[1.41496577 0.833864   0.88847777]\n [0.75880862 0.43291467 0.46076366]\n [1.35256266 1.01105592 1.084845  ]] \n---------\nactivated_hidden\n [[0.80454799 0.69717133 0.70857594]\n [0.68109502 0.60656945 0.61319532]\n [0.79454828 0.73322674 0.74740976]] \n---------\nweights2\n [[0.43328179]\n [0.56557272]\n [0.83042626]] \n---------\nactivated_output\n [[0.79105842]\n [0.7590318 ]\n [0.79893077]] \n---------\n"
    },
    {
     "data": {
      "text/plain": "[None, None, None, None, None]"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes = ['weights1', 'hidden_sum', 'activated_hidden', 'weights2', 'activated_output']\n",
    "[print(i+'\\n', getattr(nn,i), '\\n'+'---'*3) for i in attributes if i[:2]!= '__'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16Ujj6vNYQyX"
   },
   "source": [
    "### Backpropagation by Hand (Not Recommended)\n",
    "\n",
    "Our model has 9 total weights (6 in the first layer, 3 in the last layer) that could be off.\n",
    "\n",
    "1) Calculate Error for a given each observation\n",
    "\n",
    "2) Does the error indicate that I'm overestimating or underestimating in my prediction?\n",
    "\n",
    "3) Look at final layer weights to get an idea for which weights are helping pass desireable signals and which are stifling desireable signals\n",
    "\n",
    "4) Also go to the previous layer and see what can be done to boost activations that are associated with helpful weights, and limit activations that are associated with unhelpful weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want activated that correspond to negative weights to be lower\n",
    "# And we want more higher activation for positivie weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16Ujj6vNYQyX",
    "toc-hr-collapsed": true
   },
   "source": [
    "### Update Weights Based on Gradient\n",
    "\n",
    "Repeat steps 1-4 for every observation in a given batch, and then given the network's cost function, calculate its gradient using calculus and update weights associated with the (negative) gradient of the cost function. \n",
    "\n",
    "Remember that we have 9 weights in our network therefore the gradient that comes from our gradient descent calculation will be the vector that takes us in the most downward direction along some function in 9-dimensional hyperspace.\n",
    "\n",
    "\\begin{align}\n",
    "C(w1, w2, w3, w4, w5, w6, w7, w8, w9)\n",
    "\\end{align}\n",
    "\n",
    "You should also know that with neural networks it is common to have gradients that are not convex (like what we saw when we applied gradient descent to linear regression). Due to the high complexity of these models and their nonlinearity, it is common for gradient descent to get stuck in a local minimum, but there are ways to combat this:\n",
    "\n",
    "1) Stochastic Gradient Descent\n",
    "\n",
    "2) More advanced Gradient-Descent-based \"Optimizers\" - See Stretch Goals on assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want activations that correspond to negative weights to be lower\n",
    "# and activations that correspond to positive weights to be higher\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        # 2x3 Matrix Array for the First Layer\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
    "       \n",
    "        # 3x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, x):\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weighted sum of inputs => hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "        \n",
    "    def backward(self, X,y,o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        \n",
    "        # Error in Output\n",
    "        self.o_error = y - o\n",
    "        \n",
    "        # Apply Derivative of Sigmoid to error\n",
    "        # How far off are we in relation to the Sigmoid f(x) of the output\n",
    "        # ^- aka hidden => output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        # z2 error\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        \n",
    "        # How much of that \"far off\" can explained by the input => hidden\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        # Adjustment to first set of weights (input => hidden)\n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        # Adjustment to second set of weights (hidden => output)\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "        \n",
    "\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Let's look at the shape of the Gradient Componets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()\n",
    "\n",
    "nn.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our Error Associated with Each Observation \n",
    "aka how wrong were we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.30592278],\n       [0.26011069],\n       [0.27918576]])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.o_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1st Gradient \n",
    "Simple interpretation - how much more sigmoid activation would have pushed us towards the right answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.06970072],\n       [0.05951115],\n       [0.0636707 ]])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.o_delta\n",
    "# How much more a sigmoid activation would push us in the right direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2nd Error\n",
    "Justice hasn't been served yet - tho. We still have neurons to blame. Let's go back another layer. \n",
    "\n",
    "__Discussion:__ Why is this shape different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.00443959, 0.01720128, 0.02404023],\n       [0.00379057, 0.01468662, 0.02052578],\n       [0.00405551, 0.01571315, 0.02196043]])"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.z2_error\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2nd Gradient\n",
    "For each observation, how much more sigmoid activation from this layer would have pushed us towards the right answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.00098887, 0.00380275, 0.00530655],\n       [0.00086694, 0.00334329, 0.00466576],\n       [0.00089662, 0.00348446, 0.00490846]])"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.z2_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Descent\n",
    "\n",
    "*Discussion:* Input to Hidden Weight Update\n",
    "- We multiply the gradient by the inputs. Why?\n",
    "- Why do we need to transpose the inputs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.00184485, 0.00713406, 0.01000141],\n       [0.00206826, 0.00798311, 0.01117095]])"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.dot(nn.z2_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discussion:* Hidden to Output Weight Update\n",
    "- Why is output the shape 3x1? \n",
    "- We multiply the gradient by the inputs. Why?\n",
    "- Why do we need to transpose the inputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.12869822],\n       [0.13063543],\n       [0.12971368]])"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.activated_hidden.T.dot(nn.o_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Network (for real this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---------EPOCH 1---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.92]\n [0.86]\n [0.89]]\nPredicted Output: \n [[0.78081792]\n [0.74720482]\n [0.77813508]]\nLoss: \n 0.014869388021409502\n+---------EPOCH 2---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.92]\n [0.86]\n [0.89]]\nPredicted Output: \n [[0.80264233]\n [0.76788285]\n [0.80016934]]\nLoss: \n 0.010109313688334376\n+---------EPOCH 3---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.92]\n [0.86]\n [0.89]]\nPredicted Output: \n [[0.81937211]\n [0.78400209]\n [0.81706784]]\nLoss: \n 0.007073585043874914\n+---------EPOCH 4---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.92]\n [0.86]\n [0.89]]\nPredicted Output: \n [[0.83249658]\n [0.79683337]\n [0.83032891]]\nLoss: \n 0.005069170236062095\n+---------EPOCH 5---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.92]\n [0.86]\n [0.89]]\nPredicted Output: \n [[0.84299114]\n [0.8072244 ]\n [0.84093501]]\nLoss: \n 0.0037076670151604805\n+---------EPOCH 1000---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.92]\n [0.86]\n [0.89]]\nPredicted Output: \n [[0.9036691 ]\n [0.86963169]\n [0.89833171]]\nLoss: \n 0.00014296174736129288\n+---------EPOCH 2000---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.92]\n [0.86]\n [0.89]]\nPredicted Output: \n [[0.90444   ]\n [0.8705239 ]\n [0.89681156]]\nLoss: \n 0.00013308786173418306\n+---------EPOCH 3000---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.92]\n [0.86]\n [0.89]]\nPredicted Output: \n [[0.90460119]\n [0.87138304]\n [0.89587949]]\nLoss: \n 0.00013375509044804063\n+---------EPOCH 4000---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.92]\n [0.86]\n [0.89]]\nPredicted Output: \n [[0.90428043]\n [0.87231289]\n [0.89533542]]\nLoss: \n 0.00014239291160657972\n+---------EPOCH 5000---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.92]\n [0.86]\n [0.89]]\nPredicted Output: \n [[0.90356655]\n [0.87336821]\n [0.89504474]]\nLoss: \n 0.00015807226460519408\n+---------EPOCH 6000---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.92]\n [0.86]\n [0.89]]\nPredicted Output: \n [[0.90254608]\n [0.87455338]\n [0.89491327]]\nLoss: \n 0.0001801934912675381\n+---------EPOCH 7000---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.92]\n [0.86]\n [0.89]]\nPredicted Output: \n [[0.90133325]\n [0.87581087]\n [0.89487459]]\nLoss: \n 0.00020739755356193878\n+---------EPOCH 8000---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.92]\n [0.86]\n [0.89]]\nPredicted Output: \n [[0.90008899]\n [0.8770093 ]\n [0.89488927]]\nLoss: \n 0.00023655653485045742\n+---------EPOCH 9000---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.92]\n [0.86]\n [0.89]]\nPredicted Output: \n [[0.89901285]\n [0.87795452]\n [0.89494819]]\nLoss: \n 0.00026243667520131777\n+---------EPOCH 10000---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.92]\n [0.86]\n [0.89]]\nPredicted Output: \n [[0.89828966]\n [0.87845691]\n [0.89506132]]\nLoss: \n 0.00027920450973868054\n"
    }
   ],
   "source": [
    "# Train my 'net\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "# Number of Epochs / Iterations\n",
    "for i in range(10000):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "In the module project, you will implement backpropagation inside a multi-layer perceptron (aka a feedforward neural network). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The What - Stochastic Gradient Descent calculates an approximation of the gradient over the entire dataset by reviewing the predictions of a random sample. \n",
    "\n",
    "The Why - *Speed*. Calculating the gradient over the entire dataset is extremely expensive computationally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZF7UE-KluPsX"
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "A true Stochastic GD-based implementation from [Welch Labs](https://www.youtube.com/watch?v=bxe2T-V8XRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        # Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        # Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        # Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        # Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        # Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        # Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    # Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        # Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        # Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        # Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        # Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        # Make empty list to store costs:\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = trainer(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Optimization terminated successfully.\n         Current function value: 0.000176\n         Iterations: 66\n         Function evaluations: 72\n         Gradient evaluations: 72\n"
    }
   ],
   "source": [
    "T.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Predicted Output: \n[[0.90558176]\n [0.87191511]\n [0.89126275]]\nLoss: \n0.00011715000120651323\n"
    }
   ],
   "source": [
    "print(\"Predicted Output: \\n\" + str(NN.forward(X))) \n",
    "print(\"Loss: \\n\" + str(np.mean(np.square(y - NN.forward(X))))) # mean sum squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 392.14375 262.19625\" width=\"392.14375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.19625 \r\nL 392.14375 262.19625 \r\nL 392.14375 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 50.14375 224.64 \r\nL 384.94375 224.64 \r\nL 384.94375 7.2 \r\nL 50.14375 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m024d263b6b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.361932\" xlink:href=\"#m024d263b6b\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(62.180682 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"112.187107\" xlink:href=\"#m024d263b6b\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 10 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(105.824607 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"159.012281\" xlink:href=\"#m024d263b6b\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(152.649781 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"205.837456\" xlink:href=\"#m024d263b6b\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 30 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(199.474956 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"252.662631\" xlink:href=\"#m024d263b6b\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 40 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(246.300131 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"299.487806\" xlink:href=\"#m024d263b6b\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 50 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(293.125306 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"346.312981\" xlink:href=\"#m024d263b6b\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 60 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(339.950481 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_8\">\r\n     <!-- Iterations -->\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 19.671875 72.90625 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-73\"/>\r\n      <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(193.730469 252.916562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-73\"/>\r\n      <use x=\"29.492188\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"68.701172\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"130.224609\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"171.337891\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"232.617188\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"271.826172\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"299.609375\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"360.791016\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"424.169922\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mbf4ab27edf\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#mbf4ab27edf\" y=\"215.005187\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.00 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 218.804406)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#mbf4ab27edf\" y=\"186.685544\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.02 -->\r\n      <g transform=\"translate(20.878125 190.484763)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#mbf4ab27edf\" y=\"158.365902\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.04 -->\r\n      <g transform=\"translate(20.878125 162.16512)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#mbf4ab27edf\" y=\"130.046259\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.06 -->\r\n      <g transform=\"translate(20.878125 133.845478)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#mbf4ab27edf\" y=\"101.726616\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.08 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 105.525835)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#mbf4ab27edf\" y=\"73.406973\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0.10 -->\r\n      <g transform=\"translate(20.878125 77.206192)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#mbf4ab27edf\" y=\"45.087331\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 0.12 -->\r\n      <g transform=\"translate(20.878125 48.88655)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#mbf4ab27edf\" y=\"16.767688\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 0.14 -->\r\n      <g transform=\"translate(20.878125 20.566907)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_17\">\r\n     <!-- Cost -->\r\n     <defs>\r\n      <path d=\"M 64.40625 67.28125 \r\nL 64.40625 56.890625 \r\nQ 59.421875 61.53125 53.78125 63.8125 \r\nQ 48.140625 66.109375 41.796875 66.109375 \r\nQ 29.296875 66.109375 22.65625 58.46875 \r\nQ 16.015625 50.828125 16.015625 36.375 \r\nQ 16.015625 21.96875 22.65625 14.328125 \r\nQ 29.296875 6.6875 41.796875 6.6875 \r\nQ 48.140625 6.6875 53.78125 8.984375 \r\nQ 59.421875 11.28125 64.40625 15.921875 \r\nL 64.40625 5.609375 \r\nQ 59.234375 2.09375 53.4375 0.328125 \r\nQ 47.65625 -1.421875 41.21875 -1.421875 \r\nQ 24.65625 -1.421875 15.125 8.703125 \r\nQ 5.609375 18.84375 5.609375 36.375 \r\nQ 5.609375 53.953125 15.125 64.078125 \r\nQ 24.65625 74.21875 41.21875 74.21875 \r\nQ 47.75 74.21875 53.53125 72.484375 \r\nQ 59.328125 70.75 64.40625 67.28125 \r\nz\r\n\" id=\"DejaVuSans-67\"/>\r\n     </defs>\r\n     <g transform=\"translate(14.798438 127.035625)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-67\"/>\r\n      <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"131.005859\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"183.105469\" xlink:href=\"#DejaVuSans-116\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#p0f465cd748)\" d=\"M 65.361932 17.083636 \r\nL 70.044449 181.635817 \r\nL 74.726967 206.717573 \r\nL 79.409484 212.702545 \r\nL 84.092002 213.970722 \r\nL 88.774519 214.128622 \r\nL 93.457037 214.136508 \r\nL 98.139554 214.137178 \r\nL 102.822072 214.138409 \r\nL 107.504589 214.140717 \r\nL 112.187107 214.145052 \r\nL 116.869624 214.153125 \r\nL 121.552142 214.167902 \r\nL 126.234659 214.19432 \r\nL 130.917177 214.240793 \r\nL 135.599694 214.32515 \r\nL 140.282212 214.638793 \r\nL 144.964729 214.654654 \r\nL 149.647247 214.655727 \r\nL 154.329764 214.65679 \r\nL 159.012281 214.6588 \r\nL 163.694799 214.662434 \r\nL 168.377316 214.66824 \r\nL 173.059834 214.673119 \r\nL 177.742351 214.674534 \r\nL 182.424869 214.674943 \r\nL 187.107386 214.675027 \r\nL 191.789904 214.675072 \r\nL 196.472421 214.67515 \r\nL 201.154939 214.675291 \r\nL 205.837456 214.675548 \r\nL 210.519974 214.67602 \r\nL 215.202491 214.676877 \r\nL 219.885009 214.678406 \r\nL 224.567526 214.681042 \r\nL 229.250044 214.685365 \r\nL 233.932561 214.692003 \r\nL 238.615079 214.699955 \r\nL 243.297596 214.704358 \r\nL 247.980114 214.712361 \r\nL 252.662631 214.726734 \r\nL 257.345149 214.739718 \r\nL 262.027666 214.743186 \r\nL 266.710184 214.745681 \r\nL 271.392701 214.746876 \r\nL 276.075219 214.748869 \r\nL 280.757736 214.750203 \r\nL 285.440253 214.751599 \r\nL 290.122771 214.752748 \r\nL 294.805288 214.753532 \r\nL 299.487806 214.753589 \r\nL 304.170323 214.753593 \r\nL 308.852841 214.753593 \r\nL 313.535358 214.753594 \r\nL 318.217876 214.753601 \r\nL 322.900393 214.753613 \r\nL 327.582911 214.753633 \r\nL 332.265428 214.753672 \r\nL 336.947946 214.753744 \r\nL 341.630463 214.753876 \r\nL 346.312981 214.75411 \r\nL 350.995498 214.754501 \r\nL 355.678016 214.755066 \r\nL 360.360533 214.755538 \r\nL 365.043051 214.756076 \r\nL 369.725568 214.756364 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 50.14375 224.64 \r\nL 50.14375 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 384.94375 224.64 \r\nL 384.94375 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 50.14375 224.64 \r\nL 384.94375 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 50.14375 7.2 \r\nL 384.94375 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p0f465cd748\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(T.J)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "This is a reference implementation for you to explore. You will not be expected to apply it to today's module project. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NN)",
   "language": "python",
   "name": "python-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}