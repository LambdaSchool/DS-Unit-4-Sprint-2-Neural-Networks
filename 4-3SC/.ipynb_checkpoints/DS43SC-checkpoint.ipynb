{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6SKlgYrpcym"
   },
   "source": [
    "# Neural Networks Sprint Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrEbRrjVphPM"
   },
   "source": [
    "## 1) Define the following terms:\n",
    "\n",
    "- Neuron \n",
    "- Input Layer\n",
    "- Hidden Layer\n",
    "- Output Layer\n",
    "- Activation\n",
    "- Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5EksLqnp4oB"
   },
   "source": [
    "## Neuron\n",
    "![Wikipedia Neuron Diagram](http://www.ryanleeallred.com/wp-content/uploads/2019/03/Screen-Shot-2019-03-31-at-10.19.43-PM.png)\n",
    "\n",
    "Neural Networks aren't exactly a new technology, but recent breakthroughs have revitalized the area. For example the \"Perceptron\" -one of the basic building blocks of the technology- was invented in 1957. \n",
    "\n",
    "Artificial Neural Networks are a computational model that was inspired by how neural networks in the brain process information. In the brain electrochemical signals flow from earlier neurons through the dendrites of the cell toward the cell body. If the received signals surpass a certain threshold with a given timing then the neuron fires sending a large spike of energy down the axon and through the axon terminals to other neurons down the line. \n",
    "\n",
    "In Artificial Neural Networks the neurons or \"nodes\" are similar in that they receive inputs and pass on their signal to the next layer of nodes if a certain threshold is reached, but that's about where the similarities end. Remember that ANNs are not brains. Don't fall into the common trap of assuming that if an Artificial Neural Network has as many nodes as the human brain that it will be just as powerful or just as capable. The goal with ANNs is not to create a realistic model of the brain but to craft robust algorithms and data structures that can model the complex relationships found in data.\n",
    "\n",
    "## Input Layer\n",
    "The Input Layer is what receives input from our dataset. Sometimes it is called the visible layer because it's the only part that is exposed to our data and that our data interacts with directly. Typically node maps are drawn with one input node for each of the different inputs/features/columns of our dataset that will be passed to the network.\n",
    "\n",
    "## Hidden Layer\n",
    "Layers after the input layer are called Hidden Layers. This is because they cannot be accessed except through the input layer. They're inside of the network and they perform their functions, but we don't directly interact with them. The simplest possible network is to have a single neuron in the hidden layer that just outputs the value. \"Deep Learning\" apart from being a big buzzword simply means that we are using a Neural Network that has multiple hidden layers. \"Deep Learning\" is a big part of the renewed hype around ANNs because it allows networks that are structured in specific ways to accomplish tasks that were previously out of reach (image recognition for example).\n",
    "\n",
    "## Output Layer\n",
    "The final layer is called the Output Layer. The purpose of the output layer is to output a vector of values that is in a format that is suitable for the type of problem that we're trying to address. Typically the output value is modified by an \"activation function\" to transform it into a format that makes sense for our context, here's a couple of examples:\n",
    "\n",
    "NNs applied to a regression problem might have a single output node with no activation function because what we want is an unbounded continuous value.\n",
    "\n",
    "NNS applied to a binary classification problem might use a sigmoid function as its activation function in order to squishify values down to represent a probability. Outputs in this case would represent the probability of predicting the primary class of interest. We can turn this into a class-specific prediction by rounding the outputted sigmoid probability up to 1 or down to 0.\n",
    "\n",
    "NNS applied to multiclass classification problems might have multiple output nodes in the output layer, one for each class that we're trying to predict.\n",
    "\n",
    "## Activation\n",
    "Same as Transfer Function.  In Neural Networks, each node has an activation function. Each node in a given layer typically has the same activation function. These activation functions are the biggest piece of neural networks that have been inspired by actual biology. The activation function decides whether a cell \"fires\" or not. Sometimes it is said that the cell is \"activated\" or not. In Artificial Neural Networks activation functions decide how much signal to pass onto the next layer. This is why they are sometimes referred to as transfer functions because they determine how much signal is transferred to the next layer.\n",
    "\n",
    "Common Activation Functions:\n",
    "\n",
    "![Activation Functions](http://www.snee.com/bobdc.blog/img/activationfunctions.png)\n",
    "\n",
    "## Backpropagation\n",
    "Backpropagation is a method used in artificial neural networks to calculate a gradient that is needed in the calculation of the weights to be used in the network. Backpropagation is shorthand for \"the backward propagation of errors,\" since an error is computed at the output and distributed backwards throughout the network’s layers. It is commonly used to train deep neural networks.\n",
    "\n",
    "Backpropagation is a generalization of the delta rule to multi-layered feedforward networks, made possible by using the chain rule to iteratively compute gradients for each layer. It is closely related to the Gauss–Newton algorithm and is part of continuing research in neural backpropagation.\n",
    "\n",
    "Backpropagation is a special case of a more general technique called automatic differentiation. In the context of learning, backpropagation is commonly used by the gradient descent optimization algorithm to adjust the weight of neurons by calculating the gradient of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ri_gRA2Jp728"
   },
   "source": [
    "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 1  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold, \\\n",
    "                                    cross_val_score, \\\n",
    "                                    GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ig6ZTH8tpQ19"
   },
   "outputs": [],
   "source": [
    "class perceptron:\n",
    "    import numpy as np\n",
    "    from random import seed\n",
    "    from random import randrange\n",
    "    \n",
    "    # Accept training and define data size\n",
    "    \n",
    "    def __init__(self, data, inputs, correct):\n",
    "        self.data = data\n",
    "        self.inputs = inputs\n",
    "        self.correct = correct\n",
    "    \n",
    "    # put entered dat into function\n",
    "    \n",
    "    def train(self):\n",
    "        npdata = np.array (self.data)\n",
    "        npcorr = np.array (self.correct)\n",
    "    \n",
    "        # Define random weights for data size specified\n",
    "\n",
    "        weights = 2 * np.random.random((self.inputs,1)) - 1\n",
    "    \n",
    "        # Calculate weighted sum of inputs and weight\n",
    "\n",
    "        weighted_sum = np.dot(npdata, weights)\n",
    "    \n",
    "        # Sigmoid activation function for updating weights\n",
    "\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "        # Sigmoid derivative function for updating weights\n",
    "\n",
    "        def sigmoid_derivative(x):\n",
    "            return sigmoid(x) * (1 - sigmoid(x))       \n",
    "    \n",
    "        # Output the activated value for the end of 1 training epoch\n",
    "\n",
    "        activated_output = sigmoid(weighted_sum)\n",
    "    \n",
    "        # Take difference of output and true values to calculate error\n",
    "\n",
    "        error = self.correct - activated_output\n",
    "\n",
    "        # Gradient Descent / Backpropagation \n",
    "\n",
    "        adjustments = error * sigmoid_derivative(activated_output) \n",
    "\n",
    "        # Update weights\n",
    "    \n",
    "        weights += np.dot (npdata.T, adjustments)\n",
    "        \n",
    "        # Print results\n",
    "    \n",
    "        print('Optimized weights after training: ')\n",
    "        print(weights)\n",
    "\n",
    "        print(\"Output After Training:\")\n",
    "        print(activated_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized weights after training: \n",
      "[[ 0.43174544]\n",
      " [-0.83131535]\n",
      " [ 0.51515125]]\n",
      "Output After Training:\n",
      "[[0.65144807]\n",
      " [0.80417764]\n",
      " [0.5250856 ]\n",
      " [0.70840055]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "adata = [[1,1,1],[1,0,1],[0,1,1],[0,0,1]]\n",
    "ainputs = 3\n",
    "acorrect = [[1],[0],[0],[0]]\n",
    "pc = perceptron(adata, ainputs, acorrect)\n",
    "\n",
    "pc.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86HyRi8Osr3U"
   },
   "source": [
    "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
    "- Your network must have one hidden layer. \n",
    "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "- Train your model on the Heart Disease dataset from UCI:\n",
    "\n",
    "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
    "\n",
    "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n",
      "age           int64\n",
      "sex           int64\n",
      "cp            int64\n",
      "trestbps      int64\n",
      "chol          int64\n",
      "fbs           int64\n",
      "restecg       int64\n",
      "thalach       int64\n",
      "exang         int64\n",
      "oldpeak     float64\n",
      "slope         int64\n",
      "ca            int64\n",
      "thal          int64\n",
      "target        int64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Import and examin data\n",
    "url = 'https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv'\n",
    "df = pd.read_csv(url)\n",
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalach     0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n",
    "# Nothing to clean!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         float64\n",
       "sex         float64\n",
       "cp          float64\n",
       "trestbps    float64\n",
       "chol        float64\n",
       "fbs         float64\n",
       "restecg     float64\n",
       "thalach     float64\n",
       "exang       float64\n",
       "oldpeak     float64\n",
       "slope       float64\n",
       "ca          float64\n",
       "thal        float64\n",
       "target      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert all columns to float64 and check\n",
    "\n",
    "for col in df.columns:\n",
    "  if df[col].dtype == 'int64':\n",
    "    df[col] = df[col].astype('float')\n",
    "    \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((303, 13), (303,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split into X and y\n",
    "X = df.drop(['target'], axis=1)\n",
    "y = df['target']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 101\n",
    "np.random.seed(seed)\n",
    "\n",
    "class Neural_Network(object):\n",
    "    def __init__(self, num_input_nodes, num_hidden_nodes, num_output_nodes):\n",
    "        self.num_input_nodes  = num_input_nodes   # 13 input nodes\n",
    "        self.num_hidden_nodes = num_hidden_nodes  # 8 hidden nodes\n",
    "        self.num_output_nodes = num_output_nodes  # 1 output node\n",
    "        \n",
    "        self.L1_weights = np.random.randn(num_input_nodes, num_hidden_nodes)\n",
    "        self.L2_weights = np.random.randn(num_hidden_nodes, num_output_nodes)\n",
    "        \n",
    "    def feed_forward(self, X):\n",
    "        self.hidden_sum = np.dot(X, self.L1_weights)\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.L2_weights)\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        return self.activated_output\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def sigmoid_prime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "\n",
    "        self.o_error = y - o\n",
    "        self.o_delta = self.o_error * self.sigmoid_prime(o)\n",
    "        \n",
    "        self.z2_error = self.o_delta.dot(self.L2_weights.T)\n",
    "        self.z2_delta = self.z2_error * self.sigmoid_prime(self.activated_hidden)\n",
    "        \n",
    "        # Adjust weights\n",
    "        self.L1_weights += X.T.dot(self.z2_delta)\n",
    "        self.L2_weights += self.activated_hidden.T.dot(self.o_delta)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X = X.values\n",
    "y = y.values\n",
    "y_out = []\n",
    "for i in range(len(y)):\n",
    "    y_out.append([y[i]])\n",
    "y = y_out.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------- EPOCH 1 -----------+ Loss: 0.2501642263758063\n",
      "+---------- EPOCH 2 -----------+ Loss: 0.4023410425726336\n",
      "+---------- EPOCH 3 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 50 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 200 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 500 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1000 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1050 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1200 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1500 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2000 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2050 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2200 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2500 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3000 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3050 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3200 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3500 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4000 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4050 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4200 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4500 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5000 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5050 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5200 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5500 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6000 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6050 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6200 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6500 -----------+ Loss: 0.5445544554455445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------- EPOCH 6550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7000 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7050 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7200 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7500 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8000 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8050 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8200 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8500 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9000 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9050 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9200 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9500 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 10000 -----------+ Loss: 0.5445544554455445\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network(13, 8, 1)\n",
    "for i in range(10000): # trains the NN 1,000 times\n",
    "  if i+1 in [1,2,3,4,5] or (i+1) % 50 == 0:\n",
    "    print('+---------- EPOCH', i+1, '-----------+', end=' ')\n",
    "#     print(\"Input: \\n\", X) \n",
    "#     print(\"Actual Output: \\n\", y)  \n",
    "#     print(\"Predicted Output: \\n\" + str(NN.feed_forward(X))) \n",
    "    print(\"Loss: \" + str(np.mean(np.square(y - NN.feed_forward(X))))) # mean sum squared loss\n",
    "#     print(\"\\n\")\n",
    "  NN.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGT1oRzXw3H9"
   },
   "source": [
    "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
    "\n",
    "- Use the Heart Disease Dataset (binary classification)\n",
    "- Use an appropriate loss function for a binary classification task\n",
    "- Use an appropriate activation function on the final layer of your network. \n",
    "- Train your model using verbose output for ease of grading.\n",
    "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
    "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rick1270/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWw4IYxLxKwH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rick1270/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/rick1270/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/150\n",
      "242/242 [==============================] - 0s 738us/step - loss: 6.9056 - acc: 0.5455\n",
      "Epoch 2/150\n",
      "242/242 [==============================] - 0s 36us/step - loss: 6.1594 - acc: 0.5413\n",
      "Epoch 3/150\n",
      "242/242 [==============================] - 0s 22us/step - loss: 4.9671 - acc: 0.5744\n",
      "Epoch 4/150\n",
      "242/242 [==============================] - 0s 87us/step - loss: 3.9790 - acc: 0.5124\n",
      "Epoch 5/150\n",
      "242/242 [==============================] - 0s 34us/step - loss: 3.3023 - acc: 0.5124\n",
      "Epoch 6/150\n",
      "242/242 [==============================] - 0s 28us/step - loss: 2.9966 - acc: 0.4917\n",
      "Epoch 7/150\n",
      "242/242 [==============================] - 0s 42us/step - loss: 2.5233 - acc: 0.4917\n",
      "Epoch 8/150\n",
      "242/242 [==============================] - 0s 29us/step - loss: 2.0833 - acc: 0.5165\n",
      "Epoch 9/150\n",
      "242/242 [==============================] - 0s 25us/step - loss: 1.6487 - acc: 0.5744\n",
      "Epoch 10/150\n",
      "242/242 [==============================] - 0s 26us/step - loss: 1.2242 - acc: 0.6074\n",
      "Epoch 11/150\n",
      "242/242 [==============================] - 0s 26us/step - loss: 0.8604 - acc: 0.6612\n",
      "Epoch 12/150\n",
      "242/242 [==============================] - 0s 26us/step - loss: 0.7841 - acc: 0.6901\n",
      "Epoch 13/150\n",
      "242/242 [==============================] - 0s 28us/step - loss: 0.7413 - acc: 0.6860\n",
      "Epoch 14/150\n",
      "242/242 [==============================] - 0s 26us/step - loss: 0.7222 - acc: 0.6818\n",
      "Epoch 15/150\n",
      "242/242 [==============================] - 0s 26us/step - loss: 0.7071 - acc: 0.6860\n",
      "Epoch 16/150\n",
      "242/242 [==============================] - 0s 26us/step - loss: 0.6886 - acc: 0.6736\n",
      "Epoch 17/150\n",
      "242/242 [==============================] - 0s 27us/step - loss: 0.6767 - acc: 0.6777\n",
      "Epoch 18/150\n",
      "242/242 [==============================] - 0s 26us/step - loss: 0.6713 - acc: 0.6777\n",
      "Epoch 19/150\n",
      "242/242 [==============================] - 0s 24us/step - loss: 0.6583 - acc: 0.6818\n",
      "Epoch 20/150\n",
      "242/242 [==============================] - 0s 22us/step - loss: 0.6471 - acc: 0.6736\n",
      "Epoch 21/150\n",
      "242/242 [==============================] - 0s 21us/step - loss: 0.6382 - acc: 0.6777\n",
      "Epoch 22/150\n",
      "242/242 [==============================] - 0s 23us/step - loss: 0.6314 - acc: 0.6736\n",
      "Epoch 23/150\n",
      "242/242 [==============================] - 0s 21us/step - loss: 0.6224 - acc: 0.6777\n",
      "Epoch 24/150\n",
      "242/242 [==============================] - 0s 22us/step - loss: 0.6175 - acc: 0.6777\n",
      "Epoch 25/150\n",
      "242/242 [==============================] - 0s 21us/step - loss: 0.6122 - acc: 0.6818\n",
      "Epoch 26/150\n",
      "242/242 [==============================] - 0s 21us/step - loss: 0.6103 - acc: 0.6736\n",
      "Epoch 27/150\n",
      "242/242 [==============================] - 0s 23us/step - loss: 0.6072 - acc: 0.6983\n",
      "Epoch 28/150\n",
      "242/242 [==============================] - 0s 22us/step - loss: 0.6007 - acc: 0.6901\n",
      "Epoch 29/150\n",
      "242/242 [==============================] - 0s 21us/step - loss: 0.5996 - acc: 0.6694\n",
      "Epoch 30/150\n",
      "242/242 [==============================] - 0s 21us/step - loss: 0.5945 - acc: 0.6901\n",
      "Epoch 31/150\n",
      "242/242 [==============================] - 0s 21us/step - loss: 0.5920 - acc: 0.7025\n",
      "Epoch 32/150\n",
      "242/242 [==============================] - 0s 23us/step - loss: 0.5914 - acc: 0.6983\n",
      "Epoch 33/150\n",
      "242/242 [==============================] - 0s 26us/step - loss: 0.5932 - acc: 0.6983\n",
      "Epoch 34/150\n",
      "242/242 [==============================] - 0s 22us/step - loss: 0.5933 - acc: 0.6901\n",
      "Epoch 35/150\n",
      "242/242 [==============================] - 0s 25us/step - loss: 0.5873 - acc: 0.7107\n",
      "Epoch 36/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5857 - acc: 0.7149\n",
      "Epoch 37/150\n",
      "242/242 [==============================] - 0s 67us/step - loss: 0.5847 - acc: 0.6818\n",
      "Epoch 38/150\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.5829 - acc: 0.7025\n",
      "Epoch 39/150\n",
      "242/242 [==============================] - 0s 30us/step - loss: 0.5840 - acc: 0.7025\n",
      "Epoch 40/150\n",
      "242/242 [==============================] - 0s 30us/step - loss: 0.5786 - acc: 0.7066\n",
      "Epoch 41/150\n",
      "242/242 [==============================] - 0s 29us/step - loss: 0.5811 - acc: 0.7066\n",
      "Epoch 42/150\n",
      "242/242 [==============================] - 0s 27us/step - loss: 0.5778 - acc: 0.7149\n",
      "Epoch 43/150\n",
      "242/242 [==============================] - 0s 27us/step - loss: 0.5819 - acc: 0.6901\n",
      "Epoch 44/150\n",
      "242/242 [==============================] - 0s 24us/step - loss: 0.5806 - acc: 0.7149\n",
      "Epoch 45/150\n",
      "242/242 [==============================] - 0s 31us/step - loss: 0.5752 - acc: 0.7066\n",
      "Epoch 46/150\n",
      "242/242 [==============================] - 0s 21us/step - loss: 0.5769 - acc: 0.7025\n",
      "Epoch 47/150\n",
      "242/242 [==============================] - 0s 22us/step - loss: 0.5755 - acc: 0.7107\n",
      "Epoch 48/150\n",
      "242/242 [==============================] - 0s 25us/step - loss: 0.5765 - acc: 0.7149\n",
      "Epoch 49/150\n",
      "242/242 [==============================] - 0s 25us/step - loss: 0.5768 - acc: 0.7025\n",
      "Epoch 50/150\n",
      "242/242 [==============================] - 0s 22us/step - loss: 0.5771 - acc: 0.7149\n",
      "Epoch 51/150\n",
      "242/242 [==============================] - 0s 21us/step - loss: 0.5728 - acc: 0.7107\n",
      "Epoch 52/150\n",
      "242/242 [==============================] - 0s 135us/step - loss: 0.5720 - acc: 0.7149\n",
      "Epoch 53/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.5751 - acc: 0.7231\n",
      "Epoch 54/150\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.5696 - acc: 0.7149\n",
      "Epoch 55/150\n",
      "242/242 [==============================] - 0s 36us/step - loss: 0.5694 - acc: 0.7190\n",
      "Epoch 56/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5683 - acc: 0.7149\n",
      "Epoch 57/150\n",
      "242/242 [==============================] - 0s 34us/step - loss: 0.5685 - acc: 0.7190\n",
      "Epoch 58/150\n",
      "242/242 [==============================] - 0s 41us/step - loss: 0.5661 - acc: 0.7149\n",
      "Epoch 59/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5679 - acc: 0.7149\n",
      "Epoch 60/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5675 - acc: 0.7149\n",
      "Epoch 61/150\n",
      "242/242 [==============================] - 0s 31us/step - loss: 0.5654 - acc: 0.7149\n",
      "Epoch 62/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5649 - acc: 0.7273\n",
      "Epoch 63/150\n",
      "242/242 [==============================] - 0s 31us/step - loss: 0.5652 - acc: 0.7273\n",
      "Epoch 64/150\n",
      "242/242 [==============================] - 0s 37us/step - loss: 0.5628 - acc: 0.7190\n",
      "Epoch 65/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.5650 - acc: 0.7107\n",
      "Epoch 66/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5613 - acc: 0.7149\n",
      "Epoch 67/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5614 - acc: 0.7107\n",
      "Epoch 68/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5592 - acc: 0.7190\n",
      "Epoch 69/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5646 - acc: 0.7190\n",
      "Epoch 70/150\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.5628 - acc: 0.7273\n",
      "Epoch 71/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5586 - acc: 0.7231\n",
      "Epoch 72/150\n",
      "242/242 [==============================] - 0s 31us/step - loss: 0.5645 - acc: 0.7149\n",
      "Epoch 73/150\n",
      "242/242 [==============================] - 0s 31us/step - loss: 0.5599 - acc: 0.7066\n",
      "Epoch 74/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5553 - acc: 0.7231\n",
      "Epoch 75/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5689 - acc: 0.7355\n",
      "Epoch 76/150\n",
      "242/242 [==============================] - 0s 31us/step - loss: 0.5572 - acc: 0.7066\n",
      "Epoch 77/150\n",
      "242/242 [==============================] - 0s 37us/step - loss: 0.5599 - acc: 0.7149\n",
      "Epoch 78/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 33us/step - loss: 0.5637 - acc: 0.7355\n",
      "Epoch 79/150\n",
      "242/242 [==============================] - 0s 34us/step - loss: 0.5618 - acc: 0.6983\n",
      "Epoch 80/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5521 - acc: 0.7190\n",
      "Epoch 81/150\n",
      "242/242 [==============================] - 0s 37us/step - loss: 0.5561 - acc: 0.7231\n",
      "Epoch 82/150\n",
      "242/242 [==============================] - 0s 31us/step - loss: 0.5592 - acc: 0.6983\n",
      "Epoch 83/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5712 - acc: 0.7190\n",
      "Epoch 84/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5812 - acc: 0.6983\n",
      "Epoch 85/150\n",
      "242/242 [==============================] - 0s 66us/step - loss: 0.5582 - acc: 0.7314\n",
      "Epoch 86/150\n",
      "242/242 [==============================] - 0s 36us/step - loss: 0.5601 - acc: 0.7231\n",
      "Epoch 87/150\n",
      "242/242 [==============================] - 0s 46us/step - loss: 0.5498 - acc: 0.7190\n",
      "Epoch 88/150\n",
      "242/242 [==============================] - 0s 37us/step - loss: 0.5585 - acc: 0.7273\n",
      "Epoch 89/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5523 - acc: 0.7355\n",
      "Epoch 90/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5471 - acc: 0.7397\n",
      "Epoch 91/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5504 - acc: 0.7066\n",
      "Epoch 92/150\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.5550 - acc: 0.7355\n",
      "Epoch 93/150\n",
      "242/242 [==============================] - 0s 36us/step - loss: 0.5467 - acc: 0.7149\n",
      "Epoch 94/150\n",
      "242/242 [==============================] - 0s 31us/step - loss: 0.5505 - acc: 0.7273\n",
      "Epoch 95/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5456 - acc: 0.7314\n",
      "Epoch 96/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5453 - acc: 0.7231\n",
      "Epoch 97/150\n",
      "242/242 [==============================] - 0s 31us/step - loss: 0.5437 - acc: 0.7273\n",
      "Epoch 98/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5461 - acc: 0.7231\n",
      "Epoch 99/150\n",
      "242/242 [==============================] - 0s 34us/step - loss: 0.5442 - acc: 0.7355\n",
      "Epoch 100/150\n",
      "242/242 [==============================] - 0s 34us/step - loss: 0.5434 - acc: 0.7273\n",
      "Epoch 101/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5414 - acc: 0.7190\n",
      "Epoch 102/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5432 - acc: 0.7521\n",
      "Epoch 103/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5663 - acc: 0.6777\n",
      "Epoch 104/150\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.5383 - acc: 0.7355\n",
      "Epoch 105/150\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.5439 - acc: 0.7438\n",
      "Epoch 106/150\n",
      "242/242 [==============================] - 0s 34us/step - loss: 0.5497 - acc: 0.7066\n",
      "Epoch 107/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5322 - acc: 0.7355\n",
      "Epoch 108/150\n",
      "242/242 [==============================] - 0s 31us/step - loss: 0.5455 - acc: 0.7355\n",
      "Epoch 109/150\n",
      "242/242 [==============================] - 0s 31us/step - loss: 0.5349 - acc: 0.7355\n",
      "Epoch 110/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5359 - acc: 0.7397\n",
      "Epoch 111/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5364 - acc: 0.7190\n",
      "Epoch 112/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5467 - acc: 0.7521\n",
      "Epoch 113/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5383 - acc: 0.7231\n",
      "Epoch 114/150\n",
      "242/242 [==============================] - 0s 34us/step - loss: 0.5435 - acc: 0.7355\n",
      "Epoch 115/150\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.5316 - acc: 0.7314\n",
      "Epoch 116/150\n",
      "242/242 [==============================] - 0s 31us/step - loss: 0.5399 - acc: 0.7107\n",
      "Epoch 117/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5310 - acc: 0.7355\n",
      "Epoch 118/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5331 - acc: 0.7231\n",
      "Epoch 119/150\n",
      "242/242 [==============================] - 0s 34us/step - loss: 0.5421 - acc: 0.7521\n",
      "Epoch 120/150\n",
      "242/242 [==============================] - 0s 36us/step - loss: 0.5347 - acc: 0.7355\n",
      "Epoch 121/150\n",
      "242/242 [==============================] - 0s 37us/step - loss: 0.5333 - acc: 0.7314\n",
      "Epoch 122/150\n",
      "242/242 [==============================] - 0s 34us/step - loss: 0.5305 - acc: 0.7438\n",
      "Epoch 123/150\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.5440 - acc: 0.7438\n",
      "Epoch 124/150\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.5412 - acc: 0.7231\n",
      "Epoch 125/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5294 - acc: 0.7479\n",
      "Epoch 126/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5305 - acc: 0.7521\n",
      "Epoch 127/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5297 - acc: 0.7149\n",
      "Epoch 128/150\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.5275 - acc: 0.7438\n",
      "Epoch 129/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5224 - acc: 0.7521\n",
      "Epoch 130/150\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.5282 - acc: 0.7273\n",
      "Epoch 131/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5219 - acc: 0.7314\n",
      "Epoch 132/150\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.5319 - acc: 0.7438\n",
      "Epoch 133/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5171 - acc: 0.7314\n",
      "Epoch 134/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5334 - acc: 0.7107\n",
      "Epoch 135/150\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.5341 - acc: 0.7397\n",
      "Epoch 136/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5186 - acc: 0.7355\n",
      "Epoch 137/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5209 - acc: 0.7314\n",
      "Epoch 138/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5188 - acc: 0.7479\n",
      "Epoch 139/150\n",
      "242/242 [==============================] - 0s 34us/step - loss: 0.5176 - acc: 0.7438\n",
      "Epoch 140/150\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.5175 - acc: 0.7562\n",
      "Epoch 141/150\n",
      "242/242 [==============================] - 0s 37us/step - loss: 0.5180 - acc: 0.7521\n",
      "Epoch 142/150\n",
      "242/242 [==============================] - 0s 31us/step - loss: 0.5237 - acc: 0.7355\n",
      "Epoch 143/150\n",
      "242/242 [==============================] - 0s 31us/step - loss: 0.5133 - acc: 0.7562\n",
      "Epoch 144/150\n",
      "242/242 [==============================] - 0s 36us/step - loss: 0.5217 - acc: 0.7438\n",
      "Epoch 145/150\n",
      "242/242 [==============================] - 0s 36us/step - loss: 0.5115 - acc: 0.7562\n",
      "Epoch 146/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5154 - acc: 0.7603\n",
      "Epoch 147/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.5125 - acc: 0.7397\n",
      "Epoch 148/150\n",
      "242/242 [==============================] - 0s 31us/step - loss: 0.5112 - acc: 0.7438\n",
      "Epoch 149/150\n",
      "242/242 [==============================] - 0s 31us/step - loss: 0.5113 - acc: 0.7521\n",
      "Epoch 150/150\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.5097 - acc: 0.7397\n",
      "61/61 [==============================] - 0s 463us/step\n",
      "Epoch 1/150\n",
      "242/242 [==============================] - 0s 887us/step - loss: 4.8734 - acc: 0.4587\n",
      "Epoch 2/150\n",
      "242/242 [==============================] - 0s 37us/step - loss: 1.8631 - acc: 0.6157\n",
      "Epoch 3/150\n",
      "242/242 [==============================] - 0s 40us/step - loss: 1.6979 - acc: 0.6364\n",
      "Epoch 4/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 1.4597 - acc: 0.6488\n",
      "Epoch 5/150\n",
      "242/242 [==============================] - 0s 38us/step - loss: 1.1540 - acc: 0.6570\n",
      "Epoch 6/150\n",
      "242/242 [==============================] - 0s 29us/step - loss: 1.0103 - acc: 0.6736\n",
      "Epoch 7/150\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.9456 - acc: 0.6818\n",
      "Epoch 8/150\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.8334 - acc: 0.6860\n",
      "Epoch 9/150\n",
      "242/242 [==============================] - 0s 43us/step - loss: 0.7859 - acc: 0.6942\n",
      "Epoch 10/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 37us/step - loss: 0.7594 - acc: 0.6983\n",
      "Epoch 11/150\n",
      "242/242 [==============================] - 0s 30us/step - loss: 0.7218 - acc: 0.7231\n",
      "Epoch 12/150\n",
      "242/242 [==============================] - 0s 51us/step - loss: 0.7073 - acc: 0.7066\n",
      "Epoch 13/150\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.6984 - acc: 0.6942\n",
      "Epoch 14/150\n",
      "242/242 [==============================] - 0s 40us/step - loss: 0.6984 - acc: 0.7107\n",
      "Epoch 15/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.6753 - acc: 0.7066\n",
      "Epoch 16/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.6766 - acc: 0.7025\n",
      "Epoch 17/150\n",
      "242/242 [==============================] - 0s 39us/step - loss: 0.6614 - acc: 0.7107\n",
      "Epoch 18/150\n",
      "242/242 [==============================] - 0s 37us/step - loss: 0.6593 - acc: 0.7231\n",
      "Epoch 19/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.6533 - acc: 0.7107\n",
      "Epoch 20/150\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.6492 - acc: 0.7025\n",
      "Epoch 21/150\n",
      "242/242 [==============================] - 0s 42us/step - loss: 0.6489 - acc: 0.7149\n",
      "Epoch 22/150\n",
      "242/242 [==============================] - 0s 51us/step - loss: 0.6373 - acc: 0.7149\n",
      "Epoch 23/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.6310 - acc: 0.7190\n",
      "Epoch 24/150\n",
      "242/242 [==============================] - 0s 43us/step - loss: 0.6198 - acc: 0.7314\n",
      "Epoch 25/150\n",
      "242/242 [==============================] - 0s 30us/step - loss: 0.6396 - acc: 0.7149\n",
      "Epoch 26/150\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.6232 - acc: 0.7231\n",
      "Epoch 27/150\n",
      "242/242 [==============================] - 0s 41us/step - loss: 0.6096 - acc: 0.7479\n",
      "Epoch 28/150\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.6155 - acc: 0.7107\n",
      "Epoch 29/150\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.6167 - acc: 0.7190\n",
      "Epoch 30/150\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.6270 - acc: 0.7231\n",
      "Epoch 31/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.6106 - acc: 0.7190\n",
      "Epoch 32/150\n",
      "242/242 [==============================] - 0s 46us/step - loss: 0.5908 - acc: 0.7190\n",
      "Epoch 33/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5716 - acc: 0.7479\n",
      "Epoch 34/150\n",
      "242/242 [==============================] - 0s 39us/step - loss: 0.5574 - acc: 0.7397\n",
      "Epoch 35/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.5453 - acc: 0.7521\n",
      "Epoch 36/150\n",
      "242/242 [==============================] - 0s 39us/step - loss: 0.5633 - acc: 0.7438\n",
      "Epoch 37/150\n",
      "242/242 [==============================] - 0s 42us/step - loss: 0.5646 - acc: 0.7190\n",
      "Epoch 38/150\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.5449 - acc: 0.7273\n",
      "Epoch 39/150\n",
      "242/242 [==============================] - 0s 39us/step - loss: 0.5206 - acc: 0.7521\n",
      "Epoch 40/150\n",
      "242/242 [==============================] - 0s 49us/step - loss: 0.5116 - acc: 0.7645\n",
      "Epoch 41/150\n",
      "242/242 [==============================] - 0s 43us/step - loss: 0.5101 - acc: 0.7686\n",
      "Epoch 42/150\n",
      "242/242 [==============================] - 0s 47us/step - loss: 0.5108 - acc: 0.7438\n",
      "Epoch 43/150\n",
      "242/242 [==============================] - 0s 40us/step - loss: 0.4941 - acc: 0.7645\n",
      "Epoch 44/150\n",
      "242/242 [==============================] - 0s 65us/step - loss: 0.4932 - acc: 0.7521\n",
      "Epoch 45/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4936 - acc: 0.7686\n",
      "Epoch 46/150\n",
      "242/242 [==============================] - 0s 45us/step - loss: 0.5162 - acc: 0.7314\n",
      "Epoch 47/150\n",
      "242/242 [==============================] - 0s 46us/step - loss: 0.5126 - acc: 0.7603\n",
      "Epoch 48/150\n",
      "242/242 [==============================] - 0s 46us/step - loss: 0.5423 - acc: 0.7190\n",
      "Epoch 49/150\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.4787 - acc: 0.7686\n",
      "Epoch 50/150\n",
      "242/242 [==============================] - 0s 44us/step - loss: 0.4743 - acc: 0.7810\n",
      "Epoch 51/150\n",
      "242/242 [==============================] - 0s 36us/step - loss: 0.4711 - acc: 0.7851\n",
      "Epoch 52/150\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.4664 - acc: 0.7769\n",
      "Epoch 53/150\n",
      "242/242 [==============================] - 0s 45us/step - loss: 0.4740 - acc: 0.7893\n",
      "Epoch 54/150\n",
      "242/242 [==============================] - 0s 37us/step - loss: 0.4904 - acc: 0.7479\n",
      "Epoch 55/150\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.4729 - acc: 0.7851\n",
      "Epoch 56/150\n",
      "242/242 [==============================] - 0s 47us/step - loss: 0.4639 - acc: 0.7934\n",
      "Epoch 57/150\n",
      "242/242 [==============================] - 0s 42us/step - loss: 0.4589 - acc: 0.8017\n",
      "Epoch 58/150\n",
      "242/242 [==============================] - 0s 40us/step - loss: 0.4588 - acc: 0.7893\n",
      "Epoch 59/150\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.4564 - acc: 0.7893\n",
      "Epoch 60/150\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.4535 - acc: 0.7975\n",
      "Epoch 61/150\n",
      "242/242 [==============================] - 0s 40us/step - loss: 0.4546 - acc: 0.7893\n",
      "Epoch 62/150\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.4500 - acc: 0.7975\n",
      "Epoch 63/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.4531 - acc: 0.7893\n",
      "Epoch 64/150\n",
      "242/242 [==============================] - 0s 42us/step - loss: 0.4572 - acc: 0.7810\n",
      "Epoch 65/150\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.4465 - acc: 0.7934\n",
      "Epoch 66/150\n",
      "242/242 [==============================] - 0s 45us/step - loss: 0.4559 - acc: 0.7934\n",
      "Epoch 67/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.4463 - acc: 0.8058\n",
      "Epoch 68/150\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.4482 - acc: 0.7975\n",
      "Epoch 69/150\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.4455 - acc: 0.7810\n",
      "Epoch 70/150\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.4613 - acc: 0.8058\n",
      "Epoch 71/150\n",
      "242/242 [==============================] - 0s 34us/step - loss: 0.4554 - acc: 0.7851\n",
      "Epoch 72/150\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.4377 - acc: 0.8099\n",
      "Epoch 73/150\n",
      "242/242 [==============================] - 0s 34us/step - loss: 0.4334 - acc: 0.8058\n",
      "Epoch 74/150\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.4341 - acc: 0.7975\n",
      "Epoch 75/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.4377 - acc: 0.8017\n",
      "Epoch 76/150\n",
      "242/242 [==============================] - 0s 45us/step - loss: 0.4641 - acc: 0.7645\n",
      "Epoch 77/150\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.4534 - acc: 0.7727\n",
      "Epoch 78/150\n",
      "242/242 [==============================] - 0s 48us/step - loss: 0.4266 - acc: 0.8099\n",
      "Epoch 79/150\n",
      "242/242 [==============================] - 0s 42us/step - loss: 0.4492 - acc: 0.8099\n",
      "Epoch 80/150\n",
      "242/242 [==============================] - 0s 36us/step - loss: 0.4620 - acc: 0.7851\n",
      "Epoch 81/150\n",
      "242/242 [==============================] - 0s 34us/step - loss: 0.4432 - acc: 0.7851\n",
      "Epoch 82/150\n",
      "242/242 [==============================] - 0s 34us/step - loss: 0.4646 - acc: 0.7727\n",
      "Epoch 83/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.4483 - acc: 0.7686\n",
      "Epoch 84/150\n",
      "242/242 [==============================] - 0s 36us/step - loss: 0.4353 - acc: 0.7851\n",
      "Epoch 85/150\n",
      "242/242 [==============================] - 0s 39us/step - loss: 0.4276 - acc: 0.8099\n",
      "Epoch 86/150\n",
      "242/242 [==============================] - 0s 37us/step - loss: 0.4183 - acc: 0.8058\n",
      "Epoch 87/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.4228 - acc: 0.7893\n",
      "Epoch 88/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.4261 - acc: 0.8058\n",
      "Epoch 89/150\n",
      "242/242 [==============================] - 0s 41us/step - loss: 0.4255 - acc: 0.8017\n",
      "Epoch 90/150\n",
      "242/242 [==============================] - 0s 37us/step - loss: 0.4184 - acc: 0.8182\n",
      "Epoch 91/150\n",
      "242/242 [==============================] - 0s 43us/step - loss: 0.4182 - acc: 0.8017\n",
      "Epoch 92/150\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.4229 - acc: 0.8140\n",
      "Epoch 93/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 35us/step - loss: 0.4165 - acc: 0.8099\n",
      "Epoch 94/150\n",
      "242/242 [==============================] - 0s 42us/step - loss: 0.4174 - acc: 0.8058\n",
      "Epoch 95/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.4214 - acc: 0.8223\n",
      "Epoch 96/150\n",
      "242/242 [==============================] - 0s 37us/step - loss: 0.4122 - acc: 0.8099\n",
      "Epoch 97/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.4176 - acc: 0.8264\n",
      "Epoch 98/150\n",
      "242/242 [==============================] - 0s 37us/step - loss: 0.4131 - acc: 0.8058\n",
      "Epoch 99/150\n",
      "242/242 [==============================] - 0s 34us/step - loss: 0.4113 - acc: 0.8099\n",
      "Epoch 100/150\n",
      "242/242 [==============================] - 0s 40us/step - loss: 0.4175 - acc: 0.8223\n",
      "Epoch 101/150\n",
      "242/242 [==============================] - 0s 36us/step - loss: 0.4082 - acc: 0.8347\n",
      "Epoch 102/150\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.4088 - acc: 0.8223\n",
      "Epoch 103/150\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.4114 - acc: 0.8306\n",
      "Epoch 104/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.4338 - acc: 0.7975\n",
      "Epoch 105/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.4035 - acc: 0.8223\n",
      "Epoch 106/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.4051 - acc: 0.8347\n",
      "Epoch 107/150\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.4037 - acc: 0.8264\n",
      "Epoch 108/150\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.4060 - acc: 0.8306\n",
      "Epoch 109/150\n",
      "242/242 [==============================] - 0s 36us/step - loss: 0.4061 - acc: 0.8223\n",
      "Epoch 110/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.4013 - acc: 0.8223\n",
      "Epoch 111/150\n",
      "242/242 [==============================] - 0s 40us/step - loss: 0.4078 - acc: 0.8306\n",
      "Epoch 112/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.3988 - acc: 0.8347\n",
      "Epoch 113/150\n",
      "242/242 [==============================] - 0s 136us/step - loss: 0.3984 - acc: 0.8306\n",
      "Epoch 114/150\n",
      "242/242 [==============================] - 0s 44us/step - loss: 0.4029 - acc: 0.8264\n",
      "Epoch 115/150\n",
      "242/242 [==============================] - 0s 42us/step - loss: 0.4132 - acc: 0.8347\n",
      "Epoch 116/150\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.4132 - acc: 0.8099\n",
      "Epoch 117/150\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.4421 - acc: 0.8017\n",
      "Epoch 118/150\n",
      "242/242 [==============================] - 0s 65us/step - loss: 0.4282 - acc: 0.8140\n",
      "Epoch 119/150\n",
      "242/242 [==============================] - 0s 71us/step - loss: 0.3955 - acc: 0.8223\n",
      "Epoch 120/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.4212 - acc: 0.8017\n",
      "Epoch 121/150\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.4000 - acc: 0.8223\n",
      "Epoch 122/150\n",
      "242/242 [==============================] - 0s 49us/step - loss: 0.4057 - acc: 0.8140\n",
      "Epoch 123/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4268 - acc: 0.7893\n",
      "Epoch 124/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.4175 - acc: 0.7810\n",
      "Epoch 125/150\n",
      "242/242 [==============================] - 0s 48us/step - loss: 0.4412 - acc: 0.7727\n",
      "Epoch 126/150\n",
      "242/242 [==============================] - 0s 41us/step - loss: 0.4135 - acc: 0.8430\n",
      "Epoch 127/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.3938 - acc: 0.8306\n",
      "Epoch 128/150\n",
      "242/242 [==============================] - 0s 43us/step - loss: 0.3916 - acc: 0.8306\n",
      "Epoch 129/150\n",
      "242/242 [==============================] - 0s 40us/step - loss: 0.4080 - acc: 0.8140\n",
      "Epoch 130/150\n",
      "242/242 [==============================] - 0s 44us/step - loss: 0.4058 - acc: 0.8182\n",
      "Epoch 131/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.3910 - acc: 0.8264\n",
      "Epoch 132/150\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.3951 - acc: 0.8306\n",
      "Epoch 133/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.3939 - acc: 0.8264\n",
      "Epoch 134/150\n",
      "242/242 [==============================] - 0s 40us/step - loss: 0.3950 - acc: 0.8306\n",
      "Epoch 135/150\n",
      "242/242 [==============================] - 0s 51us/step - loss: 0.4078 - acc: 0.8140\n",
      "Epoch 136/150\n",
      "242/242 [==============================] - 0s 45us/step - loss: 0.3951 - acc: 0.8306\n",
      "Epoch 137/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.3923 - acc: 0.8264\n",
      "Epoch 138/150\n",
      "242/242 [==============================] - 0s 46us/step - loss: 0.3881 - acc: 0.8430\n",
      "Epoch 139/150\n",
      "242/242 [==============================] - 0s 36us/step - loss: 0.3869 - acc: 0.8347\n",
      "Epoch 140/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.3885 - acc: 0.8306\n",
      "Epoch 141/150\n",
      "242/242 [==============================] - 0s 44us/step - loss: 0.3903 - acc: 0.8306\n",
      "Epoch 142/150\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.3888 - acc: 0.8182\n",
      "Epoch 143/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.3930 - acc: 0.8264\n",
      "Epoch 144/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.3904 - acc: 0.8347\n",
      "Epoch 145/150\n",
      "242/242 [==============================] - 0s 67us/step - loss: 0.3886 - acc: 0.8140\n",
      "Epoch 146/150\n",
      "242/242 [==============================] - 0s 67us/step - loss: 0.3833 - acc: 0.8347\n",
      "Epoch 147/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.3813 - acc: 0.8306\n",
      "Epoch 148/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.3921 - acc: 0.8347\n",
      "Epoch 149/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4063 - acc: 0.8017\n",
      "Epoch 150/150\n",
      "242/242 [==============================] - 0s 49us/step - loss: 0.4053 - acc: 0.8058\n",
      "61/61 [==============================] - 0s 917us/step\n",
      "Epoch 1/150\n",
      "242/242 [==============================] - 0s 816us/step - loss: 3.8914 - acc: 0.5455\n",
      "Epoch 2/150\n",
      "242/242 [==============================] - 0s 22us/step - loss: 1.1176 - acc: 0.6322\n",
      "Epoch 3/150\n",
      "242/242 [==============================] - 0s 25us/step - loss: 1.1313 - acc: 0.5455\n",
      "Epoch 4/150\n",
      "242/242 [==============================] - 0s 47us/step - loss: 0.7389 - acc: 0.6653\n",
      "Epoch 5/150\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.8283 - acc: 0.6322\n",
      "Epoch 6/150\n",
      "242/242 [==============================] - 0s 39us/step - loss: 0.7187 - acc: 0.6777\n",
      "Epoch 7/150\n",
      "242/242 [==============================] - 0s 44us/step - loss: 0.7242 - acc: 0.6529\n",
      "Epoch 8/150\n",
      "242/242 [==============================] - 0s 37us/step - loss: 0.6903 - acc: 0.6364\n",
      "Epoch 9/150\n",
      "242/242 [==============================] - 0s 43us/step - loss: 0.6653 - acc: 0.6529\n",
      "Epoch 10/150\n",
      "242/242 [==============================] - 0s 48us/step - loss: 0.6545 - acc: 0.6777\n",
      "Epoch 11/150\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.6457 - acc: 0.6777\n",
      "Epoch 12/150\n",
      "242/242 [==============================] - 0s 44us/step - loss: 0.6414 - acc: 0.6694\n",
      "Epoch 13/150\n",
      "242/242 [==============================] - 0s 47us/step - loss: 0.6370 - acc: 0.6694\n",
      "Epoch 14/150\n",
      "242/242 [==============================] - 0s 48us/step - loss: 0.6277 - acc: 0.6901\n",
      "Epoch 15/150\n",
      "242/242 [==============================] - 0s 40us/step - loss: 0.6278 - acc: 0.6860\n",
      "Epoch 16/150\n",
      "242/242 [==============================] - 0s 45us/step - loss: 0.6289 - acc: 0.6736\n",
      "Epoch 17/150\n",
      "242/242 [==============================] - 0s 43us/step - loss: 0.6184 - acc: 0.6983\n",
      "Epoch 18/150\n",
      "242/242 [==============================] - 0s 46us/step - loss: 0.6181 - acc: 0.6942\n",
      "Epoch 19/150\n",
      "242/242 [==============================] - 0s 46us/step - loss: 0.6221 - acc: 0.6570\n",
      "Epoch 20/150\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.6135 - acc: 0.7149\n",
      "Epoch 21/150\n",
      "242/242 [==============================] - 0s 39us/step - loss: 0.6111 - acc: 0.7107\n",
      "Epoch 22/150\n",
      "242/242 [==============================] - 0s 41us/step - loss: 0.6078 - acc: 0.6942\n",
      "Epoch 23/150\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.6038 - acc: 0.6942\n",
      "Epoch 24/150\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.6012 - acc: 0.6983\n",
      "Epoch 25/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 47us/step - loss: 0.5951 - acc: 0.7066\n",
      "Epoch 26/150\n",
      "242/242 [==============================] - 0s 42us/step - loss: 0.5973 - acc: 0.6694\n",
      "Epoch 27/150\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.6017 - acc: 0.7107\n",
      "Epoch 28/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.5891 - acc: 0.7066\n",
      "Epoch 29/150\n",
      "242/242 [==============================] - 0s 40us/step - loss: 0.5984 - acc: 0.7066\n",
      "Epoch 30/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.5931 - acc: 0.6860\n",
      "Epoch 31/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5873 - acc: 0.7149\n",
      "Epoch 32/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5820 - acc: 0.7066\n",
      "Epoch 33/150\n",
      "242/242 [==============================] - 0s 51us/step - loss: 0.5825 - acc: 0.7107\n",
      "Epoch 34/150\n",
      "242/242 [==============================] - 0s 45us/step - loss: 0.5768 - acc: 0.7066\n",
      "Epoch 35/150\n",
      "242/242 [==============================] - 0s 49us/step - loss: 0.5967 - acc: 0.7107\n",
      "Epoch 36/150\n",
      "242/242 [==============================] - 0s 67us/step - loss: 0.5977 - acc: 0.7066\n",
      "Epoch 37/150\n",
      "242/242 [==============================] - 0s 65us/step - loss: 0.5722 - acc: 0.7149\n",
      "Epoch 38/150\n",
      "242/242 [==============================] - 0s 70us/step - loss: 0.5794 - acc: 0.7066\n",
      "Epoch 39/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5834 - acc: 0.7190\n",
      "Epoch 40/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5644 - acc: 0.7190\n",
      "Epoch 41/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.5668 - acc: 0.7025\n",
      "Epoch 42/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.5675 - acc: 0.7231\n",
      "Epoch 43/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.5586 - acc: 0.7190\n",
      "Epoch 44/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5573 - acc: 0.7149\n",
      "Epoch 45/150\n",
      "242/242 [==============================] - 0s 49us/step - loss: 0.5659 - acc: 0.7190\n",
      "Epoch 46/150\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.5557 - acc: 0.7231\n",
      "Epoch 47/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5518 - acc: 0.7314\n",
      "Epoch 48/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5493 - acc: 0.7231\n",
      "Epoch 49/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5648 - acc: 0.7107\n",
      "Epoch 50/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.5539 - acc: 0.7149\n",
      "Epoch 51/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.5504 - acc: 0.7438\n",
      "Epoch 52/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.5478 - acc: 0.7397\n",
      "Epoch 53/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.5424 - acc: 0.7397\n",
      "Epoch 54/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.5582 - acc: 0.7231\n",
      "Epoch 55/150\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.5526 - acc: 0.7273\n",
      "Epoch 56/150\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.5453 - acc: 0.7314\n",
      "Epoch 57/150\n",
      "242/242 [==============================] - 0s 43us/step - loss: 0.5367 - acc: 0.7314\n",
      "Epoch 58/150\n",
      "242/242 [==============================] - 0s 49us/step - loss: 0.5370 - acc: 0.7190\n",
      "Epoch 59/150\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.5680 - acc: 0.7231\n",
      "Epoch 60/150\n",
      "242/242 [==============================] - 0s 46us/step - loss: 0.5319 - acc: 0.7397\n",
      "Epoch 61/150\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.5335 - acc: 0.7479\n",
      "Epoch 62/150\n",
      "242/242 [==============================] - 0s 51us/step - loss: 0.5331 - acc: 0.7438\n",
      "Epoch 63/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.5422 - acc: 0.7479\n",
      "Epoch 64/150\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.5413 - acc: 0.7397\n",
      "Epoch 65/150\n",
      "242/242 [==============================] - 0s 45us/step - loss: 0.5168 - acc: 0.7438\n",
      "Epoch 66/150\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.5234 - acc: 0.7438\n",
      "Epoch 67/150\n",
      "242/242 [==============================] - 0s 73us/step - loss: 0.5242 - acc: 0.7521\n",
      "Epoch 68/150\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.5369 - acc: 0.7066\n",
      "Epoch 69/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.5106 - acc: 0.7479\n",
      "Epoch 70/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5125 - acc: 0.7603\n",
      "Epoch 71/150\n",
      "242/242 [==============================] - 0s 47us/step - loss: 0.5136 - acc: 0.7769\n",
      "Epoch 72/150\n",
      "242/242 [==============================] - 0s 45us/step - loss: 0.5137 - acc: 0.7727\n",
      "Epoch 73/150\n",
      "242/242 [==============================] - 0s 49us/step - loss: 0.5524 - acc: 0.7107\n",
      "Epoch 74/150\n",
      "242/242 [==============================] - 0s 51us/step - loss: 0.5172 - acc: 0.7521\n",
      "Epoch 75/150\n",
      "242/242 [==============================] - 0s 43us/step - loss: 0.5177 - acc: 0.7438\n",
      "Epoch 76/150\n",
      "242/242 [==============================] - 0s 43us/step - loss: 0.5070 - acc: 0.7562\n",
      "Epoch 77/150\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.5014 - acc: 0.7438\n",
      "Epoch 78/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.4980 - acc: 0.7727\n",
      "Epoch 79/150\n",
      "242/242 [==============================] - 0s 44us/step - loss: 0.4931 - acc: 0.7686\n",
      "Epoch 80/150\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.4933 - acc: 0.7727\n",
      "Epoch 81/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.4905 - acc: 0.7810\n",
      "Epoch 82/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.4897 - acc: 0.7562\n",
      "Epoch 83/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.4893 - acc: 0.7479\n",
      "Epoch 84/150\n",
      "242/242 [==============================] - 0s 45us/step - loss: 0.4876 - acc: 0.7562\n",
      "Epoch 85/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4836 - acc: 0.7562\n",
      "Epoch 86/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4883 - acc: 0.7727\n",
      "Epoch 87/150\n",
      "242/242 [==============================] - 0s 70us/step - loss: 0.5139 - acc: 0.7355\n",
      "Epoch 88/150\n",
      "242/242 [==============================] - 0s 75us/step - loss: 0.5186 - acc: 0.7603\n",
      "Epoch 89/150\n",
      "242/242 [==============================] - 0s 71us/step - loss: 0.5150 - acc: 0.7521\n",
      "Epoch 90/150\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.4858 - acc: 0.7562\n",
      "Epoch 91/150\n",
      "242/242 [==============================] - 0s 116us/step - loss: 0.4893 - acc: 0.7851\n",
      "Epoch 92/150\n",
      "242/242 [==============================] - 0s 84us/step - loss: 0.4787 - acc: 0.7727\n",
      "Epoch 93/150\n",
      "242/242 [==============================] - 0s 84us/step - loss: 0.4722 - acc: 0.7810\n",
      "Epoch 94/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.4658 - acc: 0.7686\n",
      "Epoch 95/150\n",
      "242/242 [==============================] - 0s 66us/step - loss: 0.4635 - acc: 0.7769\n",
      "Epoch 96/150\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.4628 - acc: 0.7727\n",
      "Epoch 97/150\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.4620 - acc: 0.7934\n",
      "Epoch 98/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4645 - acc: 0.7934\n",
      "Epoch 99/150\n",
      "242/242 [==============================] - 0s 41us/step - loss: 0.4598 - acc: 0.7769\n",
      "Epoch 100/150\n",
      "242/242 [==============================] - 0s 47us/step - loss: 0.4606 - acc: 0.7893\n",
      "Epoch 101/150\n",
      "242/242 [==============================] - 0s 48us/step - loss: 0.4598 - acc: 0.7810\n",
      "Epoch 102/150\n",
      "242/242 [==============================] - 0s 43us/step - loss: 0.4644 - acc: 0.7769\n",
      "Epoch 103/150\n",
      "242/242 [==============================] - 0s 42us/step - loss: 0.4595 - acc: 0.7810\n",
      "Epoch 104/150\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.4784 - acc: 0.7934\n",
      "Epoch 105/150\n",
      "242/242 [==============================] - 0s 48us/step - loss: 0.4923 - acc: 0.7727\n",
      "Epoch 106/150\n",
      "242/242 [==============================] - 0s 49us/step - loss: 0.4720 - acc: 0.8140\n",
      "Epoch 107/150\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.4888 - acc: 0.7686\n",
      "Epoch 108/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 47us/step - loss: 0.4679 - acc: 0.7810\n",
      "Epoch 109/150\n",
      "242/242 [==============================] - 0s 45us/step - loss: 0.4814 - acc: 0.7686\n",
      "Epoch 110/150\n",
      "242/242 [==============================] - 0s 43us/step - loss: 0.4699 - acc: 0.7727\n",
      "Epoch 111/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.4563 - acc: 0.8058\n",
      "Epoch 112/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4433 - acc: 0.7893\n",
      "Epoch 113/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.4398 - acc: 0.7975\n",
      "Epoch 114/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4373 - acc: 0.8058\n",
      "Epoch 115/150\n",
      "242/242 [==============================] - 0s 40us/step - loss: 0.4349 - acc: 0.8017\n",
      "Epoch 116/150\n",
      "242/242 [==============================] - 0s 45us/step - loss: 0.4578 - acc: 0.7893\n",
      "Epoch 117/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4420 - acc: 0.8058\n",
      "Epoch 118/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4410 - acc: 0.8017\n",
      "Epoch 119/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.4327 - acc: 0.8058\n",
      "Epoch 120/150\n",
      "242/242 [==============================] - 0s 51us/step - loss: 0.4338 - acc: 0.8058\n",
      "Epoch 121/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.4289 - acc: 0.8140\n",
      "Epoch 122/150\n",
      "242/242 [==============================] - 0s 42us/step - loss: 0.4293 - acc: 0.8099\n",
      "Epoch 123/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.4282 - acc: 0.8017\n",
      "Epoch 124/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.4364 - acc: 0.7686\n",
      "Epoch 125/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.4406 - acc: 0.8058\n",
      "Epoch 126/150\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.4389 - acc: 0.7934\n",
      "Epoch 127/150\n",
      "242/242 [==============================] - 0s 44us/step - loss: 0.4663 - acc: 0.7851\n",
      "Epoch 128/150\n",
      "242/242 [==============================] - 0s 43us/step - loss: 0.4277 - acc: 0.8058\n",
      "Epoch 129/150\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.4230 - acc: 0.8223\n",
      "Epoch 130/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.4226 - acc: 0.8017\n",
      "Epoch 131/150\n",
      "242/242 [==============================] - 0s 40us/step - loss: 0.4246 - acc: 0.8017\n",
      "Epoch 132/150\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.4203 - acc: 0.8223\n",
      "Epoch 133/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.4248 - acc: 0.8182\n",
      "Epoch 134/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.4265 - acc: 0.7975\n",
      "Epoch 135/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.4180 - acc: 0.8099\n",
      "Epoch 136/150\n",
      "242/242 [==============================] - 0s 51us/step - loss: 0.4190 - acc: 0.8058\n",
      "Epoch 137/150\n",
      "242/242 [==============================] - 0s 47us/step - loss: 0.4151 - acc: 0.8182\n",
      "Epoch 138/150\n",
      "242/242 [==============================] - 0s 48us/step - loss: 0.4159 - acc: 0.8140\n",
      "Epoch 139/150\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.4217 - acc: 0.7893\n",
      "Epoch 140/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4094 - acc: 0.8223\n",
      "Epoch 141/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4168 - acc: 0.8099\n",
      "Epoch 142/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.4179 - acc: 0.8140\n",
      "Epoch 143/150\n",
      "242/242 [==============================] - 0s 49us/step - loss: 0.4302 - acc: 0.8017\n",
      "Epoch 144/150\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.4118 - acc: 0.8140\n",
      "Epoch 145/150\n",
      "242/242 [==============================] - 0s 49us/step - loss: 0.4113 - acc: 0.8264\n",
      "Epoch 146/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.4107 - acc: 0.8182\n",
      "Epoch 147/150\n",
      "242/242 [==============================] - 0s 46us/step - loss: 0.4393 - acc: 0.7893\n",
      "Epoch 148/150\n",
      "242/242 [==============================] - 0s 46us/step - loss: 0.4260 - acc: 0.8264\n",
      "Epoch 149/150\n",
      "242/242 [==============================] - 0s 49us/step - loss: 0.4010 - acc: 0.8264\n",
      "Epoch 150/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.4061 - acc: 0.8223\n",
      "61/61 [==============================] - 0s 1ms/step\n",
      "Epoch 1/150\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.4816 - acc: 0.5802\n",
      "Epoch 2/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 1.5397 - acc: 0.6420\n",
      "Epoch 3/150\n",
      "243/243 [==============================] - 0s 75us/step - loss: 1.3999 - acc: 0.6214\n",
      "Epoch 4/150\n",
      "243/243 [==============================] - 0s 65us/step - loss: 1.0309 - acc: 0.6461\n",
      "Epoch 5/150\n",
      "243/243 [==============================] - 0s 86us/step - loss: 0.9439 - acc: 0.6955\n",
      "Epoch 6/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.9051 - acc: 0.6914\n",
      "Epoch 7/150\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.8488 - acc: 0.7078\n",
      "Epoch 8/150\n",
      "243/243 [==============================] - 0s 74us/step - loss: 0.8324 - acc: 0.7119\n",
      "Epoch 9/150\n",
      "243/243 [==============================] - 0s 80us/step - loss: 0.8015 - acc: 0.7078\n",
      "Epoch 10/150\n",
      "243/243 [==============================] - 0s 75us/step - loss: 0.7818 - acc: 0.7119\n",
      "Epoch 11/150\n",
      "243/243 [==============================] - 0s 69us/step - loss: 0.7726 - acc: 0.7243\n",
      "Epoch 12/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.7594 - acc: 0.7243\n",
      "Epoch 13/150\n",
      "243/243 [==============================] - 0s 71us/step - loss: 0.7418 - acc: 0.7202\n",
      "Epoch 14/150\n",
      "243/243 [==============================] - 0s 72us/step - loss: 0.7328 - acc: 0.7160\n",
      "Epoch 15/150\n",
      "243/243 [==============================] - 0s 44us/step - loss: 0.7145 - acc: 0.7284\n",
      "Epoch 16/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.7055 - acc: 0.7119\n",
      "Epoch 17/150\n",
      "243/243 [==============================] - 0s 56us/step - loss: 0.7095 - acc: 0.7243\n",
      "Epoch 18/150\n",
      "243/243 [==============================] - 0s 54us/step - loss: 0.7092 - acc: 0.6996\n",
      "Epoch 19/150\n",
      "243/243 [==============================] - 0s 55us/step - loss: 0.6758 - acc: 0.7037\n",
      "Epoch 20/150\n",
      "243/243 [==============================] - 0s 43us/step - loss: 0.6742 - acc: 0.7037\n",
      "Epoch 21/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.6707 - acc: 0.7078\n",
      "Epoch 22/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.6480 - acc: 0.7407\n",
      "Epoch 23/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.6555 - acc: 0.7078\n",
      "Epoch 24/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.6383 - acc: 0.7366\n",
      "Epoch 25/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.6395 - acc: 0.7202\n",
      "Epoch 26/150\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.6230 - acc: 0.7531\n",
      "Epoch 27/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.6083 - acc: 0.7490\n",
      "Epoch 28/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.6202 - acc: 0.7366\n",
      "Epoch 29/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.6057 - acc: 0.7407\n",
      "Epoch 30/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.5944 - acc: 0.7490\n",
      "Epoch 31/150\n",
      "243/243 [==============================] - 0s 47us/step - loss: 0.5925 - acc: 0.7572\n",
      "Epoch 32/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.5897 - acc: 0.7572\n",
      "Epoch 33/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.5939 - acc: 0.7613\n",
      "Epoch 34/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.5875 - acc: 0.7284\n",
      "Epoch 35/150\n",
      "243/243 [==============================] - 0s 55us/step - loss: 0.5922 - acc: 0.7695\n",
      "Epoch 36/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.6190 - acc: 0.7325\n",
      "Epoch 37/150\n",
      "243/243 [==============================] - 0s 45us/step - loss: 0.6120 - acc: 0.7531\n",
      "Epoch 38/150\n",
      "243/243 [==============================] - 0s 53us/step - loss: 0.5729 - acc: 0.7366\n",
      "Epoch 39/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.5681 - acc: 0.7737\n",
      "Epoch 40/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 0s 58us/step - loss: 0.5619 - acc: 0.7572\n",
      "Epoch 41/150\n",
      "243/243 [==============================] - 0s 53us/step - loss: 0.5583 - acc: 0.7572\n",
      "Epoch 42/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.5570 - acc: 0.7819\n",
      "Epoch 43/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.5680 - acc: 0.7449\n",
      "Epoch 44/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.5767 - acc: 0.7407\n",
      "Epoch 45/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.5712 - acc: 0.7572\n",
      "Epoch 46/150\n",
      "243/243 [==============================] - 0s 112us/step - loss: 0.5443 - acc: 0.7942\n",
      "Epoch 47/150\n",
      "243/243 [==============================] - 0s 79us/step - loss: 0.5471 - acc: 0.7695\n",
      "Epoch 48/150\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.5577 - acc: 0.7737\n",
      "Epoch 49/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.5368 - acc: 0.7819\n",
      "Epoch 50/150\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.5412 - acc: 0.7819\n",
      "Epoch 51/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.5397 - acc: 0.7778\n",
      "Epoch 52/150\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.5295 - acc: 0.7778\n",
      "Epoch 53/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.5270 - acc: 0.7778\n",
      "Epoch 54/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.5314 - acc: 0.7901\n",
      "Epoch 55/150\n",
      "243/243 [==============================] - 0s 44us/step - loss: 0.5298 - acc: 0.7819\n",
      "Epoch 56/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.5195 - acc: 0.7819\n",
      "Epoch 57/150\n",
      "243/243 [==============================] - 0s 53us/step - loss: 0.5169 - acc: 0.8025\n",
      "Epoch 58/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.5122 - acc: 0.7778\n",
      "Epoch 59/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.5088 - acc: 0.7737\n",
      "Epoch 60/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.5084 - acc: 0.8025\n",
      "Epoch 61/150\n",
      "243/243 [==============================] - 0s 47us/step - loss: 0.5207 - acc: 0.7778\n",
      "Epoch 62/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.5431 - acc: 0.7901\n",
      "Epoch 63/150\n",
      "243/243 [==============================] - 0s 53us/step - loss: 0.5495 - acc: 0.7819\n",
      "Epoch 64/150\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.5105 - acc: 0.7942\n",
      "Epoch 65/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.5073 - acc: 0.8025\n",
      "Epoch 66/150\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.5016 - acc: 0.7737\n",
      "Epoch 67/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.4950 - acc: 0.8025\n",
      "Epoch 68/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.4959 - acc: 0.7984\n",
      "Epoch 69/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.5039 - acc: 0.8107\n",
      "Epoch 70/150\n",
      "243/243 [==============================] - 0s 53us/step - loss: 0.5030 - acc: 0.7984\n",
      "Epoch 71/150\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.4928 - acc: 0.8025\n",
      "Epoch 72/150\n",
      "243/243 [==============================] - 0s 44us/step - loss: 0.4890 - acc: 0.8189\n",
      "Epoch 73/150\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.4875 - acc: 0.8025\n",
      "Epoch 74/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.4862 - acc: 0.8066\n",
      "Epoch 75/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.5006 - acc: 0.7860\n",
      "Epoch 76/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.5078 - acc: 0.8066\n",
      "Epoch 77/150\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.5496 - acc: 0.7572\n",
      "Epoch 78/150\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.4768 - acc: 0.8148\n",
      "Epoch 79/150\n",
      "243/243 [==============================] - 0s 54us/step - loss: 0.4469 - acc: 0.8025\n",
      "Epoch 80/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.4536 - acc: 0.8025\n",
      "Epoch 81/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.4535 - acc: 0.8025\n",
      "Epoch 82/150\n",
      "243/243 [==============================] - 0s 53us/step - loss: 0.4323 - acc: 0.8025\n",
      "Epoch 83/150\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.4101 - acc: 0.8272\n",
      "Epoch 84/150\n",
      "243/243 [==============================] - 0s 53us/step - loss: 0.4190 - acc: 0.8354\n",
      "Epoch 85/150\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.4214 - acc: 0.8189\n",
      "Epoch 86/150\n",
      "243/243 [==============================] - 0s 54us/step - loss: 0.4068 - acc: 0.8354\n",
      "Epoch 87/150\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.4057 - acc: 0.8107\n",
      "Epoch 88/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.4040 - acc: 0.8148\n",
      "Epoch 89/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.4040 - acc: 0.8230\n",
      "Epoch 90/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.4077 - acc: 0.8313\n",
      "Epoch 91/150\n",
      "243/243 [==============================] - 0s 45us/step - loss: 0.4141 - acc: 0.8066\n",
      "Epoch 92/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.4216 - acc: 0.8025\n",
      "Epoch 93/150\n",
      "243/243 [==============================] - 0s 54us/step - loss: 0.4082 - acc: 0.8189\n",
      "Epoch 94/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.4001 - acc: 0.7984\n",
      "Epoch 95/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.4011 - acc: 0.8066\n",
      "Epoch 96/150\n",
      "243/243 [==============================] - 0s 55us/step - loss: 0.4003 - acc: 0.8189\n",
      "Epoch 97/150\n",
      "243/243 [==============================] - 0s 53us/step - loss: 0.4405 - acc: 0.7942\n",
      "Epoch 98/150\n",
      "243/243 [==============================] - 0s 54us/step - loss: 0.4249 - acc: 0.8189\n",
      "Epoch 99/150\n",
      "243/243 [==============================] - 0s 42us/step - loss: 0.4136 - acc: 0.8148\n",
      "Epoch 100/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.4201 - acc: 0.8107\n",
      "Epoch 101/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.3996 - acc: 0.8272\n",
      "Epoch 102/150\n",
      "243/243 [==============================] - 0s 38us/step - loss: 0.4008 - acc: 0.8189\n",
      "Epoch 103/150\n",
      "243/243 [==============================] - 0s 55us/step - loss: 0.4008 - acc: 0.8395\n",
      "Epoch 104/150\n",
      "243/243 [==============================] - 0s 41us/step - loss: 0.4029 - acc: 0.8066\n",
      "Epoch 105/150\n",
      "243/243 [==============================] - 0s 70us/step - loss: 0.4051 - acc: 0.8148\n",
      "Epoch 106/150\n",
      "243/243 [==============================] - 0s 76us/step - loss: 0.3991 - acc: 0.8107\n",
      "Epoch 107/150\n",
      "243/243 [==============================] - 0s 67us/step - loss: 0.3918 - acc: 0.8189\n",
      "Epoch 108/150\n",
      "243/243 [==============================] - 0s 46us/step - loss: 0.4032 - acc: 0.8230\n",
      "Epoch 109/150\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.3912 - acc: 0.8230\n",
      "Epoch 110/150\n",
      "243/243 [==============================] - 0s 46us/step - loss: 0.3993 - acc: 0.8189\n",
      "Epoch 111/150\n",
      "243/243 [==============================] - 0s 46us/step - loss: 0.4017 - acc: 0.8107\n",
      "Epoch 112/150\n",
      "243/243 [==============================] - 0s 38us/step - loss: 0.3980 - acc: 0.8107\n",
      "Epoch 113/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.3926 - acc: 0.8272\n",
      "Epoch 114/150\n",
      "243/243 [==============================] - 0s 73us/step - loss: 0.4003 - acc: 0.8189\n",
      "Epoch 115/150\n",
      "243/243 [==============================] - 0s 95us/step - loss: 0.4017 - acc: 0.8189\n",
      "Epoch 116/150\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.4011 - acc: 0.8107\n",
      "Epoch 117/150\n",
      "243/243 [==============================] - 0s 68us/step - loss: 0.4001 - acc: 0.8230\n",
      "Epoch 118/150\n",
      "243/243 [==============================] - 0s 74us/step - loss: 0.3996 - acc: 0.8189\n",
      "Epoch 119/150\n",
      "243/243 [==============================] - 0s 66us/step - loss: 0.3892 - acc: 0.8189\n",
      "Epoch 120/150\n",
      "243/243 [==============================] - 0s 55us/step - loss: 0.3869 - acc: 0.8354\n",
      "Epoch 121/150\n",
      "243/243 [==============================] - 0s 38us/step - loss: 0.3879 - acc: 0.8066\n",
      "Epoch 122/150\n",
      "243/243 [==============================] - 0s 70us/step - loss: 0.3844 - acc: 0.8313\n",
      "Epoch 123/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 0s 61us/step - loss: 0.3932 - acc: 0.8313\n",
      "Epoch 124/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.3882 - acc: 0.8230\n",
      "Epoch 125/150\n",
      "243/243 [==============================] - 0s 55us/step - loss: 0.3935 - acc: 0.8230\n",
      "Epoch 126/150\n",
      "243/243 [==============================] - 0s 53us/step - loss: 0.3828 - acc: 0.8272\n",
      "Epoch 127/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.3919 - acc: 0.8230\n",
      "Epoch 128/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.3846 - acc: 0.8272\n",
      "Epoch 129/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.3950 - acc: 0.8107\n",
      "Epoch 130/150\n",
      "243/243 [==============================] - 0s 78us/step - loss: 0.3850 - acc: 0.8230\n",
      "Epoch 131/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.3816 - acc: 0.8313\n",
      "Epoch 132/150\n",
      "243/243 [==============================] - 0s 53us/step - loss: 0.3813 - acc: 0.8189\n",
      "Epoch 133/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.3931 - acc: 0.8395\n",
      "Epoch 134/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.3849 - acc: 0.8189\n",
      "Epoch 135/150\n",
      "243/243 [==============================] - 0s 42us/step - loss: 0.3898 - acc: 0.8230\n",
      "Epoch 136/150\n",
      "243/243 [==============================] - 0s 55us/step - loss: 0.3802 - acc: 0.8272\n",
      "Epoch 137/150\n",
      "243/243 [==============================] - 0s 54us/step - loss: 0.3838 - acc: 0.8107\n",
      "Epoch 138/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.3795 - acc: 0.8313\n",
      "Epoch 139/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.3822 - acc: 0.8436\n",
      "Epoch 140/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.3894 - acc: 0.8066\n",
      "Epoch 141/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.4040 - acc: 0.8230\n",
      "Epoch 142/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.4053 - acc: 0.8272\n",
      "Epoch 143/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.3965 - acc: 0.8148\n",
      "Epoch 144/150\n",
      "243/243 [==============================] - 0s 53us/step - loss: 0.4098 - acc: 0.7901\n",
      "Epoch 145/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.4063 - acc: 0.7901\n",
      "Epoch 146/150\n",
      "243/243 [==============================] - 0s 56us/step - loss: 0.3838 - acc: 0.8354\n",
      "Epoch 147/150\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.3872 - acc: 0.8436\n",
      "Epoch 148/150\n",
      "243/243 [==============================] - 0s 45us/step - loss: 0.3893 - acc: 0.8107\n",
      "Epoch 149/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.4018 - acc: 0.8189\n",
      "Epoch 150/150\n",
      "243/243 [==============================] - 0s 53us/step - loss: 0.3765 - acc: 0.8313\n",
      "60/60 [==============================] - 0s 1ms/step\n",
      "Epoch 1/150\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 0.8777 - acc: 0.6584\n",
      "Epoch 2/150\n",
      "243/243 [==============================] - 0s 31us/step - loss: 0.8087 - acc: 0.6461\n",
      "Epoch 3/150\n",
      "243/243 [==============================] - 0s 23us/step - loss: 0.7612 - acc: 0.6626\n",
      "Epoch 4/150\n",
      "243/243 [==============================] - 0s 28us/step - loss: 0.7421 - acc: 0.6502\n",
      "Epoch 5/150\n",
      "243/243 [==============================] - 0s 45us/step - loss: 0.7153 - acc: 0.6502\n",
      "Epoch 6/150\n",
      "243/243 [==============================] - 0s 44us/step - loss: 0.7027 - acc: 0.6543\n",
      "Epoch 7/150\n",
      "243/243 [==============================] - 0s 39us/step - loss: 0.6932 - acc: 0.6379\n",
      "Epoch 8/150\n",
      "243/243 [==============================] - 0s 55us/step - loss: 0.6763 - acc: 0.6584\n",
      "Epoch 9/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.6738 - acc: 0.6461\n",
      "Epoch 10/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.6581 - acc: 0.6584\n",
      "Epoch 11/150\n",
      "243/243 [==============================] - 0s 38us/step - loss: 0.6602 - acc: 0.6420\n",
      "Epoch 12/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.6478 - acc: 0.6502\n",
      "Epoch 13/150\n",
      "243/243 [==============================] - 0s 55us/step - loss: 0.6403 - acc: 0.6543\n",
      "Epoch 14/150\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.6380 - acc: 0.6667\n",
      "Epoch 15/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.6470 - acc: 0.6543\n",
      "Epoch 16/150\n",
      "243/243 [==============================] - 0s 45us/step - loss: 0.6276 - acc: 0.6667\n",
      "Epoch 17/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.6248 - acc: 0.6749\n",
      "Epoch 18/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.6234 - acc: 0.6543\n",
      "Epoch 19/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.6166 - acc: 0.6749\n",
      "Epoch 20/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.6147 - acc: 0.6667\n",
      "Epoch 21/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.6086 - acc: 0.6790\n",
      "Epoch 22/150\n",
      "243/243 [==============================] - 0s 47us/step - loss: 0.6069 - acc: 0.6790\n",
      "Epoch 23/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.6037 - acc: 0.6872\n",
      "Epoch 24/150\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.6009 - acc: 0.6626\n",
      "Epoch 25/150\n",
      "243/243 [==============================] - 0s 43us/step - loss: 0.5999 - acc: 0.6667\n",
      "Epoch 26/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.5915 - acc: 0.6790\n",
      "Epoch 27/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.5948 - acc: 0.6749\n",
      "Epoch 28/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.5876 - acc: 0.6872\n",
      "Epoch 29/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.5921 - acc: 0.6872\n",
      "Epoch 30/150\n",
      "243/243 [==============================] - 0s 46us/step - loss: 0.5840 - acc: 0.6708\n",
      "Epoch 31/150\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.5794 - acc: 0.6831\n",
      "Epoch 32/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.5832 - acc: 0.6790\n",
      "Epoch 33/150\n",
      "243/243 [==============================] - 0s 47us/step - loss: 0.5810 - acc: 0.6790\n",
      "Epoch 34/150\n",
      "243/243 [==============================] - 0s 54us/step - loss: 0.5827 - acc: 0.6708\n",
      "Epoch 35/150\n",
      "243/243 [==============================] - 0s 47us/step - loss: 0.6066 - acc: 0.6708\n",
      "Epoch 36/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.5763 - acc: 0.7037\n",
      "Epoch 37/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.5668 - acc: 0.6667\n",
      "Epoch 38/150\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.5690 - acc: 0.6831\n",
      "Epoch 39/150\n",
      "243/243 [==============================] - 0s 47us/step - loss: 0.5635 - acc: 0.6996\n",
      "Epoch 40/150\n",
      "243/243 [==============================] - 0s 54us/step - loss: 0.5670 - acc: 0.6872\n",
      "Epoch 41/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.5587 - acc: 0.6955\n",
      "Epoch 42/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.5609 - acc: 0.6872\n",
      "Epoch 43/150\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.5569 - acc: 0.7037\n",
      "Epoch 44/150\n",
      "243/243 [==============================] - 0s 43us/step - loss: 0.5545 - acc: 0.7078\n",
      "Epoch 45/150\n",
      "243/243 [==============================] - 0s 56us/step - loss: 0.5522 - acc: 0.6955\n",
      "Epoch 46/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.5546 - acc: 0.6914\n",
      "Epoch 47/150\n",
      "243/243 [==============================] - 0s 55us/step - loss: 0.5574 - acc: 0.7037\n",
      "Epoch 48/150\n",
      "243/243 [==============================] - 0s 45us/step - loss: 0.5473 - acc: 0.7078\n",
      "Epoch 49/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.5480 - acc: 0.6996\n",
      "Epoch 50/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.5478 - acc: 0.7119\n",
      "Epoch 51/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.5527 - acc: 0.7243\n",
      "Epoch 52/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.5478 - acc: 0.6872\n",
      "Epoch 53/150\n",
      "243/243 [==============================] - 0s 35us/step - loss: 0.5502 - acc: 0.7160\n",
      "Epoch 54/150\n",
      "243/243 [==============================] - 0s 39us/step - loss: 0.5338 - acc: 0.7243\n",
      "Epoch 55/150\n",
      "243/243 [==============================] - 0s 42us/step - loss: 0.5394 - acc: 0.7078\n",
      "Epoch 56/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 0s 39us/step - loss: 0.5337 - acc: 0.7243\n",
      "Epoch 57/150\n",
      "243/243 [==============================] - 0s 46us/step - loss: 0.5319 - acc: 0.7366\n",
      "Epoch 58/150\n",
      "243/243 [==============================] - 0s 47us/step - loss: 0.5327 - acc: 0.7202\n",
      "Epoch 59/150\n",
      "243/243 [==============================] - 0s 36us/step - loss: 0.5309 - acc: 0.7202\n",
      "Epoch 60/150\n",
      "243/243 [==============================] - 0s 56us/step - loss: 0.5279 - acc: 0.7284\n",
      "Epoch 61/150\n",
      "243/243 [==============================] - 0s 55us/step - loss: 0.5330 - acc: 0.7366\n",
      "Epoch 62/150\n",
      "243/243 [==============================] - 0s 46us/step - loss: 0.5382 - acc: 0.7119\n",
      "Epoch 63/150\n",
      "243/243 [==============================] - 0s 43us/step - loss: 0.5456 - acc: 0.7449\n",
      "Epoch 64/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.5425 - acc: 0.7202\n",
      "Epoch 65/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.5299 - acc: 0.7490\n",
      "Epoch 66/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.5178 - acc: 0.7243\n",
      "Epoch 67/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.5254 - acc: 0.7119\n",
      "Epoch 68/150\n",
      "243/243 [==============================] - 0s 46us/step - loss: 0.5283 - acc: 0.7407\n",
      "Epoch 69/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.5130 - acc: 0.7325\n",
      "Epoch 70/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.5143 - acc: 0.7407\n",
      "Epoch 71/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.5086 - acc: 0.7407\n",
      "Epoch 72/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.5068 - acc: 0.7325\n",
      "Epoch 73/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.5077 - acc: 0.7407\n",
      "Epoch 74/150\n",
      "243/243 [==============================] - 0s 46us/step - loss: 0.5049 - acc: 0.7449\n",
      "Epoch 75/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.5082 - acc: 0.7407\n",
      "Epoch 76/150\n",
      "243/243 [==============================] - 0s 53us/step - loss: 0.5066 - acc: 0.7490\n",
      "Epoch 77/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.5057 - acc: 0.7284\n",
      "Epoch 78/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.4982 - acc: 0.7490\n",
      "Epoch 79/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.4956 - acc: 0.7449\n",
      "Epoch 80/150\n",
      "243/243 [==============================] - 0s 44us/step - loss: 0.4954 - acc: 0.7531\n",
      "Epoch 81/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.4985 - acc: 0.7449\n",
      "Epoch 82/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.5124 - acc: 0.7366\n",
      "Epoch 83/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.5159 - acc: 0.7490\n",
      "Epoch 84/150\n",
      "243/243 [==============================] - 0s 47us/step - loss: 0.4846 - acc: 0.7449\n",
      "Epoch 85/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.4911 - acc: 0.7572\n",
      "Epoch 86/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.4904 - acc: 0.7490\n",
      "Epoch 87/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.4857 - acc: 0.7654\n",
      "Epoch 88/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.4846 - acc: 0.7572\n",
      "Epoch 89/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.4829 - acc: 0.7572\n",
      "Epoch 90/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.4887 - acc: 0.7613\n",
      "Epoch 91/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.4991 - acc: 0.7531\n",
      "Epoch 92/150\n",
      "243/243 [==============================] - 0s 54us/step - loss: 0.4947 - acc: 0.7654\n",
      "Epoch 93/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.4772 - acc: 0.7490\n",
      "Epoch 94/150\n",
      "243/243 [==============================] - 0s 56us/step - loss: 0.4847 - acc: 0.7449\n",
      "Epoch 95/150\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.4736 - acc: 0.7778\n",
      "Epoch 96/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.4740 - acc: 0.7572\n",
      "Epoch 97/150\n",
      "243/243 [==============================] - 0s 54us/step - loss: 0.4718 - acc: 0.7654\n",
      "Epoch 98/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.4727 - acc: 0.7778\n",
      "Epoch 99/150\n",
      "243/243 [==============================] - 0s 56us/step - loss: 0.4701 - acc: 0.7737\n",
      "Epoch 100/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.4698 - acc: 0.7695\n",
      "Epoch 101/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.4666 - acc: 0.7654\n",
      "Epoch 102/150\n",
      "243/243 [==============================] - 0s 46us/step - loss: 0.4672 - acc: 0.7695\n",
      "Epoch 103/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.4669 - acc: 0.7654\n",
      "Epoch 104/150\n",
      "243/243 [==============================] - 0s 53us/step - loss: 0.4661 - acc: 0.7737\n",
      "Epoch 105/150\n",
      "243/243 [==============================] - 0s 47us/step - loss: 0.4625 - acc: 0.7737\n",
      "Epoch 106/150\n",
      "243/243 [==============================] - 0s 46us/step - loss: 0.4593 - acc: 0.7778\n",
      "Epoch 107/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.4636 - acc: 0.7654\n",
      "Epoch 108/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.4811 - acc: 0.7778\n",
      "Epoch 109/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.4730 - acc: 0.7572\n",
      "Epoch 110/150\n",
      "243/243 [==============================] - 0s 54us/step - loss: 0.4630 - acc: 0.7901\n",
      "Epoch 111/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.4579 - acc: 0.7778\n",
      "Epoch 112/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.4537 - acc: 0.7778\n",
      "Epoch 113/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.4511 - acc: 0.7860\n",
      "Epoch 114/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.4509 - acc: 0.7860\n",
      "Epoch 115/150\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.4496 - acc: 0.7778\n",
      "Epoch 116/150\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.4490 - acc: 0.7860\n",
      "Epoch 117/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.4465 - acc: 0.7819\n",
      "Epoch 118/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.4518 - acc: 0.7695\n",
      "Epoch 119/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.4432 - acc: 0.7860\n",
      "Epoch 120/150\n",
      "243/243 [==============================] - 0s 54us/step - loss: 0.4421 - acc: 0.7984\n",
      "Epoch 121/150\n",
      "243/243 [==============================] - 0s 46us/step - loss: 0.4480 - acc: 0.7778\n",
      "Epoch 122/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.4374 - acc: 0.7942\n",
      "Epoch 123/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.4476 - acc: 0.7819\n",
      "Epoch 124/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.4560 - acc: 0.7737\n",
      "Epoch 125/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.4346 - acc: 0.7942\n",
      "Epoch 126/150\n",
      "243/243 [==============================] - 0s 55us/step - loss: 0.4306 - acc: 0.8066\n",
      "Epoch 127/150\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.4275 - acc: 0.8066\n",
      "Epoch 128/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.4337 - acc: 0.7984\n",
      "Epoch 129/150\n",
      "243/243 [==============================] - 0s 53us/step - loss: 0.4345 - acc: 0.7942\n",
      "Epoch 130/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.4237 - acc: 0.8066\n",
      "Epoch 131/150\n",
      "243/243 [==============================] - 0s 45us/step - loss: 0.4208 - acc: 0.8025\n",
      "Epoch 132/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.4228 - acc: 0.8025\n",
      "Epoch 133/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.4204 - acc: 0.8107\n",
      "Epoch 134/150\n",
      "243/243 [==============================] - 0s 42us/step - loss: 0.4180 - acc: 0.8189\n",
      "Epoch 135/150\n",
      "243/243 [==============================] - 0s 45us/step - loss: 0.4140 - acc: 0.8066\n",
      "Epoch 136/150\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.4226 - acc: 0.8025\n",
      "Epoch 137/150\n",
      "243/243 [==============================] - 0s 47us/step - loss: 0.4147 - acc: 0.8107\n",
      "Epoch 138/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.4138 - acc: 0.8148\n",
      "Epoch 139/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 0s 43us/step - loss: 0.4111 - acc: 0.7942\n",
      "Epoch 140/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.4110 - acc: 0.8107\n",
      "Epoch 141/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.4161 - acc: 0.8107\n",
      "Epoch 142/150\n",
      "243/243 [==============================] - 0s 44us/step - loss: 0.4080 - acc: 0.8148\n",
      "Epoch 143/150\n",
      "243/243 [==============================] - 0s 54us/step - loss: 0.4062 - acc: 0.8148\n",
      "Epoch 144/150\n",
      "243/243 [==============================] - 0s 55us/step - loss: 0.4061 - acc: 0.8066\n",
      "Epoch 145/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.4013 - acc: 0.8066\n",
      "Epoch 146/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.4111 - acc: 0.8189\n",
      "Epoch 147/150\n",
      "243/243 [==============================] - 0s 55us/step - loss: 0.3992 - acc: 0.8107\n",
      "Epoch 148/150\n",
      "243/243 [==============================] - 0s 42us/step - loss: 0.4063 - acc: 0.8148\n",
      "Epoch 149/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.3986 - acc: 0.8066\n",
      "Epoch 150/150\n",
      "243/243 [==============================] - 0s 54us/step - loss: 0.4021 - acc: 0.8066\n",
      "60/60 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=X.shape[1], activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "seed = 101\n",
    "np.random.seed(seed)\n",
    "estimator = KerasClassifier(build_fn=baseline_model, \n",
    "                                     epochs=150,\n",
    "                                     batch_size=32,\n",
    "                                     verbose=1)\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=seed)\n",
    "results = cross_val_score(estimator, X, y, cv=kfold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean = 0.77, stdv = 0.08\n"
     ]
    }
   ],
   "source": [
    "print(f'Results: mean = {results.mean():.2f}, stdv = {results.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3746e1183ceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best: {grid_result.best_score_} using {grid_result.best_params_}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Creating gridsearch and fitting\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=X.shape[1], activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "param_grid = {'batch_size': range(32, 64, 16),\n",
    "              'epochs':     range(60, 180, 60)}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\n",
    "\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch is taking forever to run... I am going to bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DS43SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
