{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6SKlgYrpcym"
   },
   "source": [
    "# Neural Networks Sprint Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrEbRrjVphPM"
   },
   "source": [
    "## 1) Define the following terms:\n",
    "\n",
    "- Neuron - a node in a network\n",
    "- Input Layer - first layer, receives input from the dataset\n",
    "- Hidden Layer - layers after the input layer, before the output layer. Deep learning is using two or more of these layers\n",
    "- Output Layer - the last layer of a neural network\n",
    "- Activation - function which decides how much signal to pass onto the next layer. Common functions: sigmoid, tanh, step, relu\n",
    "- Backpropagation - used to calculate a gradient needed for the weights used in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5EksLqnp4oB"
   },
   "source": [
    " YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ri_gRA2Jp728"
   },
   "source": [
    "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 1  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "\u001b[K    100% |████████████████████████████████| 317kB 33.7MB/s a 0:00:011\n",
      "\u001b[?25hCollecting keras-applications>=1.0.6 (from keras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710a78445def/Keras_Applications-1.0.7-py2.py3-none-any.whl (51kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 34.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.15.4)\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 2.8MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (2.8.0)\n",
      "Installing collected packages: keras-applications, keras-preprocessing, keras\n",
      "Successfully installed keras-2.2.4 keras-applications-1.0.7 keras-preprocessing-1.0.9\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 92.5MB 581kB/s eta 0:00:01�███ | 89.4MB 59.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/dc/5503d89e530988eb7a1aed337dcb456ef8150f7c06132233bd9e41ec0215/grpcio-1.19.0-cp36-cp36m-manylinux1_x86_64.whl (10.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 10.8MB 5.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.2MB 19.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "\u001b[K    100% |████████████████████████████████| 368kB 13.1MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.6.1)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 38.5MB/s a 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.15.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.0.9)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.0.7)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.14.1)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/e4/d8c18f2555add57ff21bf25af36d827145896a07607486cc79a2aea641af/Markdown-3.1-py2.py3-none-any.whl (87kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 37.2MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: mock>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (39.1.0)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
      "Requirement already satisfied: pbr>=0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (5.1.3)\n",
      "Building wheels for collected packages: termcolor, absl-py, gast\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "Successfully built termcolor absl-py gast\n",
      "Installing collected packages: grpcio, markdown, absl-py, tensorboard, tensorflow-estimator, termcolor, gast, astor, tensorflow\n",
      "Successfully installed absl-py-0.7.1 astor-0.7.1 gast-0.2.2 grpcio-1.19.0 markdown-3.1 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting sklearn.model_selection\n",
      "\u001b[31m  Could not find a version that satisfies the requirement sklearn.model_selection (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for sklearn.model_selection\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ig6ZTH8tpQ19"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "import numpy as np\n",
    "\n",
    "inputs = np.array([[1,1,1],\n",
    "                   [1,0,1],\n",
    "                   [0,1,1],\n",
    "                   [0,0,1]])\n",
    "\n",
    "correct_outputs = [[1],\n",
    "                   [0],\n",
    "                   [0],\n",
    "                   [0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Perceptron(object):\n",
    "    def __init__(self, rate = 0.01, niter = 10):\n",
    "        self.rate = rate\n",
    "        self.niter = niter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data\n",
    "        X : Training vectors, X.shape : [#samples, #features]\n",
    "        y : Target values, y.shape : [#samples]\n",
    "        \"\"\"\n",
    "\n",
    "        # weights\n",
    "        self.weight = np.zeros(1 + X.shape[1])\n",
    "\n",
    "        # Number of misclassifications\n",
    "        self.errors = []  # Number of misclassifications\n",
    "\n",
    "        for i in range(self.niter):\n",
    "            err = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                delta_w = self.rate * (target - self.predict(xi))\n",
    "                self.weight[1:] += delta_w * xi\n",
    "                self.weight[0] += delta_w\n",
    "                err += int(delta_w != 0.0)\n",
    "            self.errors.append(err)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.weight[1:]) + self.weight[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, 0)\n",
    "    \n",
    "pn = Perceptron(0.1, 10)\n",
    "pn.fit(inputs, correct_outputs)\n",
    "pn.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86HyRi8Osr3U"
   },
   "source": [
    "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
    "- Your network must have one hidden layer. \n",
    "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "- Train your model on the Heart Disease dataset from UCI:\n",
    "\n",
    "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
    "\n",
    "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNfiajv3v4Ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((303, 13), (303, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df.head()\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df.target\n",
    "\n",
    "X = np.array(X)\n",
    "\n",
    "y=np.array(y)\n",
    "y=np.reshape(y, (303,1))\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "        self.inputs = 13\n",
    "        self.hiddenNodes = 12\n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.L1_weights = np.random.randn(self.inputs, self.hiddenNodes)\n",
    "        self.L2_weights = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def feed_forward(self, X):\n",
    "        # Weighted sum between inputs and hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.L1_weights)\n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        # Weighted sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.L2_weights)\n",
    "        # final actiavtion of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        return self.activated_output\n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "        return 1/(1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1-s)\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        # backward propagate through the network\n",
    "        self.o_error = y - o # error in output\n",
    "        self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to error \n",
    "        \n",
    "        self.z2_error = self.o_delta.dot(self.L2_weights.T) # z2 error: how much our hidden layer weights contributed to output error\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden) # applying derivative of sigmoid to z2 error\n",
    "        \n",
    "        self.L1_weights += X.T.dot(self.z2_delta)\n",
    "        self.L2_weights += self.activated_hidden.T.dot(self.o_delta) # adjusting second set (hidden --> output) weights\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:23: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28],\n",
       "       [1.30194916e-28]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "\n",
    "for _ in range(1000):\n",
    "    NN.train(X, y)\n",
    "NN.feed_forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGT1oRzXw3H9"
   },
   "source": [
    "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
    "\n",
    "- Use the Heart Disease Dataset (binary classification)\n",
    "- Use an appropriate loss function for a binary classification task\n",
    "- Use an appropriate activation function on the final layer of your network. \n",
    "- Train your model using verbose output for ease of grading.\n",
    "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
    "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWw4IYxLxKwH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                224       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 513\n",
      "Trainable params: 513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64\n",
    "num_classes = 1\n",
    "epochs = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation = 'relu', input_shape=(13,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "model.compile(loss='mse', optimizer = 'adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 272 samples, validate on 31 samples\n",
      "Epoch 1/50\n",
      "272/272 [==============================] - 0s 916us/step - loss: 0.4224 - acc: 0.5735 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "272/272 [==============================] - 0s 42us/step - loss: 0.4191 - acc: 0.5809 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "272/272 [==============================] - 0s 41us/step - loss: 0.4401 - acc: 0.5588 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "272/272 [==============================] - 0s 38us/step - loss: 0.3769 - acc: 0.6213 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "272/272 [==============================] - 0s 37us/step - loss: 0.4221 - acc: 0.5772 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "272/272 [==============================] - 0s 38us/step - loss: 0.4069 - acc: 0.5846 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "272/272 [==============================] - 0s 37us/step - loss: 0.4262 - acc: 0.5699 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "272/272 [==============================] - 0s 35us/step - loss: 0.3771 - acc: 0.6140 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "272/272 [==============================] - 0s 35us/step - loss: 0.4063 - acc: 0.5882 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "272/272 [==============================] - 0s 39us/step - loss: 0.3867 - acc: 0.6140 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "272/272 [==============================] - 0s 35us/step - loss: 0.3934 - acc: 0.6066 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "272/272 [==============================] - 0s 36us/step - loss: 0.3934 - acc: 0.6066 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "272/272 [==============================] - 0s 36us/step - loss: 0.3892 - acc: 0.6103 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "272/272 [==============================] - 0s 38us/step - loss: 0.3943 - acc: 0.6029 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "272/272 [==============================] - 0s 37us/step - loss: 0.3932 - acc: 0.6066 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "272/272 [==============================] - 0s 36us/step - loss: 0.3856 - acc: 0.6140 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "272/272 [==============================] - 0s 32us/step - loss: 0.3825 - acc: 0.6140 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "272/272 [==============================] - 0s 33us/step - loss: 0.3970 - acc: 0.5993 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "272/272 [==============================] - 0s 35us/step - loss: 0.3969 - acc: 0.5993 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "272/272 [==============================] - 0s 36us/step - loss: 0.3886 - acc: 0.6103 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "272/272 [==============================] - 0s 36us/step - loss: 0.3941 - acc: 0.5993 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "272/272 [==============================] - 0s 35us/step - loss: 0.3950 - acc: 0.6029 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "272/272 [==============================] - 0s 34us/step - loss: 0.3897 - acc: 0.6103 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "272/272 [==============================] - 0s 36us/step - loss: 0.3912 - acc: 0.6066 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "272/272 [==============================] - 0s 35us/step - loss: 0.3857 - acc: 0.6140 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "272/272 [==============================] - 0s 35us/step - loss: 0.3843 - acc: 0.6140 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "272/272 [==============================] - 0s 34us/step - loss: 0.3892 - acc: 0.6066 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "272/272 [==============================] - 0s 36us/step - loss: 0.3725 - acc: 0.6176 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "272/272 [==============================] - 0s 34us/step - loss: 0.3820 - acc: 0.6140 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "272/272 [==============================] - 0s 35us/step - loss: 0.3922 - acc: 0.6066 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "272/272 [==============================] - 0s 37us/step - loss: 0.3987 - acc: 0.5956 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "272/272 [==============================] - 0s 33us/step - loss: 0.3934 - acc: 0.6029 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "272/272 [==============================] - 0s 35us/step - loss: 0.3928 - acc: 0.6066 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "272/272 [==============================] - 0s 35us/step - loss: 0.3905 - acc: 0.6103 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "272/272 [==============================] - 0s 35us/step - loss: 0.3904 - acc: 0.6103 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "272/272 [==============================] - 0s 34us/step - loss: 0.3911 - acc: 0.6066 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "272/272 [==============================] - 0s 39us/step - loss: 0.3898 - acc: 0.6103 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "272/272 [==============================] - 0s 34us/step - loss: 0.3800 - acc: 0.6176 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "272/272 [==============================] - 0s 34us/step - loss: 0.3794 - acc: 0.6176 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "272/272 [==============================] - 0s 36us/step - loss: 0.3898 - acc: 0.6103 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "272/272 [==============================] - 0s 33us/step - loss: 0.3679 - acc: 0.6250 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "272/272 [==============================] - 0s 34us/step - loss: 0.3758 - acc: 0.6176 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "272/272 [==============================] - 0s 33us/step - loss: 0.3878 - acc: 0.6103 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "272/272 [==============================] - 0s 36us/step - loss: 0.3849 - acc: 0.6140 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "272/272 [==============================] - 0s 35us/step - loss: 0.3827 - acc: 0.6176 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "272/272 [==============================] - 0s 35us/step - loss: 0.3971 - acc: 0.5993 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "272/272 [==============================] - 0s 35us/step - loss: 0.3934 - acc: 0.5993 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "272/272 [==============================] - 0s 34us/step - loss: 0.3788 - acc: 0.6140 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "272/272 [==============================] - 0s 34us/step - loss: 0.3768 - acc: 0.6213 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "272/272 [==============================] - 0s 35us/step - loss: 0.3747 - acc: 0.6176 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "303/303 [==============================] - 0s 17us/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, epochs=epochs, validation_split=.1)\n",
    "scores = model.evaluate(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 1ms/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 100us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 101us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 101us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 100us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 100us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 101us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 95us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 97us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 105us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 102us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 103us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 106us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 100us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 102us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 103us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 99us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 95us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 100us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 105us/step - loss: 5.1067 - acc: 0.6832\n",
      "101/101 [==============================] - 0s 550us/step\n",
      "202/202 [==============================] - 0s 52us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 6.3397 - acc: 0.4950\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 105us/step - loss: 2.4513 - acc: 0.4703\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 101us/step - loss: 1.3025 - acc: 0.5099\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 95us/step - loss: 0.9914 - acc: 0.5594\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 93us/step - loss: 0.8809 - acc: 0.6188\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 100us/step - loss: 0.8411 - acc: 0.6287\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 100us/step - loss: 0.7960 - acc: 0.6386\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 101us/step - loss: 0.7548 - acc: 0.6436\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 106us/step - loss: 0.7803 - acc: 0.6436\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 103us/step - loss: 0.7388 - acc: 0.6584\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 108us/step - loss: 0.7369 - acc: 0.6188\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 101us/step - loss: 0.7403 - acc: 0.6485\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 107us/step - loss: 0.7038 - acc: 0.6436\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 98us/step - loss: 0.6930 - acc: 0.6733\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 110us/step - loss: 0.6524 - acc: 0.6931\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 106us/step - loss: 0.6256 - acc: 0.6782\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 98us/step - loss: 0.5991 - acc: 0.6980\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 107us/step - loss: 0.5879 - acc: 0.6881\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 96us/step - loss: 0.6616 - acc: 0.6881\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 110us/step - loss: 0.5930 - acc: 0.6980\n",
      "101/101 [==============================] - 0s 695us/step\n",
      "202/202 [==============================] - 0s 51us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 1.1915 - acc: 0.5990\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 102us/step - loss: 0.6406 - acc: 0.7574\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 109us/step - loss: 0.5600 - acc: 0.7871\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 109us/step - loss: 0.5317 - acc: 0.7921\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 102us/step - loss: 0.5176 - acc: 0.7921\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 101us/step - loss: 0.5095 - acc: 0.7921\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 103us/step - loss: 0.4958 - acc: 0.7970\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 95us/step - loss: 0.4909 - acc: 0.7970\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 103us/step - loss: 0.4862 - acc: 0.7970\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 108us/step - loss: 0.4796 - acc: 0.7822\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 111us/step - loss: 0.4758 - acc: 0.7822\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 108us/step - loss: 0.4717 - acc: 0.7871\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 103us/step - loss: 0.4680 - acc: 0.7921\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 99us/step - loss: 0.4719 - acc: 0.7921\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 104us/step - loss: 0.4643 - acc: 0.7871\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 117us/step - loss: 0.4624 - acc: 0.7921\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 108us/step - loss: 0.4621 - acc: 0.7772\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 114us/step - loss: 0.4607 - acc: 0.7871\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 105us/step - loss: 0.4557 - acc: 0.7921\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 107us/step - loss: 0.4539 - acc: 0.7970\n",
      "101/101 [==============================] - 0s 837us/step\n",
      "202/202 [==============================] - 0s 50us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 5.1068 - acc: 0.6832\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 62us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 60us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 62us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 62us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 67us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 59us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 69us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 59us/step - loss: 5.1067 - acc: 0.6832\n",
      "101/101 [==============================] - 0s 972us/step\n",
      "202/202 [==============================] - 0s 44us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 1.7062 - acc: 0.6584\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 61us/step - loss: 1.3563 - acc: 0.6535\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 1.4429 - acc: 0.6733\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 1.3216 - acc: 0.6832\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 59us/step - loss: 1.1720 - acc: 0.6980\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 59us/step - loss: 1.0893 - acc: 0.6931\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 0.9665 - acc: 0.7129\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 60us/step - loss: 0.8903 - acc: 0.7178\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 0.8471 - acc: 0.6832\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 60us/step - loss: 0.7978 - acc: 0.7030\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 60us/step - loss: 0.7509 - acc: 0.6881\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 0.6928 - acc: 0.7129\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 61us/step - loss: 0.6735 - acc: 0.7030\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 0.6574 - acc: 0.7228\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 0.6562 - acc: 0.7079\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 0.7304 - acc: 0.6980\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 62us/step - loss: 0.6874 - acc: 0.6931\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 0.6501 - acc: 0.7030\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 0.6405 - acc: 0.6980\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 0.6731 - acc: 0.6782\n",
      "101/101 [==============================] - 0s 1ms/step\n",
      "202/202 [==============================] - 0s 30us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 68us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 59us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 56us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 56us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 56us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 58us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 56us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 55us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 58us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 56us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 60us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 60us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 59us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 68us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 62us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 60us/step - loss: 13.1658 - acc: 0.1832\n",
      "101/101 [==============================] - 0s 1ms/step\n",
      "202/202 [==============================] - 0s 38us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 10.8649 - acc: 0.3168\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 10.7063 - acc: 0.3168\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 36us/step - loss: 10.1958 - acc: 0.3168\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 9.1074 - acc: 0.3267\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 40us/step - loss: 7.1546 - acc: 0.3713\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 41us/step - loss: 4.5217 - acc: 0.4901\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 3.3081 - acc: 0.5990\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 3.2212 - acc: 0.6535\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 39us/step - loss: 2.9863 - acc: 0.6832\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 39us/step - loss: 2.3409 - acc: 0.6980\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 1.9706 - acc: 0.6683\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 36us/step - loss: 2.0278 - acc: 0.6436\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 37us/step - loss: 1.7937 - acc: 0.6733\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 35us/step - loss: 1.7159 - acc: 0.6931\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 35us/step - loss: 1.6908 - acc: 0.6980\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 1.6516 - acc: 0.7030\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 35us/step - loss: 1.6162 - acc: 0.7030\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 35us/step - loss: 1.5864 - acc: 0.6931\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 37us/step - loss: 1.5757 - acc: 0.6832\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 35us/step - loss: 1.4989 - acc: 0.6931\n",
      "101/101 [==============================] - 0s 1ms/step\n",
      "202/202 [==============================] - 0s 35us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 50us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 40us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 39us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 37us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 41us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 40us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 45us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 46us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 51us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 46us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 48us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 47us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 45us/step - loss: 7.9712 - acc: 0.5000\n",
      "101/101 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 0s 31us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 47us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 48us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 40us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 40us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 48us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 51us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 49us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 40us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 41us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 41us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 39us/step - loss: 13.1658 - acc: 0.1832\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "202/202 [==============================] - 0s 27us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 31us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 32us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 32us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 31us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 32us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 27us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 27us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 26us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 5.1067 - acc: 0.6832\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "202/202 [==============================] - 0s 22us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 33us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 37us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 37us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 31us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 7.9712 - acc: 0.5000\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "202/202 [==============================] - 0s 17us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 2.1567 - acc: 0.8119\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 1.5999 - acc: 0.8119\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 1.2713 - acc: 0.8168\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 32us/step - loss: 1.0795 - acc: 0.8069\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 33us/step - loss: 0.9322 - acc: 0.8020\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 0.7836 - acc: 0.7921\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 33us/step - loss: 0.7050 - acc: 0.7574\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 37us/step - loss: 0.6907 - acc: 0.7525\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 0.6727 - acc: 0.7525\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 33us/step - loss: 0.6382 - acc: 0.7525\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 32us/step - loss: 0.6215 - acc: 0.7624\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 37us/step - loss: 0.6132 - acc: 0.7673\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 32us/step - loss: 0.6019 - acc: 0.7624\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 0.5774 - acc: 0.7624\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 0.5645 - acc: 0.7574\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 0.5504 - acc: 0.7525\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 0.5330 - acc: 0.7525\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 31us/step - loss: 0.5185 - acc: 0.7475\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 0.4991 - acc: 0.7574\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 27us/step - loss: 0.4901 - acc: 0.7673\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "202/202 [==============================] - 0s 24us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 10.5948 - acc: 0.3168\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 27us/step - loss: 10.2367 - acc: 0.3168\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 9.5763 - acc: 0.3168\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 8.4073 - acc: 0.3168\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 6.7378 - acc: 0.3218\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 4.8828 - acc: 0.3416\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 3.0333 - acc: 0.3663\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 27us/step - loss: 1.8420 - acc: 0.4802\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 23us/step - loss: 1.4143 - acc: 0.5495\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 23us/step - loss: 1.5994 - acc: 0.6337\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 24us/step - loss: 1.8597 - acc: 0.6634\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 23us/step - loss: 1.9683 - acc: 0.6584\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 24us/step - loss: 1.9192 - acc: 0.6584\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 23us/step - loss: 1.7720 - acc: 0.6535\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 24us/step - loss: 1.5958 - acc: 0.6238\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 23us/step - loss: 1.4467 - acc: 0.5891\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 23us/step - loss: 1.3979 - acc: 0.5693\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 24us/step - loss: 1.3980 - acc: 0.5545\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 1.4312 - acc: 0.5297\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 23us/step - loss: 1.4334 - acc: 0.5198\n",
      "101/101 [==============================] - 0s 3ms/step\n",
      "202/202 [==============================] - 0s 26us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 24us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 23us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 26us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 24us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 24us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 26us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 27us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 23us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 24us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 23us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 24us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 23us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 23us/step - loss: 8.0590 - acc: 0.5000\n",
      "101/101 [==============================] - 0s 3ms/step\n",
      "202/202 [==============================] - 0s 16us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 5ms/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 27us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 27us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 24us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 23us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 24us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 24us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 24us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 27us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 24us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 24us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 24us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 23us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 23us/step - loss: 2.9201 - acc: 0.8168\n",
      "101/101 [==============================] - 0s 3ms/step\n",
      "202/202 [==============================] - 0s 21us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 5ms/step - loss: 10.6404 - acc: 0.3168\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 22us/step - loss: 10.1563 - acc: 0.3168\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 9.0794 - acc: 0.3168\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 7.5911 - acc: 0.3168\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 6.0417 - acc: 0.3168\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 4.4822 - acc: 0.3069\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 3.1204 - acc: 0.3317\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 24us/step - loss: 2.1629 - acc: 0.3267\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 1.6191 - acc: 0.3663\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 1.3120 - acc: 0.3663\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 23us/step - loss: 1.1203 - acc: 0.3960\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 33us/step - loss: 0.9849 - acc: 0.3861\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 26us/step - loss: 0.8965 - acc: 0.4208\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 0.8468 - acc: 0.5297\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 0.8228 - acc: 0.5990\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 0.7936 - acc: 0.6535\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 0.7592 - acc: 0.6881\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 0.7339 - acc: 0.6881\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 27us/step - loss: 0.7001 - acc: 0.6980\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 22us/step - loss: 0.6865 - acc: 0.6980\n",
      "101/101 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 0s 20us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 5ms/step - loss: 4.4681 - acc: 0.5644\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 32us/step - loss: 2.7716 - acc: 0.5941\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 27us/step - loss: 2.9259 - acc: 0.5248\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 2.9662 - acc: 0.5099\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 23us/step - loss: 2.3968 - acc: 0.5594\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 2.4289 - acc: 0.6188\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 32us/step - loss: 2.6728 - acc: 0.6238\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 2.3306 - acc: 0.6287\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 1.9301 - acc: 0.6238\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 1.7785 - acc: 0.6287\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 1.7085 - acc: 0.6139\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 1.5591 - acc: 0.6634\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 1.5061 - acc: 0.6535\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 26us/step - loss: 1.4392 - acc: 0.6535\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 27us/step - loss: 1.4489 - acc: 0.6287\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 24us/step - loss: 1.3782 - acc: 0.6485\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 1.2792 - acc: 0.6238\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 26us/step - loss: 1.2597 - acc: 0.6436\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 26us/step - loss: 1.3449 - acc: 0.6337\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 26us/step - loss: 1.3336 - acc: 0.6337\n",
      "101/101 [==============================] - 0s 4ms/step\n",
      "202/202 [==============================] - 0s 15us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 6ms/step - loss: 1.0802 - acc: 0.7574\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 28us/step - loss: 0.9360 - acc: 0.7426\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 0.8752 - acc: 0.7228\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 26us/step - loss: 0.8440 - acc: 0.7178\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 0.8230 - acc: 0.7525\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 0.8051 - acc: 0.7525\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 26us/step - loss: 0.7740 - acc: 0.7525\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 0.7444 - acc: 0.7525\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 0.7138 - acc: 0.7525\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 27us/step - loss: 0.6944 - acc: 0.7327\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 0.6723 - acc: 0.7475\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 23us/step - loss: 0.6331 - acc: 0.7525\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 23us/step - loss: 0.6078 - acc: 0.7624\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 0.5839 - acc: 0.7673\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 26us/step - loss: 0.5603 - acc: 0.7673\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 0.5503 - acc: 0.7525\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 27us/step - loss: 0.5686 - acc: 0.7327\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 0.6396 - acc: 0.7129\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 26us/step - loss: 0.7186 - acc: 0.6040\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 25us/step - loss: 0.7242 - acc: 0.5990\n",
      "101/101 [==============================] - 0s 4ms/step\n",
      "202/202 [==============================] - 0s 30us/step\n",
      "Epoch 1/20\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 2/20\n",
      "303/303 [==============================] - 0s 40us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 3/20\n",
      "303/303 [==============================] - 0s 38us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 4/20\n",
      "303/303 [==============================] - 0s 40us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 5/20\n",
      "303/303 [==============================] - 0s 38us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 6/20\n",
      "303/303 [==============================] - 0s 36us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 7/20\n",
      "303/303 [==============================] - 0s 40us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 8/20\n",
      "303/303 [==============================] - 0s 40us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 9/20\n",
      "303/303 [==============================] - 0s 43us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 10/20\n",
      "303/303 [==============================] - 0s 38us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 11/20\n",
      "303/303 [==============================] - 0s 38us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 12/20\n",
      "303/303 [==============================] - 0s 41us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 13/20\n",
      "303/303 [==============================] - 0s 39us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 14/20\n",
      "303/303 [==============================] - 0s 40us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 15/20\n",
      "303/303 [==============================] - 0s 40us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 16/20\n",
      "303/303 [==============================] - 0s 37us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 17/20\n",
      "303/303 [==============================] - 0s 39us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 18/20\n",
      "303/303 [==============================] - 0s 43us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 19/20\n",
      "303/303 [==============================] - 0s 37us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 20/20\n",
      "303/303 [==============================] - 0s 42us/step - loss: 7.2609 - acc: 0.5446\n",
      "Best: 0.7194719568337544 using {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.25412541657391163, Stdev: 0.27346973974926814 with {'batch_size': 10, 'epochs': 20}\n",
      "Means: 0.5412541269862613, Stdev: 0.4123960143520592 with {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.7194719568337544, Stdev: 0.20328532043271239 with {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.2541254127379691, Stdev: 0.27346973478873965 with {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.2607260781152807, Stdev: 0.1854656323600358 with {'batch_size': 80, 'epochs': 20}\n",
      "Means: 0.3663366243194039, Stdev: 0.236935187907703 with {'batch_size': 100, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=13, activation = 'relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# define grid search parameters\n",
    "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "              'epochs': [20]}\n",
    "\n",
    "# Create grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X,y)\n",
    "\n",
    "# Report results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "202/202 [==============================] - 1s 6ms/step - loss: 4.9206 - acc: 0.6832\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 49us/step - loss: 4.6760 - acc: 0.6832\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 48us/step - loss: 4.1654 - acc: 0.6782\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 51us/step - loss: 3.4020 - acc: 0.6238\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 46us/step - loss: 3.1734 - acc: 0.5000\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 45us/step - loss: 2.9785 - acc: 0.4802\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 47us/step - loss: 2.7212 - acc: 0.5248\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 47us/step - loss: 2.4771 - acc: 0.5149\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 46us/step - loss: 2.2776 - acc: 0.5099\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 44us/step - loss: 2.1668 - acc: 0.5495\n",
      "101/101 [==============================] - 0s 5ms/step\n",
      "202/202 [==============================] - 0s 30us/step\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 1s 7ms/step - loss: 6.6974 - acc: 0.5050\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 49us/step - loss: 5.1402 - acc: 0.5347\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 48us/step - loss: 3.0804 - acc: 0.6188\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 50us/step - loss: 1.8682 - acc: 0.6188\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 46us/step - loss: 1.9173 - acc: 0.6139\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 47us/step - loss: 1.7577 - acc: 0.6386\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 43us/step - loss: 1.4208 - acc: 0.6287\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 49us/step - loss: 1.3829 - acc: 0.6832\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 45us/step - loss: 1.2635 - acc: 0.6881\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 49us/step - loss: 1.2283 - acc: 0.6782\n",
      "101/101 [==============================] - 1s 5ms/step\n",
      "202/202 [==============================] - 0s 33us/step\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 1s 7ms/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 46us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 49us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 51us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 51us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 44us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 49us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 49us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 46us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 45us/step - loss: 2.9201 - acc: 0.8168\n",
      "101/101 [==============================] - 1s 5ms/step\n",
      "202/202 [==============================] - 0s 38us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 7ms/step - loss: 2.6972 - acc: 0.5693\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 51us/step - loss: 2.0495 - acc: 0.4802\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 54us/step - loss: 2.1882 - acc: 0.6040\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 47us/step - loss: 1.9469 - acc: 0.4554\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 51us/step - loss: 1.8407 - acc: 0.4802\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 52us/step - loss: 1.7182 - acc: 0.6089\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 1.5837 - acc: 0.5050\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 45us/step - loss: 1.4642 - acc: 0.5198\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 46us/step - loss: 1.3767 - acc: 0.5693\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 47us/step - loss: 1.3501 - acc: 0.5743\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 52us/step - loss: 1.3182 - acc: 0.5941\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 58us/step - loss: 1.4325 - acc: 0.6535\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 47us/step - loss: 1.3713 - acc: 0.5693\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 47us/step - loss: 1.5424 - acc: 0.5149\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 45us/step - loss: 1.2467 - acc: 0.6040\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 47us/step - loss: 1.1980 - acc: 0.6139\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 50us/step - loss: 1.1591 - acc: 0.6337\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 1.1684 - acc: 0.6139\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 52us/step - loss: 1.1496 - acc: 0.6337\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 49us/step - loss: 1.1101 - acc: 0.6584\n",
      "101/101 [==============================] - 1s 5ms/step\n",
      "202/202 [==============================] - 0s 34us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 2s 7ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 48us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 51us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 45us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 50us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 56us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 50us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 50us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 53us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 50us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 55us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 48us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 50us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 48us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 51us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 49us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 53us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 60us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 60us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 60us/step - loss: 7.9712 - acc: 0.5000\n",
      "101/101 [==============================] - 1s 5ms/step\n",
      "202/202 [==============================] - 0s 39us/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 2s 8ms/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 45us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 54us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 55us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 47us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 41us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 53us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 49us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 49us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 48us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 49us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 50us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 57us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 57us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 53us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 59us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 55us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 55us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 13.1658 - acc: 0.1832\n",
      "101/101 [==============================] - 1s 6ms/step\n",
      "202/202 [==============================] - 0s 36us/step\n",
      "Epoch 1/40\n",
      "202/202 [==============================] - 2s 8ms/step - loss: 4.1852 - acc: 0.6832\n",
      "Epoch 2/40\n",
      "202/202 [==============================] - 0s 48us/step - loss: 3.1736 - acc: 0.6832\n",
      "Epoch 3/40\n",
      "202/202 [==============================] - 0s 48us/step - loss: 2.3059 - acc: 0.6832\n",
      "Epoch 4/40\n",
      "202/202 [==============================] - 0s 53us/step - loss: 1.5239 - acc: 0.6683\n",
      "Epoch 5/40\n",
      "202/202 [==============================] - 0s 52us/step - loss: 1.0937 - acc: 0.6337\n",
      "Epoch 6/40\n",
      "202/202 [==============================] - 0s 52us/step - loss: 0.9693 - acc: 0.5990\n",
      "Epoch 7/40\n",
      "202/202 [==============================] - 0s 49us/step - loss: 0.9189 - acc: 0.5792\n",
      "Epoch 8/40\n",
      "202/202 [==============================] - 0s 52us/step - loss: 0.8399 - acc: 0.6089\n",
      "Epoch 9/40\n",
      "202/202 [==============================] - 0s 54us/step - loss: 0.7730 - acc: 0.6287\n",
      "Epoch 10/40\n",
      "202/202 [==============================] - 0s 51us/step - loss: 0.7494 - acc: 0.6337\n",
      "Epoch 11/40\n",
      "202/202 [==============================] - 0s 52us/step - loss: 0.7386 - acc: 0.6386\n",
      "Epoch 12/40\n",
      "202/202 [==============================] - 0s 49us/step - loss: 0.7166 - acc: 0.6485\n",
      "Epoch 13/40\n",
      "202/202 [==============================] - 0s 50us/step - loss: 0.7078 - acc: 0.6337\n",
      "Epoch 14/40\n",
      "202/202 [==============================] - 0s 51us/step - loss: 0.6989 - acc: 0.6287\n",
      "Epoch 15/40\n",
      "202/202 [==============================] - 0s 49us/step - loss: 0.6874 - acc: 0.6436\n",
      "Epoch 16/40\n",
      "202/202 [==============================] - 0s 48us/step - loss: 0.6887 - acc: 0.6584\n",
      "Epoch 17/40\n",
      "202/202 [==============================] - 0s 54us/step - loss: 0.6688 - acc: 0.6683\n",
      "Epoch 18/40\n",
      "202/202 [==============================] - 0s 46us/step - loss: 0.6692 - acc: 0.6139\n",
      "Epoch 19/40\n",
      "202/202 [==============================] - 0s 51us/step - loss: 0.6782 - acc: 0.6337\n",
      "Epoch 20/40\n",
      "202/202 [==============================] - 0s 62us/step - loss: 0.6547 - acc: 0.6584\n",
      "Epoch 21/40\n",
      "202/202 [==============================] - 0s 64us/step - loss: 0.6665 - acc: 0.6634\n",
      "Epoch 22/40\n",
      "202/202 [==============================] - 0s 55us/step - loss: 0.6476 - acc: 0.6634\n",
      "Epoch 23/40\n",
      "202/202 [==============================] - 0s 50us/step - loss: 0.6392 - acc: 0.6683\n",
      "Epoch 24/40\n",
      "202/202 [==============================] - 0s 47us/step - loss: 0.6305 - acc: 0.6634\n",
      "Epoch 25/40\n",
      "202/202 [==============================] - 0s 50us/step - loss: 0.6396 - acc: 0.6584\n",
      "Epoch 26/40\n",
      "202/202 [==============================] - 0s 53us/step - loss: 0.6310 - acc: 0.6535\n",
      "Epoch 27/40\n",
      "202/202 [==============================] - 0s 50us/step - loss: 0.6163 - acc: 0.6782\n",
      "Epoch 28/40\n",
      "202/202 [==============================] - 0s 56us/step - loss: 0.6697 - acc: 0.6931\n",
      "Epoch 29/40\n",
      "202/202 [==============================] - 0s 61us/step - loss: 0.6701 - acc: 0.6881\n",
      "Epoch 30/40\n",
      "202/202 [==============================] - 0s 55us/step - loss: 0.6152 - acc: 0.6782\n",
      "Epoch 31/40\n",
      "202/202 [==============================] - 0s 55us/step - loss: 0.6004 - acc: 0.6782\n",
      "Epoch 32/40\n",
      "202/202 [==============================] - 0s 54us/step - loss: 0.5971 - acc: 0.6733\n",
      "Epoch 33/40\n",
      "202/202 [==============================] - 0s 62us/step - loss: 0.5912 - acc: 0.7030\n",
      "Epoch 34/40\n",
      "202/202 [==============================] - 0s 61us/step - loss: 0.5998 - acc: 0.6832\n",
      "Epoch 35/40\n",
      "202/202 [==============================] - 0s 50us/step - loss: 0.6014 - acc: 0.6782\n",
      "Epoch 36/40\n",
      "202/202 [==============================] - 0s 57us/step - loss: 0.5881 - acc: 0.7079\n",
      "Epoch 37/40\n",
      "202/202 [==============================] - 0s 53us/step - loss: 0.5757 - acc: 0.7030\n",
      "Epoch 38/40\n",
      "202/202 [==============================] - 0s 50us/step - loss: 0.5735 - acc: 0.6980\n",
      "Epoch 39/40\n",
      "202/202 [==============================] - 0s 45us/step - loss: 0.5689 - acc: 0.7129\n",
      "Epoch 40/40\n",
      "202/202 [==============================] - 0s 47us/step - loss: 0.5980 - acc: 0.6782\n",
      "101/101 [==============================] - 1s 6ms/step\n",
      "202/202 [==============================] - 0s 36us/step\n",
      "Epoch 1/40\n",
      "202/202 [==============================] - 2s 8ms/step - loss: 7.4769 - acc: 0.5000\n",
      "Epoch 2/40\n",
      "202/202 [==============================] - 0s 51us/step - loss: 6.1885 - acc: 0.5000\n",
      "Epoch 3/40\n",
      "202/202 [==============================] - 0s 59us/step - loss: 4.8594 - acc: 0.5050\n",
      "Epoch 4/40\n",
      "202/202 [==============================] - 0s 51us/step - loss: 3.8326 - acc: 0.5050\n",
      "Epoch 5/40\n",
      "202/202 [==============================] - 0s 49us/step - loss: 2.9123 - acc: 0.4901\n",
      "Epoch 6/40\n",
      "202/202 [==============================] - 0s 46us/step - loss: 2.2122 - acc: 0.4703\n",
      "Epoch 7/40\n",
      "202/202 [==============================] - 0s 46us/step - loss: 1.7293 - acc: 0.4356\n",
      "Epoch 8/40\n",
      "202/202 [==============================] - 0s 48us/step - loss: 1.3846 - acc: 0.4455\n",
      "Epoch 9/40\n",
      "202/202 [==============================] - 0s 53us/step - loss: 1.1434 - acc: 0.5000\n",
      "Epoch 10/40\n",
      "202/202 [==============================] - 0s 52us/step - loss: 0.9871 - acc: 0.5050\n",
      "Epoch 11/40\n",
      "202/202 [==============================] - 0s 49us/step - loss: 0.9104 - acc: 0.5347\n",
      "Epoch 12/40\n",
      "202/202 [==============================] - 0s 52us/step - loss: 0.8671 - acc: 0.5495\n",
      "Epoch 13/40\n",
      "202/202 [==============================] - 0s 57us/step - loss: 0.8389 - acc: 0.5495\n",
      "Epoch 14/40\n",
      "202/202 [==============================] - 0s 51us/step - loss: 0.8155 - acc: 0.5495\n",
      "Epoch 15/40\n",
      "202/202 [==============================] - 0s 51us/step - loss: 0.8019 - acc: 0.5446\n",
      "Epoch 16/40\n",
      "202/202 [==============================] - 0s 56us/step - loss: 0.7882 - acc: 0.5495\n",
      "Epoch 17/40\n",
      "202/202 [==============================] - 0s 52us/step - loss: 0.7784 - acc: 0.5644\n",
      "Epoch 18/40\n",
      "202/202 [==============================] - 0s 50us/step - loss: 0.7683 - acc: 0.5693\n",
      "Epoch 19/40\n",
      "202/202 [==============================] - 0s 50us/step - loss: 0.7584 - acc: 0.5792\n",
      "Epoch 20/40\n",
      "202/202 [==============================] - 0s 54us/step - loss: 0.7493 - acc: 0.5693\n",
      "Epoch 21/40\n",
      "202/202 [==============================] - 0s 59us/step - loss: 0.7387 - acc: 0.5594\n",
      "Epoch 22/40\n",
      "202/202 [==============================] - 0s 56us/step - loss: 0.7290 - acc: 0.5693\n",
      "Epoch 23/40\n",
      "202/202 [==============================] - 0s 61us/step - loss: 0.7289 - acc: 0.5594\n",
      "Epoch 24/40\n",
      "202/202 [==============================] - 0s 51us/step - loss: 0.7287 - acc: 0.5594\n",
      "Epoch 25/40\n",
      "202/202 [==============================] - 0s 54us/step - loss: 0.7253 - acc: 0.5594\n",
      "Epoch 26/40\n",
      "202/202 [==============================] - 0s 49us/step - loss: 0.7190 - acc: 0.5644\n",
      "Epoch 27/40\n",
      "202/202 [==============================] - 0s 52us/step - loss: 0.7122 - acc: 0.5792\n",
      "Epoch 28/40\n",
      "202/202 [==============================] - 0s 52us/step - loss: 0.7073 - acc: 0.5792\n",
      "Epoch 29/40\n",
      "202/202 [==============================] - 0s 60us/step - loss: 0.7045 - acc: 0.5891\n",
      "Epoch 30/40\n",
      "202/202 [==============================] - 0s 46us/step - loss: 0.7081 - acc: 0.5743\n",
      "Epoch 31/40\n",
      "202/202 [==============================] - 0s 47us/step - loss: 0.7062 - acc: 0.5693\n",
      "Epoch 32/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 0s 47us/step - loss: 0.7028 - acc: 0.5792\n",
      "Epoch 33/40\n",
      "202/202 [==============================] - 0s 47us/step - loss: 0.6979 - acc: 0.5743\n",
      "Epoch 34/40\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.6980 - acc: 0.5594\n",
      "Epoch 35/40\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.6990 - acc: 0.5594\n",
      "Epoch 36/40\n",
      "202/202 [==============================] - 0s 52us/step - loss: 0.6970 - acc: 0.5545\n",
      "Epoch 37/40\n",
      "202/202 [==============================] - 0s 56us/step - loss: 0.6941 - acc: 0.5693\n",
      "Epoch 38/40\n",
      "202/202 [==============================] - 0s 57us/step - loss: 0.6909 - acc: 0.5792\n",
      "Epoch 39/40\n",
      "202/202 [==============================] - 0s 62us/step - loss: 0.6884 - acc: 0.5792\n",
      "Epoch 40/40\n",
      "202/202 [==============================] - 0s 56us/step - loss: 0.6867 - acc: 0.5743\n",
      "101/101 [==============================] - 1s 6ms/step\n",
      "202/202 [==============================] - 0s 36us/step\n",
      "Epoch 1/40\n",
      "202/202 [==============================] - 2s 9ms/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 2/40\n",
      "202/202 [==============================] - 0s 48us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 3/40\n",
      "202/202 [==============================] - 0s 55us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 4/40\n",
      "202/202 [==============================] - 0s 49us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 5/40\n",
      "202/202 [==============================] - 0s 47us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 6/40\n",
      "202/202 [==============================] - 0s 51us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 7/40\n",
      "202/202 [==============================] - 0s 50us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 8/40\n",
      "202/202 [==============================] - 0s 51us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 9/40\n",
      "202/202 [==============================] - 0s 49us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 10/40\n",
      "202/202 [==============================] - 0s 53us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 11/40\n",
      "202/202 [==============================] - 0s 54us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 12/40\n",
      "202/202 [==============================] - 0s 49us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 13/40\n",
      "202/202 [==============================] - 0s 55us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 14/40\n",
      "202/202 [==============================] - 0s 56us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 15/40\n",
      "202/202 [==============================] - 0s 51us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 16/40\n",
      "202/202 [==============================] - 0s 49us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 17/40\n",
      "202/202 [==============================] - 0s 51us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 18/40\n",
      "202/202 [==============================] - 0s 60us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 19/40\n",
      "202/202 [==============================] - 0s 54us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 20/40\n",
      "202/202 [==============================] - 0s 50us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 21/40\n",
      "202/202 [==============================] - 0s 56us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 22/40\n",
      "202/202 [==============================] - 0s 57us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 23/40\n",
      "202/202 [==============================] - 0s 57us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 24/40\n",
      "202/202 [==============================] - 0s 53us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 25/40\n",
      "202/202 [==============================] - 0s 56us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 26/40\n",
      "202/202 [==============================] - 0s 54us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 27/40\n",
      "202/202 [==============================] - 0s 52us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 28/40\n",
      "202/202 [==============================] - 0s 61us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 29/40\n",
      "202/202 [==============================] - 0s 58us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 30/40\n",
      "202/202 [==============================] - 0s 53us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 31/40\n",
      "202/202 [==============================] - 0s 47us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 32/40\n",
      "202/202 [==============================] - 0s 53us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 33/40\n",
      "202/202 [==============================] - 0s 50us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 34/40\n",
      "202/202 [==============================] - 0s 50us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 35/40\n",
      "202/202 [==============================] - 0s 46us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 36/40\n",
      "202/202 [==============================] - 0s 50us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 37/40\n",
      "202/202 [==============================] - 0s 46us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 38/40\n",
      "202/202 [==============================] - 0s 46us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 39/40\n",
      "202/202 [==============================] - 0s 47us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 40/40\n",
      "202/202 [==============================] - 0s 46us/step - loss: 2.9201 - acc: 0.8168\n",
      "101/101 [==============================] - 1s 7ms/step\n",
      "202/202 [==============================] - 0s 34us/step\n",
      "Epoch 1/60\n",
      "202/202 [==============================] - 2s 9ms/step - loss: 10.8171 - acc: 0.3168\n",
      "Epoch 2/60\n",
      "202/202 [==============================] - 0s 53us/step - loss: 10.5145 - acc: 0.3168\n",
      "Epoch 3/60\n",
      "202/202 [==============================] - 0s 48us/step - loss: 9.3882 - acc: 0.3218\n",
      "Epoch 4/60\n",
      "202/202 [==============================] - 0s 54us/step - loss: 7.1752 - acc: 0.3564\n",
      "Epoch 5/60\n",
      "202/202 [==============================] - 0s 48us/step - loss: 3.8937 - acc: 0.4851\n",
      "Epoch 6/60\n",
      "202/202 [==============================] - 0s 53us/step - loss: 1.7707 - acc: 0.6188\n",
      "Epoch 7/60\n",
      "202/202 [==============================] - 0s 53us/step - loss: 1.4442 - acc: 0.7129\n",
      "Epoch 8/60\n",
      "202/202 [==============================] - 0s 51us/step - loss: 1.7033 - acc: 0.7079\n",
      "Epoch 9/60\n",
      "202/202 [==============================] - 0s 50us/step - loss: 1.7089 - acc: 0.7030\n",
      "Epoch 10/60\n",
      "202/202 [==============================] - 0s 50us/step - loss: 1.5133 - acc: 0.7228\n",
      "Epoch 11/60\n",
      "202/202 [==============================] - 0s 46us/step - loss: 1.3241 - acc: 0.7327\n",
      "Epoch 12/60\n",
      "202/202 [==============================] - 0s 43us/step - loss: 1.2333 - acc: 0.6931\n",
      "Epoch 13/60\n",
      "202/202 [==============================] - 0s 52us/step - loss: 1.2731 - acc: 0.6832\n",
      "Epoch 14/60\n",
      "202/202 [==============================] - 0s 58us/step - loss: 1.2286 - acc: 0.6832\n",
      "Epoch 15/60\n",
      "202/202 [==============================] - 0s 56us/step - loss: 1.1563 - acc: 0.6980\n",
      "Epoch 16/60\n",
      "202/202 [==============================] - 0s 54us/step - loss: 1.1123 - acc: 0.7129\n",
      "Epoch 17/60\n",
      "202/202 [==============================] - 0s 54us/step - loss: 1.0448 - acc: 0.7129\n",
      "Epoch 18/60\n",
      "202/202 [==============================] - 0s 59us/step - loss: 0.9707 - acc: 0.7079\n",
      "Epoch 19/60\n",
      "202/202 [==============================] - 0s 58us/step - loss: 0.8935 - acc: 0.7277\n",
      "Epoch 20/60\n",
      "202/202 [==============================] - 0s 54us/step - loss: 0.8188 - acc: 0.7426\n",
      "Epoch 21/60\n",
      "202/202 [==============================] - 0s 54us/step - loss: 0.7618 - acc: 0.7277\n",
      "Epoch 22/60\n",
      "202/202 [==============================] - 0s 55us/step - loss: 0.7227 - acc: 0.7277\n",
      "Epoch 23/60\n",
      "202/202 [==============================] - 0s 57us/step - loss: 0.6812 - acc: 0.7228\n",
      "Epoch 24/60\n",
      "202/202 [==============================] - 0s 58us/step - loss: 0.6583 - acc: 0.7277\n",
      "Epoch 25/60\n",
      "202/202 [==============================] - 0s 50us/step - loss: 0.6476 - acc: 0.7228\n",
      "Epoch 26/60\n",
      "202/202 [==============================] - 0s 55us/step - loss: 0.6318 - acc: 0.7228\n",
      "Epoch 27/60\n",
      "202/202 [==============================] - 0s 55us/step - loss: 0.6221 - acc: 0.7228\n",
      "Epoch 28/60\n",
      "202/202 [==============================] - 0s 54us/step - loss: 0.6164 - acc: 0.7277\n",
      "Epoch 29/60\n",
      "202/202 [==============================] - 0s 57us/step - loss: 0.6178 - acc: 0.7277\n",
      "Epoch 30/60\n",
      "202/202 [==============================] - 0s 53us/step - loss: 0.6131 - acc: 0.7228\n",
      "Epoch 31/60\n",
      "202/202 [==============================] - 0s 64us/step - loss: 0.6153 - acc: 0.7277\n",
      "Epoch 32/60\n",
      "202/202 [==============================] - 0s 58us/step - loss: 0.6109 - acc: 0.7277\n",
      "Epoch 33/60\n",
      "202/202 [==============================] - 0s 53us/step - loss: 0.6092 - acc: 0.7327\n",
      "Epoch 34/60\n",
      "202/202 [==============================] - 0s 54us/step - loss: 0.6151 - acc: 0.7327\n",
      "Epoch 35/60\n",
      "202/202 [==============================] - 0s 48us/step - loss: 0.6065 - acc: 0.7277\n",
      "Epoch 36/60\n",
      "202/202 [==============================] - 0s 57us/step - loss: 0.6008 - acc: 0.7277\n",
      "Epoch 37/60\n",
      "202/202 [==============================] - 0s 60us/step - loss: 0.6016 - acc: 0.7327\n",
      "Epoch 38/60\n",
      "202/202 [==============================] - 0s 54us/step - loss: 0.6212 - acc: 0.7228\n",
      "Epoch 39/60\n",
      "202/202 [==============================] - 0s 48us/step - loss: 0.6160 - acc: 0.7327\n",
      "Epoch 40/60\n",
      "202/202 [==============================] - 0s 52us/step - loss: 0.5996 - acc: 0.7327\n",
      "Epoch 41/60\n",
      "202/202 [==============================] - 0s 63us/step - loss: 0.5951 - acc: 0.7475\n",
      "Epoch 42/60\n",
      "202/202 [==============================] - 0s 54us/step - loss: 0.5931 - acc: 0.7376\n",
      "Epoch 43/60\n",
      "202/202 [==============================] - 0s 52us/step - loss: 0.5885 - acc: 0.7277\n",
      "Epoch 44/60\n",
      "202/202 [==============================] - 0s 53us/step - loss: 0.5879 - acc: 0.7277\n",
      "Epoch 45/60\n",
      "202/202 [==============================] - 0s 49us/step - loss: 0.6047 - acc: 0.7178\n",
      "Epoch 46/60\n",
      "202/202 [==============================] - 0s 52us/step - loss: 0.5969 - acc: 0.7178\n",
      "Epoch 47/60\n",
      "202/202 [==============================] - 0s 57us/step - loss: 0.5870 - acc: 0.7228\n",
      "Epoch 48/60\n",
      "202/202 [==============================] - 0s 56us/step - loss: 0.5842 - acc: 0.7228\n",
      "Epoch 49/60\n",
      "202/202 [==============================] - 0s 54us/step - loss: 0.5789 - acc: 0.7178\n",
      "Epoch 50/60\n",
      "202/202 [==============================] - 0s 51us/step - loss: 0.5897 - acc: 0.7277\n",
      "Epoch 51/60\n",
      "202/202 [==============================] - 0s 56us/step - loss: 0.5721 - acc: 0.7178\n",
      "Epoch 52/60\n",
      "202/202 [==============================] - 0s 61us/step - loss: 0.5798 - acc: 0.7327\n",
      "Epoch 53/60\n",
      "202/202 [==============================] - 0s 62us/step - loss: 0.5740 - acc: 0.7327\n",
      "Epoch 54/60\n",
      "202/202 [==============================] - 0s 56us/step - loss: 0.5726 - acc: 0.7277\n",
      "Epoch 55/60\n",
      "202/202 [==============================] - 0s 51us/step - loss: 0.5653 - acc: 0.7327\n",
      "Epoch 56/60\n",
      "202/202 [==============================] - 0s 56us/step - loss: 0.5676 - acc: 0.7277\n",
      "Epoch 57/60\n",
      "202/202 [==============================] - 0s 54us/step - loss: 0.5646 - acc: 0.7277\n",
      "Epoch 58/60\n",
      "202/202 [==============================] - 0s 52us/step - loss: 0.5666 - acc: 0.7327\n",
      "Epoch 59/60\n",
      "202/202 [==============================] - 0s 51us/step - loss: 0.5736 - acc: 0.7376\n",
      "Epoch 60/60\n",
      "202/202 [==============================] - 0s 58us/step - loss: 0.5605 - acc: 0.7327\n",
      "101/101 [==============================] - 1s 7ms/step\n",
      "202/202 [==============================] - 0s 35us/step\n",
      "Epoch 1/60\n",
      "202/202 [==============================] - 2s 9ms/step - loss: 7.2411 - acc: 0.5000\n",
      "Epoch 2/60\n",
      "202/202 [==============================] - 0s 56us/step - loss: 4.0473 - acc: 0.5000\n",
      "Epoch 3/60\n",
      "202/202 [==============================] - 0s 52us/step - loss: 1.3012 - acc: 0.5396\n",
      "Epoch 4/60\n",
      "202/202 [==============================] - 0s 54us/step - loss: 1.4487 - acc: 0.5644\n",
      "Epoch 5/60\n",
      "202/202 [==============================] - 0s 50us/step - loss: 1.1807 - acc: 0.6089\n",
      "Epoch 6/60\n",
      "202/202 [==============================] - 0s 51us/step - loss: 0.8495 - acc: 0.6139\n",
      "Epoch 7/60\n",
      "202/202 [==============================] - 0s 52us/step - loss: 0.8376 - acc: 0.6139\n",
      "Epoch 8/60\n",
      "202/202 [==============================] - 0s 51us/step - loss: 0.7257 - acc: 0.6881\n",
      "Epoch 9/60\n",
      "202/202 [==============================] - 0s 49us/step - loss: 0.6728 - acc: 0.7228\n",
      "Epoch 10/60\n",
      "202/202 [==============================] - 0s 56us/step - loss: 0.6343 - acc: 0.7376\n",
      "Epoch 11/60\n",
      "202/202 [==============================] - 0s 58us/step - loss: 0.6355 - acc: 0.7129\n",
      "Epoch 12/60\n",
      "202/202 [==============================] - 0s 55us/step - loss: 0.6241 - acc: 0.7277\n",
      "Epoch 13/60\n",
      "202/202 [==============================] - 0s 52us/step - loss: 0.6198 - acc: 0.7277\n",
      "Epoch 14/60\n",
      "202/202 [==============================] - 0s 57us/step - loss: 0.6117 - acc: 0.7277\n",
      "Epoch 15/60\n",
      "202/202 [==============================] - 0s 57us/step - loss: 0.6378 - acc: 0.7277\n",
      "Epoch 16/60\n",
      "202/202 [==============================] - 0s 57us/step - loss: 0.6150 - acc: 0.7228\n",
      "Epoch 17/60\n",
      "202/202 [==============================] - 0s 47us/step - loss: 0.6015 - acc: 0.7327\n",
      "Epoch 18/60\n",
      "202/202 [==============================] - 0s 53us/step - loss: 0.5909 - acc: 0.7277\n",
      "Epoch 19/60\n",
      "202/202 [==============================] - 0s 57us/step - loss: 0.6155 - acc: 0.7129\n",
      "Epoch 20/60\n",
      "202/202 [==============================] - 0s 53us/step - loss: 0.5864 - acc: 0.7475\n",
      "Epoch 21/60\n",
      "202/202 [==============================] - 0s 47us/step - loss: 0.5800 - acc: 0.7525\n",
      "Epoch 22/60\n",
      "202/202 [==============================] - 0s 56us/step - loss: 0.5951 - acc: 0.7376\n",
      "Epoch 23/60\n",
      "202/202 [==============================] - 0s 56us/step - loss: 0.5827 - acc: 0.7376\n",
      "Epoch 24/60\n",
      "202/202 [==============================] - 0s 56us/step - loss: 0.5715 - acc: 0.7475\n",
      "Epoch 25/60\n",
      "202/202 [==============================] - 0s 51us/step - loss: 0.5721 - acc: 0.7228\n",
      "Epoch 26/60\n",
      "202/202 [==============================] - 0s 56us/step - loss: 0.5753 - acc: 0.7327\n",
      "Epoch 27/60\n",
      "202/202 [==============================] - 0s 60us/step - loss: 0.5724 - acc: 0.7327\n",
      "Epoch 28/60\n",
      "202/202 [==============================] - 0s 53us/step - loss: 0.5655 - acc: 0.7376\n",
      "Epoch 29/60\n",
      "202/202 [==============================] - 0s 55us/step - loss: 0.5978 - acc: 0.7079\n",
      "Epoch 30/60\n",
      "202/202 [==============================] - 0s 51us/step - loss: 0.5700 - acc: 0.7574\n",
      "Epoch 31/60\n",
      "202/202 [==============================] - 0s 49us/step - loss: 0.5608 - acc: 0.7673\n",
      "Epoch 32/60\n",
      "202/202 [==============================] - 0s 49us/step - loss: 0.5884 - acc: 0.7228\n",
      "Epoch 33/60\n",
      "202/202 [==============================] - 0s 49us/step - loss: 0.5994 - acc: 0.7277\n",
      "Epoch 34/60\n",
      "202/202 [==============================] - 0s 47us/step - loss: 0.5858 - acc: 0.7376\n",
      "Epoch 35/60\n",
      "202/202 [==============================] - 0s 50us/step - loss: 0.6204 - acc: 0.7228\n",
      "Epoch 36/60\n",
      "202/202 [==============================] - 0s 46us/step - loss: 0.5982 - acc: 0.7327\n",
      "Epoch 37/60\n",
      "202/202 [==============================] - 0s 47us/step - loss: 0.5790 - acc: 0.7426\n",
      "Epoch 38/60\n",
      "202/202 [==============================] - 0s 47us/step - loss: 0.5897 - acc: 0.7574\n",
      "Epoch 39/60\n",
      "202/202 [==============================] - 0s 46us/step - loss: 0.5651 - acc: 0.7525\n",
      "Epoch 40/60\n",
      "202/202 [==============================] - 0s 49us/step - loss: 0.5591 - acc: 0.7327\n",
      "Epoch 41/60\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.5947 - acc: 0.7178\n",
      "Epoch 42/60\n",
      "202/202 [==============================] - 0s 48us/step - loss: 0.5733 - acc: 0.7327\n",
      "Epoch 43/60\n",
      "202/202 [==============================] - 0s 49us/step - loss: 0.5524 - acc: 0.7426\n",
      "Epoch 44/60\n",
      "202/202 [==============================] - 0s 46us/step - loss: 0.5484 - acc: 0.7475\n",
      "Epoch 45/60\n",
      "202/202 [==============================] - 0s 48us/step - loss: 0.5413 - acc: 0.7822\n",
      "Epoch 46/60\n",
      "202/202 [==============================] - 0s 55us/step - loss: 0.5561 - acc: 0.7475\n",
      "Epoch 47/60\n",
      "202/202 [==============================] - 0s 54us/step - loss: 0.5520 - acc: 0.7525\n",
      "Epoch 48/60\n",
      "202/202 [==============================] - 0s 63us/step - loss: 0.5431 - acc: 0.7822\n",
      "Epoch 49/60\n",
      "202/202 [==============================] - 0s 58us/step - loss: 0.5394 - acc: 0.7624\n",
      "Epoch 50/60\n",
      "202/202 [==============================] - 0s 62us/step - loss: 0.5473 - acc: 0.7228\n",
      "Epoch 51/60\n",
      "202/202 [==============================] - 0s 53us/step - loss: 0.5365 - acc: 0.7525\n",
      "Epoch 52/60\n",
      "202/202 [==============================] - 0s 66us/step - loss: 0.5315 - acc: 0.7723\n",
      "Epoch 53/60\n",
      "202/202 [==============================] - 0s 58us/step - loss: 0.5730 - acc: 0.7030\n",
      "Epoch 54/60\n",
      "202/202 [==============================] - 0s 56us/step - loss: 0.5914 - acc: 0.7079\n",
      "Epoch 55/60\n",
      "202/202 [==============================] - 0s 61us/step - loss: 0.6214 - acc: 0.7079\n",
      "Epoch 56/60\n",
      "202/202 [==============================] - 0s 55us/step - loss: 0.5368 - acc: 0.7822\n",
      "Epoch 57/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 0s 52us/step - loss: 0.5448 - acc: 0.7475\n",
      "Epoch 58/60\n",
      "202/202 [==============================] - 0s 55us/step - loss: 0.5475 - acc: 0.7673\n",
      "Epoch 59/60\n",
      "202/202 [==============================] - 0s 56us/step - loss: 0.5283 - acc: 0.7624\n",
      "Epoch 60/60\n",
      "202/202 [==============================] - 0s 53us/step - loss: 0.5265 - acc: 0.7673\n",
      "101/101 [==============================] - 1s 7ms/step\n",
      "202/202 [==============================] - 0s 33us/step\n",
      "Epoch 1/60\n",
      "202/202 [==============================] - 2s 9ms/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 2/60\n",
      "202/202 [==============================] - 0s 49us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 3/60\n",
      "202/202 [==============================] - 0s 52us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 4/60\n",
      "202/202 [==============================] - 0s 56us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 5/60\n",
      "202/202 [==============================] - 0s 53us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 6/60\n",
      "202/202 [==============================] - 0s 57us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 7/60\n",
      "202/202 [==============================] - 0s 50us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 8/60\n",
      "202/202 [==============================] - 0s 54us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 9/60\n",
      "202/202 [==============================] - 0s 55us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 10/60\n",
      "202/202 [==============================] - 0s 48us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 11/60\n",
      "202/202 [==============================] - 0s 57us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 12/60\n",
      "202/202 [==============================] - 0s 55us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 13/60\n",
      "202/202 [==============================] - 0s 55us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 14/60\n",
      "202/202 [==============================] - 0s 55us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 15/60\n",
      "202/202 [==============================] - 0s 52us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 16/60\n",
      "202/202 [==============================] - 0s 50us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 17/60\n",
      "202/202 [==============================] - 0s 54us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 18/60\n",
      "202/202 [==============================] - 0s 52us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 19/60\n",
      "202/202 [==============================] - 0s 55us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 20/60\n",
      "202/202 [==============================] - 0s 53us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 21/60\n",
      "202/202 [==============================] - 0s 53us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 22/60\n",
      "202/202 [==============================] - 0s 57us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 23/60\n",
      "202/202 [==============================] - 0s 54us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 24/60\n",
      "202/202 [==============================] - 0s 58us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 25/60\n",
      "202/202 [==============================] - 0s 56us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 26/60\n",
      "202/202 [==============================] - 0s 54us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 27/60\n",
      "202/202 [==============================] - 0s 62us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 28/60\n",
      "202/202 [==============================] - 0s 55us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 29/60\n",
      "202/202 [==============================] - 0s 52us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 30/60\n",
      "202/202 [==============================] - 0s 49us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 31/60\n",
      "202/202 [==============================] - 0s 53us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 32/60\n",
      "202/202 [==============================] - 0s 52us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 33/60\n",
      "202/202 [==============================] - 0s 52us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 34/60\n",
      "202/202 [==============================] - 0s 51us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 35/60\n",
      "202/202 [==============================] - 0s 53us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 36/60\n",
      "202/202 [==============================] - 0s 57us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 37/60\n",
      "202/202 [==============================] - 0s 55us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 38/60\n",
      "202/202 [==============================] - 0s 56us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 39/60\n",
      "202/202 [==============================] - 0s 47us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 40/60\n",
      "202/202 [==============================] - 0s 52us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 41/60\n",
      "202/202 [==============================] - 0s 49us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 42/60\n",
      "202/202 [==============================] - 0s 50us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 43/60\n",
      "202/202 [==============================] - 0s 47us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 44/60\n",
      "202/202 [==============================] - 0s 50us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 45/60\n",
      "202/202 [==============================] - 0s 48us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 46/60\n",
      "202/202 [==============================] - 0s 51us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 47/60\n",
      "202/202 [==============================] - 0s 48us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 48/60\n",
      "202/202 [==============================] - 0s 46us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 49/60\n",
      "202/202 [==============================] - 0s 65us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 50/60\n",
      "202/202 [==============================] - 0s 57us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 51/60\n",
      "202/202 [==============================] - 0s 49us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 52/60\n",
      "202/202 [==============================] - 0s 46us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 53/60\n",
      "202/202 [==============================] - 0s 51us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 54/60\n",
      "202/202 [==============================] - 0s 58us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 55/60\n",
      "202/202 [==============================] - 0s 48us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 56/60\n",
      "202/202 [==============================] - 0s 59us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 57/60\n",
      "202/202 [==============================] - 0s 54us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 58/60\n",
      "202/202 [==============================] - 0s 61us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 59/60\n",
      "202/202 [==============================] - 0s 57us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 60/60\n",
      "202/202 [==============================] - 0s 52us/step - loss: 2.9201 - acc: 0.8168\n",
      "101/101 [==============================] - 1s 7ms/step\n",
      "202/202 [==============================] - 0s 32us/step\n",
      "Epoch 1/20\n",
      "303/303 [==============================] - 2s 7ms/step - loss: 5.3722 - acc: 0.4092\n",
      "Epoch 2/20\n",
      "303/303 [==============================] - 0s 44us/step - loss: 3.5339 - acc: 0.3993\n",
      "Epoch 3/20\n",
      "303/303 [==============================] - 0s 46us/step - loss: 3.3204 - acc: 0.4554\n",
      "Epoch 4/20\n",
      "303/303 [==============================] - 0s 41us/step - loss: 2.7330 - acc: 0.4125\n",
      "Epoch 5/20\n",
      "303/303 [==============================] - 0s 43us/step - loss: 2.4036 - acc: 0.3729\n",
      "Epoch 6/20\n",
      "303/303 [==============================] - 0s 46us/step - loss: 2.0652 - acc: 0.4026\n",
      "Epoch 7/20\n",
      "303/303 [==============================] - 0s 45us/step - loss: 1.8113 - acc: 0.4224\n",
      "Epoch 8/20\n",
      "303/303 [==============================] - 0s 41us/step - loss: 1.6089 - acc: 0.4356\n",
      "Epoch 9/20\n",
      "303/303 [==============================] - 0s 44us/step - loss: 1.4777 - acc: 0.4323\n",
      "Epoch 10/20\n",
      "303/303 [==============================] - 0s 45us/step - loss: 1.3518 - acc: 0.4389\n",
      "Epoch 11/20\n",
      "303/303 [==============================] - 0s 42us/step - loss: 1.2735 - acc: 0.4686\n",
      "Epoch 12/20\n",
      "303/303 [==============================] - 0s 45us/step - loss: 1.2159 - acc: 0.4818\n",
      "Epoch 13/20\n",
      "303/303 [==============================] - 0s 41us/step - loss: 1.1566 - acc: 0.4950\n",
      "Epoch 14/20\n",
      "303/303 [==============================] - 0s 45us/step - loss: 1.1145 - acc: 0.5116\n",
      "Epoch 15/20\n",
      "303/303 [==============================] - 0s 43us/step - loss: 1.0851 - acc: 0.5347\n",
      "Epoch 16/20\n",
      "303/303 [==============================] - 0s 47us/step - loss: 1.0550 - acc: 0.5215\n",
      "Epoch 17/20\n",
      "303/303 [==============================] - 0s 49us/step - loss: 1.0400 - acc: 0.5281\n",
      "Epoch 18/20\n",
      "303/303 [==============================] - 0s 44us/step - loss: 0.9844 - acc: 0.5413\n",
      "Epoch 19/20\n",
      "303/303 [==============================] - 0s 45us/step - loss: 0.9674 - acc: 0.5578\n",
      "Epoch 20/20\n",
      "303/303 [==============================] - 0s 48us/step - loss: 0.9516 - acc: 0.5842\n",
      "Best: 0.6138613901712714 using {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.2079207951282904, Stdev: 0.28015895367222565 with {'batch_size': 40, 'epochs': 10}\n",
      "Means: 0.6138613901712714, Stdev: 0.3236679943284156 with {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.40264026589519514, Stdev: 0.2857407381066998 with {'batch_size': 40, 'epochs': 40}\n",
      "Means: 0.3663366347452988, Stdev: 0.272592077501236 with {'batch_size': 40, 'epochs': 60}\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=13, activation = 'relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# define grid search parameters\n",
    "param_grid = {'batch_size': [40],\n",
    "              'epochs': [10,20,40,60]}\n",
    "\n",
    "# Create grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X,y)\n",
    "\n",
    "# Report results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DS43SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
