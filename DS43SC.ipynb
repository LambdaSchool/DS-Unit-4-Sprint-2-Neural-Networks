{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6SKlgYrpcym"
   },
   "source": [
    "# Neural Networks Sprint Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrEbRrjVphPM"
   },
   "source": [
    "## 1) Define the following terms:\n",
    "\n",
    "- Neuron\n",
    "- Input Layer\n",
    "- Hidden Layer\n",
    "- Output Layer\n",
    "- Activation\n",
    "- Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5EksLqnp4oB"
   },
   "source": [
    "**Neuron:**\n",
    "A node in A Neural Network. They have a structure analogous to that of biological neurons. Neurons read the activation state of a bunch of neurons in the previous layer (each weighed differently), and use that information to produce a single output value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input Layer:**\n",
    "The first layer of a NN. It's a series of variables that correspond directly to a feature in the data. In image recognition, for example, they could be the brightness of a particular pixel.\n",
    "\n",
    "**Hidden Layer:**\n",
    "Internal layers of the NN, connecting input and output layers. They are a series of functions, each of which takes in the values of all the variables in the previous layer (starting with the input layer) and produces a single number as output. The output value is generated as a sum of the values of all the nodes in the previous layer, each multiplied by a weight. That summation gets passed through a squishification function, and added to a bias to produce the output. Hidden layers don't need to correspond to any recognizable feature of the outside world.\n",
    "\n",
    "**Output Layer:**\n",
    "The final layer of the NN. Each node is a function like the hidden layers, but its output corresponds to the NN's predictions for a single outcome variable. Note that the nodes in this layer don't usually have an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation Function:**\n",
    "Each neuron must aggregate inputs and produce a single output. The activation function shapes that output to be within useful bounds. One can use several possible activation functions, but common functions (sigmoid, tanh) will map the whole numberline to a small range ((0,1) or (-1,1)) or get rid of negative numbers (ReLU). Note that all the nodes in a layer of the NN tend to have the same activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation:** Short for \"Backwards Propagation of errors\" and refers to a specific algorithm for how weights in a neural network are updated in reverse order at the end of each training epoch. It 'assigns blame' to weights and works backwards to nudge the weights until they 'improve' the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ri_gRA2Jp728"
   },
   "source": [
    "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 1  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ig6ZTH8tpQ19"
   },
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "\n",
    "    def __init__(self, no_of_inputs, threshold=100, learning_rate=0.01):\n",
    "        self.threshold = threshold\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.zeros(no_of_inputs + 1)\n",
    "           \n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        if summation > 0:\n",
    "            activation = 1\n",
    "        else:\n",
    "            activation = 0            \n",
    "        return activation\n",
    "\n",
    "    def train(self, training_inputs, labels):\n",
    "        for _ in range(self.threshold):\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                self.weights[1:] += self.learning_rate * (label - prediction) * inputs\n",
    "                self.weights[0] += self.learning_rate * (label - prediction)\n",
    "\n",
    "inputs = np.array([[1,1,1],\n",
    "                   [1,0,1],\n",
    "                   [0,1,1],\n",
    "                   [0,0,1]])\n",
    "\n",
    "correct_outputs = np.array([[1],\n",
    "                            [0],\n",
    "                            [0],\n",
    "                            [0]])\n",
    "\n",
    "pn = Perceptron(no_of_inputs=3, threshold=100, learning_rate=0.01)\n",
    "pn.train(inputs, correct_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     AND Gate\n",
      "1 1 1 --> 1\n",
      "1 0 1 --> 0\n",
      "0 1 1 --> 0\n",
      "0 0 1 --> 0\n"
     ]
    }
   ],
   "source": [
    "print(\"     AND Gate\")\n",
    "for row in inputs:\n",
    "    print(f'{row[0]} {row[1]} {row[2]} --> {pn.predict(row)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86HyRi8Osr3U"
   },
   "source": [
    "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
    "- Your network must have one hidden layer. \n",
    "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "- Train your model on the Heart Disease dataset from UCI:\n",
    "\n",
    "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
    "\n",
    "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNfiajv3v4Ed"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d82dc39b23f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle_arrays_unison\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "from mlxtend.data import mnist_data\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.preprocessing import shuffle_arrays_unison\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv\"\n",
    "df=pd.read_csv(url)\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.target.values\n",
    "X = df.drop(columns = ['target']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = 13\n",
    "# 2 hidden layers of 16 each, \n",
    "# output of 1 variables (the probability prediction)\n",
    "class NN_2L16(object):\n",
    "    def __init__(self):\n",
    "        self.inputs = 13\n",
    "        self.L1Nodes = 16\n",
    "        self.L2Nodes = 16\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initlize Weights\n",
    "        self.L1_weights = np.random.randn(self.inputs, self.L1Nodes) # (784x16)\n",
    "        self.L2_weights = np.random.randn(self.L1Nodes, self.L2Nodes) # (16x16)\n",
    "        self.output_weights = np.random.randn(self.L2Nodes, self.outputNodes) # (16x10)\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        # Sum and activate flows to L1\n",
    "        self.activated_L1 = self.sigmoid(np.dot(X, self.L1_weights)) \n",
    "        # Sum and activate flows to L2\n",
    "        self.activated_L2 = self.sigmoid(np.dot(self.activated_L1, self.L2_weights))\n",
    "        # Sum and activate flows to output\n",
    "        self.activated_output = self.sigmoid(np.dot(self.activated_L2, self.output_weights))\n",
    "        return self.activated_output\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1/(1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        ## backward propgate through the network, calculating error and delta at each layer\n",
    "        # Output\n",
    "        self.output_error = y - output # error in this layer\n",
    "        self.output_delta = self.output_error*self.sigmoidPrime(output) # apply derivative of sigmoid to error\n",
    "        \n",
    "        # L2\n",
    "        self.L2_error = self.output_delta.dot(self.output_weights.T) \n",
    "        self.L2_delta = self.L2_error*self.sigmoidPrime(self.activated_L2)\n",
    "        \n",
    "        # L1\n",
    "        self.L1_error = self.L2_delta.dot(self.L2_weights.T) \n",
    "        self.L1_delta = self.L1_error*self.sigmoidPrime(self.activated_L1)\n",
    "        \n",
    "        \n",
    "        ## Update all weights\n",
    "        self.L1_weights += X.T.dot(self.L1_delta) \n",
    "        self.L2_weights += self.activated_L1.T.dot(self.L2_delta)\n",
    "        self.output_weights += self.activated_L2.T.dot(self.output_delta)\n",
    "        \n",
    "    def train (self, X, y):\n",
    "        output = self.feed_forward(X)\n",
    "        self.backward(X, y, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_2L16()\n",
    "epochs=1000\n",
    "for i in range(epochs): \n",
    "    if i+1 in [1,2,3] or (i+1) % 500 == 0:\n",
    "        print('+---------- EPOCH', i+1, '-----------+')\n",
    "        #print(\"Input: \\n\", X) \n",
    "        #print(\"Actual Output: \\n\", y)  \n",
    "        #print(\"Predicted Output: \\n\" + str(model.feed_forward(X))) \n",
    "        print(\"Loss: \\n\" + str(np.mean(np.square(y - model.feed_forward(X))))) # mean sum squared loss\n",
    "        print(\"\\n\")\n",
    "    model.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGT1oRzXw3H9"
   },
   "source": [
    "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
    "\n",
    "- Use the Heart Disease Dataset (binary classification)\n",
    "- Use an appropriate loss function for a binary classification task\n",
    "- Use an appropriate activation function on the final layer of your network. \n",
    "- Train your model using verbose output for ease of grading.\n",
    "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
    "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWw4IYxLxKwH"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline : 1 Hidden 16 node Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# random seed to reproduce later\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=45, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# instantiate model obj\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# GridSearchCV hyperparameters\n",
    "batch_size = [20]\n",
    "epochs = [20]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "c_v = StratifiedKFold(n_splits=4,\n",
    "                      shuffle=True,\n",
    "                      random_state=seed) # 4-fold CV\n",
    "\n",
    "# Create Grid Search\n",
    "cv_grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    n_jobs=-1, \n",
    "                    cv=c_v)\n",
    "\n",
    "# Fit\n",
    "cv_grid_result1 = cv_grid.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DS43SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
