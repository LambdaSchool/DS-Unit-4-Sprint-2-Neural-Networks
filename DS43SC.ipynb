{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6SKlgYrpcym"
   },
   "source": [
    "# Neural Networks Sprint Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrEbRrjVphPM"
   },
   "source": [
    "## 1) Define the following terms:\n",
    "\n",
    "- Neuron\n",
    "- Input Layer\n",
    "- Hidden Layer\n",
    "- Output Layer\n",
    "- Activation\n",
    "- Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5EksLqnp4oB"
   },
   "source": [
    "**Neuron:**\n",
    "A node in A Neural Network. They modeled after biological neurons. Neurons read the activation state of all the neurons in the previous layer, weighs them individually, and passes the sum  thru a function that decides on a final output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input Layer:**\n",
    "The first/input layer of a neural network. A set of variables that correspond directly to a feature in the data. \n",
    "\n",
    "**Hidden Layer:**\n",
    "Internal layers of a neural network. Existing between them, it connects input and output layers. \n",
    "\n",
    "**Output Layer:**\n",
    "The last/output layer of a neural network. Each node is a function like the hidden layers, but its output corresponds to the NN's predictions for a single outcome variable. Note that the nodes in this layer don't usually have an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation Function:**\n",
    "Each neuron must sum and weight inputs then produce a single output. The activation function shapes that output to be within useful bounds. All the nodes in a layer of the NN usually have the same activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation:** Short for \"Backwards Propagation of errors\" and refers to a specific algorithm for how weights in a neural network are updated in reverse order at the end of each training epoch. It 'assigns blame' to weights and works backwards to nudge the weights until they 'improve' the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ri_gRA2Jp728"
   },
   "source": [
    "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 1  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ig6ZTH8tpQ19"
   },
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "\n",
    "    def __init__(self, no_of_inputs, threshold=100, learning_rate=0.01):\n",
    "        self.threshold = threshold\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.zeros(no_of_inputs + 1)\n",
    "           \n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        if summation > 0:\n",
    "            activation = 1\n",
    "        else:\n",
    "            activation = 0            \n",
    "        return activation\n",
    "\n",
    "    def train(self, training_inputs, labels):\n",
    "        for _ in range(self.threshold):\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                self.weights[1:] += self.learning_rate * (label - prediction) * inputs\n",
    "                self.weights[0] += self.learning_rate * (label - prediction)\n",
    "\n",
    "inputs = np.array([[1,1,1],\n",
    "                   [1,0,1],\n",
    "                   [0,1,1],\n",
    "                   [0,0,1]])\n",
    "\n",
    "correct_outputs = np.array([[1],\n",
    "                            [0],\n",
    "                            [0],\n",
    "                            [0]])\n",
    "\n",
    "pn = Perceptron(no_of_inputs=3, threshold=100, learning_rate=0.01)\n",
    "pn.train(inputs, correct_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     AND Gate\n",
      "1 1 1 --> 1\n",
      "1 0 1 --> 0\n",
      "0 1 1 --> 0\n",
      "0 0 1 --> 0\n"
     ]
    }
   ],
   "source": [
    "print(\"     AND Gate\")\n",
    "for row in inputs:\n",
    "    print(f'{row[0]} {row[1]} {row[2]} --> {pn.predict(row)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86HyRi8Osr3U"
   },
   "source": [
    "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
    "- Your network must have one hidden layer. \n",
    "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "- Train your model on the Heart Disease dataset from UCI:\n",
    "\n",
    "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
    "\n",
    "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNfiajv3v4Ed"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#from mlxtend.preprocessing import shuffle_arrays_unison\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
      "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
      "      dtype='object')\n",
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv\"\n",
    "df=pd.read_csv(url)\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.target.values\n",
    "X = df.drop(columns = ['target']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y.shape)\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = 13\n",
    "# 2 hidden layers of 16 each, \n",
    "# output of 1 variables (the probability prediction)\n",
    "class NN_2L16(object):\n",
    "    def __init__(self):\n",
    "        self.inputs = 13\n",
    "        self.L1Nodes = 16\n",
    "        self.L2Nodes = 16\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initlize Weights\n",
    "        self.L1_weights = np.random.randn(self.inputs, self.L1Nodes) # (784x16)\n",
    "        self.L2_weights = np.random.randn(self.L1Nodes, self.L2Nodes) # (16x16)\n",
    "        self.output_weights = np.random.randn(self.L2Nodes, self.outputNodes) # (16x10)\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        # Sum and activate flows to L1\n",
    "        self.activated_L1 = self.sigmoid(np.dot(X, self.L1_weights)) \n",
    "        # Sum and activate flows to L2\n",
    "        self.activated_L2 = self.sigmoid(np.dot(self.activated_L1, self.L2_weights))\n",
    "        # Sum and activate flows to output\n",
    "        self.activated_output = self.sigmoid(np.dot(self.activated_L2, self.output_weights))\n",
    "        return self.activated_output\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1/(1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        ## backward propgate through the network, calculating error and delta at each layer\n",
    "        # Output\n",
    "        self.output_error = y - output # error in this layer\n",
    "        self.output_delta = self.output_error*self.sigmoidPrime(output) # apply derivative of sigmoid to error\n",
    "        \n",
    "        # L2\n",
    "        self.L2_error = self.output_delta.dot(self.output_weights.T) \n",
    "        self.L2_delta = self.L2_error*self.sigmoidPrime(self.activated_L2)\n",
    "        \n",
    "        # L1\n",
    "        self.L1_error = self.L2_delta.dot(self.L2_weights.T) \n",
    "        self.L1_delta = self.L1_error*self.sigmoidPrime(self.activated_L1)\n",
    "        \n",
    "        \n",
    "        ## Update all weights\n",
    "        self.L1_weights += X.T.dot(self.L1_delta) \n",
    "        self.L2_weights += self.activated_L1.T.dot(self.L2_delta)\n",
    "        self.output_weights += self.activated_L2.T.dot(self.output_delta)\n",
    "        \n",
    "    def train (self, X, y):\n",
    "        output = self.feed_forward(X)\n",
    "        self.backward(X, y, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------- EPOCH 1 -----------+\n",
      "Loss: \n",
      "0.48727738719106406\n",
      "\n",
      "\n",
      "+---------- EPOCH 2 -----------+\n",
      "Loss: \n",
      "0.45544554455445546\n",
      "\n",
      "\n",
      "+---------- EPOCH 3 -----------+\n",
      "Loss: \n",
      "0.45544554455445546\n",
      "\n",
      "\n",
      "+---------- EPOCH 500 -----------+\n",
      "Loss: \n",
      "0.45544554455445546\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:26: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------- EPOCH 1000 -----------+\n",
      "Loss: \n",
      "0.45544554455445546\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = NN_2L16()\n",
    "epochs=1000\n",
    "for i in range(epochs): \n",
    "    if i+1 in [1,2,3] or (i+1) % 500 == 0:\n",
    "        print('+---------- EPOCH', i+1, '-----------+')\n",
    "        #print(\"Input: \\n\", X) \n",
    "        #print(\"Actual Output: \\n\", y)  \n",
    "        #print(\"Predicted Output: \\n\" + str(model.feed_forward(X))) \n",
    "        print(\"Loss: \\n\" + str(np.mean(np.square(y - model.feed_forward(X))))) # mean sum squared loss\n",
    "        print(\"\\n\")\n",
    "    model.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGT1oRzXw3H9"
   },
   "source": [
    "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
    "\n",
    "- Use the Heart Disease Dataset (binary classification)\n",
    "- Use an appropriate loss function for a binary classification task\n",
    "- Use an appropriate activation function on the final layer of your network. \n",
    "- Train your model using verbose output for ease of grading.\n",
    "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
    "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWw4IYxLxKwH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline : 1 Hidden 16 node Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Cross Validation Score: 0.72277228 used {'batch_size': 20, 'epochs': 20}\n",
      "\n",
      "CPU times: user 4.56 s, sys: 3.9 s, total: 8.46 s\n",
      "Wall time: 6.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# random seed to reproduce later\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# instantiate model obj\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# GridSearchCV hyperparameters\n",
    "batch_size = [20]\n",
    "epochs = [20]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "c_v = StratifiedKFold(n_splits=4,\n",
    "                      shuffle=True,\n",
    "                      random_state=seed) # 4-fold CV\n",
    "\n",
    "# instantiate GridSearchCV obj\n",
    "cv_grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    n_jobs=-1, \n",
    "                    cv=c_v)\n",
    "\n",
    "# run the cross validation\n",
    "xval_result = cv_grid.fit(X, y)\n",
    "\n",
    "# Plot the accuracy \n",
    "acc = [x*100 for x in xval_result.best_estimator_.model.history.history['acc']]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(acc)\n",
    "ax.set_xlabel('# Epochs', fontsize=16)\n",
    "ax.set_ylabel('% Accuracy', fontsize=16)\n",
    "ax.set_ylim(60,100)\n",
    "ax.axhline(80, color='k', linestyle='--', linewidth=1)\n",
    "ax.axhline(90, color='k', linestyle='--')\n",
    "plt.title('Best Estimator', fontsize=20, y=1.05)\n",
    "plt.show()\n",
    "\n",
    "# Report Results\n",
    "print(f\"Baseline Cross Validation Score: {xval_result.best_score_:.8f} used {xval_result.best_params_}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypertune Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "226/226 [==============================] - 8s 34ms/step - loss: 8.7630 - acc: 0.4336\n",
      " 40/228 [====>.........................] - ETA: 36s - loss: 5.1514 - acc: 0.5500 Epoch 2/20\n",
      "227/227 [==============================] - 8s 35ms/step - loss: 8.6689 - acc: 0.4493\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 8.3198 - acc: 0.4693\n",
      "200/228 [=========================>....] - ETA: 1s - loss: 8.3640 - acc: 0.4550Epoch 2/20\n",
      "Epoch 2/20\n",
      "228/228 [==============================] - 8s 35ms/step - loss: 8.3266 - acc: 0.4605\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 10.4768 - acc: 0.3500Epoch 2/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8.7722 - acc: 0.4558\n",
      "Epoch 3/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 8.7660 - acc: 0.4561\n",
      "200/227 [=========================>....] - ETA: 0s - loss: 8.6232 - acc: 0.4650Epoch 3/20\n",
      "227/227 [==============================] - 0s 2ms/step - loss: 8.8046 - acc: 0.4537\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 8.7660 - acc: 0.4561\n",
      " 40/228 [====>.........................] - ETA: 0s - loss: 9.6709 - acc: 0.4000Epoch 3/20\n",
      "Epoch 3/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8.7722 - acc: 0.4558\n",
      "120/228 [==============>...............] - ETA: 0s - loss: 9.4022 - acc: 0.4167Epoch 4/20\n",
      "227/227 [==============================] - 0s 447us/step - loss: 8.8046 - acc: 0.4537\n",
      "140/228 [=================>............] - ETA: 0s - loss: 9.6709 - acc: 0.4000Epoch 4/20\n",
      "228/228 [==============================] - 0s 634us/step - loss: 8.7660 - acc: 0.4561\n",
      " 80/227 [=========>....................] - ETA: 16s - loss: 4.1219 - acc: 0.6000Epoch 4/20\n",
      "228/228 [==============================] - 9s 39ms/step - loss: 4.7423 - acc: 0.4781\n",
      "227/227 [==============================] - 0s 455us/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 2/20\n",
      "220/227 [============================>.] - ETA: 0s - loss: 4.3907 - acc: 0.5273Epoch 5/20\n",
      "228/228 [==============================] - 0s 341us/step - loss: 8.7660 - acc: 0.4561\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 8.7660 - acc: 0.4561\n",
      "227/227 [==============================] - 9s 40ms/step - loss: 4.4325 - acc: 0.5198\n",
      "100/226 [============>.................] - ETA: 0s - loss: 8.2202 - acc: 0.4900Epoch 5/20\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 9.6709 - acc: 0.4000Epoch 4/20\n",
      "228/228 [==============================] - 9s 40ms/step - loss: 4.7559 - acc: 0.4693\n",
      "226/226 [==============================] - 9s 40ms/step - loss: 4.4563 - acc: 0.5221\n",
      "Epoch 2/20\n",
      " 80/228 [=========>....................] - ETA: 0s - loss: 4.4262 - acc: 0.5000Epoch 2/20\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 4.9233 - acc: 0.4000Epoch 2/20\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 8.7722 - acc: 0.4558\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 4.5673 - acc: 0.4500Epoch 5/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 8.8046 - acc: 0.4537\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 4.5188 - acc: 0.4649\n",
      "226/226 [==============================] - 0s 976us/step - loss: 4.1548 - acc: 0.4823\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 8.6452 - acc: 0.4636Epoch 6/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 4.6537 - acc: 0.4693\n",
      "Epoch 3/20\n",
      "Epoch 3/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 4.1467 - acc: 0.5286\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 8.7660 - acc: 0.4561\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 8.7660 - acc: 0.4561\n",
      "Epoch 3/20\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 4.0837 - acc: 0.4500Epoch 6/20\n",
      "Epoch 3/20\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 3.2474 - acc: 0.6500Epoch 5/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8.7722 - acc: 0.4558\n",
      " 40/227 [====>.........................] - ETA: 0s - loss: 3.3353 - acc: 0.5250Epoch 6/20\n",
      "227/227 [==============================] - 0s 926us/step - loss: 8.8046 - acc: 0.4537\n",
      " 80/227 [=========>....................] - ETA: 0s - loss: 3.4084 - acc: 0.5500Epoch 7/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 4.2019 - acc: 0.4386\n",
      " 60/226 [======>.......................] - ETA: 0s - loss: 9.1336 - acc: 0.4333Epoch 4/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 4.3057 - acc: 0.4693\n",
      "228/228 [==============================] - 0s 964us/step - loss: 8.7660 - acc: 0.4561\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 8.7660 - acc: 0.4561\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 3.2778 - acc: 0.5000Epoch 4/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 3.9546 - acc: 0.5000\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 7.2531 - acc: 0.5500Epoch 4/20\n",
      "227/227 [==============================] - 0s 2ms/step - loss: 3.9611 - acc: 0.5330\n",
      "160/227 [====================>.........] - ETA: 0s - loss: 8.8650 - acc: 0.4500Epoch 4/20\n",
      "228/228 [==============================] - 0s 895us/step - loss: 3.9799 - acc: 0.4298\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 8.7722 - acc: 0.4558\n",
      "Epoch 5/20\n",
      "227/227 [==============================] - 0s 2ms/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 7/20\n",
      "180/226 [======================>.......] - ETA: 0s - loss: 3.5363 - acc: 0.5167Epoch 8/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 8.7660 - acc: 0.4561\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 8.7660 - acc: 0.4561\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 6.4472 - acc: 0.6000Epoch 7/20\n",
      "Epoch 8/20\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 3.9764 - acc: 0.4386\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 3.6694 - acc: 0.5463\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 9.6709 - acc: 0.4000Epoch 5/20\n",
      "Epoch 5/20\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 3.7145 - acc: 0.5000\n",
      "227/227 [==============================] - 0s 696us/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 5/20\n",
      "226/226 [==============================] - 0s 985us/step - loss: 8.7722 - acc: 0.4558\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 1.5524 - acc: 0.6000Epoch 9/20\n",
      "Epoch 8/20\n",
      "228/228 [==============================] - 0s 662us/step - loss: 8.7660 - acc: 0.4561\n",
      "228/228 [==============================] - 0s 708us/step - loss: 8.7660 - acc: 0.4561\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 3.7772 - acc: 0.4254\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 10.4768 - acc: 0.3500Epoch 9/20\n",
      "Epoch 8/20\n",
      "Epoch 6/20\n",
      "227/227 [==============================] - 0s 694us/step - loss: 3.4572 - acc: 0.4978\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 3.3606 - acc: 0.5000Epoch 6/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 3.6701 - acc: 0.4430\n",
      " 40/227 [====>.........................] - ETA: 0s - loss: 2.7559 - acc: 0.6000Epoch 6/20\n",
      "226/226 [==============================] - 0s 951us/step - loss: 8.7722 - acc: 0.4558\n",
      " 60/227 [======>.......................] - ETA: 0s - loss: 2.7856 - acc: 0.5333Epoch 9/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 3.5002 - acc: 0.4912\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 8.7660 - acc: 0.4561\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 6/20\n",
      " 60/228 [======>.......................] - ETA: 0s - loss: 3.2618 - acc: 0.4500Epoch 10/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 3.2968 - acc: 0.4846\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 3.4566 - acc: 0.4649\n",
      "Epoch 10/20\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 8.7660 - acc: 0.4561\n",
      " 80/228 [=========>....................] - ETA: 0s - loss: 3.4793 - acc: 0.4500Epoch 7/20\n",
      " 40/226 [====>.........................] - ETA: 0s - loss: 3.5561 - acc: 0.5000Epoch 7/20\n",
      "228/228 [==============================] - 11s 48ms/step - loss: 4.7862 - acc: 0.4693\n",
      "Epoch 9/20\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 2.8602 - acc: 0.5000Epoch 2/20\n",
      "226/226 [==============================] - 11s 49ms/step - loss: 4.3970 - acc: 0.5221\n",
      " 80/228 [=========>....................] - ETA: 20s - loss: 5.4628 - acc: 0.4250Epoch 2/20\n",
      "227/227 [==============================] - 11s 49ms/step - loss: 4.4236 - acc: 0.5154\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 3.2122 - acc: 0.4518\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8.7722 - acc: 0.4558\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 4.6649 - acc: 0.4500Epoch 2/20\n",
      "Epoch 7/20\n",
      "Epoch 10/20\n",
      "227/227 [==============================] - 0s 932us/step - loss: 8.8046 - acc: 0.4537\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 10.4768 - acc: 0.3500Epoch 11/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 3.2480 - acc: 0.5000\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 11.2827 - acc: 0.3000Epoch 7/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 3.3179 - acc: 0.5154\n",
      " 40/227 [====>.........................] - ETA: 0s - loss: 10.8797 - acc: 0.3250Epoch 8/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 8.7660 - acc: 0.4561\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 4.7123 - acc: 0.4781\n",
      "228/228 [==============================] - 11s 49ms/step - loss: 4.8658 - acc: 0.4649\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 2.9868 - acc: 0.4500Epoch 10/20\n",
      " 40/226 [====>.........................] - ETA: 0s - loss: 3.0082 - acc: 0.3500Epoch 3/20\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 3.2013 - acc: 0.4227Epoch 2/20\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 8.7660 - acc: 0.4561\n",
      "227/227 [==============================] - 0s 945us/step - loss: 4.1597 - acc: 0.5198\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 3.2472 - acc: 0.4254\n",
      "Epoch 11/20\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 3.9393 - acc: 0.5000Epoch 3/20\n",
      "100/228 [============>.................] - ETA: 0s - loss: 9.0261 - acc: 0.4400Epoch 8/20\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 4.1345 - acc: 0.5177\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 3.0230 - acc: 0.4518\n",
      "226/226 [==============================] - 0s 992us/step - loss: 3.0210 - acc: 0.4558\n",
      " 60/228 [======>.......................] - ETA: 0s - loss: 4.4714 - acc: 0.4833Epoch 3/20\n",
      " 40/227 [====>.........................] - ETA: 0s - loss: 3.2372 - acc: 0.6000Epoch 8/20\n",
      "180/227 [======================>.......] - ETA: 0s - loss: 8.7754 - acc: 0.4556Epoch 8/20\n",
      "228/228 [==============================] - 0s 684us/step - loss: 8.7660 - acc: 0.4561\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 1.6133 - acc: 0.7000Epoch 11/20\n",
      "228/228 [==============================] - 0s 903us/step - loss: 4.4627 - acc: 0.4912\n",
      "180/227 [======================>.......] - ETA: 0s - loss: 2.8868 - acc: 0.4833Epoch 4/20\n",
      "228/228 [==============================] - 0s 666us/step - loss: 3.0770 - acc: 0.4298\n",
      "228/228 [==============================] - 0s 952us/step - loss: 8.7660 - acc: 0.4561\n",
      "226/226 [==============================] - 1s 2ms/step - loss: 8.7722 - acc: 0.4558\n",
      "227/227 [==============================] - 0s 2ms/step - loss: 8.8046 - acc: 0.4537\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 6.5529 - acc: 0.3000Epoch 9/20\n",
      "Epoch 12/20\n",
      "140/227 [=================>............] - ETA: 0s - loss: 3.3908 - acc: 0.5786Epoch 12/20\n",
      "Epoch 11/20\n",
      "227/227 [==============================] - 0s 2ms/step - loss: 2.8952 - acc: 0.4978\n",
      "226/226 [==============================] - 0s 967us/step - loss: 2.8682 - acc: 0.5044\n",
      "Epoch 9/20\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 4.6145 - acc: 0.4693\n",
      " 40/226 [====>.........................] - ETA: 0s - loss: 6.8502 - acc: 0.5750Epoch 9/20\n",
      "100/228 [============>.................] - ETA: 0s - loss: 3.0854 - acc: 0.4300Epoch 3/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 3.9784 - acc: 0.5265\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 8.7660 - acc: 0.4561\n",
      "228/228 [==============================] - 0s 700us/step - loss: 8.7660 - acc: 0.4561\n",
      "100/228 [============>.................] - ETA: 0s - loss: 4.1512 - acc: 0.4600Epoch 4/20\n",
      "227/227 [==============================] - 0s 2ms/step - loss: 3.9622 - acc: 0.5286\n",
      " 40/226 [====>.........................] - ETA: 0s - loss: 2.9264 - acc: 0.4750Epoch 13/20\n",
      "Epoch 12/20\n",
      "228/228 [==============================] - 0s 917us/step - loss: 2.7502 - acc: 0.4386\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 4.0458 - acc: 0.3500Epoch 4/20\n",
      " 60/226 [======>.......................] - ETA: 33s - loss: 5.0075 - acc: 0.4500Epoch 10/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 8.8046 - acc: 0.4537\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 2.7839 - acc: 0.4079\n",
      " 40/227 [====>.........................] - ETA: 0s - loss: 2.9494 - acc: 0.6000Epoch 13/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8.7722 - acc: 0.4558\n",
      " 60/228 [======>.......................] - ETA: 33s - loss: 4.7496 - acc: 0.4500Epoch 9/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 2.6310 - acc: 0.5110\n",
      "228/228 [==============================] - 0s 391us/step - loss: 8.7660 - acc: 0.4561\n",
      " 60/227 [======>.......................] - ETA: 0s - loss: 3.0786 - acc: 0.5333Epoch 12/20\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 2.8289 - acc: 0.4000Epoch 10/20\n",
      "Epoch 14/20\n",
      "226/226 [==============================] - 12s 54ms/step - loss: 4.4377 - acc: 0.5177\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 2.5572 - acc: 0.4779\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 4.3388 - acc: 0.4474\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 8.8650 - acc: 0.4500Epoch 2/20\n",
      "Epoch 10/20\n",
      "228/228 [==============================] - 0s 912us/step - loss: 8.7660 - acc: 0.4561\n",
      "228/228 [==============================] - 0s 679us/step - loss: 2.5570 - acc: 0.4211\n",
      "228/228 [==============================] - 1s 2ms/step - loss: 4.3487 - acc: 0.4561\n",
      "Epoch 4/20\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 4.5104 - acc: 0.4500Epoch 13/20\n",
      "Epoch 11/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 3.7463 - acc: 0.5088\n",
      "Epoch 5/20\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 8.0590 - acc: 0.5000Epoch 5/20\n",
      "227/227 [==============================] - 0s 910us/step - loss: 8.8046 - acc: 0.4537\n",
      "228/228 [==============================] - 12s 54ms/step - loss: 4.7446 - acc: 0.4649\n",
      "227/227 [==============================] - 12s 55ms/step - loss: 4.4094 - acc: 0.5198\n",
      "Epoch 14/20\n",
      "Epoch 2/20\n",
      "227/227 [==============================] - 0s 2ms/step - loss: 3.7620 - acc: 0.5198\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 2.4951 - acc: 0.4518\n",
      " 80/228 [=========>....................] - ETA: 0s - loss: 2.9892 - acc: 0.5250Epoch 2/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8.7722 - acc: 0.4558\n",
      "Epoch 5/20\n",
      "Epoch 10/20\n",
      "228/228 [==============================] - 12s 55ms/step - loss: 4.7753 - acc: 0.4781\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 2.4667 - acc: 0.4802\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 2.6633 - acc: 0.5500Epoch 13/20\n",
      "Epoch 2/20\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 1.9485 - acc: 0.5000Epoch 11/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 8.7660 - acc: 0.4561\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 2.3262 - acc: 0.4779\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 1.0476 - acc: 0.7500Epoch 15/20\n",
      "Epoch 11/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 2.2273 - acc: 0.4649\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 8.7660 - acc: 0.4561\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 2.1083 - acc: 0.4500Epoch 12/20\n",
      "Epoch 14/20\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 4.0328 - acc: 0.4823\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 3.8859 - acc: 0.4649\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 8.8650 - acc: 0.4500Epoch 3/20\n",
      "Epoch 6/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 3.5370 - acc: 0.4912\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 4.0955 - acc: 0.4342\n",
      "227/227 [==============================] - 0s 919us/step - loss: 4.0696 - acc: 0.5374\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 2.7762 - acc: 0.6000Epoch 6/20\n",
      "Epoch 5/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 3/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 3.5054 - acc: 0.4978\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 4.2993 - acc: 0.2000Epoch 15/20\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 4.5405 - acc: 0.4649\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 3.3469 - acc: 0.5000Epoch 6/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8.7722 - acc: 0.4558\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 2.7023 - acc: 0.6500Epoch 3/20\n",
      "Epoch 14/20\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 2.2004 - acc: 0.4605\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 4.4255 - acc: 0.4561\n",
      " 40/227 [====>.........................] - ETA: 0s - loss: 2.6603 - acc: 0.6250Epoch 11/20\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 8.8650 - acc: 0.4500Epoch 3/20\n",
      "227/227 [==============================] - 0s 2ms/step - loss: 2.2193 - acc: 0.5022\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 8.7660 - acc: 0.4561\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 2.1011 - acc: 0.4254\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 3.9102 - acc: 0.4000Epoch 12/20\n",
      "Epoch 16/20\n",
      "Epoch 13/20\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 2.1080 - acc: 0.4558\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 8.7660 - acc: 0.4561\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 2.1022 - acc: 0.3000Epoch 12/20\n",
      "Epoch 15/20\n",
      "227/227 [==============================] - 0s 910us/step - loss: 8.8046 - acc: 0.4537\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 3.2323 - acc: 0.4779\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 3.5509 - acc: 0.4779\n",
      "Epoch 16/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 3.8044 - acc: 0.4254\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 3.7279 - acc: 0.4649\n",
      " 40/226 [====>.........................] - ETA: 0s - loss: 1.8392 - acc: 0.5250Epoch 4/20\n",
      " 40/228 [====>.........................] - ETA: 0s - loss: 6.8502 - acc: 0.5750Epoch 7/20\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 10.4768 - acc: 0.3500Epoch 6/20\n",
      "Epoch 7/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 3.2879 - acc: 0.5066\n",
      "227/227 [==============================] - 0s 2ms/step - loss: 3.6593 - acc: 0.5110\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 4.5606 - acc: 0.2500Epoch 7/20\n",
      "Epoch 4/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 3.8707 - acc: 0.4474\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8.7722 - acc: 0.4558\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 3.2198 - acc: 0.5000Epoch 4/20\n",
      "Epoch 15/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.9889 - acc: 0.4254\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 8.7660 - acc: 0.4561\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 3.8702 - acc: 0.4430\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.8010 - acc: 0.4561\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 10.4768 - acc: 0.3500Epoch 17/20\n",
      "Epoch 12/20\n",
      "Epoch 4/20\n",
      "Epoch 14/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 2.0423 - acc: 0.5110\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 1.8958 - acc: 0.4690\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 1.4410 - acc: 0.5000Epoch 13/20\n",
      "Epoch 13/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 8.7660 - acc: 0.4561\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 1.5412 - acc: 0.5500Epoch 16/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 3.0824 - acc: 0.4469\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 3.0631 - acc: 0.4867\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 8.8046 - acc: 0.4537\n",
      " 40/226 [====>.........................] - ETA: 0s - loss: 2.2024 - acc: 0.4250Epoch 8/20\n",
      "Epoch 5/20\n",
      "Epoch 17/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 3.6035 - acc: 0.4167\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 3.2910 - acc: 0.4518\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 8.0590 - acc: 0.5000Epoch 8/20\n",
      "Epoch 7/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 3.1049 - acc: 0.5198\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 3.0339 - acc: 0.5198\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 2.6722 - acc: 0.5000Epoch 5/20\n",
      "Epoch 8/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8.7722 - acc: 0.4558\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 3.1846 - acc: 0.4474\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 8.7660 - acc: 0.4561\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 2.1257 - acc: 0.5500Epoch 5/20\n",
      "Epoch 16/20\n",
      "Epoch 18/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.7738 - acc: 0.4474\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 1.9275 - acc: 0.4978\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 7.2531 - acc: 0.5500Epoch 13/20\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 6.4472 - acc: 0.6000Epoch 14/20\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 3.3557 - acc: 0.4123\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 1.5179 - acc: 0.4737\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 8.7660 - acc: 0.4561\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 0.9000 - acc: 0.6500Epoch 5/20\n",
      "Epoch 15/20\n",
      "Epoch 17/20\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 1.7794 - acc: 0.4690\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 2.8741 - acc: 0.4867\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 9.6709 - acc: 0.4000Epoch 14/20\n",
      "Epoch 9/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 8.8046 - acc: 0.4537\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 2.5987 - acc: 0.4469\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 3.3365 - acc: 0.4386\n",
      " 40/228 [====>.........................] - ETA: 0s - loss: 10.0738 - acc: 0.3750Epoch 18/20\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 3.2990 - acc: 0.3500Epoch 6/20\n",
      "Epoch 8/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 2.7656 - acc: 0.4934\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 2.9782 - acc: 0.4298\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 3.2883 - acc: 0.3000Epoch 9/20\n",
      "Epoch 9/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 2.6597 - acc: 0.4802\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 8.7660 - acc: 0.4561\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 2.5902 - acc: 0.4342\n",
      " 40/228 [====>.........................] - ETA: 0s - loss: 4.1258 - acc: 0.3250Epoch 6/20\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 2.8723 - acc: 0.4500Epoch 6/20\n",
      "Epoch 19/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8.7722 - acc: 0.4558\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 6.4472 - acc: 0.6000Epoch 17/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.5781 - acc: 0.4868\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 1.7206 - acc: 0.5286\n",
      " 40/228 [====>.........................] - ETA: 0s - loss: 1.6395 - acc: 0.5000Epoch 14/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 2.7723 - acc: 0.4211\n",
      "Epoch 15/20\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 1.4782 - acc: 0.6000Epoch 6/20\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 1.5142 - acc: 0.4518\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 8.7660 - acc: 0.4561\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 1.5071 - acc: 0.4956\n",
      " 40/228 [====>.........................] - ETA: 0s - loss: 1.1264 - acc: 0.5250Epoch 16/20\n",
      " 40/227 [====>.........................] - ETA: 0s - loss: 1.7092 - acc: 0.5250Epoch 18/20\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 2.4443 - acc: 0.6500Epoch 15/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 8.8046 - acc: 0.4537\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 2.7403 - acc: 0.4558\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 1.9382 - acc: 0.5310\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 2.5599 - acc: 0.5110\n",
      " 40/228 [====>.........................] - ETA: 0s - loss: 2.2610 - acc: 0.6000Epoch 10/20\n",
      "Epoch 19/20\n",
      "Epoch 7/20\n",
      "Epoch 10/20\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 3.1171 - acc: 0.4430\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 2.6481 - acc: 0.4518\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 2.8241 - acc: 0.4000Epoch 9/20\n",
      "Epoch 10/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 2.2028 - acc: 0.4890\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.8424 - acc: 0.4561\n",
      " 40/227 [====>.........................] - ETA: 0s - loss: 2.0903 - acc: 0.5250Epoch 7/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8.7722 - acc: 0.4558\n",
      "Epoch 7/20\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 8.7660 - acc: 0.4561\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 1.4967 - acc: 0.5595\n",
      " 40/228 [====>.........................] - ETA: 0s - loss: 1.9860 - acc: 0.5250Epoch 18/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.3377 - acc: 0.4825\n",
      "Epoch 16/20\n",
      "Epoch 20/20\n",
      "228/228 [==============================] - 0s 884us/step - loss: 8.7660 - acc: 0.4561\n",
      " 40/228 [====>.........................] - ETA: 0s - loss: 2.7586 - acc: 0.3750Epoch 15/20\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 2.6578 - acc: 0.4000Epoch 19/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.1466 - acc: 0.5132\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 1.3972 - acc: 0.5133\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 2.1335 - acc: 0.5088\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 6.4472 - acc: 0.6000Epoch 17/20\n",
      "Epoch 7/20\n",
      "Epoch 16/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 8.8046 - acc: 0.4537\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 1.3088 - acc: 0.5500Epoch 20/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 2.2975 - acc: 0.5265\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 2.2708 - acc: 0.4934\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 1.4020 - acc: 0.5133\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 9.6709 - acc: 0.4000Epoch 11/20\n",
      "Epoch 11/20\n",
      "Epoch 8/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 2.7470 - acc: 0.4518\n",
      "226/226 [==============================] - 0s 912us/step - loss: 8.7722 - acc: 0.4558\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 1.1869 - acc: 0.7000Epoch 10/20\n",
      "Epoch 19/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 2.0339 - acc: 0.4846\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 2.2718 - acc: 0.4474\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 8.8650 - acc: 0.4500Epoch 8/20\n",
      "Epoch 11/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 8.7660 - acc: 0.4561\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 1.5289 - acc: 0.5263\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.3271 - acc: 0.4781\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 1.5182 - acc: 0.5110\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 1.9443 - acc: 0.5000Epoch 16/20\n",
      "Epoch 8/20\n",
      "Epoch 17/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 8.7660 - acc: 0.4561\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.1492 - acc: 0.4912\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 1.8997 - acc: 0.4000Epoch 18/20\n",
      "Epoch 20/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.6784 - acc: 0.4518\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 1.1754 - acc: 0.5619\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 0.8554 - acc: 0.7000Epoch 17/20\n",
      "Epoch 8/20\n",
      "228/228 [==============================] - 0s 991us/step - loss: 2.3929 - acc: 0.4298\n",
      "220/227 [============================>.] - ETA: 0s - loss: 8.7917 - acc: 0.4545Epoch 11/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 1.0950 - acc: 0.5929\n",
      "227/227 [==============================] - 0s 976us/step - loss: 1.4130 - acc: 0.5198\n",
      "227/227 [==============================] - 0s 2ms/step - loss: 8.8046 - acc: 0.4537\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 2.3232 - acc: 0.6000Epoch 9/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8.7722 - acc: 0.4558\n",
      "Epoch 9/20\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 2.0857 - acc: 0.4735\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 1.4242 - acc: 0.5500Epoch 20/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.0841 - acc: 0.5482\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 2.0336 - acc: 0.4518\n",
      " 80/228 [=========>....................] - ETA: 0s - loss: 1.9088 - acc: 0.4125Epoch 12/20\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 9.6709 - acc: 0.4000Epoch 12/20\n",
      "Epoch 17/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 1.2811 - acc: 0.5639\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.3627 - acc: 0.5044\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 1.3703 - acc: 0.4500Epoch 9/20\n",
      "Epoch 18/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.9049 - acc: 0.5614\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 8.7660 - acc: 0.4561\n",
      "227/227 [==============================] - 1s 3ms/step - loss: 2.0024 - acc: 0.4934\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 1.2276 - acc: 0.4500Epoch 19/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 1.1447 - acc: 0.5177\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 2.1270 - acc: 0.4561\n",
      "Epoch 12/20\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 0.6162 - acc: 0.6000Epoch 12/20\n",
      "Epoch 18/20\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 1.4943 - acc: 0.4737\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 0.7846 - acc: 0.6000Epoch 9/20\n",
      "226/226 [==============================] - 0s 948us/step - loss: 1.7522 - acc: 0.4690\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 1.6781 - acc: 0.2500Epoch 13/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.7125 - acc: 0.4649\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 1.2175 - acc: 0.5419\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.1747 - acc: 0.5000\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 8.7722 - acc: 0.4558\n",
      "Epoch 13/20\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 0.9744 - acc: 0.5545Epoch 19/20\n",
      "Epoch 10/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.8418 - acc: 0.5482\n",
      "226/226 [==============================] - 0s 897us/step - loss: 1.0043 - acc: 0.5619\n",
      "226/226 [==============================] - 1s 2ms/step - loss: 0.9796 - acc: 0.5354\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.9572 - acc: 0.5614\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 0.7283 - acc: 0.6500Epoch 19/20\n",
      "Epoch 20/20\n",
      "227/227 [==============================] - 1s 3ms/step - loss: 1.0269 - acc: 0.5551\n",
      "Epoch 10/20\n",
      "Epoch 18/20\n",
      "160/227 [====================>.........] - ETA: 0s - loss: 1.8669 - acc: 0.4875Epoch 10/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.1140 - acc: 0.4825\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 1.8188 - acc: 0.4693\n",
      " 60/228 [======>.......................] - ETA: 0s - loss: 0.7164 - acc: 0.6167Epoch 13/20\n",
      "Epoch 10/20\n",
      "227/227 [==============================] - 0s 2ms/step - loss: 1.7568 - acc: 0.5242\n",
      "226/226 [==============================] - 0s 512us/step - loss: 0.7150 - acc: 0.6327\n",
      " 80/228 [=========>....................] - ETA: 0s - loss: 0.7234 - acc: 0.5875Epoch 13/20\n",
      " 60/227 [======>.......................] - ETA: 0s - loss: 0.8662 - acc: 0.6667Epoch 11/20\n",
      "228/228 [==============================] - 0s 668us/step - loss: 0.8565 - acc: 0.6096\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 0.9551 - acc: 0.5903\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.3748 - acc: 0.4868\n",
      " 80/227 [=========>....................] - ETA: 0s - loss: 0.9437 - acc: 0.6375Epoch 19/20\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 1.1905 - acc: 0.5500Epoch 20/20\n",
      "Epoch 14/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.6846 - acc: 0.6711\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 0.8531 - acc: 0.6150\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 0.8278 - acc: 0.5500Epoch 11/20\n",
      "Epoch 20/20\n",
      "226/226 [==============================] - 1s 2ms/step - loss: 1.4543 - acc: 0.5619\n",
      "228/228 [==============================] - 0s 930us/step - loss: 0.7071 - acc: 0.6184\n",
      "227/227 [==============================] - 0s 680us/step - loss: 1.5575 - acc: 0.5110\n",
      "Epoch 14/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 1.0693 - acc: 0.5595\n",
      " 40/226 [====>.........................] - ETA: 0s - loss: 1.1066 - acc: 0.5000Epoch 11/20\n",
      "180/228 [======================>.......] - ETA: 0s - loss: 0.7814 - acc: 0.6000Epoch 14/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.5115 - acc: 0.4781\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 1.6548 - acc: 0.6000Epoch 11/20\n",
      "200/228 [=========================>....] - ETA: 0s - loss: 0.8157 - acc: 0.5850Epoch 14/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 0.6727 - acc: 0.6504\n",
      "228/228 [==============================] - 0s 906us/step - loss: 0.7706 - acc: 0.5965\n",
      "100/226 [============>.................] - ETA: 0s - loss: 1.3503 - acc: 0.5500Epoch 20/20\n",
      "Epoch 12/20\n",
      "228/228 [==============================] - 0s 875us/step - loss: 0.5789 - acc: 0.7237\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 0.9369 - acc: 0.6123\n",
      "75/75 [==============================] - 2s 20ms/step\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.7938 - acc: 0.5833\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 0.8847 - acc: 0.7000Epoch 12/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.1154 - acc: 0.4956\n",
      "180/228 [======================>.......] - ETA: 0sEpoch 15/20\n",
      "228/228 [==============================] - 0s 737us/step\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.8324 - acc: 0.5973\n",
      "76/76 [==============================] - 1s 19ms/step\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.2538 - acc: 0.4912\n",
      "228/228 [==============================] - 0s 797us/step - loss: 0.7261 - acc: 0.6623\n",
      " 20/227 [=>............................] - ETA: 0sEpoch 15/20\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.6200 - acc: 0.6535\n",
      "227/227 [==============================] - 0s 2ms/step - loss: 1.3341 - acc: 0.5463\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 1.2643 - acc: 0.5398\n",
      "Epoch 13/20\n",
      "180/227 [======================>.......] - ETA: 0sEpoch 12/20\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 1.0806 - acc: 0.5000Epoch 15/20\n",
      "227/227 [==============================] - 0s 2ms/step - loss: 0.9377 - acc: 0.5815\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 0.7943 - acc: 0.6227Epoch 15/20\n",
      " 40/228 [====>.........................] - ETA: 0s - loss: 1.3402 - acc: 0.5250Epoch 12/20\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.7848 - acc: 0.6272\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.5667 - acc: 0.7434\n",
      "227/227 [==============================] - 0s 688us/step\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 0.9308 - acc: 0.6000Epoch 13/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.9586 - acc: 0.5263\n",
      "75/75 [==============================] - 2s 20ms/step\n",
      "228/228 [==============================] - 0s 683us/step - loss: 0.5294 - acc: 0.7368\n",
      " 20/228 [=>............................] - ETA: 0sEpoch 16/20\n",
      "227/227 [==============================] - 0s 643us/step - loss: 1.1612 - acc: 0.5771\n",
      " 40/226 [====>.........................] - ETA: 0s - loss: 0.6086 - acc: 0.7000Epoch 14/20\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 0.7733 - acc: 0.6500Epoch 16/20\n",
      "228/228 [==============================] - 0s 873us/step - loss: 0.6648 - acc: 0.6535\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.0056 - acc: 0.5482\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 1.2911 - acc: 0.6500Epoch 13/20\n",
      "Epoch 16/20\n",
      "227/227 [==============================] - 0s 955us/step - loss: 0.6400 - acc: 0.6740\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 1.0687 - acc: 0.5575\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 1.5314 - acc: 0.3500Epoch 13/20\n",
      "226/226 [==============================] - 0s 990us/step - loss: 0.4967 - acc: 0.7522\n",
      "Epoch 16/20\n",
      "228/228 [==============================] - 0s 940us/step\n",
      "228/228 [==============================] - 0s 714us/step - loss: 0.7986 - acc: 0.5965\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 0.4936 - acc: 0.7000Epoch 14/20\n",
      "120/228 [==============>...............] - ETA: 0s - loss: 0.5223 - acc: 0.7667Epoch 17/20\n",
      "227/227 [==============================] - 0s 722us/step - loss: 1.0011 - acc: 0.5815\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 0.4875 - acc: 0.8000Epoch 17/20\n",
      "228/228 [==============================] - 0s 612us/step - loss: 0.5880 - acc: 0.7456\n",
      "228/228 [==============================] - 0s 704us/step - loss: 0.9502 - acc: 0.5044\n",
      "77/77 [==============================] - 2s 20ms/step\n",
      "160/228 [====================>.........] - ETA: 0s - loss: 0.5088 - acc: 0.7625Epoch 14/20\n",
      " 20/226 [=>............................] - ETA: 0sEpoch 17/20\n",
      "226/226 [==============================] - 0s 672us/step - loss: 0.9503 - acc: 0.5619\n",
      "120/226 [==============>...............] - ETA: 0sEpoch 17/20\n",
      "227/227 [==============================] - 0s 826us/step - loss: 0.6667 - acc: 0.6564\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.5405 - acc: 0.7412\n",
      "226/226 [==============================] - 0s 816us/step - loss: 0.6215 - acc: 0.7080\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 0.9393 - acc: 0.4500Epoch 14/20\n",
      "Epoch 15/20\n",
      "228/228 [==============================] - 0s 438us/step - loss: 0.5046 - acc: 0.7588\n",
      "228/228 [==============================] - 0s 892us/step - loss: 0.7337 - acc: 0.5921\n",
      "Epoch 15/20\n",
      " 40/226 [====>.........................] - ETA: 0s - loss: 0.8019 - acc: 0.5500Epoch 15/20\n",
      "Epoch 18/20\n",
      "227/227 [==============================] - 0s 965us/step - loss: 0.8537 - acc: 0.5815\n",
      "228/228 [==============================] - 0s 710us/step - loss: 0.8323 - acc: 0.5877\n",
      "226/226 [==============================] - 0s 964us/step\n",
      " 40/226 [====>.........................] - ETA: 0s - loss: 0.8561 - acc: 0.5000Epoch 18/20\n",
      "Epoch 18/20\n",
      "226/226 [==============================] - 0s 694us/step - loss: 0.7846 - acc: 0.6150\n",
      "227/227 [==============================] - 0s 468us/step - loss: 0.5476 - acc: 0.7181\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 0.8046 - acc: 0.5000Epoch 15/20\n",
      "Epoch 18/20\n",
      "228/228 [==============================] - 0s 860us/step - loss: 0.6776 - acc: 0.7061\n",
      "228/228 [==============================] - 0s 638us/step - loss: 0.6636 - acc: 0.6404\n",
      "226/226 [==============================] - 0s 949us/step - loss: 0.5867 - acc: 0.6903\n",
      " 40/226 [====>.........................] - ETA: 0s - loss: 0.6528 - acc: 0.6750Epoch 19/20\n",
      "Epoch 16/20\n",
      "76/76 [==============================] - 1s 17ms/step\n",
      "75/75 [==============================] - 1s 17ms/step\n",
      "Epoch 16/20\n",
      "227/227 [==============================] - 0s 603us/step - loss: 0.6996 - acc: 0.6344\n",
      "228/228 [==============================] - 0s 963us/step - loss: 0.5995 - acc: 0.7149\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 0.3997 - acc: 0.8500Epoch 19/20\n",
      "Epoch 16/20\n",
      "227/227 [==============================] - 0s 652us/step - loss: 0.4896 - acc: 0.7753\n",
      "228/228 [==============================] - 0s 886us/step - loss: 0.7244 - acc: 0.6447\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 0.7446 - acc: 0.6500Epoch 19/20\n",
      "Epoch 16/20\n",
      "228/228 [==============================] - 0s 381us/step - loss: 0.6273 - acc: 0.6886\n",
      "226/226 [==============================] - 0s 876us/step - loss: 0.7746 - acc: 0.6460\n",
      "227/227 [==============================] - 0s 438us/step\n",
      "20/77 [======>.......................] - ETA: 3sEpoch 19/20\n",
      "Epoch 20/20\n",
      "228/228 [==============================] - 0s 581us/step\n",
      "228/228 [==============================] - 0s 618us/step - loss: 0.4373 - acc: 0.7939\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 0.5526 - acc: 0.7000Epoch 17/20\n",
      "226/226 [==============================] - 0s 796us/step - loss: 0.5215 - acc: 0.7478\n",
      "227/227 [==============================] - 0s 571us/step - loss: 0.6679 - acc: 0.6696\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 0.7070 - acc: 0.6000Epoch 17/20\n",
      "Epoch 20/20\n",
      "227/227 [==============================] - 0s 595us/step - loss: 0.6836 - acc: 0.6608\n",
      "228/228 [==============================] - 0s 792us/step - loss: 0.4475 - acc: 0.7895\n",
      "77/77 [==============================] - 1s 18ms/step\n",
      "228/228 [==============================] - 0s 639us/step - loss: 0.7128 - acc: 0.6140\n",
      "228/228 [==============================] - 0s 434us/step - loss: 0.6355 - acc: 0.6930\n",
      "20/75 [=======>......................] - ETA: 3sEpoch 17/20\n",
      "Epoch 17/20\n",
      "Epoch 20/20\n",
      "226/226 [==============================] - 0s 632us/step - loss: 0.6036 - acc: 0.6991\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 0.3266 - acc: 0.8500Epoch 20/20\n",
      "228/228 [==============================] - 0s 568us/step - loss: 0.4847 - acc: 0.7500\n",
      "226/226 [==============================] - 0s 396us/step - loss: 0.5854 - acc: 0.7434\n",
      "227/227 [==============================] - 0s 404us/step - loss: 0.6131 - acc: 0.6784\n",
      "75/75 [==============================] - 1s 18ms/step\n",
      " 60/226 [======>.......................] - ETA: 0sEpoch 18/20\n",
      "Epoch 18/20\n",
      "228/228 [==============================] - 0s 445us/step - loss: 0.4764 - acc: 0.7456\n",
      "227/227 [==============================] - 0s 442us/step - loss: 0.5130 - acc: 0.7489\n",
      "228/228 [==============================] - 0s 491us/step - loss: 0.7535 - acc: 0.6316\n",
      " 40/228 [====>.........................] - ETA: 0sEpoch 18/20\n",
      "Epoch 18/20\n",
      "226/226 [==============================] - 0s 423us/step - loss: 0.5541 - acc: 0.7345\n",
      "226/226 [==============================] - 0s 1ms/step\n",
      "226/226 [==============================] - 0s 432us/step - loss: 0.4643 - acc: 0.8053\n",
      "228/228 [==============================] - 0s 423us/step - loss: 0.4765 - acc: 0.7719\n",
      " 40/228 [====>.........................] - ETA: 0s - loss: 0.4724 - acc: 0.7500Epoch 19/20\n",
      "Epoch 19/20\n",
      "228/228 [==============================] - 0s 850us/step\n",
      "227/227 [==============================] - 0s 599us/step - loss: 0.4959 - acc: 0.7709\n",
      "228/228 [==============================] - 0s 666us/step - loss: 0.4467 - acc: 0.8070\n",
      "200/228 [=========================>....] - ETA: 0s - loss: 0.5083 - acc: 0.7600Epoch 19/20\n",
      "Epoch 19/20\n",
      "228/228 [==============================] - 0s 372us/step - loss: 0.5013 - acc: 0.7675\n",
      "226/226 [==============================] - 0s 403us/step - loss: 0.4438 - acc: 0.7788\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 0.5967 - acc: 0.6500Epoch 20/20\n",
      "Epoch 20/20\n",
      "227/227 [==============================] - 0s 162us/step - loss: 0.4341 - acc: 0.8150\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 0.3576 - acc: 0.8000Epoch 20/20\n",
      "228/228 [==============================] - 0s 426us/step - loss: 0.4127 - acc: 0.8465\n",
      " 20/227 [=>............................] - ETA: 0s - loss: 0.3164 - acc: 0.9000Epoch 20/20\n",
      "228/228 [==============================] - 0s 440us/step - loss: 0.4695 - acc: 0.7719\n",
      "228/228 [==============================] - 0s 240us/step - loss: 0.4883 - acc: 0.7500\n",
      "227/227 [==============================] - 0s 485us/step - loss: 0.5526 - acc: 0.7841\n",
      "226/226 [==============================] - 0s 858us/step - loss: 0.4764 - acc: 0.7920\n",
      "75/75 [==============================] - 1s 13ms/step\n",
      "228/228 [==============================] - 0s 139us/step\n",
      "76/76 [==============================] - 1s 12ms/step\n",
      "75/75 [==============================] - 1s 12ms/step\n",
      "227/227 [==============================] - 0s 152us/step\n",
      "77/77 [==============================] - 1s 11ms/step\n",
      "228/228 [==============================] - 0s 122us/step\n",
      "226/226 [==============================] - 0s 171us/step\n",
      "75/75 [==============================] - 1s 8ms/step\n",
      "76/76 [==============================] - 1s 7ms/step\n",
      "75/75 [==============================] - 1s 8ms/step\n",
      "228/228 [==============================] - 0s 80us/step\n",
      "77/77 [==============================] - 1s 7ms/step\n",
      "227/227 [==============================] - 0s 48us/step\n",
      "228/228 [==============================] - 0s 82us/step\n",
      "226/226 [==============================] - 0s 44us/step\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.5198 - acc: 0.4983\n",
      "Epoch 2/20\n",
      "303/303 [==============================] - 0s 99us/step - loss: 3.8336 - acc: 0.4917\n",
      "Epoch 3/20\n",
      "303/303 [==============================] - 0s 96us/step - loss: 3.1123 - acc: 0.4587\n",
      "Epoch 4/20\n",
      "303/303 [==============================] - 0s 95us/step - loss: 2.2054 - acc: 0.4356\n",
      "Epoch 5/20\n",
      "303/303 [==============================] - 0s 95us/step - loss: 1.5571 - acc: 0.5215\n",
      "Epoch 6/20\n",
      "303/303 [==============================] - 0s 94us/step - loss: 1.0543 - acc: 0.5842\n",
      "Epoch 7/20\n",
      "303/303 [==============================] - 0s 95us/step - loss: 0.8017 - acc: 0.6304\n",
      "Epoch 8/20\n",
      "303/303 [==============================] - 0s 94us/step - loss: 0.7395 - acc: 0.6733\n",
      "Epoch 9/20\n",
      "303/303 [==============================] - 0s 101us/step - loss: 0.5913 - acc: 0.7327\n",
      "Epoch 10/20\n",
      "303/303 [==============================] - 0s 97us/step - loss: 0.6442 - acc: 0.6964\n",
      "Epoch 11/20\n",
      "303/303 [==============================] - 0s 95us/step - loss: 0.6092 - acc: 0.7261\n",
      "Epoch 12/20\n",
      "303/303 [==============================] - 0s 95us/step - loss: 0.6638 - acc: 0.7030\n",
      "Epoch 13/20\n",
      "303/303 [==============================] - 0s 95us/step - loss: 0.5928 - acc: 0.7459\n",
      "Epoch 14/20\n",
      "303/303 [==============================] - 0s 98us/step - loss: 0.5099 - acc: 0.7591\n",
      "Epoch 15/20\n",
      "303/303 [==============================] - 0s 94us/step - loss: 0.5332 - acc: 0.7624\n",
      "Epoch 16/20\n",
      "303/303 [==============================] - 0s 97us/step - loss: 0.5476 - acc: 0.7492\n",
      "Epoch 17/20\n",
      "303/303 [==============================] - 0s 95us/step - loss: 0.4286 - acc: 0.7987\n",
      "Epoch 18/20\n",
      "303/303 [==============================] - 0s 94us/step - loss: 0.5178 - acc: 0.7690\n",
      "Epoch 19/20\n",
      "303/303 [==============================] - 0s 94us/step - loss: 0.4397 - acc: 0.8020\n",
      "Epoch 20/20\n",
      "303/303 [==============================] - 0s 95us/step - loss: 0.4277 - acc: 0.8086\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cross Validation Score: 0.75907591 used {'batch_size': 20, 'epochs': 20, 'optimizer': 'nadam'}\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'grid_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grid_result' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Parameters -----------------------------------------------\n",
    "batch_size = [20]\n",
    "epochs = [20]\n",
    "optimizer = ['adam','sgd','rmsprop','nadam']\n",
    "param_grid = dict(batch_size=batch_size, \n",
    "                  epochs=epochs,\n",
    "                  optimizer=optimizer)\n",
    "# -----------------------------------------------\n",
    "\n",
    "# random seed to reproduce later\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "def create_model(optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# instantiate model obj\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# GridSearchCV hyperparameters\n",
    "c_v = StratifiedKFold(n_splits=4,\n",
    "                      shuffle=True,\n",
    "                      random_state=seed) # 4-fold CV\n",
    "\n",
    "# instantiate GridSearchCV obj\n",
    "cv_grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    n_jobs=-1, \n",
    "                    cv=c_v)\n",
    "\n",
    "# run the cross validation\n",
    "xval_result = cv_grid.fit(X, y)\n",
    "\n",
    "# Plot the accuracy \n",
    "acc = [x*100 for x in xval_result.best_estimator_.model.history.history['acc']]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(acc)\n",
    "ax.set_xlabel('# Epochs', fontsize=16)\n",
    "ax.set_ylabel('% Accuracy', fontsize=16)\n",
    "ax.set_ylim(60,100)\n",
    "ax.axhline(80, color='k', linestyle='--', linewidth=1)\n",
    "ax.axhline(90, color='k', linestyle='--')\n",
    "plt.title('Best Estimator', fontsize=20, y=1.05)\n",
    "plt.show()\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best Cross Validation Score: {xval_result.best_score_:.8f} used {xval_result.best_params_}\\n\")\n",
    "mean_scores = [x*100 for x in grid_result.cv_results_['mean_test_score']]\n",
    "std_scores  = [x*100 for x in grid_result.cv_results_['std_test_score']]\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(mean_scores, std_scorse, params):\n",
    "    print(f\"Means: {mean:.3f}, Stdev: {stdev:.3f} with: {param}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypertune Batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "227/227 [==============================] - 12s 51ms/step - loss: 4.4879 - acc: 0.5198\n",
      "228/228 [==============================] - 12s 51ms/step - loss: 4.7594 - acc: 0.5088\n",
      "226/226 [==============================] - 12s 51ms/step - loss: 4.5179 - acc: 0.5221\n",
      "228/228 [==============================] - 12s 51ms/step - loss: 4.7778 - acc: 0.4956\n",
      "228/228 [==============================] - 12s 51ms/step - loss: 4.9960 - acc: 0.5044\n",
      "Epoch 2/20\n",
      "Epoch 2/20\n",
      "Epoch 2/20\n",
      "228/228 [==============================] - 12s 51ms/step - loss: 4.8071 - acc: 0.4868\n",
      " 20/227 [=>............................] - ETA: 2:00 - loss: 4.8796 - acc: 0.6000Epoch 2/20\n",
      "Epoch 2/20\n",
      "100/228 [============>.................] - ETA: 0s - loss: 4.5413 - acc: 0.4400Epoch 2/20\n",
      "228/228 [==============================] - 0s 288us/step - loss: 5.0041 - acc: 0.4649\n",
      "226/226 [==============================] - 0s 294us/step - loss: 4.2873 - acc: 0.5265\n",
      "227/227 [==============================] - 12s 52ms/step - loss: 4.5421 - acc: 0.5154\n",
      "228/228 [==============================] - 12s 51ms/step - loss: 5.0202 - acc: 0.5044\n",
      "228/228 [==============================] - 0s 533us/step - loss: 4.7580 - acc: 0.4781\n",
      "120/228 [==============>...............] - ETA: 10s - loss: 5.3255 - acc: 0.4250Epoch 3/20\n",
      "227/227 [==============================] - 0s 534us/step - loss: 4.3309 - acc: 0.5110\n",
      "228/228 [==============================] - 0s 234us/step - loss: 4.7841 - acc: 0.4825\n",
      "226/226 [==============================] - 12s 52ms/step - loss: 4.4463 - acc: 0.5177\n",
      "Epoch 3/20\n",
      "228/228 [==============================] - 0s 528us/step - loss: 4.7272 - acc: 0.4605\n",
      "Epoch 2/20\n",
      " 50/228 [=====>........................] - ETA: 41s - loss: 4.4786 - acc: 0.5800Epoch 2/20\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 4.6954 - acc: 0.4864 Epoch 3/20\n",
      "150/228 [==================>...........] - ETA: 0s - loss: 4.8363 - acc: 0.4667Epoch 3/20\n",
      "Epoch 3/20\n",
      "Epoch 2/20\n",
      "150/226 [==================>...........] - ETA: 0s - loss: 3.8388 - acc: 0.5267Epoch 3/20\n",
      "226/226 [==============================] - 12s 52ms/step - loss: 4.4831 - acc: 0.5088\n",
      "228/228 [==============================] - 0s 248us/step - loss: 4.7612 - acc: 0.4737\n",
      "228/228 [==============================] - 12s 52ms/step - loss: 4.7321 - acc: 0.4825\n",
      "226/226 [==============================] - 0s 227us/step - loss: 4.1943 - acc: 0.5133\n",
      "227/227 [==============================] - 0s 221us/step - loss: 4.2861 - acc: 0.5242\n",
      "Epoch 2/20\n",
      "228/228 [==============================] - 0s 184us/step - loss: 4.7827 - acc: 0.4737\n",
      "227/227 [==============================] - 12s 52ms/step - loss: 4.4035 - acc: 0.5198\n",
      "228/228 [==============================] - 0s 240us/step - loss: 4.7209 - acc: 0.4781\n",
      "150/228 [==================>...........] - ETA: 0s - loss: 4.5529 - acc: 0.5000Epoch 4/20\n",
      "200/227 [=========================>....] - ETA: 0s - loss: 4.1327 - acc: 0.5400Epoch 2/20\n",
      "200/226 [=========================>....] - ETA: 0s - loss: 4.4729 - acc: 0.5100Epoch 4/20\n",
      "200/228 [=========================>....] - ETA: 0s - loss: 4.4871 - acc: 0.4500Epoch 3/20\n",
      " 50/226 [=====>........................] - ETA: 0s - loss: 4.5116 - acc: 0.5200Epoch 3/20\n",
      "Epoch 2/20\n",
      "228/228 [==============================] - 12s 52ms/step - loss: 4.9180 - acc: 0.4956\n",
      "Epoch 4/20\n",
      "227/227 [==============================] - 12s 52ms/step - loss: 4.4862 - acc: 0.5242\n",
      "228/228 [==============================] - 0s 415us/step - loss: 4.4792 - acc: 0.4956\n",
      "227/227 [==============================] - 0s 517us/step - loss: 4.1638 - acc: 0.5507\n",
      "226/226 [==============================] - 0s 496us/step - loss: 4.2299 - acc: 0.5265\n",
      "228/228 [==============================] - 0s 493us/step - loss: 4.5158 - acc: 0.4561\n",
      "150/228 [==================>...........] - ETA: 0s - loss: 4.8354 - acc: 0.4267Epoch 2/20\n",
      "170/228 [=====================>........] - ETA: 4s - loss: 4.8779 - acc: 0.4882Epoch 2/20\n",
      "100/228 [============>.................] - ETA: 0s - loss: 4.6075 - acc: 0.4800Epoch 4/20\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 4.3198 - acc: 0.5500Epoch 4/20\n",
      "228/228 [==============================] - 0s 239us/step - loss: 4.6110 - acc: 0.4825\n",
      "Epoch 3/20\n",
      "226/226 [==============================] - 0s 229us/step - loss: 4.1334 - acc: 0.5044\n",
      "Epoch 4/20\n",
      "227/227 [==============================] - 0s 215us/step - loss: 4.2114 - acc: 0.5419\n",
      "226/226 [==============================] - 0s 505us/step - loss: 4.2679 - acc: 0.4912\n",
      "228/228 [==============================] - 0s 224us/step - loss: 4.7381 - acc: 0.4518\n",
      "226/226 [==============================] - 12s 53ms/step - loss: 4.3753 - acc: 0.5088\n",
      "100/227 [============>.................] - ETA: 0s - loss: 3.5648 - acc: 0.5600Epoch 5/20\n",
      "100/228 [============>.................] - ETA: 0s - loss: 4.3377 - acc: 0.4300Epoch 5/20\n",
      "100/226 [============>.................] - ETA: 0s - loss: 3.7450 - acc: 0.5300Epoch 4/20\n",
      "Epoch 4/20\n",
      "Epoch 3/20\n",
      "100/227 [============>.................] - ETA: 0s - loss: 4.4909 - acc: 0.4900Epoch 2/20\n",
      "228/228 [==============================] - 12s 53ms/step - loss: 4.7202 - acc: 0.4912\n",
      "227/227 [==============================] - 0s 238us/step - loss: 4.2103 - acc: 0.5066\n",
      "228/228 [==============================] - 0s 206us/step - loss: 4.1916 - acc: 0.4518\n",
      "228/228 [==============================] - 0s 500us/step - loss: 4.7419 - acc: 0.4781\n",
      "227/227 [==============================] - 0s 235us/step - loss: 4.1257 - acc: 0.5198\n",
      "150/228 [==================>...........] - ETA: 0s - loss: 4.6340 - acc: 0.4600Epoch 2/20\n",
      "Epoch 3/20\n",
      " 10/226 [>.............................] - ETA: 0s - loss: 3.6871 - acc: 0.4000Epoch 5/20\n",
      "Epoch 5/20\n",
      "228/228 [==============================] - 0s 157us/step - loss: 4.5381 - acc: 0.4737\n",
      "Epoch 5/20\n",
      "226/226 [==============================] - 0s 237us/step - loss: 4.0300 - acc: 0.5221\n",
      "226/226 [==============================] - 0s 475us/step - loss: 4.1320 - acc: 0.5177\n",
      "228/228 [==============================] - 0s 482us/step - loss: 4.3208 - acc: 0.4561\n",
      "227/227 [==============================] - 0s 237us/step - loss: 4.1523 - acc: 0.5286\n",
      "228/228 [==============================] - 0s 250us/step - loss: 4.6497 - acc: 0.4430\n",
      "100/227 [============>.................] - ETA: 0s - loss: 4.0728 - acc: 0.4700Epoch 6/20\n",
      "Epoch 4/20\n",
      "Epoch 6/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 4.3336 - acc: 0.4518\n",
      "Epoch 5/20\n",
      "Epoch 5/20\n",
      "226/226 [==============================] - 0s 420us/step - loss: 4.0599 - acc: 0.5133\n",
      "Epoch 5/20\n",
      "228/228 [==============================] - 0s 917us/step - loss: 4.6910 - acc: 0.4605\n",
      "200/227 [=========================>....] - ETA: 0s - loss: 4.0438 - acc: 0.5100Epoch 3/20\n",
      "150/228 [==================>...........] - ETA: 0s - loss: 4.0476 - acc: 0.5133Epoch 4/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 3.7836 - acc: 0.5066\n",
      "228/228 [==============================] - 0s 412us/step - loss: 3.7075 - acc: 0.4605\n",
      "200/227 [=========================>....] - ETA: 0s - loss: 3.9842 - acc: 0.5300Epoch 3/20\n",
      "228/228 [==============================] - 0s 454us/step - loss: 4.4123 - acc: 0.4605\n",
      "227/227 [==============================] - 0s 454us/step - loss: 3.9589 - acc: 0.5066\n",
      "228/228 [==============================] - 0s 230us/step - loss: 4.4347 - acc: 0.4561\n",
      "226/226 [==============================] - 0s 240us/step - loss: 3.9264 - acc: 0.5133\n",
      "227/227 [==============================] - 0s 244us/step - loss: 4.0912 - acc: 0.5110\n",
      "228/228 [==============================] - 0s 242us/step - loss: 4.5394 - acc: 0.4956\n",
      " 50/226 [=====>........................] - ETA: 0s - loss: 3.2725 - acc: 0.4400Epoch 6/20\n",
      "Epoch 3/20\n",
      "227/227 [==============================] - 0s 614us/step - loss: 4.0072 - acc: 0.5419\n",
      " 50/228 [=====>........................] - ETA: 0s - loss: 4.2551 - acc: 0.3800Epoch 6/20\n",
      "110/226 [=============>................] - ETA: 0s - loss: 4.3919 - acc: 0.4727Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 7/20\n",
      "226/226 [==============================] - 0s 441us/step - loss: 4.0006 - acc: 0.5088\n",
      "228/228 [==============================] - 0s 432us/step - loss: 4.1035 - acc: 0.4474\n",
      "Epoch 6/20\n",
      "Epoch 6/20\n",
      "226/226 [==============================] - 0s 237us/step - loss: 3.8468 - acc: 0.4912\n",
      " 10/227 [>.............................] - ETA: 0s - loss: 3.4343 - acc: 0.4000Epoch 4/20\n",
      "150/228 [==================>...........] - ETA: 0s - loss: 4.0654 - acc: 0.4800Epoch 5/20\n",
      "150/226 [==================>...........] - ETA: 0s - loss: 3.4432 - acc: 0.5200Epoch 6/20\n",
      "150/228 [==================>...........] - ETA: 0s - loss: 4.6840 - acc: 0.4600Epoch 5/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 3.8828 - acc: 0.4735\n",
      "226/226 [==============================] - 0s 222us/step - loss: 3.8660 - acc: 0.4867\n",
      "227/227 [==============================] - 0s 226us/step - loss: 3.9121 - acc: 0.5066\n",
      "228/228 [==============================] - 0s 237us/step - loss: 4.2558 - acc: 0.4737\n",
      "227/227 [==============================] - 0s 238us/step - loss: 4.0900 - acc: 0.5066\n",
      "228/228 [==============================] - 0s 249us/step - loss: 4.5111 - acc: 0.4868\n",
      "228/228 [==============================] - 0s 475us/step - loss: 3.4461 - acc: 0.4649\n",
      "228/228 [==============================] - 0s 475us/step - loss: 4.2507 - acc: 0.4825\n",
      "Epoch 3/20\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 4.0866 - acc: 0.5000Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 8/20\n",
      "226/226 [==============================] - 0s 171us/step - loss: 3.8904 - acc: 0.5221\n",
      "228/228 [==============================] - 0s 175us/step - loss: 3.8448 - acc: 0.4254\n",
      "Epoch 7/20\n",
      "226/226 [==============================] - 0s 193us/step - loss: 3.5578 - acc: 0.5044\n",
      "Epoch 7/20\n",
      " 90/227 [==========>...................] - ETA: 0s - loss: 2.9591 - acc: 0.5444Epoch 7/20\n",
      "227/227 [==============================] - 0s 468us/step - loss: 3.7875 - acc: 0.5286\n",
      "Epoch 7/20\n",
      "228/228 [==============================] - 0s 933us/step - loss: 4.3257 - acc: 0.4649\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 4.1516 - acc: 0.4912\n",
      "150/228 [==================>...........] - ETA: 0s - loss: 4.2474 - acc: 0.4467Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 6/20\n",
      " 50/228 [=====>........................] - ETA: 0s - loss: 4.3796 - acc: 0.3800Epoch 5/20\n",
      "100/228 [============>.................] - ETA: 0s - loss: 4.8867 - acc: 0.4400Epoch 4/20\n",
      " 40/226 [====>.........................] - ETA: 0s - loss: 3.2340 - acc: 0.4250Epoch 3/20\n",
      "226/226 [==============================] - 0s 230us/step - loss: 3.7995 - acc: 0.5310\n",
      "228/228 [==============================] - 0s 235us/step - loss: 4.1074 - acc: 0.4430\n",
      "227/227 [==============================] - 0s 247us/step - loss: 3.8978 - acc: 0.5330\n",
      "228/228 [==============================] - 0s 245us/step - loss: 4.2584 - acc: 0.4825\n",
      "227/227 [==============================] - 0s 462us/step - loss: 3.9117 - acc: 0.5154\n",
      " 10/228 [>.............................] - ETA: 0s - loss: 2.2189 - acc: 0.7000Epoch 9/20\n",
      "Epoch 9/20\n",
      "228/228 [==============================] - 0s 235us/step - loss: 3.8053 - acc: 0.4474\n",
      "200/226 [=========================>....] - ETA: 0s - loss: 3.3930 - acc: 0.5000Epoch 8/20\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 3.4655 - acc: 0.4693\n",
      "Epoch 8/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 3.1462 - acc: 0.5066\n",
      "227/227 [==============================] - 0s 222us/step - loss: 3.5134 - acc: 0.5198\n",
      "228/228 [==============================] - 0s 436us/step - loss: 3.0333 - acc: 0.4474\n",
      "228/228 [==============================] - 0s 451us/step - loss: 4.0747 - acc: 0.4868\n",
      "100/226 [============>.................] - ETA: 0s - loss: 3.1196 - acc: 0.4700Epoch 8/20\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 3.0094 - acc: 0.6000Epoch 8/20\n",
      "226/226 [==============================] - 0s 475us/step - loss: 3.5935 - acc: 0.4956\n",
      "150/228 [==================>...........] - ETA: 0s - loss: 3.6553 - acc: 0.5267Epoch 4/20\n",
      "Epoch 4/20\n",
      "Epoch 6/20\n",
      "Epoch 8/20\n",
      "Epoch 8/20\n",
      "228/228 [==============================] - 0s 473us/step - loss: 4.0944 - acc: 0.4342\n",
      "226/226 [==============================] - 0s 225us/step - loss: 3.5484 - acc: 0.4779\n",
      "228/228 [==============================] - 0s 232us/step - loss: 3.9173 - acc: 0.4430\n",
      "100/228 [============>.................] - ETA: 0s - loss: 3.7290 - acc: 0.4000Epoch 7/20\n",
      "227/227 [==============================] - 0s 226us/step - loss: 3.7518 - acc: 0.5154\n",
      "226/226 [==============================] - 0s 692us/step - loss: 3.2735 - acc: 0.4690\n",
      "228/228 [==============================] - 0s 225us/step - loss: 4.0465 - acc: 0.4912\n",
      "100/228 [============>.................] - ETA: 0s - loss: 3.6875 - acc: 0.4900Epoch 5/20\n",
      "227/227 [==============================] - 0s 199us/step - loss: 3.5041 - acc: 0.5110\n",
      "140/228 [=================>............] - ETA: 0s - loss: 3.2877 - acc: 0.4143Epoch 10/20\n",
      "Epoch 10/20\n",
      "200/228 [=========================>....] - ETA: 0s - loss: 3.7482 - acc: 0.4200Epoch 9/20\n",
      "Epoch 7/20\n",
      "Epoch 9/20\n",
      "228/228 [==============================] - 0s 239us/step - loss: 2.7674 - acc: 0.4518\n",
      " 50/228 [=====>........................] - ETA: 0s - loss: 2.5950 - acc: 0.4800Epoch 9/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 3.2258 - acc: 0.4513\n",
      "228/228 [==============================] - 0s 468us/step - loss: 3.6412 - acc: 0.4298\n",
      " 40/228 [====>.........................] - ETA: 0s - loss: 2.0392 - acc: 0.4500Epoch 9/20\n",
      "228/228 [==============================] - 0s 467us/step - loss: 3.8819 - acc: 0.4474\n",
      "100/228 [============>.................] - ETA: 0s - loss: 3.5721 - acc: 0.4300Epoch 4/20\n",
      "226/226 [==============================] - 0s 222us/step - loss: 3.5485 - acc: 0.4558\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 2.9927 - acc: 0.4167\n",
      "228/228 [==============================] - 0s 192us/step - loss: 3.7626 - acc: 0.4386\n",
      "226/226 [==============================] - 0s 410us/step - loss: 3.5224 - acc: 0.4602\n",
      "Epoch 9/20\n",
      "228/228 [==============================] - 0s 165us/step - loss: 3.8330 - acc: 0.4781\n",
      "227/227 [==============================] - 0s 218us/step - loss: 3.6373 - acc: 0.5110\n",
      "226/226 [==============================] - 0s 194us/step - loss: 2.8826 - acc: 0.4690\n",
      "200/227 [=========================>....] - ETA: 0s - loss: 3.0231 - acc: 0.5200Epoch 9/20\n",
      "200/227 [=========================>....] - ETA: 0s - loss: 3.4073 - acc: 0.4950Epoch 11/20\n",
      " 10/226 [>.............................] - ETA: 0s - loss: 1.4050 - acc: 0.4000Epoch 4/20\n",
      "Epoch 11/20\n",
      "Epoch 8/20\n",
      "100/228 [============>.................] - ETA: 0s - loss: 3.6387 - acc: 0.3900Epoch 10/20\n",
      "Epoch 10/20\n",
      "Epoch 8/20\n",
      "227/227 [==============================] - 0s 957us/step - loss: 3.2110 - acc: 0.4890\n",
      "227/227 [==============================] - 0s 480us/step - loss: 3.4056 - acc: 0.5022\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 2.5513 - acc: 0.4386\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 2.3443 - acc: 0.5286\n",
      "200/228 [=========================>....] - ETA: 0s - loss: 3.5190 - acc: 0.4900Epoch 7/20\n",
      "228/228 [==============================] - 0s 915us/step - loss: 3.7541 - acc: 0.4298\n",
      "226/226 [==============================] - 0s 224us/step - loss: 3.4065 - acc: 0.4558\n",
      "Epoch 10/20\n",
      "226/226 [==============================] - 0s 242us/step - loss: 3.2298 - acc: 0.5354\n",
      "228/228 [==============================] - 0s 249us/step - loss: 3.6305 - acc: 0.4430\n",
      "228/228 [==============================] - 0s 511us/step - loss: 3.2402 - acc: 0.4167\n",
      "228/228 [==============================] - 0s 234us/step - loss: 3.7135 - acc: 0.4605\n",
      "227/227 [==============================] - 0s 250us/step - loss: 3.5129 - acc: 0.5066\n",
      "Epoch 5/20\n",
      "100/226 [============>.................] - ETA: 0s - loss: 2.9881 - acc: 0.4200Epoch 5/20\n",
      "228/228 [==============================] - 0s 637us/step - loss: 2.3340 - acc: 0.4561\n",
      "228/228 [==============================] - 0s 413us/step - loss: 3.5264 - acc: 0.4737\n",
      "Epoch 6/20\n",
      " 50/227 [=====>........................] - ETA: 0s - loss: 5.8558 - acc: 0.4000Epoch 12/20\n",
      "100/227 [============>.................] - ETA: 0s - loss: 2.9026 - acc: 0.5300Epoch 9/20\n",
      "100/226 [============>.................] - ETA: 0s - loss: 2.4712 - acc: 0.4600Epoch 11/20\n",
      "Epoch 10/20\n",
      "Epoch 12/20\n",
      "Epoch 11/20\n",
      "226/226 [==============================] - 0s 443us/step - loss: 2.8475 - acc: 0.4735\n",
      "Epoch 10/20\n",
      " 10/227 [>.............................] - ETA: 0s - loss: 0.7732 - acc: 0.7000Epoch 10/20\n",
      "227/227 [==============================] - 0s 234us/step - loss: 3.3959 - acc: 0.5154\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 1.8389 - acc: 0.5500Epoch 9/20\n",
      "100/228 [============>.................] - ETA: 0s - loss: 2.8558 - acc: 0.5000Epoch 8/20\n",
      "226/226 [==============================] - 0s 256us/step - loss: 3.2471 - acc: 0.4513\n",
      "227/227 [==============================] - 0s 450us/step - loss: 3.2406 - acc: 0.5110\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 2.4518 - acc: 0.4646\n",
      "226/226 [==============================] - 0s 238us/step - loss: 3.0434 - acc: 0.4602\n",
      "228/228 [==============================] - 0s 221us/step - loss: 3.4648 - acc: 0.4342\n",
      "228/228 [==============================] - 0s 221us/step - loss: 3.0552 - acc: 0.4254\n",
      "228/228 [==============================] - 0s 211us/step - loss: 3.5730 - acc: 0.4430\n",
      "227/227 [==============================] - 0s 212us/step - loss: 3.3309 - acc: 0.5154\n",
      " 50/227 [=====>........................] - ETA: 0s - loss: 2.9876 - acc: 0.5200Epoch 13/20\n",
      "Epoch 11/20\n",
      "Epoch 10/20\n",
      "Epoch 5/20\n",
      "Epoch 11/20\n",
      "Epoch 13/20\n",
      "Epoch 12/20\n",
      "Epoch 12/20\n",
      "228/228 [==============================] - 0s 477us/step - loss: 3.3054 - acc: 0.4561\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.9676 - acc: 0.4561\n",
      " 50/227 [=====>........................] - ETA: 0s - loss: 1.7825 - acc: 0.5400Epoch 11/20\n",
      "228/228 [==============================] - 0s 931us/step - loss: 3.4160 - acc: 0.3991\n",
      "226/226 [==============================] - 0s 223us/step - loss: 3.0795 - acc: 0.5000\n",
      "226/226 [==============================] - 0s 227us/step - loss: 2.9458 - acc: 0.4646\n",
      "228/228 [==============================] - 0s 233us/step - loss: 3.3691 - acc: 0.4211\n",
      "228/228 [==============================] - 0s 228us/step - loss: 3.3192 - acc: 0.4605\n",
      "227/227 [==============================] - 0s 230us/step - loss: 3.2057 - acc: 0.5066\n",
      "Epoch 5/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.6097 - acc: 0.4605\n",
      "228/228 [==============================] - 0s 862us/step - loss: 1.9194 - acc: 0.4605\n",
      "226/226 [==============================] - 0s 652us/step - loss: 2.3176 - acc: 0.4867\n",
      "110/227 [=============>................] - ETA: 0s - loss: 1.7275 - acc: 0.5182Epoch 7/20\n",
      "227/227 [==============================] - 0s 672us/step - loss: 2.6879 - acc: 0.4978\n",
      "Epoch 14/20\n",
      "Epoch 11/20\n",
      "227/227 [==============================] - 0s 440us/step - loss: 3.1269 - acc: 0.5066\n",
      "228/228 [==============================] - 0s 445us/step - loss: 2.9645 - acc: 0.4386\n",
      "Epoch 14/20\n",
      "Epoch 13/20\n",
      "Epoch 13/20\n",
      "Epoch 6/20\n",
      " 10/228 [>.............................] - ETA: 0s - loss: 1.6341 - acc: 0.5000Epoch 10/20\n",
      "Epoch 11/20\n",
      "228/228 [==============================] - 0s 191us/step - loss: 3.3057 - acc: 0.4649\n",
      "210/227 [==========================>...] - ETA: 0s - loss: 1.8315 - acc: 0.5048Epoch 9/20\n",
      "100/226 [============>.................] - ETA: 0s - loss: 3.0449 - acc: 0.4100Epoch 12/20\n",
      "150/228 [==================>...........] - ETA: 0s - loss: 3.4923 - acc: 0.4200Epoch 12/20\n",
      " 50/228 [=====>........................] - ETA: 0s - loss: 2.9320 - acc: 0.5200Epoch 12/20\n",
      "227/227 [==============================] - 0s 2ms/step - loss: 1.7877 - acc: 0.5110\n",
      "226/226 [==============================] - 0s 217us/step - loss: 2.9376 - acc: 0.4602\n",
      "226/226 [==============================] - 0s 235us/step - loss: 2.7032 - acc: 0.4779\n",
      "228/228 [==============================] - 0s 230us/step - loss: 3.1740 - acc: 0.4430\n",
      "228/228 [==============================] - 0s 231us/step - loss: 3.1090 - acc: 0.4561\n",
      "227/227 [==============================] - 0s 240us/step - loss: 3.0826 - acc: 0.4978\n",
      "200/228 [=========================>....] - ETA: 0s - loss: 3.1821 - acc: 0.4650Epoch 6/20\n",
      "100/227 [============>.................] - ETA: 0s - loss: 2.0941 - acc: 0.5500Epoch 15/20\n",
      "Epoch 12/20\n",
      "227/227 [==============================] - 0s 222us/step - loss: 2.9720 - acc: 0.4978\n",
      "Epoch 15/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 1.6521 - acc: 0.5088\n",
      "228/228 [==============================] - 0s 207us/step - loss: 2.5522 - acc: 0.4386\n",
      "Epoch 14/20\n",
      "140/228 [=================>............] - ETA: 0s - loss: 1.2002 - acc: 0.5571Epoch 14/20\n",
      "228/228 [==============================] - 0s 696us/step - loss: 3.1938 - acc: 0.4649\n",
      "150/226 [==================>...........] - ETA: 0s - loss: 2.8321 - acc: 0.4467Epoch 13/20\n",
      "150/228 [==================>...........] - ETA: 0s - loss: 3.1200 - acc: 0.4200Epoch 13/20\n",
      "Epoch 6/20\n",
      "228/228 [==============================] - 0s 683us/step - loss: 1.0133 - acc: 0.6096\n",
      "226/226 [==============================] - 0s 691us/step - loss: 1.9271 - acc: 0.5000\n",
      "228/228 [==============================] - 0s 704us/step - loss: 2.2018 - acc: 0.4912\n",
      "228/228 [==============================] - 0s 468us/step - loss: 2.8535 - acc: 0.4474\n",
      "Epoch 8/20\n",
      "227/227 [==============================] - 0s 715us/step - loss: 2.3954 - acc: 0.4890\n",
      "226/226 [==============================] - 0s 244us/step - loss: 2.7643 - acc: 0.4646\n",
      "228/228 [==============================] - 0s 258us/step - loss: 2.9618 - acc: 0.4342\n",
      "228/228 [==============================] - 0s 257us/step - loss: 2.9066 - acc: 0.4474\n",
      "Epoch 7/20\n",
      "Epoch 11/20\n",
      "227/227 [==============================] - 0s 203us/step - loss: 2.9397 - acc: 0.5242\n",
      "130/228 [================>.............] - ETA: 0s - loss: 1.2075 - acc: 0.5538Epoch 13/20\n",
      "Epoch 12/20\n",
      " 50/228 [=====>........................] - ETA: 0s - loss: 3.6636 - acc: 0.4400Epoch 10/20\n",
      " 90/227 [==========>...................] - ETA: 0s - loss: 1.0782 - acc: 0.6111Epoch 16/20\n",
      "226/226 [==============================] - 0s 456us/step - loss: 2.4053 - acc: 0.4602\n",
      "227/227 [==============================] - 0s 224us/step - loss: 2.7588 - acc: 0.4934\n",
      "228/228 [==============================] - 0s 237us/step - loss: 2.4987 - acc: 0.4430\n",
      "Epoch 16/20\n",
      "Epoch 15/20\n",
      " 10/228 [>.............................] - ETA: 0s - loss: 0.8805 - acc: 0.5000Epoch 15/20\n",
      "150/226 [==================>...........] - ETA: 0s - loss: 3.0142 - acc: 0.4200Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 14/20\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 1.2317 - acc: 0.5219\n",
      "228/228 [==============================] - 0s 226us/step - loss: 2.6646 - acc: 0.4693\n",
      "226/226 [==============================] - 0s 238us/step - loss: 2.7356 - acc: 0.4558\n",
      "228/228 [==============================] - 0s 228us/step - loss: 2.8249 - acc: 0.4518\n",
      "228/228 [==============================] - 0s 235us/step - loss: 2.8368 - acc: 0.4430\n",
      "227/227 [==============================] - 0s 230us/step - loss: 2.7404 - acc: 0.5110\n",
      "Epoch 6/20\n",
      "Epoch 14/20\n",
      "227/227 [==============================] - 0s 439us/step - loss: 2.1132 - acc: 0.5198\n",
      "228/228 [==============================] - 0s 423us/step - loss: 1.6218 - acc: 0.5044\n",
      "228/228 [==============================] - 0s 671us/step - loss: 2.7895 - acc: 0.4386\n",
      "Epoch 17/20\n",
      "226/226 [==============================] - 0s 238us/step - loss: 2.3312 - acc: 0.4602\n",
      "150/226 [==================>...........] - ETA: 0s - loss: 1.1258 - acc: 0.5667Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 16/20\n",
      "226/226 [==============================] - 0s 710us/step - loss: 1.6702 - acc: 0.5265\n",
      "100/228 [============>.................] - ETA: 0s - loss: 2.5486 - acc: 0.5300Epoch 11/20\n",
      "Epoch 9/20\n",
      "Epoch 13/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 1.1124 - acc: 0.5859\n",
      "227/227 [==============================] - 0s 420us/step - loss: 2.5779 - acc: 0.5110\n",
      "Epoch 14/20\n",
      "228/228 [==============================] - 0s 419us/step - loss: 2.1898 - acc: 0.4386\n",
      " 50/228 [=====>........................] - ETA: 0s - loss: 0.6607 - acc: 0.7200Epoch 12/20\n",
      " 50/228 [=====>........................] - ETA: 0s - loss: 1.7247 - acc: 0.5200Epoch 7/20\n",
      "226/226 [==============================] - 0s 260us/step - loss: 2.4603 - acc: 0.4602\n",
      "Epoch 15/20\n",
      "100/226 [============>.................] - ETA: 0s - loss: 2.3384 - acc: 0.4600Epoch 15/20\n",
      "228/228 [==============================] - 0s 198us/step - loss: 2.6167 - acc: 0.4211\n",
      "228/228 [==============================] - 0s 215us/step - loss: 2.8082 - acc: 0.4825\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 1.0652 - acc: 0.5708\n",
      "227/227 [==============================] - 0s 197us/step - loss: 2.6041 - acc: 0.4978\n",
      "228/228 [==============================] - 0s 502us/step - loss: 2.6055 - acc: 0.4825\n",
      " 10/227 [>.............................] - ETA: 0s - loss: 2.6539 - acc: 0.3000Epoch 18/20\n",
      "100/228 [============>.................] - ETA: 0s - loss: 2.9429 - acc: 0.5200Epoch 18/20\n",
      "Epoch 17/20\n",
      "Epoch 7/20\n",
      "Epoch 17/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.6747 - acc: 0.6667\n",
      "228/228 [==============================] - 0s 459us/step - loss: 1.4614 - acc: 0.5263\n",
      "Epoch 15/20\n",
      "226/226 [==============================] - 0s 486us/step - loss: 2.4121 - acc: 0.4646\n",
      "150/227 [==================>...........] - ETA: 0s - loss: 2.9584 - acc: 0.5333Epoch 8/20\n",
      "228/228 [==============================] - 0s 912us/step - loss: 0.7647 - acc: 0.6798\n",
      "150/226 [==================>...........] - ETA: 0s - loss: 1.3317 - acc: 0.5467Epoch 14/20\n",
      "228/228 [==============================] - 0s 682us/step - loss: 2.4115 - acc: 0.4254\n",
      "227/227 [==============================] - 0s 712us/step - loss: 1.8789 - acc: 0.4978\n",
      "226/226 [==============================] - 0s 231us/step - loss: 2.3265 - acc: 0.4690\n",
      " 60/227 [======>.......................] - ETA: 0s - loss: 1.0659 - acc: 0.5667Epoch 15/20\n",
      "228/228 [==============================] - 0s 454us/step - loss: 2.2141 - acc: 0.5000\n",
      "227/227 [==============================] - 0s 472us/step - loss: 2.4380 - acc: 0.5154\n",
      "228/228 [==============================] - 0s 226us/step - loss: 2.4064 - acc: 0.4211\n",
      "228/228 [==============================] - 0s 222us/step - loss: 2.4044 - acc: 0.4737\n",
      "227/227 [==============================] - 0s 236us/step - loss: 2.6149 - acc: 0.5419\n",
      " 10/228 [>.............................] - ETA: 0s - loss: 0.2662 - acc: 1.0000Epoch 7/20\n",
      "226/226 [==============================] - 0s 652us/step - loss: 1.6340 - acc: 0.5000\n",
      "Epoch 10/20\n",
      " 50/228 [=====>........................] - ETA: 0s - loss: 1.5344 - acc: 0.5800Epoch 12/20\n",
      "228/228 [==============================] - 0s 208us/step - loss: 2.5554 - acc: 0.4561\n",
      "Epoch 19/20\n",
      "120/227 [==============>...............] - ETA: 0s - loss: 0.9782 - acc: 0.5583Epoch 16/20\n",
      "Epoch 16/20\n",
      "Epoch 19/20\n",
      "Epoch 18/20\n",
      "140/226 [=================>............] - ETA: 0s - loss: 0.8669 - acc: 0.6143Epoch 18/20\n",
      " 30/228 [==>...........................] - ETA: 0s - loss: 0.8315 - acc: 0.7000Epoch 13/20\n",
      "100/228 [============>.................] - ETA: 0s - loss: 1.3181 - acc: 0.5400Epoch 16/20\n",
      "226/226 [==============================] - 0s 203us/step - loss: 1.8797 - acc: 0.5177\n",
      "100/227 [============>.................] - ETA: 0s - loss: 1.9251 - acc: 0.4600Epoch 16/20\n",
      "226/226 [==============================] - 0s 247us/step - loss: 2.2169 - acc: 0.4469\n",
      "228/228 [==============================] - 0s 207us/step - loss: 1.8212 - acc: 0.4605\n",
      "227/227 [==============================] - 0s 237us/step - loss: 2.2232 - acc: 0.4978\n",
      "228/228 [==============================] - 0s 231us/step - loss: 2.6327 - acc: 0.4474\n",
      "228/228 [==============================] - 0s 230us/step - loss: 2.2077 - acc: 0.4518\n",
      "227/227 [==============================] - 0s 231us/step - loss: 2.5910 - acc: 0.4978\n",
      "228/228 [==============================] - 0s 660us/step - loss: 1.4038 - acc: 0.5132\n",
      "200/228 [=========================>....] - ETA: 0s - loss: 2.1119 - acc: 0.4550Epoch 20/20\n",
      "Epoch 17/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 0.8468 - acc: 0.6167\n",
      "Epoch 17/20\n",
      "Epoch 20/20\n",
      "Epoch 19/20\n",
      "Epoch 19/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 0.7658 - acc: 0.6460\n",
      "226/226 [==============================] - 0s 402us/step - loss: 1.4699 - acc: 0.5000\n",
      "140/228 [=================>............] - ETA: 0s - loss: 0.7251 - acc: 0.6929Epoch 15/20\n",
      "228/228 [==============================] - 0s 694us/step - loss: 2.0563 - acc: 0.4605\n",
      "228/228 [==============================] - 0s 480us/step - loss: 2.0628 - acc: 0.4605\n",
      "100/228 [============>.................] - ETA: 0s - loss: 2.5276 - acc: 0.4600Epoch 8/20\n",
      "150/227 [==================>...........] - ETA: 0s - loss: 2.5533 - acc: 0.5400Epoch 8/20\n",
      "Epoch 14/20\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 0.8361 - acc: 0.6318Epoch 11/20\n",
      "227/227 [==============================] - 0s 908us/step - loss: 1.6952 - acc: 0.5198\n",
      "226/226 [==============================] - 0s 441us/step - loss: 1.6491 - acc: 0.5044\n",
      "226/226 [==============================] - 0s 222us/step - loss: 2.0972 - acc: 0.5221\n",
      "Epoch 17/20\n",
      "228/228 [==============================] - 0s 228us/step - loss: 2.1187 - acc: 0.4561\n",
      "228/228 [==============================] - 0s 241us/step - loss: 2.5515 - acc: 0.4956\n",
      "228/228 [==============================] - 0s 225us/step - loss: 2.2939 - acc: 0.4561\n",
      "227/227 [==============================] - 0s 239us/step - loss: 2.6084 - acc: 0.5110\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.8462 - acc: 0.6272\n",
      "228/228 [==============================] - 0s 226us/step - loss: 1.1878 - acc: 0.5395\n",
      "Epoch 13/20\n",
      "150/228 [==================>...........] - ETA: 0s - loss: 0.6829 - acc: 0.6867Epoch 17/20\n",
      "100/228 [============>.................] - ETA: 0s - loss: 2.7732 - acc: 0.4700Epoch 18/20\n",
      " 30/227 [==>...........................] - ETA: 0s - loss: 0.6401 - acc: 0.7333Epoch 20/20\n",
      "227/227 [==============================] - 0s 456us/step - loss: 2.0914 - acc: 0.5110\n",
      "Epoch 20/20\n",
      "100/226 [============>.................] - ETA: 0s - loss: 1.0948 - acc: 0.6200Epoch 9/20\n",
      "Epoch 16/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.6618 - acc: 0.6798\n",
      "100/228 [============>.................] - ETA: 0s - loss: 1.3729 - acc: 0.4400Epoch 18/20\n",
      "226/226 [==============================] - 0s 468us/step - loss: 1.2935 - acc: 0.5575\n",
      " 50/228 [=====>........................] - ETA: 0s - loss: 0.9841 - acc: 0.5800Epoch 8/20\n",
      "226/226 [==============================] - 0s 214us/step - loss: 1.4838 - acc: 0.4779\n",
      "228/228 [==============================] - 0s 441us/step - loss: 2.2517 - acc: 0.4693\n",
      "228/228 [==============================] - 0s 511us/step - loss: 2.2653 - acc: 0.4868\n",
      "228/228 [==============================] - 0s 335us/step - loss: 2.3980 - acc: 0.4781\n",
      "227/227 [==============================] - 0s 338us/step - loss: 2.1662 - acc: 0.5374\n",
      "Epoch 15/20\n",
      "150/227 [==================>...........] - ETA: 0s - loss: 1.4317 - acc: 0.5800Epoch 12/20\n",
      "Epoch 18/20\n",
      "Epoch 18/20\n",
      "228/228 [==============================] - 0s 480us/step - loss: 1.4556 - acc: 0.4649\n",
      "227/227 [==============================] - 0s 168us/step - loss: 1.8808 - acc: 0.5463\n",
      "100/228 [============>.................] - ETA: 0s - loss: 1.8835 - acc: 0.5100Epoch 19/20\n",
      "Epoch 19/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 0.7079 - acc: 0.6696\n",
      "226/226 [==============================] - 0s 220us/step - loss: 0.9626 - acc: 0.5752\n",
      "228/228 [==============================] - 0s 792us/step - loss: 0.9230 - acc: 0.5526\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 1.5449 - acc: 0.5463\n",
      "200/228 [=========================>....] - ETA: 0s - loss: 1.8879 - acc: 0.4800Epoch 9/20\n",
      "100/227 [============>.................] - ETA: 0s - loss: 2.5506 - acc: 0.5100Epoch 16/20\n",
      "100/226 [============>.................] - ETA: 0s - loss: 0.4885 - acc: 0.7600Epoch 17/20\n",
      " 40/228 [====>.........................] - ETA: 0s - loss: 1.1387 - acc: 0.5250Epoch 14/20\n",
      "226/226 [==============================] - 0s 493us/step - loss: 1.5205 - acc: 0.5575\n",
      "228/228 [==============================] - 0s 479us/step - loss: 1.7954 - acc: 0.4956\n",
      "227/227 [==============================] - 0s 144us/step - loss: 2.1932 - acc: 0.4978\n",
      " 60/228 [======>.......................] - ETA: 0s - loss: 0.5930 - acc: 0.7333Epoch 19/20\n",
      "200/228 [=========================>....] - ETA: 0s - loss: 1.8498 - acc: 0.4750Epoch 19/20\n",
      "228/228 [==============================] - 0s 547us/step - loss: 1.2800 - acc: 0.5044\n",
      "Epoch 20/20\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.5437 - acc: 0.7566\n",
      "228/228 [==============================] - 0s 977us/step - loss: 1.7275 - acc: 0.4912\n",
      "100/227 [============>.................] - ETA: 0s - loss: 1.9274 - acc: 0.5800Epoch 20/20\n",
      " 60/227 [======>.......................] - ETA: 0s - loss: 0.5771 - acc: 0.7333Epoch 9/20\n",
      "227/227 [==============================] - 0s 423us/step - loss: 1.4593 - acc: 0.5198\n",
      "200/226 [=========================>....] - ETA: 0s - loss: 1.4640 - acc: 0.5200Epoch 13/20\n",
      "228/228 [==============================] - 0s 239us/step - loss: 1.6802 - acc: 0.5044\n",
      "227/227 [==============================] - 0s 230us/step - loss: 1.8331 - acc: 0.5110\n",
      "226/226 [==============================] - 0s 742us/step - loss: 0.9257 - acc: 0.5796\n",
      "228/228 [==============================] - 0s 742us/step - loss: 1.1034 - acc: 0.5219\n",
      "Epoch 15/20\n",
      "226/226 [==============================] - 0s 476us/step - loss: 1.4161 - acc: 0.5133\n",
      "Epoch 20/20\n",
      "110/227 [=============>................] - ETA: 0s - loss: 0.5427 - acc: 0.7636Epoch 17/20\n",
      " 30/226 [==>...........................] - ETA: 0s - loss: 0.7887 - acc: 0.6667Epoch 18/20\n",
      "140/228 [=================>............] - ETA: 0s - loss: 0.7210 - acc: 0.6714Epoch 20/20\n",
      "228/228 [==============================] - 0s 484us/step - loss: 1.2003 - acc: 0.5175\n",
      "227/227 [==============================] - 0s 224us/step - loss: 1.2109 - acc: 0.5463\n",
      "100/228 [============>.................] - ETA: 0s - loss: 0.5944 - acc: 0.6700Epoch 16/20\n",
      "228/228 [==============================] - 0s 519us/step - loss: 1.6191 - acc: 0.5044\n",
      "226/226 [==============================] - 0s 595us/step - loss: 1.2793 - acc: 0.5088\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 1.3488 - acc: 0.5088\n",
      "180/228 [======================>.......] - ETA: 0s - loss: 0.6829 - acc: 0.6722Epoch 14/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 0.7019 - acc: 0.6726\n",
      "227/227 [==============================] - 1s 2ms/step - loss: 0.5874 - acc: 0.7357\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.6718 - acc: 0.6623\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.5282 - acc: 0.7544\n",
      " 50/228 [=====>........................] - ETA: 0s - loss: 1.2682 - acc: 0.5400Epoch 18/20\n",
      "Epoch 10/20\n",
      "Epoch 19/20\n",
      "180/226 [======================>.......] - ETA: 0s - loss: 0.5078 - acc: 0.7833Epoch 10/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 1.1644 - acc: 0.5683\n",
      "100/228 [============>.................] - ETA: 0s - loss: 0.8948 - acc: 0.5600Epoch 17/20\n",
      "226/226 [==============================] - 0s 392us/step - loss: 0.9985 - acc: 0.5664\n",
      "228/228 [==============================] - 0s 686us/step - loss: 1.4480 - acc: 0.4561\n",
      " 30/228 [==>...........................] - ETA: 0s - loss: 0.3484 - acc: 0.8000Epoch 19/20\n",
      "228/228 [==============================] - 1s 4ms/step - loss: 0.7940 - acc: 0.6360\n",
      "Epoch 15/20\n",
      "228/228 [==============================] - 0s 698us/step - loss: 0.8519 - acc: 0.5614\n",
      "226/226 [==============================] - 1s 3ms/step - loss: 0.5016 - acc: 0.7788\n",
      "227/227 [==============================] - 0s 227us/step - loss: 0.8981 - acc: 0.5859\n",
      " 50/226 [=====>........................] - ETA: 0s - loss: 0.7890 - acc: 0.5800Epoch 9/20\n",
      "110/227 [=============>................] - ETA: 0s - loss: 0.5065 - acc: 0.7273Epoch 20/20\n",
      "Epoch 10/20\n",
      "120/228 [==============>...............] - ETA: 0s - loss: 0.5290 - acc: 0.7083Epoch 18/20\n",
      "228/228 [==============================] - 0s 480us/step - loss: 1.2751 - acc: 0.5658\n",
      "226/226 [==============================] - 0s 673us/step - loss: 0.6832 - acc: 0.6549\n",
      " 40/228 [====>.........................] - ETA: 0s - loss: 0.7430 - acc: 0.6500Epoch 16/20\n",
      " 30/226 [==>...........................] - ETA: 0s - loss: 0.4347 - acc: 0.8000Epoch 20/20\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.4719 - acc: 0.7456\n",
      "227/227 [==============================] - 0s 457us/step - loss: 0.7260 - acc: 0.6432\n",
      "75/75 [==============================] - 1s 18ms/step\n",
      "228/228 [==============================] - 0s 722us/step - loss: 0.5919 - acc: 0.7193\n",
      "77/77 [==============================] - 1s 18ms/step\n",
      " 50/226 [=====>........................] - ETA: 0s - loss: 0.6655 - acc: 0.7600Epoch 11/20\n",
      "Epoch 19/20\n",
      "228/228 [==============================] - 0s 209us/step - loss: 0.9856 - acc: 0.5219\n",
      "100/228 [============>.................] - ETA: 0s - loss: 0.6096 - acc: 0.7100Epoch 17/20\n",
      "228/228 [==============================] - 0s 214us/step\n",
      "226/226 [==============================] - 0s 212us/step\n",
      "226/226 [==============================] - 0s 425us/step - loss: 0.5828 - acc: 0.7080\n",
      "76/76 [==============================] - 1s 18ms/step\n",
      "75/75 [==============================] - 1s 18ms/step\n",
      "227/227 [==============================] - 0s 322us/step\n",
      "228/228 [==============================] - 0s 320us/step\n",
      "228/228 [==============================] - 0s 753us/step - loss: 1.2252 - acc: 0.4868\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 1.1288 - acc: 0.5727\n",
      " 70/228 [========>.....................] - ETA: 0s - loss: 0.4475 - acc: 0.7857Epoch 18/20\n",
      "Epoch 20/20\n",
      "227/227 [==============================] - 1s 4ms/step - loss: 0.5419 - acc: 0.7269\n",
      " 50/227 [=====>........................] - ETA: 0s - loss: 0.6080 - acc: 0.7800Epoch 11/20\n",
      "226/226 [==============================] - 1s 3ms/step - loss: 0.5014 - acc: 0.7699\n",
      " 10/227 [>.............................] - ETA: 0s - loss: 0.5689 - acc: 0.7000Epoch 11/20\n",
      "76/76 [==============================] - 1s 19ms/step\n",
      "228/228 [==============================] - 0s 699us/step - loss: 0.6423 - acc: 0.6754\n",
      " 30/227 [==>...........................] - ETA: 0s - loss: 0.3601 - acc: 0.8000Epoch 19/20\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.4919 - acc: 0.7544\n",
      "228/228 [==============================] - 1s 4ms/step - loss: 0.6580 - acc: 0.6754\n",
      "227/227 [==============================] - 0s 937us/step - loss: 0.8949 - acc: 0.6123\n",
      "227/227 [==============================] - 0s 188us/step\n",
      "75/75 [==============================] - 1s 19ms/step\n",
      " 50/228 [=====>........................] - ETA: 0s - loss: 0.8625 - acc: 0.5400Epoch 12/20\n",
      "Epoch 10/20\n",
      "75/75 [==============================] - 1s 18ms/step\n",
      "228/228 [==============================] - 0s 225us/step\n",
      "77/77 [==============================] - 1s 18ms/step\n",
      "228/228 [==============================] - 0s 498us/step\n",
      "226/226 [==============================] - 0s 228us/step\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.9196 - acc: 0.5219\n",
      "160/227 [====================>.........] - ETA: 0s - loss: 0.5461 - acc: 0.7125Epoch 20/20\n",
      "226/226 [==============================] - 1s 3ms/step - loss: 0.5223 - acc: 0.7832\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.5925 - acc: 0.7018\n",
      "Epoch 12/20\n",
      "227/227 [==============================] - 1s 4ms/step - loss: 0.5621 - acc: 0.7137\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.4548 - acc: 0.7719\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.5143 - acc: 0.7675\n",
      " 20/226 [=>............................] - ETA: 0s - loss: 0.4660 - acc: 0.7500Epoch 12/20\n",
      "Epoch 13/20\n",
      "75/75 [==============================] - 1s 19ms/step\n",
      "Epoch 11/20\n",
      "228/228 [==============================] - 0s 242us/step\n",
      "77/77 [==============================] - 1s 19ms/step\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.5014 - acc: 0.7851\n",
      "226/226 [==============================] - 0s 1ms/step\n",
      "Epoch 12/20\n",
      "226/226 [==============================] - 1s 3ms/step - loss: 0.5287 - acc: 0.7389\n",
      "110/228 [=============>................] - ETA: 0s - loss: 0.4680 - acc: 0.7455Epoch 13/20\n",
      "227/227 [==============================] - 1s 2ms/step - loss: 0.4960 - acc: 0.7841\n",
      " 10/226 [>.............................] - ETA: 0s - loss: 0.3833 - acc: 0.8000Epoch 13/20\n",
      "76/76 [==============================] - 1s 20ms/step\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.4555 - acc: 0.7807\n",
      " 50/227 [=====>........................] - ETA: 0sEpoch 14/20\n",
      "227/227 [==============================] - 0s 472us/step\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.4974 - acc: 0.7632\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.4777 - acc: 0.8053\n",
      "120/227 [==============>...............] - ETA: 0s - loss: 0.5116 - acc: 0.7833Epoch 13/20\n",
      "Epoch 14/20\n",
      "227/227 [==============================] - 0s 2ms/step - loss: 0.5397 - acc: 0.7665\n",
      " 90/228 [==========>...................] - ETA: 0s - loss: 0.4585 - acc: 0.7778Epoch 14/20\n",
      "75/75 [==============================] - 1s 19ms/step\n",
      "228/228 [==============================] - 1s 2ms/step - loss: 0.4946 - acc: 0.7851\n",
      "210/228 [==========================>...] - ETA: 0s - loss: 0.4748 - acc: 0.7857Epoch 15/20\n",
      "227/227 [==============================] - 0s 942us/step - loss: 0.5264 - acc: 0.7753\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.4478 - acc: 0.7920\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.4637 - acc: 0.7982\n",
      "228/228 [==============================] - 0s 729us/step\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 1.4680 - acc: 0.5000Epoch 15/20\n",
      "Epoch 15/20\n",
      "Epoch 14/20\n",
      "228/228 [==============================] - 0s 634us/step - loss: 0.5867 - acc: 0.7588\n",
      " 70/226 [========>.....................] - ETA: 0s - loss: 0.3929 - acc: 0.8143Epoch 16/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 0.4842 - acc: 0.7832\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.6026 - acc: 0.7193\n",
      "170/228 [=====================>........] - ETA: 0s - loss: 0.5375 - acc: 0.7588Epoch 16/20\n",
      "Epoch 15/20\n",
      "228/228 [==============================] - 0s 883us/step - loss: 0.5033 - acc: 0.7895\n",
      "227/227 [==============================] - 0s 2ms/step - loss: 0.5318 - acc: 0.7533\n",
      "Epoch 17/20\n",
      "Epoch 16/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.4263 - acc: 0.8289\n",
      "200/226 [=========================>....] - ETA: 0s - loss: 0.4309 - acc: 0.8000Epoch 18/20\n",
      "227/227 [==============================] - 0s 2ms/step - loss: 0.5414 - acc: 0.7445\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.5804 - acc: 0.7456\n",
      "220/226 [============================>.] - ETA: 0s - loss: 0.4794 - acc: 0.7727Epoch 17/20\n",
      "226/226 [==============================] - 1s 2ms/step - loss: 0.5129 - acc: 0.7611\n",
      "Epoch 16/20\n",
      " 10/227 [>.............................] - ETA: 0s - loss: 0.4917 - acc: 0.8000Epoch 17/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.4923 - acc: 0.7588\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 0.5467 - acc: 0.7533\n",
      "120/228 [==============>...............] - ETA: 0s - loss: 0.4921 - acc: 0.7583Epoch 19/20\n",
      "Epoch 18/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 0.4016 - acc: 0.8363\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.4730 - acc: 0.7675\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 0.5922 - acc: 0.6000Epoch 18/20\n",
      "Epoch 17/20\n",
      "228/228 [==============================] - 0s 933us/step - loss: 0.4047 - acc: 0.8246\n",
      "227/227 [==============================] - 0s 940us/step - loss: 0.5168 - acc: 0.7401\n",
      " 30/226 [==>...........................] - ETA: 0s - loss: 0.4811 - acc: 0.8000Epoch 20/20\n",
      "Epoch 19/20\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 0.4356 - acc: 0.8053\n",
      " 70/227 [========>.....................] - ETA: 0s - loss: 0.4608 - acc: 0.7429Epoch 19/20\n",
      "227/227 [==============================] - 0s 1ms/step - loss: 0.4852 - acc: 0.7665\n",
      " 90/226 [==========>...................] - ETA: 0s - loss: 0.3680 - acc: 0.8333Epoch 20/20\n",
      "228/228 [==============================] - 1s 2ms/step - loss: 0.4907 - acc: 0.7719\n",
      "226/226 [==============================] - 0s 702us/step - loss: 0.5501 - acc: 0.7478\n",
      " 10/227 [>.............................] - ETA: 0s - loss: 0.2734 - acc: 1.0000Epoch 18/20\n",
      "Epoch 20/20\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.4463 - acc: 0.7939\n",
      " 20/228 [=>............................] - ETA: 0s - loss: 0.4757 - acc: 0.8500Epoch 1/20\n",
      "227/227 [==============================] - 0s 602us/step - loss: 0.5208 - acc: 0.7753\n",
      "Epoch 1/20\n",
      " 40/228 [====>.........................] - ETA: 0s - loss: 0.5158 - acc: 0.8250Epoch 1/20\n",
      " 90/226 [==========>...................] - ETA: 0s - loss: 0.4196 - acc: 0.8444Epoch 1/20\n",
      "220/226 [============================>.] - ETA: 0s - loss: 0.4393 - acc: 0.8045Epoch 1/20\n",
      "226/226 [==============================] - 1s 2ms/step - loss: 0.4405 - acc: 0.8009\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 0.4949 - acc: 0.7864Epoch 1/20\n",
      "Epoch 1/20\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.4869 - acc: 0.7895\n",
      "Epoch 1/20\n",
      "Epoch 19/20\n",
      "228/228 [==============================] - 0s 1ms/step - loss: 0.4415 - acc: 0.7939\n",
      "Epoch 20/20\n",
      "75/75 [==============================] - 1s 15ms/step\n",
      "76/76 [==============================] - 1s 15ms/step\n",
      "228/228 [==============================] - 0s 650us/step\n",
      "227/227 [==============================] - 0s 397us/step\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.4603 - acc: 0.7851\n",
      "77/77 [==============================] - 1s 15ms/step\n",
      "226/226 [==============================] - 0s 432us/step\n",
      "75/75 [==============================] - 1s 11ms/step\n",
      "228/228 [==============================] - 0s 220us/step\n",
      "228/228 [==============================] - 7s 30ms/step - loss: 4.3603 - acc: 0.5570\n",
      "226/226 [==============================] - 7s 30ms/step - loss: 4.7418 - acc: 0.5000\n",
      "227/227 [==============================] - 7s 30ms/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 2/20\n",
      "Epoch 2/20\n",
      "Epoch 2/20\n",
      "228/228 [==============================] - 7s 30ms/step - loss: 7.2720 - acc: 0.5439\n",
      "226/226 [==============================] - 0s 14us/step - loss: 4.8355 - acc: 0.4469\n",
      "228/228 [==============================] - 0s 23us/step - loss: 4.3747 - acc: 0.4912\n",
      "Epoch 2/20\n",
      "227/227 [==============================] - 0s 10us/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 3/20\n",
      "Epoch 3/20\n",
      "228/228 [==============================] - 0s 17us/step - loss: 4.0249 - acc: 0.5439\n",
      "Epoch 3/20\n",
      "228/228 [==============================] - 0s 25us/step - loss: 7.2720 - acc: 0.5439\n",
      "226/226 [==============================] - 0s 10us/step - loss: 4.4605 - acc: 0.4956\n",
      "227/227 [==============================] - 0s 10us/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 4/20\n",
      "Epoch 3/20\n",
      "228/228 [==============================] - 0s 11us/step - loss: 3.8673 - acc: 0.5351\n",
      "Epoch 4/20\n",
      "Epoch 4/20\n",
      "228/228 [==============================] - 0s 13us/step - loss: 7.2720 - acc: 0.5439\n",
      "Epoch 5/20\n",
      "226/226 [==============================] - 0s 10us/step - loss: 4.3039 - acc: 0.5000\n",
      "227/227 [==============================] - 0s 10us/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "228/228 [==============================] - 0s 9us/step - loss: 3.7378 - acc: 0.5439\n",
      "Epoch 5/20\n",
      "228/228 [==============================] - 0s 10us/step - loss: 7.2720 - acc: 0.5439\n",
      "Epoch 6/20\n",
      "226/226 [==============================] - 0s 11us/step - loss: 4.1802 - acc: 0.5133\n",
      "227/227 [==============================] - 0s 10us/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "228/228 [==============================] - 0s 11us/step - loss: 3.6109 - acc: 0.5395\n",
      "226/226 [==============================] - 7s 29ms/step - loss: 7.2658 - acc: 0.5442\n",
      "Epoch 6/20\n",
      "228/228 [==============================] - 0s 10us/step - loss: 7.2720 - acc: 0.5439\n",
      "Epoch 7/20\n",
      "Epoch 2/20\n",
      "226/226 [==============================] - 0s 10us/step - loss: 4.0444 - acc: 0.5221\n",
      "227/227 [==============================] - 0s 9us/step - loss: 8.8046 - acc: 0.4537\n",
      "228/228 [==============================] - 0s 11us/step - loss: 3.4840 - acc: 0.5439\n",
      "Epoch 6/20\n",
      "228/228 [==============================] - 7s 29ms/step - loss: 8.7660 - acc: 0.4561\n",
      "Epoch 7/20\n",
      "226/226 [==============================] - 0s 14us/step - loss: 7.2658 - acc: 0.5442\n",
      "Epoch 7/20\n",
      "227/227 [==============================] - 7s 29ms/step - loss: 4.4917 - acc: 0.5066\n",
      "228/228 [==============================] - 0s 10us/step - loss: 7.2720 - acc: 0.5439\n",
      "Epoch 8/20\n",
      "Epoch 3/20\n",
      "226/226 [==============================] - 0s 10us/step - loss: 3.8984 - acc: 0.5177\n",
      "Epoch 2/20\n",
      "228/228 [==============================] - 6s 28ms/step - loss: 4.4609 - acc: 0.5351\n",
      "Epoch 2/20\n",
      "227/227 [==============================] - 0s 9us/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 7/20\n",
      "228/228 [==============================] - 0s 9us/step - loss: 3.3576 - acc: 0.5526\n",
      "Epoch 8/20\n",
      "228/228 [==============================] - 0s 14us/step - loss: 8.7660 - acc: 0.4561\n",
      "226/226 [==============================] - 0s 9us/step - loss: 7.2658 - acc: 0.5442\n",
      "Epoch 2/20\n",
      "227/227 [==============================] - 0s 13us/step - loss: 4.2365 - acc: 0.5154\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "228/228 [==============================] - 0s 9us/step - loss: 7.2720 - acc: 0.5439\n",
      "226/226 [==============================] - 0s 10us/step - loss: 3.7435 - acc: 0.5177\n",
      "Epoch 3/20\n",
      "228/228 [==============================] - 0s 14us/step - loss: 4.6061 - acc: 0.4342\n",
      "Epoch 4/20\n",
      "227/227 [==============================] - 0s 10us/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 3/20\n",
      "Epoch 8/20\n",
      "228/228 [==============================] - 0s 9us/step - loss: 3.2259 - acc: 0.5439\n",
      "Epoch 9/20\n",
      "228/228 [==============================] - 0s 11us/step - loss: 8.7660 - acc: 0.4561\n",
      "Epoch 3/20\n",
      "226/226 [==============================] - 0s 9us/step - loss: 7.2658 - acc: 0.5442\n",
      "227/227 [==============================] - 0s 13us/step - loss: 4.0485 - acc: 0.5066\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "228/228 [==============================] - 0s 9us/step - loss: 7.2720 - acc: 0.5439\n",
      "Epoch 4/20\n",
      "226/226 [==============================] - 0s 10us/step - loss: 3.5692 - acc: 0.5133\n",
      "Epoch 5/20\n",
      "228/228 [==============================] - 0s 9us/step - loss: 4.2020 - acc: 0.5088\n",
      "Epoch 4/20\n",
      "227/227 [==============================] - 0s 10us/step - loss: 8.8046 - acc: 0.4537\n",
      "228/228 [==============================] - 0s 10us/step - loss: 8.7660 - acc: 0.4561\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "226/226 [==============================] - 0s 9us/step - loss: 7.2658 - acc: 0.5442\n",
      "Epoch 4/20\n",
      "228/228 [==============================] - 0s 41us/step - loss: 3.0833 - acc: 0.5526\n",
      "Epoch 10/20\n",
      "227/227 [==============================] - 0s 10us/step - loss: 3.8415 - acc: 0.5242\n",
      "228/228 [==============================] - 0s 9us/step - loss: 7.2720 - acc: 0.5439\n",
      "Epoch 5/20\n",
      "226/226 [==============================] - 0s 9us/step - loss: 3.3937 - acc: 0.5133\n",
      "Epoch 6/20\n",
      "227/227 [==============================] - 0s 10us/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 5/20\n",
      "228/228 [==============================] - 0s 9us/step - loss: 4.0309 - acc: 0.5175\n",
      "Epoch 11/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "228/228 [==============================] - 0s 8us/step - loss: 8.7660 - acc: 0.4561\n",
      "226/226 [==============================] - 0s 9us/step - loss: 7.2658 - acc: 0.5442\n",
      "Epoch 5/20\n",
      "Epoch 11/20\n",
      "227/227 [==============================] - 0s 12us/step - loss: 3.6545 - acc: 0.5198\n",
      "228/228 [==============================] - 0s 37us/step - loss: 2.9377 - acc: 0.5658\n",
      "228/228 [==============================] - 0s 16us/step - loss: 7.2720 - acc: 0.5439\n",
      "Epoch 6/20\n",
      "226/226 [==============================] - 0s 10us/step - loss: 3.2255 - acc: 0.5133\n",
      "Epoch 7/20\n",
      "227/227 [==============================] - 0s 10us/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 6/20\n",
      "228/228 [==============================] - 0s 8us/step - loss: 3.9032 - acc: 0.5219\n",
      "Epoch 12/20\n",
      "Epoch 11/20\n",
      "228/228 [==============================] - 0s 9us/step - loss: 8.7660 - acc: 0.4561\n",
      "Epoch 12/20\n",
      "226/226 [==============================] - 0s 9us/step - loss: 7.2658 - acc: 0.5442\n",
      "Epoch 6/20\n",
      "Epoch 12/20\n",
      "227/227 [==============================] - 0s 10us/step - loss: 3.4856 - acc: 0.5419\n",
      "228/228 [==============================] - 0s 10us/step - loss: 2.7968 - acc: 0.5658\n",
      "228/228 [==============================] - 0s 15us/step - loss: 7.2720 - acc: 0.5439\n",
      "Epoch 8/20\n",
      "Epoch 7/20\n",
      "226/226 [==============================] - 0s 10us/step - loss: 3.0639 - acc: 0.5354\n",
      "228/228 [==============================] - 0s 11us/step - loss: 3.7646 - acc: 0.5263\n",
      "227/227 [==============================] - 0s 16us/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 7/20\n",
      "Epoch 13/20\n",
      "Epoch 12/20\n",
      "226/226 [==============================] - 0s 11us/step - loss: 7.2658 - acc: 0.5442\n",
      "228/228 [==============================] - 0s 9us/step - loss: 8.7660 - acc: 0.4561\n",
      "Epoch 13/20\n",
      "Epoch 7/20\n",
      "Epoch 13/20\n",
      "227/227 [==============================] - 0s 12us/step - loss: 3.3050 - acc: 0.5330\n",
      "228/228 [==============================] - 0s 16us/step - loss: 7.2720 - acc: 0.5439\n",
      "228/228 [==============================] - 0s 24us/step - loss: 2.6644 - acc: 0.5702\n",
      "Epoch 9/20\n",
      "Epoch 8/20\n",
      "226/226 [==============================] - 0s 10us/step - loss: 2.8944 - acc: 0.5221\n",
      "228/228 [==============================] - 0s 12us/step - loss: 3.6307 - acc: 0.5175\n",
      "227/227 [==============================] - 0s 18us/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 13/20\n",
      "Epoch 8/20\n",
      "Epoch 14/20\n",
      "226/226 [==============================] - 0s 22us/step - loss: 7.2658 - acc: 0.5442\n",
      "228/228 [==============================] - 0s 15us/step - loss: 8.7660 - acc: 0.4561\n",
      "Epoch 14/20\n",
      "Epoch 8/20\n",
      "Epoch 14/20\n",
      "228/228 [==============================] - 0s 17us/step - loss: 7.2720 - acc: 0.5439\n",
      "227/227 [==============================] - 0s 13us/step - loss: 3.1622 - acc: 0.5507\n",
      "228/228 [==============================] - 0s 9us/step - loss: 2.5304 - acc: 0.5658\n",
      "Epoch 10/20\n",
      "Epoch 9/20\n",
      "226/226 [==============================] - 0s 9us/step - loss: 2.7452 - acc: 0.5487\n",
      "227/227 [==============================] - 0s 16us/step - loss: 8.8046 - acc: 0.4537\n",
      "228/228 [==============================] - 0s 11us/step - loss: 3.4916 - acc: 0.5263\n",
      "Epoch 14/20\n",
      "Epoch 9/20\n",
      "Epoch 15/20\n",
      "226/226 [==============================] - 0s 16us/step - loss: 7.2658 - acc: 0.5442\n",
      "228/228 [==============================] - 0s 15us/step - loss: 8.7660 - acc: 0.4561\n",
      "Epoch 15/20\n",
      "Epoch 15/20\n",
      "Epoch 9/20\n",
      "228/228 [==============================] - 0s 9us/step - loss: 7.2720 - acc: 0.5439\n",
      "227/227 [==============================] - 0s 9us/step - loss: 3.0084 - acc: 0.5154\n",
      "228/228 [==============================] - 0s 11us/step - loss: 2.4205 - acc: 0.5877\n",
      "Epoch 11/20\n",
      "Epoch 10/20\n",
      "227/227 [==============================] - 0s 17us/step - loss: 8.8046 - acc: 0.4537\n",
      "226/226 [==============================] - 0s 9us/step - loss: 2.6025 - acc: 0.5310\n",
      "228/228 [==============================] - 0s 12us/step - loss: 3.3536 - acc: 0.5132\n",
      "Epoch 15/20\n",
      "Epoch 10/20\n",
      "Epoch 16/20\n",
      "228/228 [==============================] - 0s 12us/step - loss: 8.7660 - acc: 0.4561\n",
      "226/226 [==============================] - 0s 20us/step - loss: 7.2658 - acc: 0.5442\n",
      "Epoch 16/20\n",
      "Epoch 10/20\n",
      "Epoch 16/20\n",
      "228/228 [==============================] - 0s 10us/step - loss: 7.2720 - acc: 0.5439\n",
      "227/227 [==============================] - 0s 9us/step - loss: 2.8892 - acc: 0.5595\n",
      "228/228 [==============================] - 0s 11us/step - loss: 2.3517 - acc: 0.5746\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "226/226 [==============================] - 0s 15us/step - loss: 2.4756 - acc: 0.5664\n",
      "228/228 [==============================] - 0s 13us/step - loss: 3.2136 - acc: 0.5263\n",
      "227/227 [==============================] - 0s 9us/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 16/20\n",
      "Epoch 11/20\n",
      "Epoch 17/20\n",
      "228/228 [==============================] - 0s 16us/step - loss: 8.7660 - acc: 0.4561\n",
      "226/226 [==============================] - 0s 17us/step - loss: 7.2658 - acc: 0.5442\n",
      "Epoch 17/20\n",
      "Epoch 11/20\n",
      "Epoch 17/20\n",
      "228/228 [==============================] - 0s 9us/step - loss: 7.2720 - acc: 0.5439\n",
      "227/227 [==============================] - 0s 9us/step - loss: 2.7838 - acc: 0.5374\n",
      "228/228 [==============================] - 0s 10us/step - loss: 2.3610 - acc: 0.5702\n",
      "Epoch 13/20\n",
      "Epoch 12/20\n",
      "226/226 [==============================] - 0s 25us/step - loss: 2.4196 - acc: 0.5619\n",
      "228/228 [==============================] - 0s 12us/step - loss: 3.0871 - acc: 0.5351\n",
      "Epoch 17/20\n",
      "227/227 [==============================] - 0s 9us/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 12/20\n",
      "Epoch 18/20\n",
      "228/228 [==============================] - 0s 17us/step - loss: 8.7660 - acc: 0.4561\n",
      "226/226 [==============================] - 0s 20us/step - loss: 7.2658 - acc: 0.5442\n",
      "Epoch 18/20\n",
      "Epoch 12/20\n",
      "228/228 [==============================] - 0s 10us/step - loss: 7.2720 - acc: 0.5439\n",
      "Epoch 18/20\n",
      "227/227 [==============================] - 0s 9us/step - loss: 2.7000 - acc: 0.5551\n",
      "228/228 [==============================] - 0s 11us/step - loss: 2.4526 - acc: 0.5789\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "226/226 [==============================] - 0s 17us/step - loss: 2.5116 - acc: 0.5708\n",
      "228/228 [==============================] - 0s 16us/step - loss: 2.9592 - acc: 0.5263\n",
      "Epoch 18/20\n",
      "227/227 [==============================] - 0s 9us/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 13/20\n",
      "228/228 [==============================] - 0s 10us/step - loss: 8.7660 - acc: 0.4561\n",
      "Epoch 19/20\n",
      "226/226 [==============================] - 0s 9us/step - loss: 7.2658 - acc: 0.5442\n",
      "Epoch 13/20\n",
      "Epoch 19/20\n",
      "Epoch 19/20\n",
      "228/228 [==============================] - 0s 9us/step - loss: 7.2720 - acc: 0.5439\n",
      "227/227 [==============================] - 0s 11us/step - loss: 2.6013 - acc: 0.5551\n",
      "Epoch 14/20\n",
      "228/228 [==============================] - 0s 9us/step - loss: 2.3604 - acc: 0.5921\n",
      "Epoch 15/20\n",
      "228/228 [==============================] - 0s 10us/step - loss: 2.8378 - acc: 0.5351\n",
      "226/226 [==============================] - 0s 19us/step - loss: 2.6690 - acc: 0.5619\n",
      "Epoch 19/20\n",
      "227/227 [==============================] - 0s 8us/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 14/20\n",
      "228/228 [==============================] - 0s 9us/step - loss: 8.7660 - acc: 0.4561\n",
      "Epoch 20/20\n",
      "226/226 [==============================] - 0s 10us/step - loss: 7.2658 - acc: 0.5442\n",
      "Epoch 14/20\n",
      "Epoch 20/20\n",
      "Epoch 20/20\n",
      "227/227 [==============================] - 0s 9us/step - loss: 2.5219 - acc: 0.5551\n",
      "228/228 [==============================] - 0s 9us/step - loss: 7.2720 - acc: 0.5439\n",
      "Epoch 15/20\n",
      "228/228 [==============================] - 0s 9us/step - loss: 2.1672 - acc: 0.5965\n",
      "228/228 [==============================] - 0s 10us/step - loss: 2.7245 - acc: 0.5351\n",
      "Epoch 16/20\n",
      "226/226 [==============================] - 0s 15us/step - loss: 2.4182 - acc: 0.5796\n",
      "227/227 [==============================] - 0s 15us/step - loss: 8.8046 - acc: 0.4537\n",
      "Epoch 20/20\n",
      "Epoch 15/20\n",
      "228/228 [==============================] - 0s 9us/step - loss: 8.7660 - acc: 0.4561\n",
      "Epoch 15/20\n",
      "226/226 [==============================] - 0s 10us/step - loss: 7.2658 - acc: 0.5442\n",
      "228/228 [==============================] - 0s 10us/step - loss: 7.2720 - acc: 0.5439\n",
      "227/227 [==============================] - 0s 9us/step - loss: 2.4107 - acc: 0.5727\n",
      "Epoch 16/20\n",
      "228/228 [==============================] - 0s 9us/step - loss: 2.6186 - acc: 0.5482\n",
      "Epoch 17/20\n",
      "Epoch 16/20\n",
      "Epoch 16/20\n",
      "228/228 [==============================] - 0s 10us/step - loss: 8.7660 - acc: 0.4561\n",
      "226/226 [==============================] - 0s 10us/step - loss: 7.2658 - acc: 0.5442\n",
      "227/227 [==============================] - 0s 10us/step - loss: 2.3181 - acc: 0.5595\n",
      "Epoch 17/20\n",
      "228/228 [==============================] - 0s 10us/step - loss: 2.5216 - acc: 0.5351\n",
      "Epoch 18/20\n",
      "Epoch 17/20\n",
      "228/228 [==============================] - 0s 10us/step - loss: 8.7660 - acc: 0.4561\n",
      "Epoch 17/20\n",
      "227/227 [==============================] - 0s 9us/step - loss: 2.1995 - acc: 0.5947\n",
      "226/226 [==============================] - 0s 10us/step - loss: 7.2658 - acc: 0.5442\n",
      "Epoch 18/20\n",
      "228/228 [==============================] - 0s 9us/step - loss: 2.4614 - acc: 0.5789\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 18/20\n",
      "228/228 [==============================] - 0s 10us/step - loss: 8.7660 - acc: 0.4561\n",
      "227/227 [==============================] - 0s 19us/step - loss: 2.1172 - acc: 0.5727\n",
      "226/226 [==============================] - 0s 17us/step - loss: 7.2658 - acc: 0.5442\n",
      "Epoch 19/20\n",
      "228/228 [==============================] - 0s 10us/step - loss: 2.5078 - acc: 0.5526\n",
      "Epoch 20/20\n",
      "Epoch 19/20\n",
      "228/228 [==============================] - 0s 10us/step - loss: 8.7660 - acc: 0.4561\n",
      "Epoch 19/20\n",
      "227/227 [==============================] - 0s 17us/step - loss: 2.0409 - acc: 0.6079\n",
      "226/226 [==============================] - 0s 16us/step - loss: 7.2658 - acc: 0.5442\n",
      "Epoch 20/20\n",
      "228/228 [==============================] - 0s 10us/step - loss: 2.6262 - acc: 0.5702\n",
      "Epoch 20/20\n",
      "228/228 [==============================] - 0s 10us/step - loss: 8.7660 - acc: 0.4561\n",
      "Epoch 20/20\n",
      "227/227 [==============================] - 0s 9us/step - loss: 1.9849 - acc: 0.5815\n",
      "228/228 [==============================] - 0s 8us/step - loss: 2.5540 - acc: 0.5614\n",
      "76/76 [==============================] - 1s 10ms/step\n",
      "77/77 [==============================] - 1s 11ms/step\n",
      "75/75 [==============================] - 1s 10ms/step\n",
      "75/75 [==============================] - 1s 11ms/step\n",
      "227/227 [==============================] - 0s 7us/step\n",
      "226/226 [==============================] - 0s 7us/step\n",
      "228/228 [==============================] - 0s 9us/step\n",
      "228/228 [==============================] - 0s 8us/step\n",
      "77/77 [==============================] - 1s 9ms/step\n",
      "76/76 [==============================] - 1s 8ms/step\n",
      "226/226 [==============================] - 0s 7us/step\n",
      "75/75 [==============================] - 1s 9ms/step\n",
      "75/75 [==============================] - 1s 8ms/step\n",
      "228/228 [==============================] - 0s 7us/step\n",
      "227/227 [==============================] - 0s 15us/step\n",
      "228/228 [==============================] - 0s 7us/step\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.5439 - acc: 0.4983\n",
      "Epoch 2/20\n",
      "303/303 [==============================] - 0s 166us/step - loss: 3.6310 - acc: 0.5050\n",
      "Epoch 3/20\n",
      "303/303 [==============================] - 0s 163us/step - loss: 2.4460 - acc: 0.4455\n",
      "Epoch 4/20\n",
      "303/303 [==============================] - 0s 161us/step - loss: 1.4267 - acc: 0.5116\n",
      "Epoch 5/20\n",
      "303/303 [==============================] - 0s 170us/step - loss: 0.7344 - acc: 0.6403\n",
      "Epoch 6/20\n",
      "303/303 [==============================] - 0s 160us/step - loss: 0.5513 - acc: 0.7492\n",
      "Epoch 7/20\n",
      "303/303 [==============================] - 0s 169us/step - loss: 0.5891 - acc: 0.7195\n",
      "Epoch 8/20\n",
      "303/303 [==============================] - 0s 162us/step - loss: 0.5106 - acc: 0.7756\n",
      "Epoch 9/20\n",
      "303/303 [==============================] - 0s 172us/step - loss: 0.4942 - acc: 0.7558\n",
      "Epoch 10/20\n",
      "303/303 [==============================] - 0s 161us/step - loss: 0.5057 - acc: 0.7690\n",
      "Epoch 11/20\n",
      "303/303 [==============================] - 0s 158us/step - loss: 0.5214 - acc: 0.7657\n",
      "Epoch 12/20\n",
      "303/303 [==============================] - 0s 160us/step - loss: 0.5908 - acc: 0.7525\n",
      "Epoch 13/20\n",
      "303/303 [==============================] - 0s 172us/step - loss: 0.5074 - acc: 0.7822\n",
      "Epoch 14/20\n",
      "303/303 [==============================] - 0s 169us/step - loss: 0.5680 - acc: 0.7558\n",
      "Epoch 15/20\n",
      "303/303 [==============================] - 0s 164us/step - loss: 0.4872 - acc: 0.7756\n",
      "Epoch 16/20\n",
      "303/303 [==============================] - 0s 167us/step - loss: 0.5012 - acc: 0.7723\n",
      "Epoch 17/20\n",
      "303/303 [==============================] - 0s 171us/step - loss: 0.4356 - acc: 0.7756\n",
      "Epoch 18/20\n",
      "303/303 [==============================] - 0s 164us/step - loss: 0.4631 - acc: 0.7987\n",
      "Epoch 19/20\n",
      "303/303 [==============================] - 0s 167us/step - loss: 0.4018 - acc: 0.8284\n",
      "Epoch 20/20\n",
      "303/303 [==============================] - 0s 167us/step - loss: 0.4929 - acc: 0.7591\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cross Validation Score: 0.77557756 used {'batch_size': 10, 'epochs': 20}\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'grid_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grid_result' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Parameters -----------------------------------------------\n",
    "batch_size = [10, 50, 100, 150, 250, 300]\n",
    "epochs = [20]\n",
    "param_grid = dict(batch_size=batch_size, \n",
    "                  epochs=epochs)\n",
    "# -----------------------------------------------\n",
    "\n",
    "# random seed to reproduce later\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "def create_model(optimizer='nadam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# instantiate model obj\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# GridSearchCV hyperparameters\n",
    "c_v = StratifiedKFold(n_splits=4,\n",
    "                      shuffle=True,\n",
    "                      random_state=seed) # 4-fold CV\n",
    "\n",
    "# instantiate GridSearchCV obj\n",
    "cv_grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    n_jobs=-1, \n",
    "                    cv=c_v)\n",
    "\n",
    "# run the cross validation\n",
    "xval_result = cv_grid.fit(X, y)\n",
    "\n",
    "# Plot the accuracy \n",
    "acc = [x*100 for x in xval_result.best_estimator_.model.history.history['acc']]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(acc)\n",
    "ax.set_xlabel('# Epochs', fontsize=16)\n",
    "ax.set_ylabel('% Accuracy', fontsize=16)\n",
    "ax.set_ylim(60,100)\n",
    "ax.axhline(80, color='k', linestyle='--', linewidth=1)\n",
    "ax.axhline(90, color='k', linestyle='--')\n",
    "plt.title('Best Estimator', fontsize=20, y=1.05)\n",
    "plt.show()\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best Cross Validation Score: {xval_result.best_score_:.8f} used {xval_result.best_params_}\\n\")\n",
    "mean_scores = [x*100 for x in grid_result.cv_results_['mean_test_score']]\n",
    "std_scores  = [x*100 for x in grid_result.cv_results_['std_test_score']]\n",
    "params = xval_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(mean_scores, std_scorse, params):\n",
    "    print(f\"Means: {mean:.3f}, Stdev: {stdev:.3f} with: {param}\") "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DS43SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
