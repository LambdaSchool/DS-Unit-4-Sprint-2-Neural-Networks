{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS43SC.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samirgadkari/DS-Unit-4-Sprint-3-Neural-Networks/blob/master/DS43SC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Y6SKlgYrpcym"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Networks Sprint Challenge"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BrEbRrjVphPM"
      },
      "cell_type": "markdown",
      "source": [
        "## 1) Define the following terms:\n",
        "\n",
        "- Neuron\n",
        "- Input Layer\n",
        "- Hidden Layer\n",
        "- Output Layer\n",
        "- Activation\n",
        "- Backpropagation"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Q5EksLqnp4oB"
      },
      "cell_type": "markdown",
      "source": [
        "  - Neuron:\n",
        "In machine learning, a neuron is a function that takes multiple inputs and outputs one value. If the neuron is in the first hidden layer, it's inputs are all the feature values of the input layer + the bias value. If the neuron is in a deeper hidden layer, it's inputs are all the neurons to it's left + the bias value. Each neuron multiplies the weights and inputs coming into it, adds the bias, and applies the activation function to the result to get the output value of the neuron.\n",
        "\n",
        "\n",
        "  - Input Layer:\n",
        "Input layer contains a set of cells. Each cell represents one feature, and contains the value present in that feature for that observation.\n",
        "\n",
        "\n",
        "  - Hidden Layer:\n",
        "Hidden layers are all the layers to the right of the input layer. These are called hidden, because we cannot directly influence them.\n",
        "\n",
        "\n",
        "  - Output Layer:\n",
        "An output layer contains as many neurons as you expect to solve your particular machine-learning problem. The output layer is activated by an activation function specific to the problem. For example for regression, the activation function is the linear function. The output values will be unbounded with this function, which is what you want. For binary classfication, you might use the sigmoid function to restrict values between 0 and 1. These values are then the probability of predicting the primary class.\n",
        "\n",
        "\n",
        "  - Activation Function (also called Transfer Function):\n",
        "An activation function takes a value and returns another value. It has to be differentiable, since we will differentiate it during the backpropagation step. You may choose different activation functions for different layers. An activation function is applied after a neuron has weighed all input values, added them together, and then added a bias.\n",
        "Common activation functions are the Sigmoid, tanh, step, and relu.\n",
        "\n",
        "  - Backpropagation\n",
        "On each iteration, we will get an output value. Backpropagation is the step that updates the weights/bias in the correct direction so that our error between the output value and the training output value is lower than the last error."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Ri_gRA2Jp728"
      },
      "cell_type": "markdown",
      "source": [
        "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
        "\n",
        "| x1 | x2 | x3 | y |\n",
        "|----|----|----|---|\n",
        "| 1  | 1  | 1  | 1 |\n",
        "| 1  | 0  | 1  | 0 |\n",
        "| 0  | 1  | 1  | 0 |\n",
        "| 0  | 0  | 1  | 0 |"
      ]
    },
    {
      "metadata": {
        "id": "FYceyLVUpuT_",
        "colab_type": "code",
        "colab": {},
        "outputId": "6189ecbb-fed4-433c-ed7f-4e3e96ca0e4e"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import StratifiedKFold, \\\n",
        "                                    cross_val_score, \\\n",
        "                                    GridSearchCV\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "AevtiwY2puUE",
        "colab_type": "code",
        "colab": {},
        "outputId": "247f5f86-40fe-4f5d-b2f6-03afcf2d3438"
      },
      "cell_type": "code",
      "source": [
        "X = np.array([[1, 1, 1], \n",
        "              [1, 0, 1],\n",
        "              [0, 1, 1], \n",
        "              [0, 0, 1]])\n",
        "y = np.array([[1], \n",
        "              [0], \n",
        "              [0], \n",
        "              [0]])\n",
        "df = pd.DataFrame(data=np.concatenate([X, y], axis=1))\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  1  2  3\n",
              "0  1  1  1  1\n",
              "1  1  0  1  0\n",
              "2  0  1  1  0\n",
              "3  0  0  1  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "BC0QFi7opuUH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, inputs, weights, bias, outputs):\n",
        "        self.inputs = inputs\n",
        "        self.weights = weights\n",
        "        self.outputs = outputs\n",
        "    \n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # Explanation for why this is is given here:\n",
        "    # https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e\n",
        "    def sigmoid_derivative(x):\n",
        "        return sigmoid(x) * (1 - sigmoid(x))\n",
        "    \n",
        "    def iterate(self, num_iters):\n",
        "        for iteration in range(num_iters):\n",
        "\n",
        "          # Weighted sum of inputs and weights\n",
        "          weighted_sum = np.dot(self.inputs, \n",
        "                                self.weights) + bias\n",
        "\n",
        "          # Activate with sigmoid function\n",
        "          activated_output = sigmoid(weighted_sum)\n",
        "\n",
        "          # Calculate Error\n",
        "          error = self.outputs - activated_output\n",
        "\n",
        "          # Calculate weight adjustments with sigmoid_derivative\n",
        "          adjustments = error * sigmoid_derivative(activated_output)\n",
        "\n",
        "          # Update weights\n",
        "          self.weights += np.dot(self.inputs.T, adjustments)\n",
        "\n",
        "        print('optimized weights after training: ')\n",
        "        print(weights)\n",
        "\n",
        "        np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
        "        print('activated_output:', activated_output)\n",
        "\n",
        "        return activated_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M1D--KzBpuUJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "a5943ded-84d3-4b70-aed2-c5a42ba9531a"
      },
      "cell_type": "code",
      "source": [
        "inputs  = X\n",
        "weights = 2 * np.random.random((3, 1)) - 1\n",
        "bias    = 1\n",
        "correct_outputs = y\n",
        "perceptron = Perceptron(inputs, weights, bias, correct_outputs)\n",
        "perceptron.iterate(10000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "optimized weights after training: \n",
            "[[11.839]\n",
            " [11.839]\n",
            " [-19.048]]\n",
            "activated_output: [[0.996]\n",
            " [0.002]\n",
            " [0.002]\n",
            " [0.000]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.996],\n",
              "       [0.002],\n",
              "       [0.002],\n",
              "       [0.000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "86HyRi8Osr3U"
      },
      "cell_type": "markdown",
      "source": [
        "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
        "- Your network must have one hidden layer. \n",
        "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
        "- Train your model on the Heart Disease dataset from UCI:\n",
        "\n",
        "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
        "\n",
        "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
      ]
    },
    {
      "metadata": {
        "id": "0i5xUmAlpuUP",
        "colab_type": "code",
        "colab": {},
        "outputId": "6d7e34d3-a2e3-4c19-f39a-509e2491a2cc"
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv',\n",
        "                 header=0)\n",
        "print(df.shape)\n",
        "print(df.dtypes)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(303, 14)\n",
            "age           int64\n",
            "sex           int64\n",
            "cp            int64\n",
            "trestbps      int64\n",
            "chol          int64\n",
            "fbs           int64\n",
            "restecg       int64\n",
            "thalach       int64\n",
            "exang         int64\n",
            "oldpeak     float64\n",
            "slope         int64\n",
            "ca            int64\n",
            "thal          int64\n",
            "target        int64\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
              "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
              "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
              "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
              "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
              "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
              "\n",
              "   ca  thal  target  \n",
              "0   0     1       1  \n",
              "1   0     2       1  \n",
              "2   0     2       1  \n",
              "3   0     2       1  \n",
              "4   0     2       1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "NGHpmKWnpuUS",
        "colab_type": "code",
        "colab": {},
        "outputId": "f148a079-d20d-44f1-ac1e-fd837489c08a"
      },
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age         0\n",
              "sex         0\n",
              "cp          0\n",
              "trestbps    0\n",
              "chol        0\n",
              "fbs         0\n",
              "restecg     0\n",
              "thalach     0\n",
              "exang       0\n",
              "oldpeak     0\n",
              "slope       0\n",
              "ca          0\n",
              "thal        0\n",
              "target      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "id": "2bh-4BDBpuUV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "  if df[col].dtype == 'int64':\n",
        "    df[col] = df[col].astype('float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f-2-7gaQpuUX",
        "colab_type": "code",
        "colab": {},
        "outputId": "d89842f2-d248-4768-eb5c-0814363026da"
      },
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age         float64\n",
              "sex         float64\n",
              "cp          float64\n",
              "trestbps    float64\n",
              "chol        float64\n",
              "fbs         float64\n",
              "restecg     float64\n",
              "thalach     float64\n",
              "exang       float64\n",
              "oldpeak     float64\n",
              "slope       float64\n",
              "ca          float64\n",
              "thal        float64\n",
              "target      float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "metadata": {
        "id": "tXsssEYnpuUa",
        "colab_type": "code",
        "colab": {},
        "outputId": "37c8e16d-d59c-4daf-fcce-cac6df6f54f7"
      },
      "cell_type": "code",
      "source": [
        "df_X = df.drop(['target'], axis=1)\n",
        "df_y = df['target']\n",
        "df_X.shape, df_y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((303, 13), (303,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CNfiajv3v4Ed",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seed = 101\n",
        "np.random.seed(seed)\n",
        "\n",
        "class Neural_Network(object):\n",
        "    def __init__(self, num_input_nodes, num_hidden_nodes, num_output_nodes, learning_rate):\n",
        "        self.num_input_nodes  = num_input_nodes   # 13 input nodes\n",
        "        self.num_hidden_nodes = num_hidden_nodes  # 8 hidden nodes\n",
        "        self.num_output_nodes = num_output_nodes  # 1 output node\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        self.L1_weights = np.random.randn(num_input_nodes, num_hidden_nodes)\n",
        "        self.L2_weights = np.random.randn(num_hidden_nodes, num_output_nodes)\n",
        "        \n",
        "    def feed_forward(self, X):\n",
        "        self.hidden_sum = np.dot(X, self.L1_weights)\n",
        "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "        self.output_sum = np.dot(self.activated_hidden, self.L2_weights)\n",
        "        self.activated_output = self.sigmoid(self.output_sum)\n",
        "        return self.activated_output\n",
        "        \n",
        "    def sigmoid(self, s):\n",
        "        return 1 / (1 + np.exp(-s))\n",
        "    \n",
        "    def sigmoid_prime(self, s):\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    def backward(self, X, y, o):\n",
        "\n",
        "        self.o_error = y - o\n",
        "        self.o_delta = self.o_error * self.sigmoid_prime(o)\n",
        "        \n",
        "        self.z2_error = self.o_delta.dot(self.L2_weights.T)\n",
        "        self.z2_delta = self.z2_error * self.sigmoid_prime(self.activated_hidden)\n",
        "        \n",
        "        # Adjust weights\n",
        "        self.L1_weights += self.learning_rate * X.T.dot(self.z2_delta)\n",
        "        self.L2_weights += self.learning_rate * self.activated_hidden.T.dot(self.o_delta)\n",
        "\n",
        "    def train(self, X, y):\n",
        "        o = self.feed_forward(X)\n",
        "        self.backward(X, y, o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Il47NGopuUg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = df_X.values\n",
        "y = df_y.values\n",
        "y_out = []\n",
        "for i in range(len(y)):\n",
        "    y_out.append([y[i]])\n",
        "y = y_out.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EzrCU4c2puUi",
        "colab_type": "code",
        "colab": {},
        "outputId": "90e6a03f-c9ec-458f-c252-03e095219ac7"
      },
      "cell_type": "code",
      "source": [
        "NN = Neural_Network(13, 8, 1, 0.01)\n",
        "for i in range(100000): # trains the NN 1,000 times\n",
        "  if i+1 in [1,2,3,4,5] or (i+1) % 50 == 0:\n",
        "    print('+---------- EPOCH', i+1, '-----------+', end=' ')\n",
        "#     print(\"Input: \\n\", X) \n",
        "#     print(\"Actual Output: \\n\", y)  \n",
        "#     print(\"Predicted Output: \\n\" + str(NN.feed_forward(X))) \n",
        "    print(\"Loss: \" + str(np.mean(np.square(y - NN.feed_forward(X))))) # mean sum squared loss\n",
        "#     print(\"\\n\")\n",
        "  NN.train(X, y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 1 -----------+ Loss: 0.49187528649037016\n",
            "+---------- EPOCH 2 -----------+ Loss: 0.4762493648106615\n",
            "+---------- EPOCH 3 -----------+ Loss: 0.4525266892820536\n",
            "+---------- EPOCH 4 -----------+ Loss: 0.41377119210066793\n",
            "+---------- EPOCH 5 -----------+ Loss: 0.3527827310980319\n",
            "+---------- EPOCH 50 -----------+ Loss: 0.24741927143550208\n",
            "+---------- EPOCH 100 -----------+ Loss: 0.24741797478714142\n",
            "+---------- EPOCH 150 -----------+ Loss: 0.24741671096803017\n",
            "+---------- EPOCH 200 -----------+ Loss: 0.24741547881916798\n",
            "+---------- EPOCH 250 -----------+ Loss: 0.24741427723310577\n",
            "+---------- EPOCH 300 -----------+ Loss: 0.24741310515121642\n",
            "+---------- EPOCH 350 -----------+ Loss: 0.24741196156113096\n",
            "+---------- EPOCH 400 -----------+ Loss: 0.24741084549432832\n",
            "+---------- EPOCH 450 -----------+ Loss: 0.247409756023871\n",
            "+---------- EPOCH 500 -----------+ Loss: 0.24740869226227383\n",
            "+---------- EPOCH 550 -----------+ Loss: 0.24740765335949932\n",
            "+---------- EPOCH 600 -----------+ Loss: 0.24740663850106948\n",
            "+---------- EPOCH 650 -----------+ Loss: 0.2474056469062877\n",
            "+---------- EPOCH 700 -----------+ Loss: 0.24740467782656345\n",
            "+---------- EPOCH 750 -----------+ Loss: 0.24740373054383158\n",
            "+---------- EPOCH 800 -----------+ Loss: 0.24740280436906287\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: RuntimeWarning: overflow encountered in exp\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 850 -----------+ Loss: 0.2474018986408573\n",
            "+---------- EPOCH 900 -----------+ Loss: 0.24740101272411655\n",
            "+---------- EPOCH 950 -----------+ Loss: 0.24740014600879012\n",
            "+---------- EPOCH 1000 -----------+ Loss: 0.24739929790869075\n",
            "+---------- EPOCH 1050 -----------+ Loss: 0.24739846786037356\n",
            "+---------- EPOCH 1100 -----------+ Loss: 0.24739765532207764\n",
            "+---------- EPOCH 1150 -----------+ Loss: 0.24739685977272374\n",
            "+---------- EPOCH 1200 -----------+ Loss: 0.2473960807109653\n",
            "+---------- EPOCH 1250 -----------+ Loss: 0.24739531765429104\n",
            "+---------- EPOCH 1300 -----------+ Loss: 0.24739457013817348\n",
            "+---------- EPOCH 1350 -----------+ Loss: 0.24739383771526288\n",
            "+---------- EPOCH 1400 -----------+ Loss: 0.24739311995462357\n",
            "+---------- EPOCH 1450 -----------+ Loss: 0.2473924164410081\n",
            "+---------- EPOCH 1500 -----------+ Loss: 0.2473917267741699\n",
            "+---------- EPOCH 1550 -----------+ Loss: 0.24739105056821079\n",
            "+---------- EPOCH 1600 -----------+ Loss: 0.2473903874509609\n",
            "+---------- EPOCH 1650 -----------+ Loss: 0.24738973706339015\n",
            "+---------- EPOCH 1700 -----------+ Loss: 0.2473890990590487\n",
            "+---------- EPOCH 1750 -----------+ Loss: 0.24738847310353565\n",
            "+---------- EPOCH 1800 -----------+ Loss: 0.24738785887399306\n",
            "+---------- EPOCH 1850 -----------+ Loss: 0.2473872560586255\n",
            "+---------- EPOCH 1900 -----------+ Loss: 0.24738666435624207\n",
            "+---------- EPOCH 1950 -----------+ Loss: 0.2473860834758211\n",
            "+---------- EPOCH 2000 -----------+ Loss: 0.24738551313609544\n",
            "+---------- EPOCH 2050 -----------+ Loss: 0.24738495306515723\n",
            "+---------- EPOCH 2100 -----------+ Loss: 0.2473844030000816\n",
            "+---------- EPOCH 2150 -----------+ Loss: 0.24738386268656803\n",
            "+---------- EPOCH 2200 -----------+ Loss: 0.2473833318785981\n",
            "+---------- EPOCH 2250 -----------+ Loss: 0.24738281033810908\n",
            "+---------- EPOCH 2300 -----------+ Loss: 0.24738229783468316\n",
            "+---------- EPOCH 2350 -----------+ Loss: 0.24738179414524988\n",
            "+---------- EPOCH 2400 -----------+ Loss: 0.2473812990538026\n",
            "+---------- EPOCH 2450 -----------+ Loss: 0.24738081235112813\n",
            "+---------- EPOCH 2500 -----------+ Loss: 0.2473803338345472\n",
            "+---------- EPOCH 2550 -----------+ Loss: 0.24737986330766787\n",
            "+---------- EPOCH 2600 -----------+ Loss: 0.2473794005801489\n",
            "+---------- EPOCH 2650 -----------+ Loss: 0.24737894546747427\n",
            "+---------- EPOCH 2700 -----------+ Loss: 0.24737849779073653\n",
            "+---------- EPOCH 2750 -----------+ Loss: 0.24737805737643032\n",
            "+---------- EPOCH 2800 -----------+ Loss: 0.24737762405625446\n",
            "+---------- EPOCH 2850 -----------+ Loss: 0.24737719766692312\n",
            "+---------- EPOCH 2900 -----------+ Loss: 0.24737677804998365\n",
            "+---------- EPOCH 2950 -----------+ Loss: 0.2473763650516439\n",
            "+---------- EPOCH 3000 -----------+ Loss: 0.2473759585226048\n",
            "+---------- EPOCH 3050 -----------+ Loss: 0.24737555831790214\n",
            "+---------- EPOCH 3100 -----------+ Loss: 0.24737516429675274\n",
            "+---------- EPOCH 3150 -----------+ Loss: 0.24737477632240862\n",
            "+---------- EPOCH 3200 -----------+ Loss: 0.2473743942620165\n",
            "+---------- EPOCH 3250 -----------+ Loss: 0.24737401798648273\n",
            "+---------- EPOCH 3300 -----------+ Loss: 0.24737364737034406\n",
            "+---------- EPOCH 3350 -----------+ Loss: 0.2473732822916437\n",
            "+---------- EPOCH 3400 -----------+ Loss: 0.24737292263181235\n",
            "+---------- EPOCH 3450 -----------+ Loss: 0.2473725682755534\n",
            "+---------- EPOCH 3500 -----------+ Loss: 0.24737221911073332\n",
            "+---------- EPOCH 3550 -----------+ Loss: 0.24737187502827607\n",
            "+---------- EPOCH 3600 -----------+ Loss: 0.24737153592206187\n",
            "+---------- EPOCH 3650 -----------+ Loss: 0.2473712016888294\n",
            "+---------- EPOCH 3700 -----------+ Loss: 0.2473708722280823\n",
            "+---------- EPOCH 3750 -----------+ Loss: 0.24737054744199907\n",
            "+---------- EPOCH 3800 -----------+ Loss: 0.24737022723534646\n",
            "+---------- EPOCH 3850 -----------+ Loss: 0.24736991151539586\n",
            "+---------- EPOCH 3900 -----------+ Loss: 0.24736960019184334\n",
            "+---------- EPOCH 3950 -----------+ Loss: 0.2473692931767324\n",
            "+---------- EPOCH 4000 -----------+ Loss: 0.24736899038437954\n",
            "+---------- EPOCH 4050 -----------+ Loss: 0.24736869173130277\n",
            "+---------- EPOCH 4100 -----------+ Loss: 0.24736839713615272\n",
            "+---------- EPOCH 4150 -----------+ Loss: 0.24736810651964636\n",
            "+---------- EPOCH 4200 -----------+ Loss: 0.24736781980450254\n",
            "+---------- EPOCH 4250 -----------+ Loss: 0.2473675369153812\n",
            "+---------- EPOCH 4300 -----------+ Loss: 0.24736725777882324\n",
            "+---------- EPOCH 4350 -----------+ Loss: 0.2473669823231935\n",
            "+---------- EPOCH 4400 -----------+ Loss: 0.24736671047862566\n",
            "+---------- EPOCH 4450 -----------+ Loss: 0.24736644217696874\n",
            "+---------- EPOCH 4500 -----------+ Loss: 0.2473661773517361\n",
            "+---------- EPOCH 4550 -----------+ Loss: 0.24736591593805526\n",
            "+---------- EPOCH 4600 -----------+ Loss: 0.24736565787262074\n",
            "+---------- EPOCH 4650 -----------+ Loss: 0.24736540309364743\n",
            "+---------- EPOCH 4700 -----------+ Loss: 0.24736515154082636\n",
            "+---------- EPOCH 4750 -----------+ Loss: 0.2473649031552812\n",
            "+---------- EPOCH 4800 -----------+ Loss: 0.24736465787952733\n",
            "+---------- EPOCH 4850 -----------+ Loss: 0.24736441565743092\n",
            "+---------- EPOCH 4900 -----------+ Loss: 0.24736417643417127\n",
            "+---------- EPOCH 4950 -----------+ Loss: 0.24736394015620192\n",
            "+---------- EPOCH 5000 -----------+ Loss: 0.24736370677121555\n",
            "+---------- EPOCH 5050 -----------+ Loss: 0.2473634762281088\n",
            "+---------- EPOCH 5100 -----------+ Loss: 0.24736324847694785\n",
            "+---------- EPOCH 5150 -----------+ Loss: 0.24736302346893632\n",
            "+---------- EPOCH 5200 -----------+ Loss: 0.24736280115638334\n",
            "+---------- EPOCH 5250 -----------+ Loss: 0.2473625814926729\n",
            "+---------- EPOCH 5300 -----------+ Loss: 0.24736236443223455\n",
            "+---------- EPOCH 5350 -----------+ Loss: 0.24736214993051447\n",
            "+---------- EPOCH 5400 -----------+ Loss: 0.24736193794394767\n",
            "+---------- EPOCH 5450 -----------+ Loss: 0.2473617284299315\n",
            "+---------- EPOCH 5500 -----------+ Loss: 0.24736152134679937\n",
            "+---------- EPOCH 5550 -----------+ Loss: 0.24736131665379563\n",
            "+---------- EPOCH 5600 -----------+ Loss: 0.2473611143110514\n",
            "+---------- EPOCH 5650 -----------+ Loss: 0.247360914279561\n",
            "+---------- EPOCH 5700 -----------+ Loss: 0.24736071652115857\n",
            "+---------- EPOCH 5750 -----------+ Loss: 0.24736052099849665\n",
            "+---------- EPOCH 5800 -----------+ Loss: 0.24736032767502428\n",
            "+---------- EPOCH 5850 -----------+ Loss: 0.24736013651496644\n",
            "+---------- EPOCH 5900 -----------+ Loss: 0.24735994748330356\n",
            "+---------- EPOCH 5950 -----------+ Loss: 0.2473597605457525\n",
            "+---------- EPOCH 6000 -----------+ Loss: 0.24735957566874717\n",
            "+---------- EPOCH 6050 -----------+ Loss: 0.24735939281942038\n",
            "+---------- EPOCH 6100 -----------+ Loss: 0.24735921196558588\n",
            "+---------- EPOCH 6150 -----------+ Loss: 0.24735903307572152\n",
            "+---------- EPOCH 6200 -----------+ Loss: 0.24735885611895173\n",
            "+---------- EPOCH 6250 -----------+ Loss: 0.24735868106503192\n",
            "+---------- EPOCH 6300 -----------+ Loss: 0.24735850788433245\n",
            "+---------- EPOCH 6350 -----------+ Loss: 0.24735833654782335\n",
            "+---------- EPOCH 6400 -----------+ Loss: 0.24735816702705943\n",
            "+---------- EPOCH 6450 -----------+ Loss: 0.24735799929416583\n",
            "+---------- EPOCH 6500 -----------+ Loss: 0.24735783332182418\n",
            "+---------- EPOCH 6550 -----------+ Loss: 0.24735766908325857\n",
            "+---------- EPOCH 6600 -----------+ Loss: 0.24735750655222316\n",
            "+---------- EPOCH 6650 -----------+ Loss: 0.24735734570298817\n",
            "+---------- EPOCH 6700 -----------+ Loss: 0.24735718651032867\n",
            "+---------- EPOCH 6750 -----------+ Loss: 0.24735702894951161\n",
            "+---------- EPOCH 6800 -----------+ Loss: 0.24735687299628412\n",
            "+---------- EPOCH 6850 -----------+ Loss: 0.24735671862686268\n",
            "+---------- EPOCH 6900 -----------+ Loss: 0.24735656581792126\n",
            "+---------- EPOCH 6950 -----------+ Loss: 0.24735641454658105\n",
            "+---------- EPOCH 7000 -----------+ Loss: 0.2473562647903997\n",
            "+---------- EPOCH 7050 -----------+ Loss: 0.24735611652736117\n",
            "+---------- EPOCH 7100 -----------+ Loss: 0.24735596973586588\n",
            "+---------- EPOCH 7150 -----------+ Loss: 0.24735582439472079\n",
            "+---------- EPOCH 7200 -----------+ Loss: 0.2473556804831303\n",
            "+---------- EPOCH 7250 -----------+ Loss: 0.24735553798068716\n",
            "+---------- EPOCH 7300 -----------+ Loss: 0.2473553968673633\n",
            "+---------- EPOCH 7350 -----------+ Loss: 0.24735525712350115\n",
            "+---------- EPOCH 7400 -----------+ Loss: 0.24735511872980565\n",
            "+---------- EPOCH 7450 -----------+ Loss: 0.24735498166733574\n",
            "+---------- EPOCH 7500 -----------+ Loss: 0.24735484591749643\n",
            "+---------- EPOCH 7550 -----------+ Loss: 0.24735471146203108\n",
            "+---------- EPOCH 7600 -----------+ Loss: 0.2473545782830137\n",
            "+---------- EPOCH 7650 -----------+ Loss: 0.247354446362842\n",
            "+---------- EPOCH 7700 -----------+ Loss: 0.24735431568422958\n",
            "+---------- EPOCH 7750 -----------+ Loss: 0.24735418623019978\n",
            "+---------- EPOCH 7800 -----------+ Loss: 0.2473540579840782\n",
            "+---------- EPOCH 7850 -----------+ Loss: 0.24735393092948613\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 7900 -----------+ Loss: 0.2473538050503346\n",
            "+---------- EPOCH 7950 -----------+ Loss: 0.2473536803308179\n",
            "+---------- EPOCH 8000 -----------+ Loss: 0.24735355675540704\n",
            "+---------- EPOCH 8050 -----------+ Loss: 0.24735343430884413\n",
            "+---------- EPOCH 8100 -----------+ Loss: 0.24735331297613697\n",
            "+---------- EPOCH 8150 -----------+ Loss: 0.24735319274255246\n",
            "+---------- EPOCH 8200 -----------+ Loss: 0.2473530735936119\n",
            "+---------- EPOCH 8250 -----------+ Loss: 0.24735295551508518\n",
            "+---------- EPOCH 8300 -----------+ Loss: 0.24735283849298573\n",
            "+---------- EPOCH 8350 -----------+ Loss: 0.2473527225135652\n",
            "+---------- EPOCH 8400 -----------+ Loss: 0.2473526075633088\n",
            "+---------- EPOCH 8450 -----------+ Loss: 0.24735249362893\n",
            "+---------- EPOCH 8500 -----------+ Loss: 0.2473523806973663\n",
            "+---------- EPOCH 8550 -----------+ Loss: 0.24735226875577399\n",
            "+---------- EPOCH 8600 -----------+ Loss: 0.24735215779152417\n",
            "+---------- EPOCH 8650 -----------+ Loss: 0.24735204779219816\n",
            "+---------- EPOCH 8700 -----------+ Loss: 0.24735193874558303\n",
            "+---------- EPOCH 8750 -----------+ Loss: 0.2473518306396678\n",
            "+---------- EPOCH 8800 -----------+ Loss: 0.24735172346263884\n",
            "+---------- EPOCH 8850 -----------+ Loss: 0.24735161720287627\n",
            "+---------- EPOCH 8900 -----------+ Loss: 0.2473515118489499\n",
            "+---------- EPOCH 8950 -----------+ Loss: 0.24735140738961542\n",
            "+---------- EPOCH 9000 -----------+ Loss: 0.2473513038138108\n",
            "+---------- EPOCH 9050 -----------+ Loss: 0.2473512011106525\n",
            "+---------- EPOCH 9100 -----------+ Loss: 0.2473510992694322\n",
            "+---------- EPOCH 9150 -----------+ Loss: 0.2473509982796127\n",
            "+---------- EPOCH 9200 -----------+ Loss: 0.24735089813082536\n",
            "+---------- EPOCH 9250 -----------+ Loss: 0.2473507988128665\n",
            "+---------- EPOCH 9300 -----------+ Loss: 0.2473507003156936\n",
            "+---------- EPOCH 9350 -----------+ Loss: 0.24735060262942332\n",
            "+---------- EPOCH 9400 -----------+ Loss: 0.2473505057443271\n",
            "+---------- EPOCH 9450 -----------+ Loss: 0.24735040965082933\n",
            "+---------- EPOCH 9500 -----------+ Loss: 0.2473503143395035\n",
            "+---------- EPOCH 9550 -----------+ Loss: 0.2473502198010701\n",
            "+---------- EPOCH 9600 -----------+ Loss: 0.24735012602639309\n",
            "+---------- EPOCH 9650 -----------+ Loss: 0.24735003300647782\n",
            "+---------- EPOCH 9700 -----------+ Loss: 0.24734994073246772\n",
            "+---------- EPOCH 9750 -----------+ Loss: 0.24734984919564218\n",
            "+---------- EPOCH 9800 -----------+ Loss: 0.24734975838741377\n",
            "+---------- EPOCH 9850 -----------+ Loss: 0.24734966829932584\n",
            "+---------- EPOCH 9900 -----------+ Loss: 0.24734957892305004\n",
            "+---------- EPOCH 9950 -----------+ Loss: 0.24734949025038358\n",
            "+---------- EPOCH 10000 -----------+ Loss: 0.24734940227324748\n",
            "+---------- EPOCH 10050 -----------+ Loss: 0.24734931498368407\n",
            "+---------- EPOCH 10100 -----------+ Loss: 0.24734922837385445\n",
            "+---------- EPOCH 10150 -----------+ Loss: 0.24734914243603673\n",
            "+---------- EPOCH 10200 -----------+ Loss: 0.2473490571626235\n",
            "+---------- EPOCH 10250 -----------+ Loss: 0.24734897254612018\n",
            "+---------- EPOCH 10300 -----------+ Loss: 0.2473488885791424\n",
            "+---------- EPOCH 10350 -----------+ Loss: 0.24734880525441472\n",
            "+---------- EPOCH 10400 -----------+ Loss: 0.2473487225647678\n",
            "+---------- EPOCH 10450 -----------+ Loss: 0.24734864050313715\n",
            "+---------- EPOCH 10500 -----------+ Loss: 0.24734855906256126\n",
            "+---------- EPOCH 10550 -----------+ Loss: 0.2473484782361791\n",
            "+---------- EPOCH 10600 -----------+ Loss: 0.24734839801722916\n",
            "+---------- EPOCH 10650 -----------+ Loss: 0.247348318399047\n",
            "+---------- EPOCH 10700 -----------+ Loss: 0.24734823937506395\n",
            "+---------- EPOCH 10750 -----------+ Loss: 0.24734816093880513\n",
            "+---------- EPOCH 10800 -----------+ Loss: 0.2473480830838884\n",
            "+---------- EPOCH 10850 -----------+ Loss: 0.24734800580402144\n",
            "+---------- EPOCH 10900 -----------+ Loss: 0.24734792909300196\n",
            "+---------- EPOCH 10950 -----------+ Loss: 0.24734785294471448\n",
            "+---------- EPOCH 11000 -----------+ Loss: 0.2473477773531299\n",
            "+---------- EPOCH 11050 -----------+ Loss: 0.24734770231230355\n",
            "+---------- EPOCH 11100 -----------+ Loss: 0.24734762781637365\n",
            "+---------- EPOCH 11150 -----------+ Loss: 0.24734755385956048\n",
            "+---------- EPOCH 11200 -----------+ Loss: 0.247347480436164\n",
            "+---------- EPOCH 11250 -----------+ Loss: 0.24734740754056336\n",
            "+---------- EPOCH 11300 -----------+ Loss: 0.24734733516721527\n",
            "+---------- EPOCH 11350 -----------+ Loss: 0.24734726331065243\n",
            "+---------- EPOCH 11400 -----------+ Loss: 0.24734719196548247\n",
            "+---------- EPOCH 11450 -----------+ Loss: 0.24734712112638702\n",
            "+---------- EPOCH 11500 -----------+ Loss: 0.24734705078811944\n",
            "+---------- EPOCH 11550 -----------+ Loss: 0.2473469809455049\n",
            "+---------- EPOCH 11600 -----------+ Loss: 0.24734691159343827\n",
            "+---------- EPOCH 11650 -----------+ Loss: 0.24734684272688331\n",
            "+---------- EPOCH 11700 -----------+ Loss: 0.2473467743408715\n",
            "+---------- EPOCH 11750 -----------+ Loss: 0.24734670643050088\n",
            "+---------- EPOCH 11800 -----------+ Loss: 0.24734663899093484\n",
            "+---------- EPOCH 11850 -----------+ Loss: 0.24734657201740134\n",
            "+---------- EPOCH 11900 -----------+ Loss: 0.24734650550519163\n",
            "+---------- EPOCH 11950 -----------+ Loss: 0.24734643944965917\n",
            "+---------- EPOCH 12000 -----------+ Loss: 0.2473463738462188\n",
            "+---------- EPOCH 12050 -----------+ Loss: 0.24734630869034555\n",
            "+---------- EPOCH 12100 -----------+ Loss: 0.24734624397757396\n",
            "+---------- EPOCH 12150 -----------+ Loss: 0.24734617970349662\n",
            "+---------- EPOCH 12200 -----------+ Loss: 0.24734611586376373\n",
            "+---------- EPOCH 12250 -----------+ Loss: 0.24734605245408198\n",
            "+---------- EPOCH 12300 -----------+ Loss: 0.2473459894702137\n",
            "+---------- EPOCH 12350 -----------+ Loss: 0.24734592690797558\n",
            "+---------- EPOCH 12400 -----------+ Loss: 0.2473458647632387\n",
            "+---------- EPOCH 12450 -----------+ Loss: 0.24734580303192655\n",
            "+---------- EPOCH 12500 -----------+ Loss: 0.24734574171001517\n",
            "+---------- EPOCH 12550 -----------+ Loss: 0.2473456807935317\n",
            "+---------- EPOCH 12600 -----------+ Loss: 0.24734562027855386\n",
            "+---------- EPOCH 12650 -----------+ Loss: 0.2473455601612091\n",
            "+---------- EPOCH 12700 -----------+ Loss: 0.24734550043767375\n",
            "+---------- EPOCH 12750 -----------+ Loss: 0.24734544110417236\n",
            "+---------- EPOCH 12800 -----------+ Loss: 0.2473453821569769\n",
            "+---------- EPOCH 12850 -----------+ Loss: 0.247345323592406\n",
            "+---------- EPOCH 12900 -----------+ Loss: 0.2473452654068243\n",
            "+---------- EPOCH 12950 -----------+ Loss: 0.24734520759664194\n",
            "+---------- EPOCH 13000 -----------+ Loss: 0.24734515015831304\n",
            "+---------- EPOCH 13050 -----------+ Loss: 0.24734509308833613\n",
            "+---------- EPOCH 13100 -----------+ Loss: 0.2473450363832528\n",
            "+---------- EPOCH 13150 -----------+ Loss: 0.24734498003964722\n",
            "+---------- EPOCH 13200 -----------+ Loss: 0.2473449240541453\n",
            "+---------- EPOCH 13250 -----------+ Loss: 0.24734486842341452\n",
            "+---------- EPOCH 13300 -----------+ Loss: 0.24734481314416282\n",
            "+---------- EPOCH 13350 -----------+ Loss: 0.2473447582131382\n",
            "+---------- EPOCH 13400 -----------+ Loss: 0.2473447036271281\n",
            "+---------- EPOCH 13450 -----------+ Loss: 0.247344649382959\n",
            "+---------- EPOCH 13500 -----------+ Loss: 0.24734459547749538\n",
            "+---------- EPOCH 13550 -----------+ Loss: 0.24734454190763963\n",
            "+---------- EPOCH 13600 -----------+ Loss: 0.24734448867033115\n",
            "+---------- EPOCH 13650 -----------+ Loss: 0.24734443576254606\n",
            "+---------- EPOCH 13700 -----------+ Loss: 0.24734438318129637\n",
            "+---------- EPOCH 13750 -----------+ Loss: 0.24734433092362973\n",
            "+---------- EPOCH 13800 -----------+ Loss: 0.2473442789866289\n",
            "+---------- EPOCH 13850 -----------+ Loss: 0.24734422736741085\n",
            "+---------- EPOCH 13900 -----------+ Loss: 0.24734417606312675\n",
            "+---------- EPOCH 13950 -----------+ Loss: 0.24734412507096135\n",
            "+---------- EPOCH 14000 -----------+ Loss: 0.24734407438813213\n",
            "+---------- EPOCH 14050 -----------+ Loss: 0.24734402401188932\n",
            "+---------- EPOCH 14100 -----------+ Loss: 0.247343973939515\n",
            "+---------- EPOCH 14150 -----------+ Loss: 0.24734392416832313\n",
            "+---------- EPOCH 14200 -----------+ Loss: 0.24734387469565866\n",
            "+---------- EPOCH 14250 -----------+ Loss: 0.24734382551889722\n",
            "+---------- EPOCH 14300 -----------+ Loss: 0.24734377663544474\n",
            "+---------- EPOCH 14350 -----------+ Loss: 0.24734372804273702\n",
            "+---------- EPOCH 14400 -----------+ Loss: 0.24734367973823923\n",
            "+---------- EPOCH 14450 -----------+ Loss: 0.24734363171944546\n",
            "+---------- EPOCH 14500 -----------+ Loss: 0.24734358398387854\n",
            "+---------- EPOCH 14550 -----------+ Loss: 0.24734353652908933\n",
            "+---------- EPOCH 14600 -----------+ Loss: 0.24734348935265665\n",
            "+---------- EPOCH 14650 -----------+ Loss: 0.24734344245218656\n",
            "+---------- EPOCH 14700 -----------+ Loss: 0.2473433958253121\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 14750 -----------+ Loss: 0.2473433494696931\n",
            "+---------- EPOCH 14800 -----------+ Loss: 0.24734330338301577\n",
            "+---------- EPOCH 14850 -----------+ Loss: 0.24734325756299178\n",
            "+---------- EPOCH 14900 -----------+ Loss: 0.24734321200735865\n",
            "+---------- EPOCH 14950 -----------+ Loss: 0.2473431667138792\n",
            "+---------- EPOCH 15000 -----------+ Loss: 0.24734312168034078\n",
            "+---------- EPOCH 15050 -----------+ Loss: 0.24734307690455554\n",
            "+---------- EPOCH 15100 -----------+ Loss: 0.24734303238435965\n",
            "+---------- EPOCH 15150 -----------+ Loss: 0.24734298811761302\n",
            "+---------- EPOCH 15200 -----------+ Loss: 0.24734294410219945\n",
            "+---------- EPOCH 15250 -----------+ Loss: 0.2473429003360255\n",
            "+---------- EPOCH 15300 -----------+ Loss: 0.24734285681702092\n",
            "+---------- EPOCH 15350 -----------+ Loss: 0.24734281354313778\n",
            "+---------- EPOCH 15400 -----------+ Loss: 0.24734277051235073\n",
            "+---------- EPOCH 15450 -----------+ Loss: 0.2473427277226562\n",
            "+---------- EPOCH 15500 -----------+ Loss: 0.24734268517207222\n",
            "+---------- EPOCH 15550 -----------+ Loss: 0.24734264285863833\n",
            "+---------- EPOCH 15600 -----------+ Loss: 0.24734260078041523\n",
            "+---------- EPOCH 15650 -----------+ Loss: 0.2473425589354843\n",
            "+---------- EPOCH 15700 -----------+ Loss: 0.24734251732194762\n",
            "+---------- EPOCH 15750 -----------+ Loss: 0.2473424759379274\n",
            "+---------- EPOCH 15800 -----------+ Loss: 0.24734243478156584\n",
            "+---------- EPOCH 15850 -----------+ Loss: 0.24734239385102516\n",
            "+---------- EPOCH 15900 -----------+ Loss: 0.2473423531444866\n",
            "+---------- EPOCH 15950 -----------+ Loss: 0.24734231266015116\n",
            "+---------- EPOCH 16000 -----------+ Loss: 0.2473422723962384\n",
            "+---------- EPOCH 16050 -----------+ Loss: 0.24734223235098668\n",
            "+---------- EPOCH 16100 -----------+ Loss: 0.24734219252265308\n",
            "+---------- EPOCH 16150 -----------+ Loss: 0.2473421529095128\n",
            "+---------- EPOCH 16200 -----------+ Loss: 0.24734211350985902\n",
            "+---------- EPOCH 16250 -----------+ Loss: 0.2473420743220025\n",
            "+---------- EPOCH 16300 -----------+ Loss: 0.24734203534427207\n",
            "+---------- EPOCH 16350 -----------+ Loss: 0.24734199657501335\n",
            "+---------- EPOCH 16400 -----------+ Loss: 0.24734195801258943\n",
            "+---------- EPOCH 16450 -----------+ Loss: 0.24734191965537997\n",
            "+---------- EPOCH 16500 -----------+ Loss: 0.2473418815017817\n",
            "+---------- EPOCH 16550 -----------+ Loss: 0.24734184355020736\n",
            "+---------- EPOCH 16600 -----------+ Loss: 0.24734180579908638\n",
            "+---------- EPOCH 16650 -----------+ Loss: 0.24734176824686396\n",
            "+---------- EPOCH 16700 -----------+ Loss: 0.24734173089200095\n",
            "+---------- EPOCH 16750 -----------+ Loss: 0.2473416937329744\n",
            "+---------- EPOCH 16800 -----------+ Loss: 0.2473416567682763\n",
            "+---------- EPOCH 16850 -----------+ Loss: 0.2473416199964142\n",
            "+---------- EPOCH 16900 -----------+ Loss: 0.24734158341591062\n",
            "+---------- EPOCH 16950 -----------+ Loss: 0.24734154702530278\n",
            "+---------- EPOCH 17000 -----------+ Loss: 0.24734151082314298\n",
            "+---------- EPOCH 17050 -----------+ Loss: 0.24734147480799756\n",
            "+---------- EPOCH 17100 -----------+ Loss: 0.24734143897844751\n",
            "+---------- EPOCH 17150 -----------+ Loss: 0.24734140333308777\n",
            "+---------- EPOCH 17200 -----------+ Loss: 0.2473413678705275\n",
            "+---------- EPOCH 17250 -----------+ Loss: 0.24734133258938945\n",
            "+---------- EPOCH 17300 -----------+ Loss: 0.24734129748831013\n",
            "+---------- EPOCH 17350 -----------+ Loss: 0.24734126256593938\n",
            "+---------- EPOCH 17400 -----------+ Loss: 0.2473412278209405\n",
            "+---------- EPOCH 17450 -----------+ Loss: 0.24734119325198983\n",
            "+---------- EPOCH 17500 -----------+ Loss: 0.24734115885777666\n",
            "+---------- EPOCH 17550 -----------+ Loss: 0.2473411246370032\n",
            "+---------- EPOCH 17600 -----------+ Loss: 0.24734109058838435\n",
            "+---------- EPOCH 17650 -----------+ Loss: 0.24734105671064743\n",
            "+---------- EPOCH 17700 -----------+ Loss: 0.2473410230025321\n",
            "+---------- EPOCH 17750 -----------+ Loss: 0.24734098946279037\n",
            "+---------- EPOCH 17800 -----------+ Loss: 0.24734095609018622\n",
            "+---------- EPOCH 17850 -----------+ Loss: 0.24734092288349557\n",
            "+---------- EPOCH 17900 -----------+ Loss: 0.24734088984150615\n",
            "+---------- EPOCH 17950 -----------+ Loss: 0.24734085696301727\n",
            "+---------- EPOCH 18000 -----------+ Loss: 0.24734082424683992\n",
            "+---------- EPOCH 18050 -----------+ Loss: 0.2473407916917963\n",
            "+---------- EPOCH 18100 -----------+ Loss: 0.24734075929671984\n",
            "+---------- EPOCH 18150 -----------+ Loss: 0.24734072706045507\n",
            "+---------- EPOCH 18200 -----------+ Loss: 0.24734069498185762\n",
            "+---------- EPOCH 18250 -----------+ Loss: 0.24734066305979382\n",
            "+---------- EPOCH 18300 -----------+ Loss: 0.2473406312931407\n",
            "+---------- EPOCH 18350 -----------+ Loss: 0.24734059968078612\n",
            "+---------- EPOCH 18400 -----------+ Loss: 0.24734056822162825\n",
            "+---------- EPOCH 18450 -----------+ Loss: 0.2473405369145755\n",
            "+---------- EPOCH 18500 -----------+ Loss: 0.2473405057585465\n",
            "+---------- EPOCH 18550 -----------+ Loss: 0.2473404747524703\n",
            "+---------- EPOCH 18600 -----------+ Loss: 0.24734044389528556\n",
            "+---------- EPOCH 18650 -----------+ Loss: 0.24734041318594105\n",
            "+---------- EPOCH 18700 -----------+ Loss: 0.24734038262339522\n",
            "+---------- EPOCH 18750 -----------+ Loss: 0.24734035220661618\n",
            "+---------- EPOCH 18800 -----------+ Loss: 0.24734032193458164\n",
            "+---------- EPOCH 18850 -----------+ Loss: 0.2473402918062786\n",
            "+---------- EPOCH 18900 -----------+ Loss: 0.24734026182070346\n",
            "+---------- EPOCH 18950 -----------+ Loss: 0.24734023197686186\n",
            "+---------- EPOCH 19000 -----------+ Loss: 0.24734020227376857\n",
            "+---------- EPOCH 19050 -----------+ Loss: 0.24734017271044714\n",
            "+---------- EPOCH 19100 -----------+ Loss: 0.2473401432859305\n",
            "+---------- EPOCH 19150 -----------+ Loss: 0.24734011399925981\n",
            "+---------- EPOCH 19200 -----------+ Loss: 0.24734008484948564\n",
            "+---------- EPOCH 19250 -----------+ Loss: 0.24734005583566626\n",
            "+---------- EPOCH 19300 -----------+ Loss: 0.2473400269568691\n",
            "+---------- EPOCH 19350 -----------+ Loss: 0.24733999821217\n",
            "+---------- EPOCH 19400 -----------+ Loss: 0.24733996960065277\n",
            "+---------- EPOCH 19450 -----------+ Loss: 0.2473399411214098\n",
            "+---------- EPOCH 19500 -----------+ Loss: 0.24733991277354148\n",
            "+---------- EPOCH 19550 -----------+ Loss: 0.2473398845561562\n",
            "+---------- EPOCH 19600 -----------+ Loss: 0.24733985646837028\n",
            "+---------- EPOCH 19650 -----------+ Loss: 0.24733982850930808\n",
            "+---------- EPOCH 19700 -----------+ Loss: 0.2473398006781016\n",
            "+---------- EPOCH 19750 -----------+ Loss: 0.24733977297389048\n",
            "+---------- EPOCH 19800 -----------+ Loss: 0.24733974539582237\n",
            "+---------- EPOCH 19850 -----------+ Loss: 0.24733971794305187\n",
            "+---------- EPOCH 19900 -----------+ Loss: 0.24733969061474137\n",
            "+---------- EPOCH 19950 -----------+ Loss: 0.2473396634100607\n",
            "+---------- EPOCH 20000 -----------+ Loss: 0.24733963632818673\n",
            "+---------- EPOCH 20050 -----------+ Loss: 0.24733960936830382\n",
            "+---------- EPOCH 20100 -----------+ Loss: 0.24733958252960309\n",
            "+---------- EPOCH 20150 -----------+ Loss: 0.24733955581128314\n",
            "+---------- EPOCH 20200 -----------+ Loss: 0.24733952921254923\n",
            "+---------- EPOCH 20250 -----------+ Loss: 0.2473395027326135\n",
            "+---------- EPOCH 20300 -----------+ Loss: 0.24733947637069525\n",
            "+---------- EPOCH 20350 -----------+ Loss: 0.24733945012602027\n",
            "+---------- EPOCH 20400 -----------+ Loss: 0.24733942399782102\n",
            "+---------- EPOCH 20450 -----------+ Loss: 0.2473393979853367\n",
            "+---------- EPOCH 20500 -----------+ Loss: 0.24733937208781298\n",
            "+---------- EPOCH 20550 -----------+ Loss: 0.24733934630450205\n",
            "+---------- EPOCH 20600 -----------+ Loss: 0.2473393206346624\n",
            "+---------- EPOCH 20650 -----------+ Loss: 0.24733929507755895\n",
            "+---------- EPOCH 20700 -----------+ Loss: 0.2473392696324629\n",
            "+---------- EPOCH 20750 -----------+ Loss: 0.24733924429865162\n",
            "+---------- EPOCH 20800 -----------+ Loss: 0.24733921907540835\n",
            "+---------- EPOCH 20850 -----------+ Loss: 0.2473391939620229\n",
            "+---------- EPOCH 20900 -----------+ Loss: 0.24733916895779087\n",
            "+---------- EPOCH 20950 -----------+ Loss: 0.2473391440620137\n",
            "+---------- EPOCH 21000 -----------+ Loss: 0.24733911927399874\n",
            "+---------- EPOCH 21050 -----------+ Loss: 0.2473390945930593\n",
            "+---------- EPOCH 21100 -----------+ Loss: 0.24733907001851427\n",
            "+---------- EPOCH 21150 -----------+ Loss: 0.24733904554968847\n",
            "+---------- EPOCH 21200 -----------+ Loss: 0.24733902118591214\n",
            "+---------- EPOCH 21250 -----------+ Loss: 0.24733899692652114\n",
            "+---------- EPOCH 21300 -----------+ Loss: 0.24733897277085704\n",
            "+---------- EPOCH 21350 -----------+ Loss: 0.2473389487182666\n",
            "+---------- EPOCH 21400 -----------+ Loss: 0.2473389247681022\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 21450 -----------+ Loss: 0.2473389009197216\n",
            "+---------- EPOCH 21500 -----------+ Loss: 0.24733887717248765\n",
            "+---------- EPOCH 21550 -----------+ Loss: 0.2473388535257687\n",
            "+---------- EPOCH 21600 -----------+ Loss: 0.2473388299789381\n",
            "+---------- EPOCH 21650 -----------+ Loss: 0.2473388065313744\n",
            "+---------- EPOCH 21700 -----------+ Loss: 0.24733878318246127\n",
            "+---------- EPOCH 21750 -----------+ Loss: 0.24733875993158744\n",
            "+---------- EPOCH 21800 -----------+ Loss: 0.2473387367781466\n",
            "+---------- EPOCH 21850 -----------+ Loss: 0.24733871372153723\n",
            "+---------- EPOCH 21900 -----------+ Loss: 0.24733869076116302\n",
            "+---------- EPOCH 21950 -----------+ Loss: 0.24733866789643222\n",
            "+---------- EPOCH 22000 -----------+ Loss: 0.2473386451267579\n",
            "+---------- EPOCH 22050 -----------+ Loss: 0.24733862245155808\n",
            "+---------- EPOCH 22100 -----------+ Loss: 0.24733859987025525\n",
            "+---------- EPOCH 22150 -----------+ Loss: 0.24733857738227658\n",
            "+---------- EPOCH 22200 -----------+ Loss: 0.24733855498705395\n",
            "+---------- EPOCH 22250 -----------+ Loss: 0.24733853268402367\n",
            "+---------- EPOCH 22300 -----------+ Loss: 0.2473385104726267\n",
            "+---------- EPOCH 22350 -----------+ Loss: 0.2473384883523083\n",
            "+---------- EPOCH 22400 -----------+ Loss: 0.24733846632251835\n",
            "+---------- EPOCH 22450 -----------+ Loss: 0.24733844438271085\n",
            "+---------- EPOCH 22500 -----------+ Loss: 0.24733842253234445\n",
            "+---------- EPOCH 22550 -----------+ Loss: 0.24733840077088173\n",
            "+---------- EPOCH 22600 -----------+ Loss: 0.24733837909778983\n",
            "+---------- EPOCH 22650 -----------+ Loss: 0.24733835751253994\n",
            "+---------- EPOCH 22700 -----------+ Loss: 0.2473383360146075\n",
            "+---------- EPOCH 22750 -----------+ Loss: 0.24733831460347194\n",
            "+---------- EPOCH 22800 -----------+ Loss: 0.24733829327861698\n",
            "+---------- EPOCH 22850 -----------+ Loss: 0.24733827203953024\n",
            "+---------- EPOCH 22900 -----------+ Loss: 0.2473382508857032\n",
            "+---------- EPOCH 22950 -----------+ Loss: 0.24733822981663173\n",
            "+---------- EPOCH 23000 -----------+ Loss: 0.2473382088318153\n",
            "+---------- EPOCH 23050 -----------+ Loss: 0.2473381879307574\n",
            "+---------- EPOCH 23100 -----------+ Loss: 0.2473381671129652\n",
            "+---------- EPOCH 23150 -----------+ Loss: 0.24733814637794999\n",
            "+---------- EPOCH 23200 -----------+ Loss: 0.24733812572522662\n",
            "+---------- EPOCH 23250 -----------+ Loss: 0.24733810515431362\n",
            "+---------- EPOCH 23300 -----------+ Loss: 0.24733808466473345\n",
            "+---------- EPOCH 23350 -----------+ Loss: 0.2473380642560122\n",
            "+---------- EPOCH 23400 -----------+ Loss: 0.2473380439276794\n",
            "+---------- EPOCH 23450 -----------+ Loss: 0.2473380236792685\n",
            "+---------- EPOCH 23500 -----------+ Loss: 0.24733800351031615\n",
            "+---------- EPOCH 23550 -----------+ Loss: 0.24733798342036284\n",
            "+---------- EPOCH 23600 -----------+ Loss: 0.2473379634089523\n",
            "+---------- EPOCH 23650 -----------+ Loss: 0.24733794347563215\n",
            "+---------- EPOCH 23700 -----------+ Loss: 0.24733792361995288\n",
            "+---------- EPOCH 23750 -----------+ Loss: 0.24733790384146886\n",
            "+---------- EPOCH 23800 -----------+ Loss: 0.2473378841397377\n",
            "+---------- EPOCH 23850 -----------+ Loss: 0.24733786451432016\n",
            "+---------- EPOCH 23900 -----------+ Loss: 0.24733784496478045\n",
            "+---------- EPOCH 23950 -----------+ Loss: 0.24733782549068614\n",
            "+---------- EPOCH 24000 -----------+ Loss: 0.247337806091608\n",
            "+---------- EPOCH 24050 -----------+ Loss: 0.2473377867671199\n",
            "+---------- EPOCH 24100 -----------+ Loss: 0.24733776751679903\n",
            "+---------- EPOCH 24150 -----------+ Loss: 0.24733774834022562\n",
            "+---------- EPOCH 24200 -----------+ Loss: 0.24733772923698327\n",
            "+---------- EPOCH 24250 -----------+ Loss: 0.24733771020665835\n",
            "+---------- EPOCH 24300 -----------+ Loss: 0.24733769124884047\n",
            "+---------- EPOCH 24350 -----------+ Loss: 0.24733767236312235\n",
            "+---------- EPOCH 24400 -----------+ Loss: 0.24733765354909973\n",
            "+---------- EPOCH 24450 -----------+ Loss: 0.24733763480637114\n",
            "+---------- EPOCH 24500 -----------+ Loss: 0.2473376161345382\n",
            "+---------- EPOCH 24550 -----------+ Loss: 0.24733759753320544\n",
            "+---------- EPOCH 24600 -----------+ Loss: 0.24733757900198036\n",
            "+---------- EPOCH 24650 -----------+ Loss: 0.24733756054047332\n",
            "+---------- EPOCH 24700 -----------+ Loss: 0.24733754214829748\n",
            "+---------- EPOCH 24750 -----------+ Loss: 0.24733752382506877\n",
            "+---------- EPOCH 24800 -----------+ Loss: 0.2473375055704061\n",
            "+---------- EPOCH 24850 -----------+ Loss: 0.24733748738393102\n",
            "+---------- EPOCH 24900 -----------+ Loss: 0.24733746926526803\n",
            "+---------- EPOCH 24950 -----------+ Loss: 0.24733745121404405\n",
            "+---------- EPOCH 25000 -----------+ Loss: 0.24733743322988874\n",
            "+---------- EPOCH 25050 -----------+ Loss: 0.24733741531243492\n",
            "+---------- EPOCH 25100 -----------+ Loss: 0.24733739746131733\n",
            "+---------- EPOCH 25150 -----------+ Loss: 0.24733737967617395\n",
            "+---------- EPOCH 25200 -----------+ Loss: 0.247337361956645\n",
            "+---------- EPOCH 25250 -----------+ Loss: 0.24733734430237345\n",
            "+---------- EPOCH 25300 -----------+ Loss: 0.24733732671300476\n",
            "+---------- EPOCH 25350 -----------+ Loss: 0.24733730918818703\n",
            "+---------- EPOCH 25400 -----------+ Loss: 0.2473372917275708\n",
            "+---------- EPOCH 25450 -----------+ Loss: 0.24733727433080904\n",
            "+---------- EPOCH 25500 -----------+ Loss: 0.24733725699755738\n",
            "+---------- EPOCH 25550 -----------+ Loss: 0.24733723972747382\n",
            "+---------- EPOCH 25600 -----------+ Loss: 0.24733722252021845\n",
            "+---------- EPOCH 25650 -----------+ Loss: 0.24733720537545437\n",
            "+---------- EPOCH 25700 -----------+ Loss: 0.24733718829284665\n",
            "+---------- EPOCH 25750 -----------+ Loss: 0.24733717127206278\n",
            "+---------- EPOCH 25800 -----------+ Loss: 0.24733715431277276\n",
            "+---------- EPOCH 25850 -----------+ Loss: 0.24733713741464886\n",
            "+---------- EPOCH 25900 -----------+ Loss: 0.24733712057736534\n",
            "+---------- EPOCH 25950 -----------+ Loss: 0.24733710380059923\n",
            "+---------- EPOCH 26000 -----------+ Loss: 0.24733708708402963\n",
            "+---------- EPOCH 26050 -----------+ Loss: 0.24733707042733763\n",
            "+---------- EPOCH 26100 -----------+ Loss: 0.24733705383020688\n",
            "+---------- EPOCH 26150 -----------+ Loss: 0.2473370372923231\n",
            "+---------- EPOCH 26200 -----------+ Loss: 0.24733702081337433\n",
            "+---------- EPOCH 26250 -----------+ Loss: 0.24733700439305045\n",
            "+---------- EPOCH 26300 -----------+ Loss: 0.24733698803104387\n",
            "+---------- EPOCH 26350 -----------+ Loss: 0.24733697172704883\n",
            "+---------- EPOCH 26400 -----------+ Loss: 0.24733695548076193\n",
            "+---------- EPOCH 26450 -----------+ Loss: 0.24733693929188186\n",
            "+---------- EPOCH 26500 -----------+ Loss: 0.24733692316010886\n",
            "+---------- EPOCH 26550 -----------+ Loss: 0.24733690708514605\n",
            "+---------- EPOCH 26600 -----------+ Loss: 0.24733689106669793\n",
            "+---------- EPOCH 26650 -----------+ Loss: 0.2473368751044713\n",
            "+---------- EPOCH 26700 -----------+ Loss: 0.24733685919817514\n",
            "+---------- EPOCH 26750 -----------+ Loss: 0.24733684334751999\n",
            "+---------- EPOCH 26800 -----------+ Loss: 0.24733682755221872\n",
            "+---------- EPOCH 26850 -----------+ Loss: 0.2473368118119859\n",
            "+---------- EPOCH 26900 -----------+ Loss: 0.24733679612653825\n",
            "+---------- EPOCH 26950 -----------+ Loss: 0.24733678049559438\n",
            "+---------- EPOCH 27000 -----------+ Loss: 0.2473367649188747\n",
            "+---------- EPOCH 27050 -----------+ Loss: 0.2473367493961015\n",
            "+---------- EPOCH 27100 -----------+ Loss: 0.2473367339269991\n",
            "+---------- EPOCH 27150 -----------+ Loss: 0.24733671851129352\n",
            "+---------- EPOCH 27200 -----------+ Loss: 0.2473367031487128\n",
            "+---------- EPOCH 27250 -----------+ Loss: 0.24733668783898655\n",
            "+---------- EPOCH 27300 -----------+ Loss: 0.24733667258184638\n",
            "+---------- EPOCH 27350 -----------+ Loss: 0.24733665737702584\n",
            "+---------- EPOCH 27400 -----------+ Loss: 0.24733664222425986\n",
            "+---------- EPOCH 27450 -----------+ Loss: 0.24733662712328539\n",
            "+---------- EPOCH 27500 -----------+ Loss: 0.24733661207384117\n",
            "+---------- EPOCH 27550 -----------+ Loss: 0.2473365970756676\n",
            "+---------- EPOCH 27600 -----------+ Loss: 0.24733658212850676\n",
            "+---------- EPOCH 27650 -----------+ Loss: 0.24733656723210262\n",
            "+---------- EPOCH 27700 -----------+ Loss: 0.24733655238620061\n",
            "+---------- EPOCH 27750 -----------+ Loss: 0.24733653759054797\n",
            "+---------- EPOCH 27800 -----------+ Loss: 0.2473365228448937\n",
            "+---------- EPOCH 27850 -----------+ Loss: 0.2473365081489882\n",
            "+---------- EPOCH 27900 -----------+ Loss: 0.24733649350258374\n",
            "+---------- EPOCH 27950 -----------+ Loss: 0.24733647890543417\n",
            "+---------- EPOCH 28000 -----------+ Loss: 0.2473364643572948\n",
            "+---------- EPOCH 28050 -----------+ Loss: 0.24733644985792286\n",
            "+---------- EPOCH 28100 -----------+ Loss: 0.2473364354070768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 28150 -----------+ Loss: 0.24733642100451692\n",
            "+---------- EPOCH 28200 -----------+ Loss: 0.2473364066500049\n",
            "+---------- EPOCH 28250 -----------+ Loss: 0.24733639234330415\n",
            "+---------- EPOCH 28300 -----------+ Loss: 0.2473363780841795\n",
            "+---------- EPOCH 28350 -----------+ Loss: 0.2473363638723972\n",
            "+---------- EPOCH 28400 -----------+ Loss: 0.24733634970772525\n",
            "+---------- EPOCH 28450 -----------+ Loss: 0.24733633558993307\n",
            "+---------- EPOCH 28500 -----------+ Loss: 0.24733632151879154\n",
            "+---------- EPOCH 28550 -----------+ Loss: 0.2473363074940729\n",
            "+---------- EPOCH 28600 -----------+ Loss: 0.24733629351555111\n",
            "+---------- EPOCH 28650 -----------+ Loss: 0.24733627958300142\n",
            "+---------- EPOCH 28700 -----------+ Loss: 0.2473362656962005\n",
            "+---------- EPOCH 28750 -----------+ Loss: 0.24733625185492647\n",
            "+---------- EPOCH 28800 -----------+ Loss: 0.24733623805895893\n",
            "+---------- EPOCH 28850 -----------+ Loss: 0.24733622430807894\n",
            "+---------- EPOCH 28900 -----------+ Loss: 0.24733621060206876\n",
            "+---------- EPOCH 28950 -----------+ Loss: 0.24733619694071224\n",
            "+---------- EPOCH 29000 -----------+ Loss: 0.24733618332379442\n",
            "+---------- EPOCH 29050 -----------+ Loss: 0.2473361697511018\n",
            "+---------- EPOCH 29100 -----------+ Loss: 0.24733615622242236\n",
            "+---------- EPOCH 29150 -----------+ Loss: 0.2473361427375452\n",
            "+---------- EPOCH 29200 -----------+ Loss: 0.24733612929626073\n",
            "+---------- EPOCH 29250 -----------+ Loss: 0.24733611589836108\n",
            "+---------- EPOCH 29300 -----------+ Loss: 0.24733610254363922\n",
            "+---------- EPOCH 29350 -----------+ Loss: 0.24733608923188952\n",
            "+---------- EPOCH 29400 -----------+ Loss: 0.247336075962908\n",
            "+---------- EPOCH 29450 -----------+ Loss: 0.24733606273649142\n",
            "+---------- EPOCH 29500 -----------+ Loss: 0.24733604955243826\n",
            "+---------- EPOCH 29550 -----------+ Loss: 0.247336036410548\n",
            "+---------- EPOCH 29600 -----------+ Loss: 0.24733602331062146\n",
            "+---------- EPOCH 29650 -----------+ Loss: 0.24733601025246077\n",
            "+---------- EPOCH 29700 -----------+ Loss: 0.2473359972358691\n",
            "+---------- EPOCH 29750 -----------+ Loss: 0.24733598426065112\n",
            "+---------- EPOCH 29800 -----------+ Loss: 0.24733597132661245\n",
            "+---------- EPOCH 29850 -----------+ Loss: 0.24733595843355993\n",
            "+---------- EPOCH 29900 -----------+ Loss: 0.24733594558130179\n",
            "+---------- EPOCH 29950 -----------+ Loss: 0.24733593276964733\n",
            "+---------- EPOCH 30000 -----------+ Loss: 0.24733591999840696\n",
            "+---------- EPOCH 30050 -----------+ Loss: 0.24733590726739252\n",
            "+---------- EPOCH 30100 -----------+ Loss: 0.24733589457641664\n",
            "+---------- EPOCH 30150 -----------+ Loss: 0.24733588192529332\n",
            "+---------- EPOCH 30200 -----------+ Loss: 0.24733586931383766\n",
            "+---------- EPOCH 30250 -----------+ Loss: 0.24733585674186595\n",
            "+---------- EPOCH 30300 -----------+ Loss: 0.2473358442091955\n",
            "+---------- EPOCH 30350 -----------+ Loss: 0.24733583171564466\n",
            "+---------- EPOCH 30400 -----------+ Loss: 0.24733581926103326\n",
            "+---------- EPOCH 30450 -----------+ Loss: 0.24733580684518178\n",
            "+---------- EPOCH 30500 -----------+ Loss: 0.24733579446791204\n",
            "+---------- EPOCH 30550 -----------+ Loss: 0.2473357821290469\n",
            "+---------- EPOCH 30600 -----------+ Loss: 0.2473357698284102\n",
            "+---------- EPOCH 30650 -----------+ Loss: 0.2473357575658271\n",
            "+---------- EPOCH 30700 -----------+ Loss: 0.24733574534112346\n",
            "+---------- EPOCH 30750 -----------+ Loss: 0.24733573315412632\n",
            "+---------- EPOCH 30800 -----------+ Loss: 0.24733572100466408\n",
            "+---------- EPOCH 30850 -----------+ Loss: 0.24733570889256573\n",
            "+---------- EPOCH 30900 -----------+ Loss: 0.24733569681766146\n",
            "+---------- EPOCH 30950 -----------+ Loss: 0.24733568477978254\n",
            "+---------- EPOCH 31000 -----------+ Loss: 0.24733567277876115\n",
            "+---------- EPOCH 31050 -----------+ Loss: 0.24733566081443042\n",
            "+---------- EPOCH 31100 -----------+ Loss: 0.24733564888662488\n",
            "+---------- EPOCH 31150 -----------+ Loss: 0.24733563699517938\n",
            "+---------- EPOCH 31200 -----------+ Loss: 0.2473356251399304\n",
            "+---------- EPOCH 31250 -----------+ Loss: 0.24733561332071488\n",
            "+---------- EPOCH 31300 -----------+ Loss: 0.24733560153737114\n",
            "+---------- EPOCH 31350 -----------+ Loss: 0.24733558978973813\n",
            "+---------- EPOCH 31400 -----------+ Loss: 0.24733557807765588\n",
            "+---------- EPOCH 31450 -----------+ Loss: 0.2473355664009655\n",
            "+---------- EPOCH 31500 -----------+ Loss: 0.2473355547595089\n",
            "+---------- EPOCH 31550 -----------+ Loss: 0.24733554315312886\n",
            "+---------- EPOCH 31600 -----------+ Loss: 0.2473355315816692\n",
            "+---------- EPOCH 31650 -----------+ Loss: 0.2473355200449747\n",
            "+---------- EPOCH 31700 -----------+ Loss: 0.24733550854289096\n",
            "+---------- EPOCH 31750 -----------+ Loss: 0.2473354970752643\n",
            "+---------- EPOCH 31800 -----------+ Loss: 0.2473354856419424\n",
            "+---------- EPOCH 31850 -----------+ Loss: 0.24733547424277338\n",
            "+---------- EPOCH 31900 -----------+ Loss: 0.2473354628776067\n",
            "+---------- EPOCH 31950 -----------+ Loss: 0.2473354515462921\n",
            "+---------- EPOCH 32000 -----------+ Loss: 0.24733544024868062\n",
            "+---------- EPOCH 32050 -----------+ Loss: 0.2473354289846243\n",
            "+---------- EPOCH 32100 -----------+ Loss: 0.24733541775397558\n",
            "+---------- EPOCH 32150 -----------+ Loss: 0.24733540655658803\n",
            "+---------- EPOCH 32200 -----------+ Loss: 0.24733539539231617\n",
            "+---------- EPOCH 32250 -----------+ Loss: 0.24733538426101495\n",
            "+---------- EPOCH 32300 -----------+ Loss: 0.2473353731625407\n",
            "+---------- EPOCH 32350 -----------+ Loss: 0.2473353620967501\n",
            "+---------- EPOCH 32400 -----------+ Loss: 0.24733535106350102\n",
            "+---------- EPOCH 32450 -----------+ Loss: 0.2473353400626519\n",
            "+---------- EPOCH 32500 -----------+ Loss: 0.24733532909406206\n",
            "+---------- EPOCH 32550 -----------+ Loss: 0.24733531815759163\n",
            "+---------- EPOCH 32600 -----------+ Loss: 0.24733530725310163\n",
            "+---------- EPOCH 32650 -----------+ Loss: 0.24733529638045376\n",
            "+---------- EPOCH 32700 -----------+ Loss: 0.24733528553951062\n",
            "+---------- EPOCH 32750 -----------+ Loss: 0.24733527473013528\n",
            "+---------- EPOCH 32800 -----------+ Loss: 0.24733526395219205\n",
            "+---------- EPOCH 32850 -----------+ Loss: 0.24733525320554575\n",
            "+---------- EPOCH 32900 -----------+ Loss: 0.247335242490062\n",
            "+---------- EPOCH 32950 -----------+ Loss: 0.24733523180560715\n",
            "+---------- EPOCH 33000 -----------+ Loss: 0.24733522115204837\n",
            "+---------- EPOCH 33050 -----------+ Loss: 0.24733521052925364\n",
            "+---------- EPOCH 33100 -----------+ Loss: 0.2473351999370915\n",
            "+---------- EPOCH 33150 -----------+ Loss: 0.24733518937543136\n",
            "+---------- EPOCH 33200 -----------+ Loss: 0.24733517884414327\n",
            "+---------- EPOCH 33250 -----------+ Loss: 0.2473351683430983\n",
            "+---------- EPOCH 33300 -----------+ Loss: 0.24733515787216778\n",
            "+---------- EPOCH 33350 -----------+ Loss: 0.24733514743122414\n",
            "+---------- EPOCH 33400 -----------+ Loss: 0.2473351370201404\n",
            "+---------- EPOCH 33450 -----------+ Loss: 0.24733512663879018\n",
            "+---------- EPOCH 33500 -----------+ Loss: 0.24733511628704793\n",
            "+---------- EPOCH 33550 -----------+ Loss: 0.24733510596478886\n",
            "+---------- EPOCH 33600 -----------+ Loss: 0.24733509567188872\n",
            "+---------- EPOCH 33650 -----------+ Loss: 0.247335085408224\n",
            "+---------- EPOCH 33700 -----------+ Loss: 0.247335075173672\n",
            "+---------- EPOCH 33750 -----------+ Loss: 0.24733506496811042\n",
            "+---------- EPOCH 33800 -----------+ Loss: 0.247335054791418\n",
            "+---------- EPOCH 33850 -----------+ Loss: 0.2473350446434739\n",
            "+---------- EPOCH 33900 -----------+ Loss: 0.247335034524158\n",
            "+---------- EPOCH 33950 -----------+ Loss: 0.24733502443335084\n",
            "+---------- EPOCH 34000 -----------+ Loss: 0.2473350143709337\n",
            "+---------- EPOCH 34050 -----------+ Loss: 0.24733500433678837\n",
            "+---------- EPOCH 34100 -----------+ Loss: 0.24733499433079745\n",
            "+---------- EPOCH 34150 -----------+ Loss: 0.247334984352844\n",
            "+---------- EPOCH 34200 -----------+ Loss: 0.24733497440281196\n",
            "+---------- EPOCH 34250 -----------+ Loss: 0.24733496448058562\n",
            "+---------- EPOCH 34300 -----------+ Loss: 0.24733495458605018\n",
            "+---------- EPOCH 34350 -----------+ Loss: 0.2473349447190912\n",
            "+---------- EPOCH 34400 -----------+ Loss: 0.24733493487959507\n",
            "+---------- EPOCH 34450 -----------+ Loss: 0.24733492506744886\n",
            "+---------- EPOCH 34500 -----------+ Loss: 0.24733491528253979\n",
            "+---------- EPOCH 34550 -----------+ Loss: 0.2473349055247564\n",
            "+---------- EPOCH 34600 -----------+ Loss: 0.24733489579398726\n",
            "+---------- EPOCH 34650 -----------+ Loss: 0.2473348860901217\n",
            "+---------- EPOCH 34700 -----------+ Loss: 0.24733487641304988\n",
            "+---------- EPOCH 34750 -----------+ Loss: 0.24733486676266223\n",
            "+---------- EPOCH 34800 -----------+ Loss: 0.24733485713884987\n",
            "+---------- EPOCH 34850 -----------+ Loss: 0.24733484754150467\n",
            "+---------- EPOCH 34900 -----------+ Loss: 0.24733483797051883\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 34950 -----------+ Loss: 0.24733482842578533\n",
            "+---------- EPOCH 35000 -----------+ Loss: 0.24733481890719766\n",
            "+---------- EPOCH 35050 -----------+ Loss: 0.24733480941464978\n",
            "+---------- EPOCH 35100 -----------+ Loss: 0.2473347999480364\n",
            "+---------- EPOCH 35150 -----------+ Loss: 0.24733479050725266\n",
            "+---------- EPOCH 35200 -----------+ Loss: 0.2473347810921943\n",
            "+---------- EPOCH 35250 -----------+ Loss: 0.24733477170275764\n",
            "+---------- EPOCH 35300 -----------+ Loss: 0.24733476233883947\n",
            "+---------- EPOCH 35350 -----------+ Loss: 0.2473347530003373\n",
            "+---------- EPOCH 35400 -----------+ Loss: 0.24733474368714894\n",
            "+---------- EPOCH 35450 -----------+ Loss: 0.24733473439917286\n",
            "+---------- EPOCH 35500 -----------+ Loss: 0.2473347251363082\n",
            "+---------- EPOCH 35550 -----------+ Loss: 0.24733471589845452\n",
            "+---------- EPOCH 35600 -----------+ Loss: 0.24733470668551163\n",
            "+---------- EPOCH 35650 -----------+ Loss: 0.24733469749738057\n",
            "+---------- EPOCH 35700 -----------+ Loss: 0.24733468833396224\n",
            "+---------- EPOCH 35750 -----------+ Loss: 0.24733467919515825\n",
            "+---------- EPOCH 35800 -----------+ Loss: 0.24733467008087084\n",
            "+---------- EPOCH 35850 -----------+ Loss: 0.24733466099100274\n",
            "+---------- EPOCH 35900 -----------+ Loss: 0.24733465192545706\n",
            "+---------- EPOCH 35950 -----------+ Loss: 0.24733464288413756\n",
            "+---------- EPOCH 36000 -----------+ Loss: 0.24733463386694848\n",
            "+---------- EPOCH 36050 -----------+ Loss: 0.24733462487379437\n",
            "+---------- EPOCH 36100 -----------+ Loss: 0.24733461590458064\n",
            "+---------- EPOCH 36150 -----------+ Loss: 0.2473346069592128\n",
            "+---------- EPOCH 36200 -----------+ Loss: 0.24733459803759725\n",
            "+---------- EPOCH 36250 -----------+ Loss: 0.2473345891396405\n",
            "+---------- EPOCH 36300 -----------+ Loss: 0.2473345802652497\n",
            "+---------- EPOCH 36350 -----------+ Loss: 0.24733457141433263\n",
            "+---------- EPOCH 36400 -----------+ Loss: 0.2473345625867974\n",
            "+---------- EPOCH 36450 -----------+ Loss: 0.24733455378255242\n",
            "+---------- EPOCH 36500 -----------+ Loss: 0.2473345450015069\n",
            "+---------- EPOCH 36550 -----------+ Loss: 0.24733453624357035\n",
            "+---------- EPOCH 36600 -----------+ Loss: 0.2473345275086528\n",
            "+---------- EPOCH 36650 -----------+ Loss: 0.2473345187966648\n",
            "+---------- EPOCH 36700 -----------+ Loss: 0.24733451010751703\n",
            "+---------- EPOCH 36750 -----------+ Loss: 0.24733450144112099\n",
            "+---------- EPOCH 36800 -----------+ Loss: 0.24733449279738853\n",
            "+---------- EPOCH 36850 -----------+ Loss: 0.2473344841762321\n",
            "+---------- EPOCH 36900 -----------+ Loss: 0.24733447557756416\n",
            "+---------- EPOCH 36950 -----------+ Loss: 0.24733446700129802\n",
            "+---------- EPOCH 37000 -----------+ Loss: 0.2473344584473474\n",
            "+---------- EPOCH 37050 -----------+ Loss: 0.24733444991562617\n",
            "+---------- EPOCH 37100 -----------+ Loss: 0.24733444140604902\n",
            "+---------- EPOCH 37150 -----------+ Loss: 0.24733443291853072\n",
            "+---------- EPOCH 37200 -----------+ Loss: 0.24733442445298673\n",
            "+---------- EPOCH 37250 -----------+ Loss: 0.24733441600933295\n",
            "+---------- EPOCH 37300 -----------+ Loss: 0.24733440758748537\n",
            "+---------- EPOCH 37350 -----------+ Loss: 0.24733439918736078\n",
            "+---------- EPOCH 37400 -----------+ Loss: 0.24733439080887623\n",
            "+---------- EPOCH 37450 -----------+ Loss: 0.24733438245194928\n",
            "+---------- EPOCH 37500 -----------+ Loss: 0.24733437411649764\n",
            "+---------- EPOCH 37550 -----------+ Loss: 0.2473343658024398\n",
            "+---------- EPOCH 37600 -----------+ Loss: 0.2473343575096944\n",
            "+---------- EPOCH 37650 -----------+ Loss: 0.24733434923818073\n",
            "+---------- EPOCH 37700 -----------+ Loss: 0.247334340987818\n",
            "+---------- EPOCH 37750 -----------+ Loss: 0.2473343327585265\n",
            "+---------- EPOCH 37800 -----------+ Loss: 0.24733432455022644\n",
            "+---------- EPOCH 37850 -----------+ Loss: 0.24733431636283848\n",
            "+---------- EPOCH 37900 -----------+ Loss: 0.24733430819628385\n",
            "+---------- EPOCH 37950 -----------+ Loss: 0.24733430005048404\n",
            "+---------- EPOCH 38000 -----------+ Loss: 0.24733429192536102\n",
            "+---------- EPOCH 38050 -----------+ Loss: 0.24733428382083708\n",
            "+---------- EPOCH 38100 -----------+ Loss: 0.2473342757368349\n",
            "+---------- EPOCH 38150 -----------+ Loss: 0.24733426767327746\n",
            "+---------- EPOCH 38200 -----------+ Loss: 0.24733425963008843\n",
            "+---------- EPOCH 38250 -----------+ Loss: 0.24733425160719152\n",
            "+---------- EPOCH 38300 -----------+ Loss: 0.2473342436045109\n",
            "+---------- EPOCH 38350 -----------+ Loss: 0.2473342356219713\n",
            "+---------- EPOCH 38400 -----------+ Loss: 0.2473342276594975\n",
            "+---------- EPOCH 38450 -----------+ Loss: 0.24733421971701497\n",
            "+---------- EPOCH 38500 -----------+ Loss: 0.2473342117944493\n",
            "+---------- EPOCH 38550 -----------+ Loss: 0.24733420389172664\n",
            "+---------- EPOCH 38600 -----------+ Loss: 0.2473341960087734\n",
            "+---------- EPOCH 38650 -----------+ Loss: 0.2473341881455163\n",
            "+---------- EPOCH 38700 -----------+ Loss: 0.2473341803018826\n",
            "+---------- EPOCH 38750 -----------+ Loss: 0.24733417247779976\n",
            "+---------- EPOCH 38800 -----------+ Loss: 0.24733416467319538\n",
            "+---------- EPOCH 38850 -----------+ Loss: 0.2473341568879981\n",
            "+---------- EPOCH 38900 -----------+ Loss: 0.24733414912213617\n",
            "+---------- EPOCH 38950 -----------+ Loss: 0.24733414137553858\n",
            "+---------- EPOCH 39000 -----------+ Loss: 0.2473341336481346\n",
            "+---------- EPOCH 39050 -----------+ Loss: 0.24733412593985377\n",
            "+---------- EPOCH 39100 -----------+ Loss: 0.247334118250626\n",
            "+---------- EPOCH 39150 -----------+ Loss: 0.24733411058038168\n",
            "+---------- EPOCH 39200 -----------+ Loss: 0.24733410292905136\n",
            "+---------- EPOCH 39250 -----------+ Loss: 0.24733409529656591\n",
            "+---------- EPOCH 39300 -----------+ Loss: 0.24733408768285667\n",
            "+---------- EPOCH 39350 -----------+ Loss: 0.24733408008785523\n",
            "+---------- EPOCH 39400 -----------+ Loss: 0.24733407251149359\n",
            "+---------- EPOCH 39450 -----------+ Loss: 0.247334064953704\n",
            "+---------- EPOCH 39500 -----------+ Loss: 0.24733405741441897\n",
            "+---------- EPOCH 39550 -----------+ Loss: 0.2473340498935714\n",
            "+---------- EPOCH 39600 -----------+ Loss: 0.24733404239109474\n",
            "+---------- EPOCH 39650 -----------+ Loss: 0.2473340349069223\n",
            "+---------- EPOCH 39700 -----------+ Loss: 0.24733402744098798\n",
            "+---------- EPOCH 39750 -----------+ Loss: 0.2473340199932261\n",
            "+---------- EPOCH 39800 -----------+ Loss: 0.2473340125635711\n",
            "+---------- EPOCH 39850 -----------+ Loss: 0.24733400515195766\n",
            "+---------- EPOCH 39900 -----------+ Loss: 0.24733399775832113\n",
            "+---------- EPOCH 39950 -----------+ Loss: 0.2473339903825969\n",
            "+---------- EPOCH 40000 -----------+ Loss: 0.2473339830247206\n",
            "+---------- EPOCH 40050 -----------+ Loss: 0.2473339756846283\n",
            "+---------- EPOCH 40100 -----------+ Loss: 0.24733396836225632\n",
            "+---------- EPOCH 40150 -----------+ Loss: 0.2473339610575414\n",
            "+---------- EPOCH 40200 -----------+ Loss: 0.2473339537704205\n",
            "+---------- EPOCH 40250 -----------+ Loss: 0.24733394650083076\n",
            "+---------- EPOCH 40300 -----------+ Loss: 0.24733393924870967\n",
            "+---------- EPOCH 40350 -----------+ Loss: 0.24733393201399534\n",
            "+---------- EPOCH 40400 -----------+ Loss: 0.24733392479662564\n",
            "+---------- EPOCH 40450 -----------+ Loss: 0.24733391759653903\n",
            "+---------- EPOCH 40500 -----------+ Loss: 0.24733391041367428\n",
            "+---------- EPOCH 40550 -----------+ Loss: 0.2473339032479704\n",
            "+---------- EPOCH 40600 -----------+ Loss: 0.24733389609936648\n",
            "+---------- EPOCH 40650 -----------+ Loss: 0.2473338889678024\n",
            "+---------- EPOCH 40700 -----------+ Loss: 0.24733388185321772\n",
            "+---------- EPOCH 40750 -----------+ Loss: 0.24733387475555277\n",
            "+---------- EPOCH 40800 -----------+ Loss: 0.2473338676747478\n",
            "+---------- EPOCH 40850 -----------+ Loss: 0.24733386061074364\n",
            "+---------- EPOCH 40900 -----------+ Loss: 0.24733385356348103\n",
            "+---------- EPOCH 40950 -----------+ Loss: 0.2473338465329015\n",
            "+---------- EPOCH 41000 -----------+ Loss: 0.24733383951894627\n",
            "+---------- EPOCH 41050 -----------+ Loss: 0.24733383252155727\n",
            "+---------- EPOCH 41100 -----------+ Loss: 0.24733382554067662\n",
            "+---------- EPOCH 41150 -----------+ Loss: 0.24733381857624645\n",
            "+---------- EPOCH 41200 -----------+ Loss: 0.24733381162820947\n",
            "+---------- EPOCH 41250 -----------+ Loss: 0.2473338046965085\n",
            "+---------- EPOCH 41300 -----------+ Loss: 0.2473337977810865\n",
            "+---------- EPOCH 41350 -----------+ Loss: 0.24733379088188712\n",
            "+---------- EPOCH 41400 -----------+ Loss: 0.24733378399885372\n",
            "+---------- EPOCH 41450 -----------+ Loss: 0.24733377713193053\n",
            "+---------- EPOCH 41500 -----------+ Loss: 0.24733377028106127\n",
            "+---------- EPOCH 41550 -----------+ Loss: 0.2473337634461907\n",
            "+---------- EPOCH 41600 -----------+ Loss: 0.24733375662726323\n",
            "+---------- EPOCH 41650 -----------+ Loss: 0.24733374982422396\n",
            "+---------- EPOCH 41700 -----------+ Loss: 0.24733374303701794\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 41750 -----------+ Loss: 0.24733373626559063\n",
            "+---------- EPOCH 41800 -----------+ Loss: 0.24733372950988777\n",
            "+---------- EPOCH 41850 -----------+ Loss: 0.24733372276985502\n",
            "+---------- EPOCH 41900 -----------+ Loss: 0.2473337160454388\n",
            "+---------- EPOCH 41950 -----------+ Loss: 0.2473337093365853\n",
            "+---------- EPOCH 42000 -----------+ Loss: 0.24733370264324142\n",
            "+---------- EPOCH 42050 -----------+ Loss: 0.24733369596535382\n",
            "+---------- EPOCH 42100 -----------+ Loss: 0.24733368930286964\n",
            "+---------- EPOCH 42150 -----------+ Loss: 0.24733368265573644\n",
            "+---------- EPOCH 42200 -----------+ Loss: 0.24733367602390163\n",
            "+---------- EPOCH 42250 -----------+ Loss: 0.24733366940731305\n",
            "+---------- EPOCH 42300 -----------+ Loss: 0.24733366280591887\n",
            "+---------- EPOCH 42350 -----------+ Loss: 0.24733365621966738\n",
            "+---------- EPOCH 42400 -----------+ Loss: 0.24733364964850707\n",
            "+---------- EPOCH 42450 -----------+ Loss: 0.24733364309238678\n",
            "+---------- EPOCH 42500 -----------+ Loss: 0.24733363655125554\n",
            "+---------- EPOCH 42550 -----------+ Loss: 0.2473336300250624\n",
            "+---------- EPOCH 42600 -----------+ Loss: 0.24733362351375696\n",
            "+---------- EPOCH 42650 -----------+ Loss: 0.24733361701728893\n",
            "+---------- EPOCH 42700 -----------+ Loss: 0.2473336105356081\n",
            "+---------- EPOCH 42750 -----------+ Loss: 0.24733360406866475\n",
            "+---------- EPOCH 42800 -----------+ Loss: 0.24733359761640897\n",
            "+---------- EPOCH 42850 -----------+ Loss: 0.2473335911787918\n",
            "+---------- EPOCH 42900 -----------+ Loss: 0.24733358475576359\n",
            "+---------- EPOCH 42950 -----------+ Loss: 0.24733357834727557\n",
            "+---------- EPOCH 43000 -----------+ Loss: 0.24733357195327896\n",
            "+---------- EPOCH 43050 -----------+ Loss: 0.24733356557372507\n",
            "+---------- EPOCH 43100 -----------+ Loss: 0.24733355920856567\n",
            "+---------- EPOCH 43150 -----------+ Loss: 0.24733355285775277\n",
            "+---------- EPOCH 43200 -----------+ Loss: 0.24733354652123818\n",
            "+---------- EPOCH 43250 -----------+ Loss: 0.24733354019897438\n",
            "+---------- EPOCH 43300 -----------+ Loss: 0.24733353389091386\n",
            "+---------- EPOCH 43350 -----------+ Loss: 0.24733352759700938\n",
            "+---------- EPOCH 43400 -----------+ Loss: 0.24733352131721378\n",
            "+---------- EPOCH 43450 -----------+ Loss: 0.2473335150514802\n",
            "+---------- EPOCH 43500 -----------+ Loss: 0.247333508799762\n",
            "+---------- EPOCH 43550 -----------+ Loss: 0.24733350256201284\n",
            "+---------- EPOCH 43600 -----------+ Loss: 0.24733349633818638\n",
            "+---------- EPOCH 43650 -----------+ Loss: 0.24733349012823644\n",
            "+---------- EPOCH 43700 -----------+ Loss: 0.24733348393211735\n",
            "+---------- EPOCH 43750 -----------+ Loss: 0.24733347774978345\n",
            "+---------- EPOCH 43800 -----------+ Loss: 0.24733347158118915\n",
            "+---------- EPOCH 43850 -----------+ Loss: 0.24733346542628942\n",
            "+---------- EPOCH 43900 -----------+ Loss: 0.24733345928503908\n",
            "+---------- EPOCH 43950 -----------+ Loss: 0.2473334531573933\n",
            "+---------- EPOCH 44000 -----------+ Loss: 0.24733344704330743\n",
            "+---------- EPOCH 44050 -----------+ Loss: 0.24733344094273688\n",
            "+---------- EPOCH 44100 -----------+ Loss: 0.24733343485563755\n",
            "+---------- EPOCH 44150 -----------+ Loss: 0.2473334287819654\n",
            "+---------- EPOCH 44200 -----------+ Loss: 0.24733342272167633\n",
            "+---------- EPOCH 44250 -----------+ Loss: 0.24733341667472666\n",
            "+---------- EPOCH 44300 -----------+ Loss: 0.24733341064107311\n",
            "+---------- EPOCH 44350 -----------+ Loss: 0.24733340462067208\n",
            "+---------- EPOCH 44400 -----------+ Loss: 0.2473333986134807\n",
            "+---------- EPOCH 44450 -----------+ Loss: 0.2473333926194557\n",
            "+---------- EPOCH 44500 -----------+ Loss: 0.24733338663855461\n",
            "+---------- EPOCH 44550 -----------+ Loss: 0.2473333806707346\n",
            "+---------- EPOCH 44600 -----------+ Loss: 0.2473333747159536\n",
            "+---------- EPOCH 44650 -----------+ Loss: 0.24733336877416903\n",
            "+---------- EPOCH 44700 -----------+ Loss: 0.247333362845339\n",
            "+---------- EPOCH 44750 -----------+ Loss: 0.24733335692942168\n",
            "+---------- EPOCH 44800 -----------+ Loss: 0.24733335102637538\n",
            "+---------- EPOCH 44850 -----------+ Loss: 0.24733334513615857\n",
            "+---------- EPOCH 44900 -----------+ Loss: 0.24733333925873\n",
            "+---------- EPOCH 44950 -----------+ Loss: 0.2473333333940483\n",
            "+---------- EPOCH 45000 -----------+ Loss: 0.24733332754207274\n",
            "+---------- EPOCH 45050 -----------+ Loss: 0.24733332170276248\n",
            "+---------- EPOCH 45100 -----------+ Loss: 0.24733331587607688\n",
            "+---------- EPOCH 45150 -----------+ Loss: 0.24733331006197537\n",
            "+---------- EPOCH 45200 -----------+ Loss: 0.24733330426041775\n",
            "+---------- EPOCH 45250 -----------+ Loss: 0.2473332984713639\n",
            "+---------- EPOCH 45300 -----------+ Loss: 0.24733329269477397\n",
            "+---------- EPOCH 45350 -----------+ Loss: 0.24733328693060802\n",
            "+---------- EPOCH 45400 -----------+ Loss: 0.24733328117882653\n",
            "+---------- EPOCH 45450 -----------+ Loss: 0.24733327543939004\n",
            "+---------- EPOCH 45500 -----------+ Loss: 0.24733326971225938\n",
            "+---------- EPOCH 45550 -----------+ Loss: 0.2473332639973953\n",
            "+---------- EPOCH 45600 -----------+ Loss: 0.24733325829475902\n",
            "+---------- EPOCH 45650 -----------+ Loss: 0.2473332526043115\n",
            "+---------- EPOCH 45700 -----------+ Loss: 0.24733324692601433\n",
            "+---------- EPOCH 45750 -----------+ Loss: 0.24733324125982903\n",
            "+---------- EPOCH 45800 -----------+ Loss: 0.24733323560571724\n",
            "+---------- EPOCH 45850 -----------+ Loss: 0.247333229963641\n",
            "+---------- EPOCH 45900 -----------+ Loss: 0.24733322433356214\n",
            "+---------- EPOCH 45950 -----------+ Loss: 0.24733321871544284\n",
            "+---------- EPOCH 46000 -----------+ Loss: 0.24733321310924544\n",
            "+---------- EPOCH 46050 -----------+ Loss: 0.24733320751493265\n",
            "+---------- EPOCH 46100 -----------+ Loss: 0.24733320193246688\n",
            "+---------- EPOCH 46150 -----------+ Loss: 0.24733319636181114\n",
            "+---------- EPOCH 46200 -----------+ Loss: 0.2473331908029282\n",
            "+---------- EPOCH 46250 -----------+ Loss: 0.24733318525578138\n",
            "+---------- EPOCH 46300 -----------+ Loss: 0.2473331797203337\n",
            "+---------- EPOCH 46350 -----------+ Loss: 0.24733317419654882\n",
            "+---------- EPOCH 46400 -----------+ Loss: 0.2473331686843902\n",
            "+---------- EPOCH 46450 -----------+ Loss: 0.2473331631838216\n",
            "+---------- EPOCH 46500 -----------+ Loss: 0.24733315769480674\n",
            "+---------- EPOCH 46550 -----------+ Loss: 0.24733315221730998\n",
            "+---------- EPOCH 46600 -----------+ Loss: 0.24733314675129522\n",
            "+---------- EPOCH 46650 -----------+ Loss: 0.24733314129672682\n",
            "+---------- EPOCH 46700 -----------+ Loss: 0.24733313585356925\n",
            "+---------- EPOCH 46750 -----------+ Loss: 0.2473331304217872\n",
            "+---------- EPOCH 46800 -----------+ Loss: 0.24733312500134533\n",
            "+---------- EPOCH 46850 -----------+ Loss: 0.2473331195922086\n",
            "+---------- EPOCH 46900 -----------+ Loss: 0.24733311419434206\n",
            "+---------- EPOCH 46950 -----------+ Loss: 0.24733310880771084\n",
            "+---------- EPOCH 47000 -----------+ Loss: 0.24733310343228038\n",
            "+---------- EPOCH 47050 -----------+ Loss: 0.24733309806801596\n",
            "+---------- EPOCH 47100 -----------+ Loss: 0.24733309271488338\n",
            "+---------- EPOCH 47150 -----------+ Loss: 0.2473330873728483\n",
            "+---------- EPOCH 47200 -----------+ Loss: 0.2473330820418766\n",
            "+---------- EPOCH 47250 -----------+ Loss: 0.2473330767219344\n",
            "+---------- EPOCH 47300 -----------+ Loss: 0.24733307141298774\n",
            "+---------- EPOCH 47350 -----------+ Loss: 0.24733306611500308\n",
            "+---------- EPOCH 47400 -----------+ Loss: 0.24733306082794668\n",
            "+---------- EPOCH 47450 -----------+ Loss: 0.24733305555178528\n",
            "+---------- EPOCH 47500 -----------+ Loss: 0.24733305028648558\n",
            "+---------- EPOCH 47550 -----------+ Loss: 0.2473330450320143\n",
            "+---------- EPOCH 47600 -----------+ Loss: 0.2473330397883385\n",
            "+---------- EPOCH 47650 -----------+ Loss: 0.2473330345554253\n",
            "+---------- EPOCH 47700 -----------+ Loss: 0.24733302933324197\n",
            "+---------- EPOCH 47750 -----------+ Loss: 0.24733302412175584\n",
            "+---------- EPOCH 47800 -----------+ Loss: 0.24733301892093443\n",
            "+---------- EPOCH 47850 -----------+ Loss: 0.24733301373074537\n",
            "+---------- EPOCH 47900 -----------+ Loss: 0.24733300855115645\n",
            "+---------- EPOCH 47950 -----------+ Loss: 0.2473330033821356\n",
            "+---------- EPOCH 48000 -----------+ Loss: 0.2473329982236508\n",
            "+---------- EPOCH 48050 -----------+ Loss: 0.24733299307567025\n",
            "+---------- EPOCH 48100 -----------+ Loss: 0.24733298793816216\n",
            "+---------- EPOCH 48150 -----------+ Loss: 0.2473329828110951\n",
            "+---------- EPOCH 48200 -----------+ Loss: 0.24733297769443738\n",
            "+---------- EPOCH 48250 -----------+ Loss: 0.24733297258815787\n",
            "+---------- EPOCH 48300 -----------+ Loss: 0.24733296749222525\n",
            "+---------- EPOCH 48350 -----------+ Loss: 0.24733296240660835\n",
            "+---------- EPOCH 48400 -----------+ Loss: 0.24733295733127633\n",
            "+---------- EPOCH 48450 -----------+ Loss: 0.24733295226619842\n",
            "+---------- EPOCH 48500 -----------+ Loss: 0.2473329472113437\n",
            "+---------- EPOCH 48550 -----------+ Loss: 0.24733294216668172\n",
            "+---------- EPOCH 48600 -----------+ Loss: 0.24733293713218196\n",
            "+---------- EPOCH 48650 -----------+ Loss: 0.24733293210781407\n",
            "+---------- EPOCH 48700 -----------+ Loss: 0.2473329270935478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 48750 -----------+ Loss: 0.24733292208935298\n",
            "+---------- EPOCH 48800 -----------+ Loss: 0.24733291709519967\n",
            "+---------- EPOCH 48850 -----------+ Loss: 0.24733291211105804\n",
            "+---------- EPOCH 48900 -----------+ Loss: 0.2473329071368982\n",
            "+---------- EPOCH 48950 -----------+ Loss: 0.2473329021726907\n",
            "+---------- EPOCH 49000 -----------+ Loss: 0.24733289721840582\n",
            "+---------- EPOCH 49050 -----------+ Loss: 0.24733289227401423\n",
            "+---------- EPOCH 49100 -----------+ Loss: 0.24733288733948655\n",
            "+---------- EPOCH 49150 -----------+ Loss: 0.24733288241479368\n",
            "+---------- EPOCH 49200 -----------+ Loss: 0.24733287749990668\n",
            "+---------- EPOCH 49250 -----------+ Loss: 0.2473328725947963\n",
            "+---------- EPOCH 49300 -----------+ Loss: 0.24733286769943388\n",
            "+---------- EPOCH 49350 -----------+ Loss: 0.24733286281379072\n",
            "+---------- EPOCH 49400 -----------+ Loss: 0.24733285793783813\n",
            "+---------- EPOCH 49450 -----------+ Loss: 0.2473328530715475\n",
            "+---------- EPOCH 49500 -----------+ Loss: 0.2473328482148907\n",
            "+---------- EPOCH 49550 -----------+ Loss: 0.24733284336783923\n",
            "+---------- EPOCH 49600 -----------+ Loss: 0.24733283853036506\n",
            "+---------- EPOCH 49650 -----------+ Loss: 0.24733283370244\n",
            "+---------- EPOCH 49700 -----------+ Loss: 0.24733282888403615\n",
            "+---------- EPOCH 49750 -----------+ Loss: 0.24733282407512566\n",
            "+---------- EPOCH 49800 -----------+ Loss: 0.2473328192756808\n",
            "+---------- EPOCH 49850 -----------+ Loss: 0.24733281448567387\n",
            "+---------- EPOCH 49900 -----------+ Loss: 0.24733280970507743\n",
            "+---------- EPOCH 49950 -----------+ Loss: 0.247332804933864\n",
            "+---------- EPOCH 50000 -----------+ Loss: 0.2473328001720063\n",
            "+---------- EPOCH 50050 -----------+ Loss: 0.2473327954194772\n",
            "+---------- EPOCH 50100 -----------+ Loss: 0.2473327906762494\n",
            "+---------- EPOCH 50150 -----------+ Loss: 0.24733278594229602\n",
            "+---------- EPOCH 50200 -----------+ Loss: 0.24733278121759014\n",
            "+---------- EPOCH 50250 -----------+ Loss: 0.24733277650210506\n",
            "+---------- EPOCH 50300 -----------+ Loss: 0.247332771795814\n",
            "+---------- EPOCH 50350 -----------+ Loss: 0.24733276709869034\n",
            "+---------- EPOCH 50400 -----------+ Loss: 0.24733276241070765\n",
            "+---------- EPOCH 50450 -----------+ Loss: 0.24733275773183952\n",
            "+---------- EPOCH 50500 -----------+ Loss: 0.24733275306205968\n",
            "+---------- EPOCH 50550 -----------+ Loss: 0.24733274840134192\n",
            "+---------- EPOCH 50600 -----------+ Loss: 0.24733274374966022\n",
            "+---------- EPOCH 50650 -----------+ Loss: 0.24733273910698847\n",
            "+---------- EPOCH 50700 -----------+ Loss: 0.2473327344733009\n",
            "+---------- EPOCH 50750 -----------+ Loss: 0.24733272984857174\n",
            "+---------- EPOCH 50800 -----------+ Loss: 0.24733272523277525\n",
            "+---------- EPOCH 50850 -----------+ Loss: 0.24733272062588574\n",
            "+---------- EPOCH 50900 -----------+ Loss: 0.24733271602787787\n",
            "+---------- EPOCH 50950 -----------+ Loss: 0.24733271143872623\n",
            "+---------- EPOCH 51000 -----------+ Loss: 0.24733270685840542\n",
            "+---------- EPOCH 51050 -----------+ Loss: 0.2473327022868903\n",
            "+---------- EPOCH 51100 -----------+ Loss: 0.24733269772415573\n",
            "+---------- EPOCH 51150 -----------+ Loss: 0.24733269317017664\n",
            "+---------- EPOCH 51200 -----------+ Loss: 0.24733268862492824\n",
            "+---------- EPOCH 51250 -----------+ Loss: 0.2473326840883857\n",
            "+---------- EPOCH 51300 -----------+ Loss: 0.24733267956052407\n",
            "+---------- EPOCH 51350 -----------+ Loss: 0.24733267504131898\n",
            "+---------- EPOCH 51400 -----------+ Loss: 0.24733267053074562\n",
            "+---------- EPOCH 51450 -----------+ Loss: 0.24733266602877982\n",
            "+---------- EPOCH 51500 -----------+ Loss: 0.247332661535397\n",
            "+---------- EPOCH 51550 -----------+ Loss: 0.24733265705057292\n",
            "+---------- EPOCH 51600 -----------+ Loss: 0.2473326525742835\n",
            "+---------- EPOCH 51650 -----------+ Loss: 0.24733264810650446\n",
            "+---------- EPOCH 51700 -----------+ Loss: 0.24733264364721194\n",
            "+---------- EPOCH 51750 -----------+ Loss: 0.247332639196382\n",
            "+---------- EPOCH 51800 -----------+ Loss: 0.24733263475399087\n",
            "+---------- EPOCH 51850 -----------+ Loss: 0.24733263032001468\n",
            "+---------- EPOCH 51900 -----------+ Loss: 0.24733262589442986\n",
            "+---------- EPOCH 51950 -----------+ Loss: 0.2473326214772129\n",
            "+---------- EPOCH 52000 -----------+ Loss: 0.2473326170683402\n",
            "+---------- EPOCH 52050 -----------+ Loss: 0.24733261266778855\n",
            "+---------- EPOCH 52100 -----------+ Loss: 0.2473326082755345\n",
            "+---------- EPOCH 52150 -----------+ Loss: 0.2473326038915549\n",
            "+---------- EPOCH 52200 -----------+ Loss: 0.2473325995158266\n",
            "+---------- EPOCH 52250 -----------+ Loss: 0.24733259514832653\n",
            "+---------- EPOCH 52300 -----------+ Loss: 0.24733259078903186\n",
            "+---------- EPOCH 52350 -----------+ Loss: 0.24733258643791967\n",
            "+---------- EPOCH 52400 -----------+ Loss: 0.24733258209496709\n",
            "+---------- EPOCH 52450 -----------+ Loss: 0.2473325777601514\n",
            "+---------- EPOCH 52500 -----------+ Loss: 0.24733257343345016\n",
            "+---------- EPOCH 52550 -----------+ Loss: 0.24733256911484067\n",
            "+---------- EPOCH 52600 -----------+ Loss: 0.24733256480430052\n",
            "+---------- EPOCH 52650 -----------+ Loss: 0.24733256050180744\n",
            "+---------- EPOCH 52700 -----------+ Loss: 0.24733255620733902\n",
            "+---------- EPOCH 52750 -----------+ Loss: 0.24733255192087308\n",
            "+---------- EPOCH 52800 -----------+ Loss: 0.24733254764238746\n",
            "+---------- EPOCH 52850 -----------+ Loss: 0.24733254337186025\n",
            "+---------- EPOCH 52900 -----------+ Loss: 0.2473325391092694\n",
            "+---------- EPOCH 52950 -----------+ Loss: 0.247332534854593\n",
            "+---------- EPOCH 53000 -----------+ Loss: 0.2473325306078092\n",
            "+---------- EPOCH 53050 -----------+ Loss: 0.24733252636889647\n",
            "+---------- EPOCH 53100 -----------+ Loss: 0.24733252213783294\n",
            "+---------- EPOCH 53150 -----------+ Loss: 0.2473325179145973\n",
            "+---------- EPOCH 53200 -----------+ Loss: 0.24733251369916778\n",
            "+---------- EPOCH 53250 -----------+ Loss: 0.24733250949152308\n",
            "+---------- EPOCH 53300 -----------+ Loss: 0.24733250529164205\n",
            "+---------- EPOCH 53350 -----------+ Loss: 0.2473325010995032\n",
            "+---------- EPOCH 53400 -----------+ Loss: 0.2473324969150855\n",
            "+---------- EPOCH 53450 -----------+ Loss: 0.24733249273836755\n",
            "+---------- EPOCH 53500 -----------+ Loss: 0.24733248856932882\n",
            "+---------- EPOCH 53550 -----------+ Loss: 0.24733248440794792\n",
            "+---------- EPOCH 53600 -----------+ Loss: 0.2473324802542042\n",
            "+---------- EPOCH 53650 -----------+ Loss: 0.24733247610807688\n",
            "+---------- EPOCH 53700 -----------+ Loss: 0.2473324719695451\n",
            "+---------- EPOCH 53750 -----------+ Loss: 0.24733246783858842\n",
            "+---------- EPOCH 53800 -----------+ Loss: 0.247332463715186\n",
            "+---------- EPOCH 53850 -----------+ Loss: 0.24733245959931763\n",
            "+---------- EPOCH 53900 -----------+ Loss: 0.24733245549096258\n",
            "+---------- EPOCH 53950 -----------+ Loss: 0.24733245139010063\n",
            "+---------- EPOCH 54000 -----------+ Loss: 0.24733244729671158\n",
            "+---------- EPOCH 54050 -----------+ Loss: 0.24733244321077527\n",
            "+---------- EPOCH 54100 -----------+ Loss: 0.24733243913227135\n",
            "+---------- EPOCH 54150 -----------+ Loss: 0.24733243506117988\n",
            "+---------- EPOCH 54200 -----------+ Loss: 0.24733243099748087\n",
            "+---------- EPOCH 54250 -----------+ Loss: 0.2473324269411544\n",
            "+---------- EPOCH 54300 -----------+ Loss: 0.24733242289218055\n",
            "+---------- EPOCH 54350 -----------+ Loss: 0.24733241885053955\n",
            "+---------- EPOCH 54400 -----------+ Loss: 0.247332414816212\n",
            "+---------- EPOCH 54450 -----------+ Loss: 0.24733241078917773\n",
            "+---------- EPOCH 54500 -----------+ Loss: 0.24733240676941756\n",
            "+---------- EPOCH 54550 -----------+ Loss: 0.2473324027569119\n",
            "+---------- EPOCH 54600 -----------+ Loss: 0.24733239875164123\n",
            "+---------- EPOCH 54650 -----------+ Loss: 0.24733239475358634\n",
            "+---------- EPOCH 54700 -----------+ Loss: 0.2473323907627278\n",
            "+---------- EPOCH 54750 -----------+ Loss: 0.2473323867790465\n",
            "+---------- EPOCH 54800 -----------+ Loss: 0.2473323828025232\n",
            "+---------- EPOCH 54850 -----------+ Loss: 0.24733237883313883\n",
            "+---------- EPOCH 54900 -----------+ Loss: 0.24733237487087445\n",
            "+---------- EPOCH 54950 -----------+ Loss: 0.247332370915711\n",
            "+---------- EPOCH 55000 -----------+ Loss: 0.24733236696762964\n",
            "+---------- EPOCH 55050 -----------+ Loss: 0.24733236302661157\n",
            "+---------- EPOCH 55100 -----------+ Loss: 0.247332359092638\n",
            "+---------- EPOCH 55150 -----------+ Loss: 0.24733235516569024\n",
            "+---------- EPOCH 55200 -----------+ Loss: 0.2473323512457497\n",
            "+---------- EPOCH 55250 -----------+ Loss: 0.2473323473327979\n",
            "+---------- EPOCH 55300 -----------+ Loss: 0.24733234342681612\n",
            "+---------- EPOCH 55350 -----------+ Loss: 0.24733233952778616\n",
            "+---------- EPOCH 55400 -----------+ Loss: 0.2473323356356895\n",
            "+---------- EPOCH 55450 -----------+ Loss: 0.247332331750508\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 55500 -----------+ Loss: 0.24733232787222326\n",
            "+---------- EPOCH 55550 -----------+ Loss: 0.24733232400081723\n",
            "+---------- EPOCH 55600 -----------+ Loss: 0.24733232013627168\n",
            "+---------- EPOCH 55650 -----------+ Loss: 0.24733231627856872\n",
            "+---------- EPOCH 55700 -----------+ Loss: 0.24733231242769033\n",
            "+---------- EPOCH 55750 -----------+ Loss: 0.24733230858361846\n",
            "+---------- EPOCH 55800 -----------+ Loss: 0.24733230474633544\n",
            "+---------- EPOCH 55850 -----------+ Loss: 0.2473323009158234\n",
            "+---------- EPOCH 55900 -----------+ Loss: 0.24733229709206447\n",
            "+---------- EPOCH 55950 -----------+ Loss: 0.24733229327504125\n",
            "+---------- EPOCH 56000 -----------+ Loss: 0.2473322894647359\n",
            "+---------- EPOCH 56050 -----------+ Loss: 0.24733228566113097\n",
            "+---------- EPOCH 56100 -----------+ Loss: 0.2473322818642089\n",
            "+---------- EPOCH 56150 -----------+ Loss: 0.24733227807395242\n",
            "+---------- EPOCH 56200 -----------+ Loss: 0.24733227429034405\n",
            "+---------- EPOCH 56250 -----------+ Loss: 0.24733227051336637\n",
            "+---------- EPOCH 56300 -----------+ Loss: 0.2473322667430024\n",
            "+---------- EPOCH 56350 -----------+ Loss: 0.24733226297923472\n",
            "+---------- EPOCH 56400 -----------+ Loss: 0.24733225922204635\n",
            "+---------- EPOCH 56450 -----------+ Loss: 0.24733225547142013\n",
            "+---------- EPOCH 56500 -----------+ Loss: 0.24733225172733908\n",
            "+---------- EPOCH 56550 -----------+ Loss: 0.24733224798978629\n",
            "+---------- EPOCH 56600 -----------+ Loss: 0.24733224425874467\n",
            "+---------- EPOCH 56650 -----------+ Loss: 0.24733224053419764\n",
            "+---------- EPOCH 56700 -----------+ Loss: 0.24733223681612834\n",
            "+---------- EPOCH 56750 -----------+ Loss: 0.2473322331045199\n",
            "+---------- EPOCH 56800 -----------+ Loss: 0.24733222939935587\n",
            "+---------- EPOCH 56850 -----------+ Loss: 0.24733222570061952\n",
            "+---------- EPOCH 56900 -----------+ Loss: 0.24733222200829433\n",
            "+---------- EPOCH 56950 -----------+ Loss: 0.24733221832236382\n",
            "+---------- EPOCH 57000 -----------+ Loss: 0.24733221464281147\n",
            "+---------- EPOCH 57050 -----------+ Loss: 0.24733221096962094\n",
            "+---------- EPOCH 57100 -----------+ Loss: 0.247332207302776\n",
            "+---------- EPOCH 57150 -----------+ Loss: 0.24733220364226027\n",
            "+---------- EPOCH 57200 -----------+ Loss: 0.2473321999880575\n",
            "+---------- EPOCH 57250 -----------+ Loss: 0.24733219634015163\n",
            "+---------- EPOCH 57300 -----------+ Loss: 0.2473321926985265\n",
            "+---------- EPOCH 57350 -----------+ Loss: 0.24733218906316606\n",
            "+---------- EPOCH 57400 -----------+ Loss: 0.24733218543405436\n",
            "+---------- EPOCH 57450 -----------+ Loss: 0.24733218181117544\n",
            "+---------- EPOCH 57500 -----------+ Loss: 0.24733217819451328\n",
            "+---------- EPOCH 57550 -----------+ Loss: 0.24733217458405224\n",
            "+---------- EPOCH 57600 -----------+ Loss: 0.24733217097977636\n",
            "+---------- EPOCH 57650 -----------+ Loss: 0.24733216738167013\n",
            "+---------- EPOCH 57700 -----------+ Loss: 0.24733216378971765\n",
            "+---------- EPOCH 57750 -----------+ Loss: 0.24733216020390336\n",
            "+---------- EPOCH 57800 -----------+ Loss: 0.24733215662421176\n",
            "+---------- EPOCH 57850 -----------+ Loss: 0.24733215305062728\n",
            "+---------- EPOCH 57900 -----------+ Loss: 0.24733214948313445\n",
            "+---------- EPOCH 57950 -----------+ Loss: 0.24733214592171787\n",
            "+---------- EPOCH 58000 -----------+ Loss: 0.24733214236636217\n",
            "+---------- EPOCH 58050 -----------+ Loss: 0.247332138817052\n",
            "+---------- EPOCH 58100 -----------+ Loss: 0.2473321352737722\n",
            "+---------- EPOCH 58150 -----------+ Loss: 0.24733213173650753\n",
            "+---------- EPOCH 58200 -----------+ Loss: 0.24733212820524272\n",
            "+---------- EPOCH 58250 -----------+ Loss: 0.24733212467996277\n",
            "+---------- EPOCH 58300 -----------+ Loss: 0.2473321211606525\n",
            "+---------- EPOCH 58350 -----------+ Loss: 0.24733211764729704\n",
            "+---------- EPOCH 58400 -----------+ Loss: 0.24733211413988146\n",
            "+---------- EPOCH 58450 -----------+ Loss: 0.2473321106383907\n",
            "+---------- EPOCH 58500 -----------+ Loss: 0.24733210714280995\n",
            "+---------- EPOCH 58550 -----------+ Loss: 0.24733210365312452\n",
            "+---------- EPOCH 58600 -----------+ Loss: 0.24733210016931956\n",
            "+---------- EPOCH 58650 -----------+ Loss: 0.2473320966913803\n",
            "+---------- EPOCH 58700 -----------+ Loss: 0.24733209321929217\n",
            "+---------- EPOCH 58750 -----------+ Loss: 0.2473320897530404\n",
            "+---------- EPOCH 58800 -----------+ Loss: 0.24733208629261066\n",
            "+---------- EPOCH 58850 -----------+ Loss: 0.24733208283798833\n",
            "+---------- EPOCH 58900 -----------+ Loss: 0.24733207938915885\n",
            "+---------- EPOCH 58950 -----------+ Loss: 0.2473320759461079\n",
            "+---------- EPOCH 59000 -----------+ Loss: 0.24733207250882108\n",
            "+---------- EPOCH 59050 -----------+ Loss: 0.2473320690772841\n",
            "+---------- EPOCH 59100 -----------+ Loss: 0.24733206565148264\n",
            "+---------- EPOCH 59150 -----------+ Loss: 0.24733206223140233\n",
            "+---------- EPOCH 59200 -----------+ Loss: 0.24733205881702922\n",
            "+---------- EPOCH 59250 -----------+ Loss: 0.24733205540834918\n",
            "+---------- EPOCH 59300 -----------+ Loss: 0.24733205200534789\n",
            "+---------- EPOCH 59350 -----------+ Loss: 0.24733204860801147\n",
            "+---------- EPOCH 59400 -----------+ Loss: 0.24733204521632582\n",
            "+---------- EPOCH 59450 -----------+ Loss: 0.24733204183027707\n",
            "+---------- EPOCH 59500 -----------+ Loss: 0.24733203844985127\n",
            "+---------- EPOCH 59550 -----------+ Loss: 0.2473320350750346\n",
            "+---------- EPOCH 59600 -----------+ Loss: 0.24733203170581317\n",
            "+---------- EPOCH 59650 -----------+ Loss: 0.24733202834217322\n",
            "+---------- EPOCH 59700 -----------+ Loss: 0.24733202498410106\n",
            "+---------- EPOCH 59750 -----------+ Loss: 0.24733202163158297\n",
            "+---------- EPOCH 59800 -----------+ Loss: 0.24733201828460535\n",
            "+---------- EPOCH 59850 -----------+ Loss: 0.24733201494315457\n",
            "+---------- EPOCH 59900 -----------+ Loss: 0.24733201160721716\n",
            "+---------- EPOCH 59950 -----------+ Loss: 0.24733200827677945\n",
            "+---------- EPOCH 60000 -----------+ Loss: 0.24733200495182806\n",
            "+---------- EPOCH 60050 -----------+ Loss: 0.24733200163234964\n",
            "+---------- EPOCH 60100 -----------+ Loss: 0.24733199831833075\n",
            "+---------- EPOCH 60150 -----------+ Loss: 0.24733199500975817\n",
            "+---------- EPOCH 60200 -----------+ Loss: 0.24733199170661838\n",
            "+---------- EPOCH 60250 -----------+ Loss: 0.24733198840889822\n",
            "+---------- EPOCH 60300 -----------+ Loss: 0.2473319851165846\n",
            "+---------- EPOCH 60350 -----------+ Loss: 0.24733198182966423\n",
            "+---------- EPOCH 60400 -----------+ Loss: 0.24733197854812405\n",
            "+---------- EPOCH 60450 -----------+ Loss: 0.247331975271951\n",
            "+---------- EPOCH 60500 -----------+ Loss: 0.247331972001132\n",
            "+---------- EPOCH 60550 -----------+ Loss: 0.24733196873565405\n",
            "+---------- EPOCH 60600 -----------+ Loss: 0.24733196547550432\n",
            "+---------- EPOCH 60650 -----------+ Loss: 0.2473319622206697\n",
            "+---------- EPOCH 60700 -----------+ Loss: 0.2473319589711375\n",
            "+---------- EPOCH 60750 -----------+ Loss: 0.2473319557268948\n",
            "+---------- EPOCH 60800 -----------+ Loss: 0.24733195248792889\n",
            "+---------- EPOCH 60850 -----------+ Loss: 0.24733194925422694\n",
            "+---------- EPOCH 60900 -----------+ Loss: 0.2473319460257762\n",
            "+---------- EPOCH 60950 -----------+ Loss: 0.24733194280256413\n",
            "+---------- EPOCH 61000 -----------+ Loss: 0.24733193958457814\n",
            "+---------- EPOCH 61050 -----------+ Loss: 0.24733193637180556\n",
            "+---------- EPOCH 61100 -----------+ Loss: 0.24733193316423377\n",
            "+---------- EPOCH 61150 -----------+ Loss: 0.24733192996185038\n",
            "+---------- EPOCH 61200 -----------+ Loss: 0.24733192676464297\n",
            "+---------- EPOCH 61250 -----------+ Loss: 0.24733192357259895\n",
            "+---------- EPOCH 61300 -----------+ Loss: 0.247331920385706\n",
            "+---------- EPOCH 61350 -----------+ Loss: 0.247331917203952\n",
            "+---------- EPOCH 61400 -----------+ Loss: 0.24733191402732432\n",
            "+---------- EPOCH 61450 -----------+ Loss: 0.24733191085581097\n",
            "+---------- EPOCH 61500 -----------+ Loss: 0.24733190768939944\n",
            "+---------- EPOCH 61550 -----------+ Loss: 0.24733190452807774\n",
            "+---------- EPOCH 61600 -----------+ Loss: 0.24733190137183358\n",
            "+---------- EPOCH 61650 -----------+ Loss: 0.247331898220655\n",
            "+---------- EPOCH 61700 -----------+ Loss: 0.2473318950745299\n",
            "+---------- EPOCH 61750 -----------+ Loss: 0.24733189193344615\n",
            "+---------- EPOCH 61800 -----------+ Loss: 0.24733188879739187\n",
            "+---------- EPOCH 61850 -----------+ Loss: 0.247331885666355\n",
            "+---------- EPOCH 61900 -----------+ Loss: 0.2473318825403236\n",
            "+---------- EPOCH 61950 -----------+ Loss: 0.24733187941928597\n",
            "+---------- EPOCH 62000 -----------+ Loss: 0.24733187630323003\n",
            "+---------- EPOCH 62050 -----------+ Loss: 0.2473318731921441\n",
            "+---------- EPOCH 62100 -----------+ Loss: 0.24733187008601645\n",
            "+---------- EPOCH 62150 -----------+ Loss: 0.2473318669848352\n",
            "+---------- EPOCH 62200 -----------+ Loss: 0.2473318638885887\n",
            "+---------- EPOCH 62250 -----------+ Loss: 0.24733186079726546\n",
            "+---------- EPOCH 62300 -----------+ Loss: 0.24733185771085361\n",
            "+---------- EPOCH 62350 -----------+ Loss: 0.24733185462934168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 62400 -----------+ Loss: 0.24733185155271808\n",
            "+---------- EPOCH 62450 -----------+ Loss: 0.2473318484809713\n",
            "+---------- EPOCH 62500 -----------+ Loss: 0.24733184541408992\n",
            "+---------- EPOCH 62550 -----------+ Loss: 0.2473318423520625\n",
            "+---------- EPOCH 62600 -----------+ Loss: 0.24733183929487756\n",
            "+---------- EPOCH 62650 -----------+ Loss: 0.24733183624252367\n",
            "+---------- EPOCH 62700 -----------+ Loss: 0.24733183319498964\n",
            "+---------- EPOCH 62750 -----------+ Loss: 0.24733183015226404\n",
            "+---------- EPOCH 62800 -----------+ Loss: 0.24733182711433563\n",
            "+---------- EPOCH 62850 -----------+ Loss: 0.24733182408119322\n",
            "+---------- EPOCH 62900 -----------+ Loss: 0.24733182105282564\n",
            "+---------- EPOCH 62950 -----------+ Loss: 0.2473318180292217\n",
            "+---------- EPOCH 63000 -----------+ Loss: 0.24733181501037013\n",
            "+---------- EPOCH 63050 -----------+ Loss: 0.24733181199626003\n",
            "+---------- EPOCH 63100 -----------+ Loss: 0.2473318089868802\n",
            "+---------- EPOCH 63150 -----------+ Loss: 0.24733180598221977\n",
            "+---------- EPOCH 63200 -----------+ Loss: 0.24733180298226762\n",
            "+---------- EPOCH 63250 -----------+ Loss: 0.24733179998701293\n",
            "+---------- EPOCH 63300 -----------+ Loss: 0.2473317969964445\n",
            "+---------- EPOCH 63350 -----------+ Loss: 0.24733179401055175\n",
            "+---------- EPOCH 63400 -----------+ Loss: 0.24733179102932354\n",
            "+---------- EPOCH 63450 -----------+ Loss: 0.2473317880527493\n",
            "+---------- EPOCH 63500 -----------+ Loss: 0.24733178508081805\n",
            "+---------- EPOCH 63550 -----------+ Loss: 0.24733178211351922\n",
            "+---------- EPOCH 63600 -----------+ Loss: 0.24733177915084192\n",
            "+---------- EPOCH 63650 -----------+ Loss: 0.2473317761927755\n",
            "+---------- EPOCH 63700 -----------+ Loss: 0.24733177323930933\n",
            "+---------- EPOCH 63750 -----------+ Loss: 0.24733177029043282\n",
            "+---------- EPOCH 63800 -----------+ Loss: 0.24733176734613532\n",
            "+---------- EPOCH 63850 -----------+ Loss: 0.2473317644064062\n",
            "+---------- EPOCH 63900 -----------+ Loss: 0.2473317614712351\n",
            "+---------- EPOCH 63950 -----------+ Loss: 0.2473317585406114\n",
            "+---------- EPOCH 64000 -----------+ Loss: 0.24733175561452456\n",
            "+---------- EPOCH 64050 -----------+ Loss: 0.24733175269296437\n",
            "+---------- EPOCH 64100 -----------+ Loss: 0.24733174977592032\n",
            "+---------- EPOCH 64150 -----------+ Loss: 0.24733174686338202\n",
            "+---------- EPOCH 64200 -----------+ Loss: 0.24733174395533908\n",
            "+---------- EPOCH 64250 -----------+ Loss: 0.24733174105178127\n",
            "+---------- EPOCH 64300 -----------+ Loss: 0.24733173815269838\n",
            "+---------- EPOCH 64350 -----------+ Loss: 0.2473317352580801\n",
            "+---------- EPOCH 64400 -----------+ Loss: 0.24733173236791606\n",
            "+---------- EPOCH 64450 -----------+ Loss: 0.24733172948219634\n",
            "+---------- EPOCH 64500 -----------+ Loss: 0.2473317266009106\n",
            "+---------- EPOCH 64550 -----------+ Loss: 0.2473317237240488\n",
            "+---------- EPOCH 64600 -----------+ Loss: 0.2473317208516009\n",
            "+---------- EPOCH 64650 -----------+ Loss: 0.24733171798355674\n",
            "+---------- EPOCH 64700 -----------+ Loss: 0.24733171511990634\n",
            "+---------- EPOCH 64750 -----------+ Loss: 0.24733171226063974\n",
            "+---------- EPOCH 64800 -----------+ Loss: 0.24733170940574686\n",
            "+---------- EPOCH 64850 -----------+ Loss: 0.24733170655521786\n",
            "+---------- EPOCH 64900 -----------+ Loss: 0.24733170370904278\n",
            "+---------- EPOCH 64950 -----------+ Loss: 0.24733170086721173\n",
            "+---------- EPOCH 65000 -----------+ Loss: 0.247331698029715\n",
            "+---------- EPOCH 65050 -----------+ Loss: 0.2473316951965426\n",
            "+---------- EPOCH 65100 -----------+ Loss: 0.2473316923676847\n",
            "+---------- EPOCH 65150 -----------+ Loss: 0.2473316895431318\n",
            "+---------- EPOCH 65200 -----------+ Loss: 0.24733168672287395\n",
            "+---------- EPOCH 65250 -----------+ Loss: 0.24733168390690144\n",
            "+---------- EPOCH 65300 -----------+ Loss: 0.24733168109520476\n",
            "+---------- EPOCH 65350 -----------+ Loss: 0.2473316782877741\n",
            "+---------- EPOCH 65400 -----------+ Loss: 0.24733167548459994\n",
            "+---------- EPOCH 65450 -----------+ Loss: 0.24733167268567263\n",
            "+---------- EPOCH 65500 -----------+ Loss: 0.2473316698909827\n",
            "+---------- EPOCH 65550 -----------+ Loss: 0.2473316671005205\n",
            "+---------- EPOCH 65600 -----------+ Loss: 0.24733166431427664\n",
            "+---------- EPOCH 65650 -----------+ Loss: 0.2473316615322416\n",
            "+---------- EPOCH 65700 -----------+ Loss: 0.2473316587544059\n",
            "+---------- EPOCH 65750 -----------+ Loss: 0.24733165598076015\n",
            "+---------- EPOCH 65800 -----------+ Loss: 0.24733165321129505\n",
            "+---------- EPOCH 65850 -----------+ Loss: 0.2473316504460011\n",
            "+---------- EPOCH 65900 -----------+ Loss: 0.24733164768486907\n",
            "+---------- EPOCH 65950 -----------+ Loss: 0.24733164492788956\n",
            "+---------- EPOCH 66000 -----------+ Loss: 0.24733164217505335\n",
            "+---------- EPOCH 66050 -----------+ Loss: 0.24733163942635128\n",
            "+---------- EPOCH 66100 -----------+ Loss: 0.2473316366817739\n",
            "+---------- EPOCH 66150 -----------+ Loss: 0.2473316339413122\n",
            "+---------- EPOCH 66200 -----------+ Loss: 0.24733163120495694\n",
            "+---------- EPOCH 66250 -----------+ Loss: 0.24733162847269904\n",
            "+---------- EPOCH 66300 -----------+ Loss: 0.24733162574452927\n",
            "+---------- EPOCH 66350 -----------+ Loss: 0.24733162302043865\n",
            "+---------- EPOCH 66400 -----------+ Loss: 0.24733162030041808\n",
            "+---------- EPOCH 66450 -----------+ Loss: 0.24733161758445846\n",
            "+---------- EPOCH 66500 -----------+ Loss: 0.24733161487255098\n",
            "+---------- EPOCH 66550 -----------+ Loss: 0.2473316121646864\n",
            "+---------- EPOCH 66600 -----------+ Loss: 0.24733160946085586\n",
            "+---------- EPOCH 66650 -----------+ Loss: 0.24733160676105062\n",
            "+---------- EPOCH 66700 -----------+ Loss: 0.24733160406526147\n",
            "+---------- EPOCH 66750 -----------+ Loss: 0.2473316013734797\n",
            "+---------- EPOCH 66800 -----------+ Loss: 0.24733159868569657\n",
            "+---------- EPOCH 66850 -----------+ Loss: 0.24733159600190296\n",
            "+---------- EPOCH 66900 -----------+ Loss: 0.24733159332209037\n",
            "+---------- EPOCH 66950 -----------+ Loss: 0.24733159064624985\n",
            "+---------- EPOCH 67000 -----------+ Loss: 0.24733158797437266\n",
            "+---------- EPOCH 67050 -----------+ Loss: 0.24733158530645014\n",
            "+---------- EPOCH 67100 -----------+ Loss: 0.24733158264247357\n",
            "+---------- EPOCH 67150 -----------+ Loss: 0.24733157998243427\n",
            "+---------- EPOCH 67200 -----------+ Loss: 0.2473315773263236\n",
            "+---------- EPOCH 67250 -----------+ Loss: 0.24733157467413294\n",
            "+---------- EPOCH 67300 -----------+ Loss: 0.24733157202585362\n",
            "+---------- EPOCH 67350 -----------+ Loss: 0.2473315693814773\n",
            "+---------- EPOCH 67400 -----------+ Loss: 0.24733156674099516\n",
            "+---------- EPOCH 67450 -----------+ Loss: 0.24733156410439894\n",
            "+---------- EPOCH 67500 -----------+ Loss: 0.2473315614716798\n",
            "+---------- EPOCH 67550 -----------+ Loss: 0.24733155884282962\n",
            "+---------- EPOCH 67600 -----------+ Loss: 0.24733155621783973\n",
            "+---------- EPOCH 67650 -----------+ Loss: 0.2473315535967018\n",
            "+---------- EPOCH 67700 -----------+ Loss: 0.24733155097940732\n",
            "+---------- EPOCH 67750 -----------+ Loss: 0.24733154836594817\n",
            "+---------- EPOCH 67800 -----------+ Loss: 0.24733154575631577\n",
            "+---------- EPOCH 67850 -----------+ Loss: 0.2473315431505018\n",
            "+---------- EPOCH 67900 -----------+ Loss: 0.2473315405484981\n",
            "+---------- EPOCH 67950 -----------+ Loss: 0.24733153795029622\n",
            "+---------- EPOCH 68000 -----------+ Loss: 0.24733153535588812\n",
            "+---------- EPOCH 68050 -----------+ Loss: 0.24733153276526534\n",
            "+---------- EPOCH 68100 -----------+ Loss: 0.2473315301784198\n",
            "+---------- EPOCH 68150 -----------+ Loss: 0.24733152759534333\n",
            "+---------- EPOCH 68200 -----------+ Loss: 0.24733152501602768\n",
            "+---------- EPOCH 68250 -----------+ Loss: 0.24733152244046477\n",
            "+---------- EPOCH 68300 -----------+ Loss: 0.24733151986864657\n",
            "+---------- EPOCH 68350 -----------+ Loss: 0.24733151730056493\n",
            "+---------- EPOCH 68400 -----------+ Loss: 0.2473315147362116\n",
            "+---------- EPOCH 68450 -----------+ Loss: 0.2473315121755787\n",
            "+---------- EPOCH 68500 -----------+ Loss: 0.24733150961865835\n",
            "+---------- EPOCH 68550 -----------+ Loss: 0.2473315070654423\n",
            "+---------- EPOCH 68600 -----------+ Loss: 0.2473315045159227\n",
            "+---------- EPOCH 68650 -----------+ Loss: 0.2473315019700916\n",
            "+---------- EPOCH 68700 -----------+ Loss: 0.24733149942794103\n",
            "+---------- EPOCH 68750 -----------+ Loss: 0.24733149688946318\n",
            "+---------- EPOCH 68800 -----------+ Loss: 0.24733149435465\n",
            "+---------- EPOCH 68850 -----------+ Loss: 0.24733149182349373\n",
            "+---------- EPOCH 68900 -----------+ Loss: 0.24733148929598647\n",
            "+---------- EPOCH 68950 -----------+ Loss: 0.24733148677212058\n",
            "+---------- EPOCH 69000 -----------+ Loss: 0.247331484251888\n",
            "+---------- EPOCH 69050 -----------+ Loss: 0.2473314817352812\n",
            "+---------- EPOCH 69100 -----------+ Loss: 0.24733147922229232\n",
            "+---------- EPOCH 69150 -----------+ Loss: 0.24733147671291358\n",
            "+---------- EPOCH 69200 -----------+ Loss: 0.24733147420713733\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 69250 -----------+ Loss: 0.24733147170495587\n",
            "+---------- EPOCH 69300 -----------+ Loss: 0.24733146920636156\n",
            "+---------- EPOCH 69350 -----------+ Loss: 0.2473314667113468\n",
            "+---------- EPOCH 69400 -----------+ Loss: 0.24733146421990387\n",
            "+---------- EPOCH 69450 -----------+ Loss: 0.24733146173202525\n",
            "+---------- EPOCH 69500 -----------+ Loss: 0.24733145924770328\n",
            "+---------- EPOCH 69550 -----------+ Loss: 0.2473314567669305\n",
            "+---------- EPOCH 69600 -----------+ Loss: 0.24733145428969927\n",
            "+---------- EPOCH 69650 -----------+ Loss: 0.2473314518160021\n",
            "+---------- EPOCH 69700 -----------+ Loss: 0.2473314493458316\n",
            "+---------- EPOCH 69750 -----------+ Loss: 0.2473314468791802\n",
            "+---------- EPOCH 69800 -----------+ Loss: 0.24733144441604046\n",
            "+---------- EPOCH 69850 -----------+ Loss: 0.247331441956405\n",
            "+---------- EPOCH 69900 -----------+ Loss: 0.2473314395002664\n",
            "+---------- EPOCH 69950 -----------+ Loss: 0.24733143704761718\n",
            "+---------- EPOCH 70000 -----------+ Loss: 0.24733143459845014\n",
            "+---------- EPOCH 70050 -----------+ Loss: 0.2473314321527578\n",
            "+---------- EPOCH 70100 -----------+ Loss: 0.24733142971053282\n",
            "+---------- EPOCH 70150 -----------+ Loss: 0.247331427271768\n",
            "+---------- EPOCH 70200 -----------+ Loss: 0.24733142483645604\n",
            "+---------- EPOCH 70250 -----------+ Loss: 0.24733142240458966\n",
            "+---------- EPOCH 70300 -----------+ Loss: 0.2473314199761614\n",
            "+---------- EPOCH 70350 -----------+ Loss: 0.24733141755116442\n",
            "+---------- EPOCH 70400 -----------+ Loss: 0.24733141512959125\n",
            "+---------- EPOCH 70450 -----------+ Loss: 0.24733141271143474\n",
            "+---------- EPOCH 70500 -----------+ Loss: 0.2473314102966878\n",
            "+---------- EPOCH 70550 -----------+ Loss: 0.24733140788534325\n",
            "+---------- EPOCH 70600 -----------+ Loss: 0.24733140547739402\n",
            "+---------- EPOCH 70650 -----------+ Loss: 0.24733140307283288\n",
            "+---------- EPOCH 70700 -----------+ Loss: 0.24733140067165277\n",
            "+---------- EPOCH 70750 -----------+ Loss: 0.24733139827384676\n",
            "+---------- EPOCH 70800 -----------+ Loss: 0.2473313958794077\n",
            "+---------- EPOCH 70850 -----------+ Loss: 0.2473313934883285\n",
            "+---------- EPOCH 70900 -----------+ Loss: 0.24733139110060234\n",
            "+---------- EPOCH 70950 -----------+ Loss: 0.24733138871622198\n",
            "+---------- EPOCH 71000 -----------+ Loss: 0.24733138633518076\n",
            "+---------- EPOCH 71050 -----------+ Loss: 0.2473313839574714\n",
            "+---------- EPOCH 71100 -----------+ Loss: 0.24733138158308732\n",
            "+---------- EPOCH 71150 -----------+ Loss: 0.2473313792120213\n",
            "+---------- EPOCH 71200 -----------+ Loss: 0.24733137684426656\n",
            "+---------- EPOCH 71250 -----------+ Loss: 0.2473313744798162\n",
            "+---------- EPOCH 71300 -----------+ Loss: 0.24733137211866352\n",
            "+---------- EPOCH 71350 -----------+ Loss: 0.24733136976080156\n",
            "+---------- EPOCH 71400 -----------+ Loss: 0.2473313674062234\n",
            "+---------- EPOCH 71450 -----------+ Loss: 0.2473313650549225\n",
            "+---------- EPOCH 71500 -----------+ Loss: 0.24733136270689182\n",
            "+---------- EPOCH 71550 -----------+ Loss: 0.2473313603621248\n",
            "+---------- EPOCH 71600 -----------+ Loss: 0.2473313580206145\n",
            "+---------- EPOCH 71650 -----------+ Loss: 0.24733135568235443\n",
            "+---------- EPOCH 71700 -----------+ Loss: 0.24733135334733766\n",
            "+---------- EPOCH 71750 -----------+ Loss: 0.24733135101555773\n",
            "+---------- EPOCH 71800 -----------+ Loss: 0.24733134868700773\n",
            "+---------- EPOCH 71850 -----------+ Loss: 0.24733134636168122\n",
            "+---------- EPOCH 71900 -----------+ Loss: 0.2473313440395714\n",
            "+---------- EPOCH 71950 -----------+ Loss: 0.24733134172067175\n",
            "+---------- EPOCH 72000 -----------+ Loss: 0.2473313394049757\n",
            "+---------- EPOCH 72050 -----------+ Loss: 0.24733133709247662\n",
            "+---------- EPOCH 72100 -----------+ Loss: 0.24733133478316788\n",
            "+---------- EPOCH 72150 -----------+ Loss: 0.2473313324770431\n",
            "+---------- EPOCH 72200 -----------+ Loss: 0.24733133017409567\n",
            "+---------- EPOCH 72250 -----------+ Loss: 0.2473313278743191\n",
            "+---------- EPOCH 72300 -----------+ Loss: 0.24733132557770685\n",
            "+---------- EPOCH 72350 -----------+ Loss: 0.24733132328425242\n",
            "+---------- EPOCH 72400 -----------+ Loss: 0.24733132099394955\n",
            "+---------- EPOCH 72450 -----------+ Loss: 0.24733131870679162\n",
            "+---------- EPOCH 72500 -----------+ Loss: 0.24733131642277226\n",
            "+---------- EPOCH 72550 -----------+ Loss: 0.247331314141885\n",
            "+---------- EPOCH 72600 -----------+ Loss: 0.24733131186412358\n",
            "+---------- EPOCH 72650 -----------+ Loss: 0.24733130958948168\n",
            "+---------- EPOCH 72700 -----------+ Loss: 0.24733130731795272\n",
            "+---------- EPOCH 72750 -----------+ Loss: 0.24733130504953055\n",
            "+---------- EPOCH 72800 -----------+ Loss: 0.24733130278420878\n",
            "+---------- EPOCH 72850 -----------+ Loss: 0.24733130052198118\n",
            "+---------- EPOCH 72900 -----------+ Loss: 0.2473312982628414\n",
            "+---------- EPOCH 72950 -----------+ Loss: 0.24733129600678325\n",
            "+---------- EPOCH 73000 -----------+ Loss: 0.24733129375380034\n",
            "+---------- EPOCH 73050 -----------+ Loss: 0.24733129150388655\n",
            "+---------- EPOCH 73100 -----------+ Loss: 0.24733128925703557\n",
            "+---------- EPOCH 73150 -----------+ Loss: 0.24733128701324142\n",
            "+---------- EPOCH 73200 -----------+ Loss: 0.24733128477249766\n",
            "+---------- EPOCH 73250 -----------+ Loss: 0.24733128253479833\n",
            "+---------- EPOCH 73300 -----------+ Loss: 0.2473312803001372\n",
            "+---------- EPOCH 73350 -----------+ Loss: 0.247331278068508\n",
            "+---------- EPOCH 73400 -----------+ Loss: 0.24733127583990483\n",
            "+---------- EPOCH 73450 -----------+ Loss: 0.24733127361432158\n",
            "+---------- EPOCH 73500 -----------+ Loss: 0.24733127139175196\n",
            "+---------- EPOCH 73550 -----------+ Loss: 0.24733126917219\n",
            "+---------- EPOCH 73600 -----------+ Loss: 0.24733126695562974\n",
            "+---------- EPOCH 73650 -----------+ Loss: 0.24733126474206518\n",
            "+---------- EPOCH 73700 -----------+ Loss: 0.2473312625314901\n",
            "+---------- EPOCH 73750 -----------+ Loss: 0.24733126032389868\n",
            "+---------- EPOCH 73800 -----------+ Loss: 0.24733125811928489\n",
            "+---------- EPOCH 73850 -----------+ Loss: 0.24733125591764266\n",
            "+---------- EPOCH 73900 -----------+ Loss: 0.24733125371896614\n",
            "+---------- EPOCH 73950 -----------+ Loss: 0.24733125152324936\n",
            "+---------- EPOCH 74000 -----------+ Loss: 0.24733124933048642\n",
            "+---------- EPOCH 74050 -----------+ Loss: 0.24733124714067134\n",
            "+---------- EPOCH 74100 -----------+ Loss: 0.24733124495379832\n",
            "+---------- EPOCH 74150 -----------+ Loss: 0.24733124276986132\n",
            "+---------- EPOCH 74200 -----------+ Loss: 0.24733124058885475\n",
            "+---------- EPOCH 74250 -----------+ Loss: 0.24733123841077262\n",
            "+---------- EPOCH 74300 -----------+ Loss: 0.24733123623560915\n",
            "+---------- EPOCH 74350 -----------+ Loss: 0.24733123406335833\n",
            "+---------- EPOCH 74400 -----------+ Loss: 0.2473312318940145\n",
            "+---------- EPOCH 74450 -----------+ Loss: 0.24733122972757207\n",
            "+---------- EPOCH 74500 -----------+ Loss: 0.24733122756402492\n",
            "+---------- EPOCH 74550 -----------+ Loss: 0.24733122540336755\n",
            "+---------- EPOCH 74600 -----------+ Loss: 0.2473312232455941\n",
            "+---------- EPOCH 74650 -----------+ Loss: 0.24733122109069888\n",
            "+---------- EPOCH 74700 -----------+ Loss: 0.2473312189386762\n",
            "+---------- EPOCH 74750 -----------+ Loss: 0.2473312167895204\n",
            "+---------- EPOCH 74800 -----------+ Loss: 0.2473312146432257\n",
            "+---------- EPOCH 74850 -----------+ Loss: 0.24733121249978643\n",
            "+---------- EPOCH 74900 -----------+ Loss: 0.24733121035919714\n",
            "+---------- EPOCH 74950 -----------+ Loss: 0.24733120822145196\n",
            "+---------- EPOCH 75000 -----------+ Loss: 0.2473312060865454\n",
            "+---------- EPOCH 75050 -----------+ Loss: 0.2473312039544719\n",
            "+---------- EPOCH 75100 -----------+ Loss: 0.24733120182522575\n",
            "+---------- EPOCH 75150 -----------+ Loss: 0.24733119969880143\n",
            "+---------- EPOCH 75200 -----------+ Loss: 0.24733119757519342\n",
            "+---------- EPOCH 75250 -----------+ Loss: 0.24733119545439602\n",
            "+---------- EPOCH 75300 -----------+ Loss: 0.24733119333640383\n",
            "+---------- EPOCH 75350 -----------+ Loss: 0.24733119122121144\n",
            "+---------- EPOCH 75400 -----------+ Loss: 0.2473311891088131\n",
            "+---------- EPOCH 75450 -----------+ Loss: 0.24733118699920356\n",
            "+---------- EPOCH 75500 -----------+ Loss: 0.24733118489237718\n",
            "+---------- EPOCH 75550 -----------+ Loss: 0.24733118278832858\n",
            "+---------- EPOCH 75600 -----------+ Loss: 0.24733118068705226\n",
            "+---------- EPOCH 75650 -----------+ Loss: 0.24733117858854275\n",
            "+---------- EPOCH 75700 -----------+ Loss: 0.24733117649279485\n",
            "+---------- EPOCH 75750 -----------+ Loss: 0.24733117439980293\n",
            "+---------- EPOCH 75800 -----------+ Loss: 0.2473311723095617\n",
            "+---------- EPOCH 75850 -----------+ Loss: 0.24733117022206574\n",
            "+---------- EPOCH 75900 -----------+ Loss: 0.2473311681373097\n",
            "+---------- EPOCH 75950 -----------+ Loss: 0.24733116605528832\n",
            "+---------- EPOCH 76000 -----------+ Loss: 0.24733116397599617\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 76050 -----------+ Loss: 0.24733116189942797\n",
            "+---------- EPOCH 76100 -----------+ Loss: 0.24733115982557832\n",
            "+---------- EPOCH 76150 -----------+ Loss: 0.24733115775444212\n",
            "+---------- EPOCH 76200 -----------+ Loss: 0.24733115568601394\n",
            "+---------- EPOCH 76250 -----------+ Loss: 0.2473311536202885\n",
            "+---------- EPOCH 76300 -----------+ Loss: 0.24733115155726068\n",
            "+---------- EPOCH 76350 -----------+ Loss: 0.2473311494969252\n",
            "+---------- EPOCH 76400 -----------+ Loss: 0.24733114743927673\n",
            "+---------- EPOCH 76450 -----------+ Loss: 0.24733114538431022\n",
            "+---------- EPOCH 76500 -----------+ Loss: 0.24733114333202033\n",
            "+---------- EPOCH 76550 -----------+ Loss: 0.24733114128240197\n",
            "+---------- EPOCH 76600 -----------+ Loss: 0.24733113923544983\n",
            "+---------- EPOCH 76650 -----------+ Loss: 0.247331137191159\n",
            "+---------- EPOCH 76700 -----------+ Loss: 0.2473311351495241\n",
            "+---------- EPOCH 76750 -----------+ Loss: 0.2473311331105401\n",
            "+---------- EPOCH 76800 -----------+ Loss: 0.24733113107420193\n",
            "+---------- EPOCH 76850 -----------+ Loss: 0.2473311290405044\n",
            "+---------- EPOCH 76900 -----------+ Loss: 0.24733112700944246\n",
            "+---------- EPOCH 76950 -----------+ Loss: 0.24733112498101095\n",
            "+---------- EPOCH 77000 -----------+ Loss: 0.247331122955205\n",
            "+---------- EPOCH 77050 -----------+ Loss: 0.2473311209320193\n",
            "+---------- EPOCH 77100 -----------+ Loss: 0.24733111891144907\n",
            "+---------- EPOCH 77150 -----------+ Loss: 0.2473311168934891\n",
            "+---------- EPOCH 77200 -----------+ Loss: 0.24733111487813442\n",
            "+---------- EPOCH 77250 -----------+ Loss: 0.24733111286538007\n",
            "+---------- EPOCH 77300 -----------+ Loss: 0.24733111085522108\n",
            "+---------- EPOCH 77350 -----------+ Loss: 0.24733110884765233\n",
            "+---------- EPOCH 77400 -----------+ Loss: 0.247331106842669\n",
            "+---------- EPOCH 77450 -----------+ Loss: 0.24733110484026605\n",
            "+---------- EPOCH 77500 -----------+ Loss: 0.2473311028404386\n",
            "+---------- EPOCH 77550 -----------+ Loss: 0.2473311008431818\n",
            "+---------- EPOCH 77600 -----------+ Loss: 0.24733109884849058\n",
            "+---------- EPOCH 77650 -----------+ Loss: 0.2473310968563601\n",
            "+---------- EPOCH 77700 -----------+ Loss: 0.24733109486678542\n",
            "+---------- EPOCH 77750 -----------+ Loss: 0.24733109287976185\n",
            "+---------- EPOCH 77800 -----------+ Loss: 0.24733109089528438\n",
            "+---------- EPOCH 77850 -----------+ Loss: 0.24733108891334812\n",
            "+---------- EPOCH 77900 -----------+ Loss: 0.24733108693394826\n",
            "+---------- EPOCH 77950 -----------+ Loss: 0.24733108495708012\n",
            "+---------- EPOCH 78000 -----------+ Loss: 0.24733108298273862\n",
            "+---------- EPOCH 78050 -----------+ Loss: 0.2473310810109192\n",
            "+---------- EPOCH 78100 -----------+ Loss: 0.24733107904161697\n",
            "+---------- EPOCH 78150 -----------+ Loss: 0.24733107707482713\n",
            "+---------- EPOCH 78200 -----------+ Loss: 0.24733107511054495\n",
            "+---------- EPOCH 78250 -----------+ Loss: 0.24733107314876568\n",
            "+---------- EPOCH 78300 -----------+ Loss: 0.24733107118948455\n",
            "+---------- EPOCH 78350 -----------+ Loss: 0.24733106923269696\n",
            "+---------- EPOCH 78400 -----------+ Loss: 0.2473310672783979\n",
            "+---------- EPOCH 78450 -----------+ Loss: 0.24733106532658292\n",
            "+---------- EPOCH 78500 -----------+ Loss: 0.24733106337724722\n",
            "+---------- EPOCH 78550 -----------+ Loss: 0.24733106143038616\n",
            "+---------- EPOCH 78600 -----------+ Loss: 0.24733105948599507\n",
            "+---------- EPOCH 78650 -----------+ Loss: 0.24733105754406925\n",
            "+---------- EPOCH 78700 -----------+ Loss: 0.24733105560460406\n",
            "+---------- EPOCH 78750 -----------+ Loss: 0.24733105366759492\n",
            "+---------- EPOCH 78800 -----------+ Loss: 0.24733105173303713\n",
            "+---------- EPOCH 78850 -----------+ Loss: 0.24733104980092607\n",
            "+---------- EPOCH 78900 -----------+ Loss: 0.2473310478712572\n",
            "+---------- EPOCH 78950 -----------+ Loss: 0.24733104594402597\n",
            "+---------- EPOCH 79000 -----------+ Loss: 0.24733104401922773\n",
            "+---------- EPOCH 79050 -----------+ Loss: 0.24733104209685788\n",
            "+---------- EPOCH 79100 -----------+ Loss: 0.2473310401769119\n",
            "+---------- EPOCH 79150 -----------+ Loss: 0.24733103825938524\n",
            "+---------- EPOCH 79200 -----------+ Loss: 0.24733103634427342\n",
            "+---------- EPOCH 79250 -----------+ Loss: 0.2473310344315718\n",
            "+---------- EPOCH 79300 -----------+ Loss: 0.24733103252127597\n",
            "+---------- EPOCH 79350 -----------+ Loss: 0.24733103061338146\n",
            "+---------- EPOCH 79400 -----------+ Loss: 0.24733102870788368\n",
            "+---------- EPOCH 79450 -----------+ Loss: 0.24733102680477825\n",
            "+---------- EPOCH 79500 -----------+ Loss: 0.24733102490406064\n",
            "+---------- EPOCH 79550 -----------+ Loss: 0.24733102300572626\n",
            "+---------- EPOCH 79600 -----------+ Loss: 0.24733102110977098\n",
            "+---------- EPOCH 79650 -----------+ Loss: 0.24733101921619016\n",
            "+---------- EPOCH 79700 -----------+ Loss: 0.2473310173249794\n",
            "+---------- EPOCH 79750 -----------+ Loss: 0.2473310154361343\n",
            "+---------- EPOCH 79800 -----------+ Loss: 0.24733101354965042\n",
            "+---------- EPOCH 79850 -----------+ Loss: 0.2473310116655235\n",
            "+---------- EPOCH 79900 -----------+ Loss: 0.24733100978374903\n",
            "+---------- EPOCH 79950 -----------+ Loss: 0.24733100790432275\n",
            "+---------- EPOCH 80000 -----------+ Loss: 0.24733100602724015\n",
            "+---------- EPOCH 80050 -----------+ Loss: 0.247331004152497\n",
            "+---------- EPOCH 80100 -----------+ Loss: 0.24733100228008897\n",
            "+---------- EPOCH 80150 -----------+ Loss: 0.2473310004100117\n",
            "+---------- EPOCH 80200 -----------+ Loss: 0.24733099854226076\n",
            "+---------- EPOCH 80250 -----------+ Loss: 0.24733099667683212\n",
            "+---------- EPOCH 80300 -----------+ Loss: 0.24733099481372123\n",
            "+---------- EPOCH 80350 -----------+ Loss: 0.2473309929529239\n",
            "+---------- EPOCH 80400 -----------+ Loss: 0.247330991094436\n",
            "+---------- EPOCH 80450 -----------+ Loss: 0.24733098923825292\n",
            "+---------- EPOCH 80500 -----------+ Loss: 0.24733098738437076\n",
            "+---------- EPOCH 80550 -----------+ Loss: 0.24733098553278512\n",
            "+---------- EPOCH 80600 -----------+ Loss: 0.24733098368349185\n",
            "+---------- EPOCH 80650 -----------+ Loss: 0.24733098183648658\n",
            "+---------- EPOCH 80700 -----------+ Loss: 0.2473309799917652\n",
            "+---------- EPOCH 80750 -----------+ Loss: 0.24733097814932348\n",
            "+---------- EPOCH 80800 -----------+ Loss: 0.24733097630915732\n",
            "+---------- EPOCH 80850 -----------+ Loss: 0.24733097447126237\n",
            "+---------- EPOCH 80900 -----------+ Loss: 0.24733097263563464\n",
            "+---------- EPOCH 80950 -----------+ Loss: 0.24733097080226984\n",
            "+---------- EPOCH 81000 -----------+ Loss: 0.24733096897116388\n",
            "+---------- EPOCH 81050 -----------+ Loss: 0.24733096714231262\n",
            "+---------- EPOCH 81100 -----------+ Loss: 0.24733096531571197\n",
            "+---------- EPOCH 81150 -----------+ Loss: 0.24733096349135772\n",
            "+---------- EPOCH 81200 -----------+ Loss: 0.2473309616692458\n",
            "+---------- EPOCH 81250 -----------+ Loss: 0.2473309598493721\n",
            "+---------- EPOCH 81300 -----------+ Loss: 0.24733095803173258\n",
            "+---------- EPOCH 81350 -----------+ Loss: 0.24733095621632303\n",
            "+---------- EPOCH 81400 -----------+ Loss: 0.24733095440313965\n",
            "+---------- EPOCH 81450 -----------+ Loss: 0.2473309525921781\n",
            "+---------- EPOCH 81500 -----------+ Loss: 0.24733095078343445\n",
            "+---------- EPOCH 81550 -----------+ Loss: 0.24733094897690477\n",
            "+---------- EPOCH 81600 -----------+ Loss: 0.24733094717258472\n",
            "+---------- EPOCH 81650 -----------+ Loss: 0.2473309453704706\n",
            "+---------- EPOCH 81700 -----------+ Loss: 0.24733094357055815\n",
            "+---------- EPOCH 81750 -----------+ Loss: 0.2473309417728436\n",
            "+---------- EPOCH 81800 -----------+ Loss: 0.2473309399773228\n",
            "+---------- EPOCH 81850 -----------+ Loss: 0.24733093818399188\n",
            "+---------- EPOCH 81900 -----------+ Loss: 0.24733093639284667\n",
            "+---------- EPOCH 81950 -----------+ Loss: 0.24733093460388347\n",
            "+---------- EPOCH 82000 -----------+ Loss: 0.24733093281709811\n",
            "+---------- EPOCH 82050 -----------+ Loss: 0.2473309310324868\n",
            "+---------- EPOCH 82100 -----------+ Loss: 0.24733092925004552\n",
            "+---------- EPOCH 82150 -----------+ Loss: 0.2473309274697703\n",
            "+---------- EPOCH 82200 -----------+ Loss: 0.24733092569165738\n",
            "+---------- EPOCH 82250 -----------+ Loss: 0.24733092391570277\n",
            "+---------- EPOCH 82300 -----------+ Loss: 0.24733092214190247\n",
            "+---------- EPOCH 82350 -----------+ Loss: 0.24733092037025275\n",
            "+---------- EPOCH 82400 -----------+ Loss: 0.24733091860074963\n",
            "+---------- EPOCH 82450 -----------+ Loss: 0.24733091683338934\n",
            "+---------- EPOCH 82500 -----------+ Loss: 0.24733091506816798\n",
            "+---------- EPOCH 82550 -----------+ Loss: 0.24733091330508164\n",
            "+---------- EPOCH 82600 -----------+ Loss: 0.2473309115441265\n",
            "+---------- EPOCH 82650 -----------+ Loss: 0.24733090978529879\n",
            "+---------- EPOCH 82700 -----------+ Loss: 0.24733090802859467\n",
            "+---------- EPOCH 82750 -----------+ Loss: 0.24733090627401028\n",
            "+---------- EPOCH 82800 -----------+ Loss: 0.24733090452154177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 82850 -----------+ Loss: 0.2473309027711855\n",
            "+---------- EPOCH 82900 -----------+ Loss: 0.24733090102293764\n",
            "+---------- EPOCH 82950 -----------+ Loss: 0.2473308992767943\n",
            "+---------- EPOCH 83000 -----------+ Loss: 0.24733089753275186\n",
            "+---------- EPOCH 83050 -----------+ Loss: 0.24733089579080636\n",
            "+---------- EPOCH 83100 -----------+ Loss: 0.2473308940509543\n",
            "+---------- EPOCH 83150 -----------+ Loss: 0.24733089231319164\n",
            "+---------- EPOCH 83200 -----------+ Loss: 0.24733089057751495\n",
            "+---------- EPOCH 83250 -----------+ Loss: 0.2473308888439204\n",
            "+---------- EPOCH 83300 -----------+ Loss: 0.24733088711240425\n",
            "+---------- EPOCH 83350 -----------+ Loss: 0.24733088538296275\n",
            "+---------- EPOCH 83400 -----------+ Loss: 0.24733088365559214\n",
            "+---------- EPOCH 83450 -----------+ Loss: 0.24733088193028901\n",
            "+---------- EPOCH 83500 -----------+ Loss: 0.24733088020704946\n",
            "+---------- EPOCH 83550 -----------+ Loss: 0.24733087848586974\n",
            "+---------- EPOCH 83600 -----------+ Loss: 0.2473308767667465\n",
            "+---------- EPOCH 83650 -----------+ Loss: 0.24733087504967594\n",
            "+---------- EPOCH 83700 -----------+ Loss: 0.24733087333465412\n",
            "+---------- EPOCH 83750 -----------+ Loss: 0.24733087162167786\n",
            "+---------- EPOCH 83800 -----------+ Loss: 0.24733086991074324\n",
            "+---------- EPOCH 83850 -----------+ Loss: 0.24733086820184677\n",
            "+---------- EPOCH 83900 -----------+ Loss: 0.24733086649498476\n",
            "+---------- EPOCH 83950 -----------+ Loss: 0.24733086479015365\n",
            "+---------- EPOCH 84000 -----------+ Loss: 0.2473308630873498\n",
            "+---------- EPOCH 84050 -----------+ Loss: 0.2473308613865697\n",
            "+---------- EPOCH 84100 -----------+ Loss: 0.24733085968780977\n",
            "+---------- EPOCH 84150 -----------+ Loss: 0.24733085799106644\n",
            "+---------- EPOCH 84200 -----------+ Loss: 0.24733085629633605\n",
            "+---------- EPOCH 84250 -----------+ Loss: 0.24733085460361506\n",
            "+---------- EPOCH 84300 -----------+ Loss: 0.2473308529129001\n",
            "+---------- EPOCH 84350 -----------+ Loss: 0.24733085122418752\n",
            "+---------- EPOCH 84400 -----------+ Loss: 0.2473308495374737\n",
            "+---------- EPOCH 84450 -----------+ Loss: 0.24733084785275514\n",
            "+---------- EPOCH 84500 -----------+ Loss: 0.24733084617002857\n",
            "+---------- EPOCH 84550 -----------+ Loss: 0.24733084448929019\n",
            "+---------- EPOCH 84600 -----------+ Loss: 0.24733084281053663\n",
            "+---------- EPOCH 84650 -----------+ Loss: 0.2473308411337645\n",
            "+---------- EPOCH 84700 -----------+ Loss: 0.2473308394589701\n",
            "+---------- EPOCH 84750 -----------+ Loss: 0.24733083778615023\n",
            "+---------- EPOCH 84800 -----------+ Loss: 0.24733083611530118\n",
            "+---------- EPOCH 84850 -----------+ Loss: 0.24733083444641954\n",
            "+---------- EPOCH 84900 -----------+ Loss: 0.24733083277950205\n",
            "+---------- EPOCH 84950 -----------+ Loss: 0.24733083111454507\n",
            "+---------- EPOCH 85000 -----------+ Loss: 0.24733082945154528\n",
            "+---------- EPOCH 85050 -----------+ Loss: 0.24733082779049917\n",
            "+---------- EPOCH 85100 -----------+ Loss: 0.2473308261314033\n",
            "+---------- EPOCH 85150 -----------+ Loss: 0.24733082447425447\n",
            "+---------- EPOCH 85200 -----------+ Loss: 0.24733082281904917\n",
            "+---------- EPOCH 85250 -----------+ Loss: 0.24733082116578392\n",
            "+---------- EPOCH 85300 -----------+ Loss: 0.24733081951445537\n",
            "+---------- EPOCH 85350 -----------+ Loss: 0.24733081786506023\n",
            "+---------- EPOCH 85400 -----------+ Loss: 0.2473308162175951\n",
            "+---------- EPOCH 85450 -----------+ Loss: 0.2473308145720566\n",
            "+---------- EPOCH 85500 -----------+ Loss: 0.24733081292844136\n",
            "+---------- EPOCH 85550 -----------+ Loss: 0.24733081128674606\n",
            "+---------- EPOCH 85600 -----------+ Loss: 0.24733080964696735\n",
            "+---------- EPOCH 85650 -----------+ Loss: 0.24733080800910195\n",
            "+---------- EPOCH 85700 -----------+ Loss: 0.24733080637314644\n",
            "+---------- EPOCH 85750 -----------+ Loss: 0.24733080473909758\n",
            "+---------- EPOCH 85800 -----------+ Loss: 0.2473308031069521\n",
            "+---------- EPOCH 85850 -----------+ Loss: 0.24733080147670664\n",
            "+---------- EPOCH 85900 -----------+ Loss: 0.2473307998483579\n",
            "+---------- EPOCH 85950 -----------+ Loss: 0.24733079822190265\n",
            "+---------- EPOCH 86000 -----------+ Loss: 0.2473307965973376\n",
            "+---------- EPOCH 86050 -----------+ Loss: 0.2473307949746594\n",
            "+---------- EPOCH 86100 -----------+ Loss: 0.24733079335386496\n",
            "+---------- EPOCH 86150 -----------+ Loss: 0.24733079173495087\n",
            "+---------- EPOCH 86200 -----------+ Loss: 0.2473307901179139\n",
            "+---------- EPOCH 86250 -----------+ Loss: 0.24733078850275092\n",
            "+---------- EPOCH 86300 -----------+ Loss: 0.24733078688945853\n",
            "+---------- EPOCH 86350 -----------+ Loss: 0.2473307852780337\n",
            "+---------- EPOCH 86400 -----------+ Loss: 0.2473307836684731\n",
            "+---------- EPOCH 86450 -----------+ Loss: 0.24733078206077358\n",
            "+---------- EPOCH 86500 -----------+ Loss: 0.2473307804549319\n",
            "+---------- EPOCH 86550 -----------+ Loss: 0.24733077885094476\n",
            "+---------- EPOCH 86600 -----------+ Loss: 0.24733077724880914\n",
            "+---------- EPOCH 86650 -----------+ Loss: 0.24733077564852185\n",
            "+---------- EPOCH 86700 -----------+ Loss: 0.2473307740500796\n",
            "+---------- EPOCH 86750 -----------+ Loss: 0.2473307724534794\n",
            "+---------- EPOCH 86800 -----------+ Loss: 0.2473307708587179\n",
            "+---------- EPOCH 86850 -----------+ Loss: 0.24733076926579212\n",
            "+---------- EPOCH 86900 -----------+ Loss: 0.24733076767469875\n",
            "+---------- EPOCH 86950 -----------+ Loss: 0.24733076608543472\n",
            "+---------- EPOCH 87000 -----------+ Loss: 0.24733076449799693\n",
            "+---------- EPOCH 87050 -----------+ Loss: 0.24733076291238232\n",
            "+---------- EPOCH 87100 -----------+ Loss: 0.24733076132858756\n",
            "+---------- EPOCH 87150 -----------+ Loss: 0.2473307597466098\n",
            "+---------- EPOCH 87200 -----------+ Loss: 0.24733075816644587\n",
            "+---------- EPOCH 87250 -----------+ Loss: 0.24733075658809248\n",
            "+---------- EPOCH 87300 -----------+ Loss: 0.24733075501154683\n",
            "+---------- EPOCH 87350 -----------+ Loss: 0.24733075343680566\n",
            "+---------- EPOCH 87400 -----------+ Loss: 0.24733075186386594\n",
            "+---------- EPOCH 87450 -----------+ Loss: 0.24733075029272455\n",
            "+---------- EPOCH 87500 -----------+ Loss: 0.24733074872337854\n",
            "+---------- EPOCH 87550 -----------+ Loss: 0.24733074715582473\n",
            "+---------- EPOCH 87600 -----------+ Loss: 0.2473307455900603\n",
            "+---------- EPOCH 87650 -----------+ Loss: 0.24733074402608188\n",
            "+---------- EPOCH 87700 -----------+ Loss: 0.24733074246388664\n",
            "+---------- EPOCH 87750 -----------+ Loss: 0.24733074090347165\n",
            "+---------- EPOCH 87800 -----------+ Loss: 0.2473307393448337\n",
            "+---------- EPOCH 87850 -----------+ Loss: 0.2473307377879698\n",
            "+---------- EPOCH 87900 -----------+ Loss: 0.2473307362328771\n",
            "+---------- EPOCH 87950 -----------+ Loss: 0.24733073467955238\n",
            "+---------- EPOCH 88000 -----------+ Loss: 0.24733073312799284\n",
            "+---------- EPOCH 88050 -----------+ Loss: 0.2473307315781955\n",
            "+---------- EPOCH 88100 -----------+ Loss: 0.24733073003015724\n",
            "+---------- EPOCH 88150 -----------+ Loss: 0.2473307284838751\n",
            "+---------- EPOCH 88200 -----------+ Loss: 0.24733072693934627\n",
            "+---------- EPOCH 88250 -----------+ Loss: 0.24733072539656767\n",
            "+---------- EPOCH 88300 -----------+ Loss: 0.2473307238555363\n",
            "+---------- EPOCH 88350 -----------+ Loss: 0.24733072231624936\n",
            "+---------- EPOCH 88400 -----------+ Loss: 0.2473307207787039\n",
            "+---------- EPOCH 88450 -----------+ Loss: 0.24733071924289685\n",
            "+---------- EPOCH 88500 -----------+ Loss: 0.24733071770882536\n",
            "+---------- EPOCH 88550 -----------+ Loss: 0.24733071617648658\n",
            "+---------- EPOCH 88600 -----------+ Loss: 0.2473307146458775\n",
            "+---------- EPOCH 88650 -----------+ Loss: 0.24733071311699523\n",
            "+---------- EPOCH 88700 -----------+ Loss: 0.24733071158983694\n",
            "+---------- EPOCH 88750 -----------+ Loss: 0.2473307100643996\n",
            "+---------- EPOCH 88800 -----------+ Loss: 0.24733070854068048\n",
            "+---------- EPOCH 88850 -----------+ Loss: 0.2473307070186767\n",
            "+---------- EPOCH 88900 -----------+ Loss: 0.24733070549838526\n",
            "+---------- EPOCH 88950 -----------+ Loss: 0.24733070397980333\n",
            "+---------- EPOCH 89000 -----------+ Loss: 0.2473307024629281\n",
            "+---------- EPOCH 89050 -----------+ Loss: 0.24733070094775664\n",
            "+---------- EPOCH 89100 -----------+ Loss: 0.24733069943428626\n",
            "+---------- EPOCH 89150 -----------+ Loss: 0.24733069792251391\n",
            "+---------- EPOCH 89200 -----------+ Loss: 0.24733069641243693\n",
            "+---------- EPOCH 89250 -----------+ Loss: 0.2473306949040524\n",
            "+---------- EPOCH 89300 -----------+ Loss: 0.2473306933973575\n",
            "+---------- EPOCH 89350 -----------+ Loss: 0.2473306918923495\n",
            "+---------- EPOCH 89400 -----------+ Loss: 0.24733069038902547\n",
            "+---------- EPOCH 89450 -----------+ Loss: 0.2473306888873826\n",
            "+---------- EPOCH 89500 -----------+ Loss: 0.2473306873874182\n",
            "+---------- EPOCH 89550 -----------+ Loss: 0.2473306858891295\n",
            "+---------- EPOCH 89600 -----------+ Loss: 0.24733068439251354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 89650 -----------+ Loss: 0.24733068289756766\n",
            "+---------- EPOCH 89700 -----------+ Loss: 0.24733068140428915\n",
            "+---------- EPOCH 89750 -----------+ Loss: 0.24733067991267507\n",
            "+---------- EPOCH 89800 -----------+ Loss: 0.24733067842272277\n",
            "+---------- EPOCH 89850 -----------+ Loss: 0.24733067693442948\n",
            "+---------- EPOCH 89900 -----------+ Loss: 0.24733067544779247\n",
            "+---------- EPOCH 89950 -----------+ Loss: 0.24733067396280894\n",
            "+---------- EPOCH 90000 -----------+ Loss: 0.24733067247947632\n",
            "+---------- EPOCH 90050 -----------+ Loss: 0.24733067099779163\n",
            "+---------- EPOCH 90100 -----------+ Loss: 0.24733066951775237\n",
            "+---------- EPOCH 90150 -----------+ Loss: 0.2473306680393556\n",
            "+---------- EPOCH 90200 -----------+ Loss: 0.24733066656259875\n",
            "+---------- EPOCH 90250 -----------+ Loss: 0.24733066508747917\n",
            "+---------- EPOCH 90300 -----------+ Loss: 0.247330663613994\n",
            "+---------- EPOCH 90350 -----------+ Loss: 0.24733066214214064\n",
            "+---------- EPOCH 90400 -----------+ Loss: 0.2473306606719164\n",
            "+---------- EPOCH 90450 -----------+ Loss: 0.24733065920331854\n",
            "+---------- EPOCH 90500 -----------+ Loss: 0.24733065773634447\n",
            "+---------- EPOCH 90550 -----------+ Loss: 0.24733065627099143\n",
            "+---------- EPOCH 90600 -----------+ Loss: 0.24733065480725686\n",
            "+---------- EPOCH 90650 -----------+ Loss: 0.24733065334513799\n",
            "+---------- EPOCH 90700 -----------+ Loss: 0.24733065188463216\n",
            "+---------- EPOCH 90750 -----------+ Loss: 0.24733065042573682\n",
            "+---------- EPOCH 90800 -----------+ Loss: 0.24733064896844945\n",
            "+---------- EPOCH 90850 -----------+ Loss: 0.24733064751276698\n",
            "+---------- EPOCH 90900 -----------+ Loss: 0.24733064605868718\n",
            "+---------- EPOCH 90950 -----------+ Loss: 0.24733064460620727\n",
            "+---------- EPOCH 91000 -----------+ Loss: 0.24733064315532471\n",
            "+---------- EPOCH 91050 -----------+ Loss: 0.2473306417060367\n",
            "+---------- EPOCH 91100 -----------+ Loss: 0.24733064025834092\n",
            "+---------- EPOCH 91150 -----------+ Loss: 0.24733063881223444\n",
            "+---------- EPOCH 91200 -----------+ Loss: 0.24733063736771493\n",
            "+---------- EPOCH 91250 -----------+ Loss: 0.2473306359247797\n",
            "+---------- EPOCH 91300 -----------+ Loss: 0.24733063448342607\n",
            "+---------- EPOCH 91350 -----------+ Loss: 0.2473306330436517\n",
            "+---------- EPOCH 91400 -----------+ Loss: 0.2473306316054537\n",
            "+---------- EPOCH 91450 -----------+ Loss: 0.24733063016882983\n",
            "+---------- EPOCH 91500 -----------+ Loss: 0.2473306287337772\n",
            "+---------- EPOCH 91550 -----------+ Loss: 0.24733062730029357\n",
            "+---------- EPOCH 91600 -----------+ Loss: 0.24733062586837615\n",
            "+---------- EPOCH 91650 -----------+ Loss: 0.24733062443802242\n",
            "+---------- EPOCH 91700 -----------+ Loss: 0.24733062300923003\n",
            "+---------- EPOCH 91750 -----------+ Loss: 0.24733062158199628\n",
            "+---------- EPOCH 91800 -----------+ Loss: 0.2473306201563186\n",
            "+---------- EPOCH 91850 -----------+ Loss: 0.24733061873219467\n",
            "+---------- EPOCH 91900 -----------+ Loss: 0.24733061730962175\n",
            "+---------- EPOCH 91950 -----------+ Loss: 0.24733061588859745\n",
            "+---------- EPOCH 92000 -----------+ Loss: 0.2473306144691191\n",
            "+---------- EPOCH 92050 -----------+ Loss: 0.24733061305118445\n",
            "+---------- EPOCH 92100 -----------+ Loss: 0.24733061163479086\n",
            "+---------- EPOCH 92150 -----------+ Loss: 0.2473306102199358\n",
            "+---------- EPOCH 92200 -----------+ Loss: 0.24733060880661686\n",
            "+---------- EPOCH 92250 -----------+ Loss: 0.24733060739483156\n",
            "+---------- EPOCH 92300 -----------+ Loss: 0.24733060598457746\n",
            "+---------- EPOCH 92350 -----------+ Loss: 0.2473306045758519\n",
            "+---------- EPOCH 92400 -----------+ Loss: 0.24733060316865263\n",
            "+---------- EPOCH 92450 -----------+ Loss: 0.247330601762977\n",
            "+---------- EPOCH 92500 -----------+ Loss: 0.2473306003588227\n",
            "+---------- EPOCH 92550 -----------+ Loss: 0.2473305989561873\n",
            "+---------- EPOCH 92600 -----------+ Loss: 0.24733059755506825\n",
            "+---------- EPOCH 92650 -----------+ Loss: 0.2473305961554631\n",
            "+---------- EPOCH 92700 -----------+ Loss: 0.2473305947573695\n",
            "+---------- EPOCH 92750 -----------+ Loss: 0.24733059336078494\n",
            "+---------- EPOCH 92800 -----------+ Loss: 0.24733059196570711\n",
            "+---------- EPOCH 92850 -----------+ Loss: 0.24733059057213347\n",
            "+---------- EPOCH 92900 -----------+ Loss: 0.24733058918006165\n",
            "+---------- EPOCH 92950 -----------+ Loss: 0.24733058778948933\n",
            "+---------- EPOCH 93000 -----------+ Loss: 0.24733058640041397\n",
            "+---------- EPOCH 93050 -----------+ Loss: 0.24733058501283323\n",
            "+---------- EPOCH 93100 -----------+ Loss: 0.24733058362674473\n",
            "+---------- EPOCH 93150 -----------+ Loss: 0.24733058224214602\n",
            "+---------- EPOCH 93200 -----------+ Loss: 0.24733058085903487\n",
            "+---------- EPOCH 93250 -----------+ Loss: 0.24733057947740872\n",
            "+---------- EPOCH 93300 -----------+ Loss: 0.24733057809726525\n",
            "+---------- EPOCH 93350 -----------+ Loss: 0.24733057671860217\n",
            "+---------- EPOCH 93400 -----------+ Loss: 0.24733057534141697\n",
            "+---------- EPOCH 93450 -----------+ Loss: 0.2473305739657075\n",
            "+---------- EPOCH 93500 -----------+ Loss: 0.24733057259147126\n",
            "+---------- EPOCH 93550 -----------+ Loss: 0.24733057121870589\n",
            "+---------- EPOCH 93600 -----------+ Loss: 0.24733056984740906\n",
            "+---------- EPOCH 93650 -----------+ Loss: 0.24733056847757856\n",
            "+---------- EPOCH 93700 -----------+ Loss: 0.24733056710921195\n",
            "+---------- EPOCH 93750 -----------+ Loss: 0.24733056574230677\n",
            "+---------- EPOCH 93800 -----------+ Loss: 0.247330564376861\n",
            "+---------- EPOCH 93850 -----------+ Loss: 0.2473305630128721\n",
            "+---------- EPOCH 93900 -----------+ Loss: 0.2473305616503378\n",
            "+---------- EPOCH 93950 -----------+ Loss: 0.2473305602892558\n",
            "+---------- EPOCH 94000 -----------+ Loss: 0.24733055892962388\n",
            "+---------- EPOCH 94050 -----------+ Loss: 0.24733055757143954\n",
            "+---------- EPOCH 94100 -----------+ Loss: 0.24733055621470068\n",
            "+---------- EPOCH 94150 -----------+ Loss: 0.247330554859405\n",
            "+---------- EPOCH 94200 -----------+ Loss: 0.24733055350555005\n",
            "+---------- EPOCH 94250 -----------+ Loss: 0.24733055215313368\n",
            "+---------- EPOCH 94300 -----------+ Loss: 0.24733055080215363\n",
            "+---------- EPOCH 94350 -----------+ Loss: 0.24733054945260752\n",
            "+---------- EPOCH 94400 -----------+ Loss: 0.2473305481044932\n",
            "+---------- EPOCH 94450 -----------+ Loss: 0.24733054675780836\n",
            "+---------- EPOCH 94500 -----------+ Loss: 0.24733054541255078\n",
            "+---------- EPOCH 94550 -----------+ Loss: 0.24733054406871816\n",
            "+---------- EPOCH 94600 -----------+ Loss: 0.24733054272630822\n",
            "+---------- EPOCH 94650 -----------+ Loss: 0.24733054138531885\n",
            "+---------- EPOCH 94700 -----------+ Loss: 0.24733054004574767\n",
            "+---------- EPOCH 94750 -----------+ Loss: 0.24733053870759258\n",
            "+---------- EPOCH 94800 -----------+ Loss: 0.2473305373708512\n",
            "+---------- EPOCH 94850 -----------+ Loss: 0.24733053603552138\n",
            "+---------- EPOCH 94900 -----------+ Loss: 0.247330534701601\n",
            "+---------- EPOCH 94950 -----------+ Loss: 0.2473305333690877\n",
            "+---------- EPOCH 95000 -----------+ Loss: 0.2473305320379794\n",
            "+---------- EPOCH 95050 -----------+ Loss: 0.2473305307082737\n",
            "+---------- EPOCH 95100 -----------+ Loss: 0.2473305293799686\n",
            "+---------- EPOCH 95150 -----------+ Loss: 0.2473305280530618\n",
            "+---------- EPOCH 95200 -----------+ Loss: 0.24733052672755118\n",
            "+---------- EPOCH 95250 -----------+ Loss: 0.24733052540343445\n",
            "+---------- EPOCH 95300 -----------+ Loss: 0.24733052408070955\n",
            "+---------- EPOCH 95350 -----------+ Loss: 0.24733052275937414\n",
            "+---------- EPOCH 95400 -----------+ Loss: 0.24733052143942635\n",
            "+---------- EPOCH 95450 -----------+ Loss: 0.24733052012086362\n",
            "+---------- EPOCH 95500 -----------+ Loss: 0.24733051880368406\n",
            "+---------- EPOCH 95550 -----------+ Loss: 0.24733051748788537\n",
            "+---------- EPOCH 95600 -----------+ Loss: 0.2473305161734655\n",
            "+---------- EPOCH 95650 -----------+ Loss: 0.24733051486042223\n",
            "+---------- EPOCH 95700 -----------+ Loss: 0.2473305135487535\n",
            "+---------- EPOCH 95750 -----------+ Loss: 0.24733051223845703\n",
            "+---------- EPOCH 95800 -----------+ Loss: 0.2473305109295308\n",
            "+---------- EPOCH 95850 -----------+ Loss: 0.2473305096219725\n",
            "+---------- EPOCH 95900 -----------+ Loss: 0.24733050831578035\n",
            "+---------- EPOCH 95950 -----------+ Loss: 0.24733050701095186\n",
            "+---------- EPOCH 96000 -----------+ Loss: 0.2473305057074852\n",
            "+---------- EPOCH 96050 -----------+ Loss: 0.24733050440537796\n",
            "+---------- EPOCH 96100 -----------+ Loss: 0.24733050310462826\n",
            "+---------- EPOCH 96150 -----------+ Loss: 0.24733050180523392\n",
            "+---------- EPOCH 96200 -----------+ Loss: 0.24733050050719285\n",
            "+---------- EPOCH 96250 -----------+ Loss: 0.24733049921050299\n",
            "+---------- EPOCH 96300 -----------+ Loss: 0.24733049791516204\n",
            "+---------- EPOCH 96350 -----------+ Loss: 0.24733049662116824\n",
            "+---------- EPOCH 96400 -----------+ Loss: 0.24733049532851925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 96450 -----------+ Loss: 0.24733049403721308\n",
            "+---------- EPOCH 96500 -----------+ Loss: 0.24733049274724778\n",
            "+---------- EPOCH 96550 -----------+ Loss: 0.247330491458621\n",
            "+---------- EPOCH 96600 -----------+ Loss: 0.2473304901713308\n",
            "+---------- EPOCH 96650 -----------+ Loss: 0.24733048888537518\n",
            "+---------- EPOCH 96700 -----------+ Loss: 0.247330487600752\n",
            "+---------- EPOCH 96750 -----------+ Loss: 0.24733048631745932\n",
            "+---------- EPOCH 96800 -----------+ Loss: 0.24733048503549493\n",
            "+---------- EPOCH 96850 -----------+ Loss: 0.24733048375485694\n",
            "+---------- EPOCH 96900 -----------+ Loss: 0.24733048247554318\n",
            "+---------- EPOCH 96950 -----------+ Loss: 0.24733048119755166\n",
            "+---------- EPOCH 97000 -----------+ Loss: 0.24733047992088036\n",
            "+---------- EPOCH 97050 -----------+ Loss: 0.24733047864552718\n",
            "+---------- EPOCH 97100 -----------+ Loss: 0.24733047737149022\n",
            "+---------- EPOCH 97150 -----------+ Loss: 0.24733047609876735\n",
            "+---------- EPOCH 97200 -----------+ Loss: 0.24733047482735665\n",
            "+---------- EPOCH 97250 -----------+ Loss: 0.24733047355725607\n",
            "+---------- EPOCH 97300 -----------+ Loss: 0.24733047228846355\n",
            "+---------- EPOCH 97350 -----------+ Loss: 0.24733047102097705\n",
            "+---------- EPOCH 97400 -----------+ Loss: 0.2473304697547947\n",
            "+---------- EPOCH 97450 -----------+ Loss: 0.2473304684899144\n",
            "+---------- EPOCH 97500 -----------+ Loss: 0.24733046722633417\n",
            "+---------- EPOCH 97550 -----------+ Loss: 0.2473304659640521\n",
            "+---------- EPOCH 97600 -----------+ Loss: 0.24733046470306616\n",
            "+---------- EPOCH 97650 -----------+ Loss: 0.2473304634433743\n",
            "+---------- EPOCH 97700 -----------+ Loss: 0.24733046218497465\n",
            "+---------- EPOCH 97750 -----------+ Loss: 0.24733046092786518\n",
            "+---------- EPOCH 97800 -----------+ Loss: 0.2473304596720439\n",
            "+---------- EPOCH 97850 -----------+ Loss: 0.24733045841750884\n",
            "+---------- EPOCH 97900 -----------+ Loss: 0.2473304571642581\n",
            "+---------- EPOCH 97950 -----------+ Loss: 0.24733045591228978\n",
            "+---------- EPOCH 98000 -----------+ Loss: 0.24733045466160178\n",
            "+---------- EPOCH 98050 -----------+ Loss: 0.24733045341219226\n",
            "+---------- EPOCH 98100 -----------+ Loss: 0.24733045216405913\n",
            "+---------- EPOCH 98150 -----------+ Loss: 0.24733045091720066\n",
            "+---------- EPOCH 98200 -----------+ Loss: 0.2473304496716148\n",
            "+---------- EPOCH 98250 -----------+ Loss: 0.2473304484272995\n",
            "+---------- EPOCH 98300 -----------+ Loss: 0.24733044718425307\n",
            "+---------- EPOCH 98350 -----------+ Loss: 0.2473304459424733\n",
            "+---------- EPOCH 98400 -----------+ Loss: 0.24733044470195856\n",
            "+---------- EPOCH 98450 -----------+ Loss: 0.24733044346270683\n",
            "+---------- EPOCH 98500 -----------+ Loss: 0.24733044222471604\n",
            "+---------- EPOCH 98550 -----------+ Loss: 0.24733044098798448\n",
            "+---------- EPOCH 98600 -----------+ Loss: 0.24733043975251018\n",
            "+---------- EPOCH 98650 -----------+ Loss: 0.24733043851829112\n",
            "+---------- EPOCH 98700 -----------+ Loss: 0.24733043728532567\n",
            "+---------- EPOCH 98750 -----------+ Loss: 0.24733043605361169\n",
            "+---------- EPOCH 98800 -----------+ Loss: 0.24733043482314745\n",
            "+---------- EPOCH 98850 -----------+ Loss: 0.24733043359393092\n",
            "+---------- EPOCH 98900 -----------+ Loss: 0.24733043236596028\n",
            "+---------- EPOCH 98950 -----------+ Loss: 0.24733043113923367\n",
            "+---------- EPOCH 99000 -----------+ Loss: 0.2473304299137492\n",
            "+---------- EPOCH 99050 -----------+ Loss: 0.24733042868950494\n",
            "+---------- EPOCH 99100 -----------+ Loss: 0.24733042746649914\n",
            "+---------- EPOCH 99150 -----------+ Loss: 0.2473304262447298\n",
            "+---------- EPOCH 99200 -----------+ Loss: 0.24733042502419525\n",
            "+---------- EPOCH 99250 -----------+ Loss: 0.2473304238048934\n",
            "+---------- EPOCH 99300 -----------+ Loss: 0.2473304225868226\n",
            "+---------- EPOCH 99350 -----------+ Loss: 0.2473304213699809\n",
            "+---------- EPOCH 99400 -----------+ Loss: 0.24733042015436638\n",
            "+---------- EPOCH 99450 -----------+ Loss: 0.24733041893997731\n",
            "+---------- EPOCH 99500 -----------+ Loss: 0.24733041772681189\n",
            "+---------- EPOCH 99550 -----------+ Loss: 0.24733041651486812\n",
            "+---------- EPOCH 99600 -----------+ Loss: 0.24733041530414435\n",
            "+---------- EPOCH 99650 -----------+ Loss: 0.2473304140946386\n",
            "+---------- EPOCH 99700 -----------+ Loss: 0.2473304128863491\n",
            "+---------- EPOCH 99750 -----------+ Loss: 0.24733041167927414\n",
            "+---------- EPOCH 99800 -----------+ Loss: 0.2473304104734118\n",
            "+---------- EPOCH 99850 -----------+ Loss: 0.2473304092687602\n",
            "+---------- EPOCH 99900 -----------+ Loss: 0.2473304080653176\n",
            "+---------- EPOCH 99950 -----------+ Loss: 0.24733040686308227\n",
            "+---------- EPOCH 100000 -----------+ Loss: 0.24733040566205222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GGT1oRzXw3H9"
      },
      "cell_type": "markdown",
      "source": [
        "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
        "\n",
        "- Use the Heart Disease Dataset (binary classification)\n",
        "- Use an appropriate loss function for a binary classification task\n",
        "- Use an appropriate activation function on the final layer of your network. \n",
        "- Train your model using verbose output for ease of grading.\n",
        "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
        "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
        "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
        "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XWw4IYxLxKwH",
        "colab": {},
        "outputId": "436ba9ca-801a-4142-c646-6de743a634f7"
      },
      "cell_type": "code",
      "source": [
        "def baseline_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(13, input_dim=X.shape[1], activation='relu'))\n",
        "  model.add(Dense(8, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "seed = 101\n",
        "np.random.seed(seed)\n",
        "estimator = KerasClassifier(build_fn=baseline_model, \n",
        "                                     epochs=150,\n",
        "                                     batch_size=32,\n",
        "                                     verbose=1)\n",
        "kfold = StratifiedKFold(n_splits=5, random_state=seed)\n",
        "results = cross_val_score(estimator, X, y, cv=kfold)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 6.9056 - acc: 0.5455\n",
            "Epoch 2/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 6.1594 - acc: 0.5413\n",
            "Epoch 3/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 4.9671 - acc: 0.5744\n",
            "Epoch 4/150\n",
            "242/242 [==============================] - 0s 75us/step - loss: 3.9814 - acc: 0.5124\n",
            "Epoch 5/150\n",
            "242/242 [==============================] - 0s 72us/step - loss: 3.3002 - acc: 0.5124\n",
            "Epoch 6/150\n",
            "242/242 [==============================] - 0s 63us/step - loss: 2.9909 - acc: 0.4917\n",
            "Epoch 7/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 2.5253 - acc: 0.4917\n",
            "Epoch 8/150\n",
            "242/242 [==============================] - 0s 71us/step - loss: 2.0864 - acc: 0.5165\n",
            "Epoch 9/150\n",
            "242/242 [==============================] - 0s 64us/step - loss: 1.6528 - acc: 0.5744\n",
            "Epoch 10/150\n",
            "242/242 [==============================] - 0s 72us/step - loss: 1.2276 - acc: 0.6074\n",
            "Epoch 11/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.8600 - acc: 0.6612\n",
            "Epoch 12/150\n",
            "242/242 [==============================] - 0s 74us/step - loss: 0.7857 - acc: 0.6901\n",
            "Epoch 13/150\n",
            "242/242 [==============================] - 0s 73us/step - loss: 0.7412 - acc: 0.6901\n",
            "Epoch 14/150\n",
            "242/242 [==============================] - 0s 64us/step - loss: 0.7227 - acc: 0.6818\n",
            "Epoch 15/150\n",
            "242/242 [==============================] - 0s 69us/step - loss: 0.7068 - acc: 0.6860\n",
            "Epoch 16/150\n",
            "242/242 [==============================] - 0s 64us/step - loss: 0.6884 - acc: 0.6777\n",
            "Epoch 17/150\n",
            "242/242 [==============================] - 0s 82us/step - loss: 0.6769 - acc: 0.6777\n",
            "Epoch 18/150\n",
            "242/242 [==============================] - 0s 67us/step - loss: 0.6707 - acc: 0.6777\n",
            "Epoch 19/150\n",
            "242/242 [==============================] - 0s 77us/step - loss: 0.6575 - acc: 0.6818\n",
            "Epoch 20/150\n",
            "242/242 [==============================] - 0s 79us/step - loss: 0.6458 - acc: 0.6736\n",
            "Epoch 21/150\n",
            "242/242 [==============================] - 0s 66us/step - loss: 0.6368 - acc: 0.6777\n",
            "Epoch 22/150\n",
            "242/242 [==============================] - 0s 67us/step - loss: 0.6297 - acc: 0.6736\n",
            "Epoch 23/150\n",
            "242/242 [==============================] - 0s 74us/step - loss: 0.6211 - acc: 0.6736\n",
            "Epoch 24/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.6166 - acc: 0.6818\n",
            "Epoch 25/150\n",
            "242/242 [==============================] - 0s 74us/step - loss: 0.6115 - acc: 0.6818\n",
            "Epoch 26/150\n",
            "242/242 [==============================] - ETA: 0s - loss: 0.5468 - acc: 0.625 - 0s 70us/step - loss: 0.6096 - acc: 0.6736\n",
            "Epoch 27/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.6067 - acc: 0.6983\n",
            "Epoch 28/150\n",
            "242/242 [==============================] - 0s 69us/step - loss: 0.6003 - acc: 0.6901\n",
            "Epoch 29/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.5993 - acc: 0.6653\n",
            "Epoch 30/150\n",
            "242/242 [==============================] - 0s 75us/step - loss: 0.5944 - acc: 0.6901\n",
            "Epoch 31/150\n",
            "242/242 [==============================] - 0s 76us/step - loss: 0.5918 - acc: 0.7025\n",
            "Epoch 32/150\n",
            "242/242 [==============================] - 0s 53us/step - loss: 0.5913 - acc: 0.7066\n",
            "Epoch 33/150\n",
            "242/242 [==============================] - 0s 71us/step - loss: 0.5931 - acc: 0.6983\n",
            "Epoch 34/150\n",
            "242/242 [==============================] - 0s 65us/step - loss: 0.5933 - acc: 0.6901\n",
            "Epoch 35/150\n",
            "242/242 [==============================] - 0s 69us/step - loss: 0.5871 - acc: 0.7149\n",
            "Epoch 36/150\n",
            "242/242 [==============================] - 0s 77us/step - loss: 0.5855 - acc: 0.7149\n",
            "Epoch 37/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.5845 - acc: 0.6860\n",
            "Epoch 38/150\n",
            "242/242 [==============================] - 0s 72us/step - loss: 0.5826 - acc: 0.7025\n",
            "Epoch 39/150\n",
            "242/242 [==============================] - 0s 71us/step - loss: 0.5837 - acc: 0.7025\n",
            "Epoch 40/150\n",
            "242/242 [==============================] - 0s 53us/step - loss: 0.5783 - acc: 0.7066\n",
            "Epoch 41/150\n",
            "242/242 [==============================] - 0s 71us/step - loss: 0.5809 - acc: 0.7066\n",
            "Epoch 42/150\n",
            "242/242 [==============================] - 0s 51us/step - loss: 0.5775 - acc: 0.7149\n",
            "Epoch 43/150\n",
            "242/242 [==============================] - 0s 69us/step - loss: 0.5818 - acc: 0.6901\n",
            "Epoch 44/150\n",
            "242/242 [==============================] - 0s 55us/step - loss: 0.5806 - acc: 0.7149\n",
            "Epoch 45/150\n",
            "242/242 [==============================] - 0s 81us/step - loss: 0.5750 - acc: 0.7066\n",
            "Epoch 46/150\n",
            "242/242 [==============================] - 0s 63us/step - loss: 0.5767 - acc: 0.7025\n",
            "Epoch 47/150\n",
            "242/242 [==============================] - 0s 80us/step - loss: 0.5754 - acc: 0.7107\n",
            "Epoch 48/150\n",
            "242/242 [==============================] - 0s 72us/step - loss: 0.5763 - acc: 0.7149\n",
            "Epoch 49/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.5766 - acc: 0.7025\n",
            "Epoch 50/150\n",
            "242/242 [==============================] - 0s 73us/step - loss: 0.5770 - acc: 0.7149\n",
            "Epoch 51/150\n",
            "242/242 [==============================] - 0s 69us/step - loss: 0.5727 - acc: 0.7107\n",
            "Epoch 52/150\n",
            "242/242 [==============================] - 0s 67us/step - loss: 0.5719 - acc: 0.7190\n",
            "Epoch 53/150\n",
            "242/242 [==============================] - 0s 68us/step - loss: 0.5751 - acc: 0.7231\n",
            "Epoch 54/150\n",
            "242/242 [==============================] - 0s 66us/step - loss: 0.5695 - acc: 0.7149\n",
            "Epoch 55/150\n",
            "242/242 [==============================] - 0s 76us/step - loss: 0.5693 - acc: 0.7190\n",
            "Epoch 56/150\n",
            "242/242 [==============================] - 0s 75us/step - loss: 0.5682 - acc: 0.7149\n",
            "Epoch 57/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.5683 - acc: 0.7231\n",
            "Epoch 58/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.5660 - acc: 0.7149\n",
            "Epoch 59/150\n",
            "242/242 [==============================] - 0s 63us/step - loss: 0.5677 - acc: 0.7149\n",
            "Epoch 60/150\n",
            "242/242 [==============================] - 0s 66us/step - loss: 0.5673 - acc: 0.7149\n",
            "Epoch 61/150\n",
            "242/242 [==============================] - 0s 72us/step - loss: 0.5652 - acc: 0.7149\n",
            "Epoch 62/150\n",
            "242/242 [==============================] - 0s 83us/step - loss: 0.5648 - acc: 0.7231\n",
            "Epoch 63/150\n",
            "242/242 [==============================] - 0s 65us/step - loss: 0.5651 - acc: 0.7273\n",
            "Epoch 64/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.5626 - acc: 0.7190\n",
            "Epoch 65/150\n",
            "242/242 [==============================] - 0s 67us/step - loss: 0.5648 - acc: 0.7107\n",
            "Epoch 66/150\n",
            "242/242 [==============================] - 0s 66us/step - loss: 0.5611 - acc: 0.7149\n",
            "Epoch 67/150\n",
            "242/242 [==============================] - 0s 69us/step - loss: 0.5612 - acc: 0.7107\n",
            "Epoch 68/150\n",
            "242/242 [==============================] - 0s 66us/step - loss: 0.5591 - acc: 0.7190\n",
            "Epoch 69/150\n",
            "242/242 [==============================] - 0s 70us/step - loss: 0.5645 - acc: 0.7190\n",
            "Epoch 70/150\n",
            "242/242 [==============================] - 0s 70us/step - loss: 0.5627 - acc: 0.7273\n",
            "Epoch 71/150\n",
            "242/242 [==============================] - 0s 55us/step - loss: 0.5584 - acc: 0.7231\n",
            "Epoch 72/150\n",
            "242/242 [==============================] - 0s 65us/step - loss: 0.5644 - acc: 0.7149\n",
            "Epoch 73/150\n",
            "242/242 [==============================] - 0s 66us/step - loss: 0.5597 - acc: 0.7066\n",
            "Epoch 74/150\n",
            "242/242 [==============================] - 0s 63us/step - loss: 0.5551 - acc: 0.7190\n",
            "Epoch 75/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.5687 - acc: 0.7355\n",
            "Epoch 76/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.5572 - acc: 0.7066\n",
            "Epoch 77/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.5596 - acc: 0.7149\n",
            "Epoch 78/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.5635 - acc: 0.7397\n",
            "Epoch 79/150\n",
            "242/242 [==============================] - 0s 53us/step - loss: 0.5617 - acc: 0.6983\n",
            "Epoch 80/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.5520 - acc: 0.7190\n",
            "Epoch 81/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.5559 - acc: 0.7273\n",
            "Epoch 82/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.5590 - acc: 0.6983\n",
            "Epoch 83/150\n",
            "242/242 [==============================] - 0s 53us/step - loss: 0.5713 - acc: 0.7190\n",
            "Epoch 84/150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 0s 68us/step - loss: 0.5813 - acc: 0.6983\n",
            "Epoch 85/150\n",
            "242/242 [==============================] - 0s 55us/step - loss: 0.5581 - acc: 0.7314\n",
            "Epoch 86/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.5600 - acc: 0.7231\n",
            "Epoch 87/150\n",
            "242/242 [==============================] - 0s 69us/step - loss: 0.5496 - acc: 0.7190\n",
            "Epoch 88/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.5584 - acc: 0.7273\n",
            "Epoch 89/150\n",
            "242/242 [==============================] - 0s 54us/step - loss: 0.5522 - acc: 0.7355\n",
            "Epoch 90/150\n",
            "242/242 [==============================] - 0s 65us/step - loss: 0.5469 - acc: 0.7397\n",
            "Epoch 91/150\n",
            "242/242 [==============================] - 0s 73us/step - loss: 0.5503 - acc: 0.7066\n",
            "Epoch 92/150\n",
            "242/242 [==============================] - 0s 67us/step - loss: 0.5549 - acc: 0.7355\n",
            "Epoch 93/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.5466 - acc: 0.7149\n",
            "Epoch 94/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.5504 - acc: 0.7273\n",
            "Epoch 95/150\n",
            "242/242 [==============================] - 0s 53us/step - loss: 0.5455 - acc: 0.7314\n",
            "Epoch 96/150\n",
            "242/242 [==============================] - 0s 63us/step - loss: 0.5452 - acc: 0.7231\n",
            "Epoch 97/150\n",
            "242/242 [==============================] - 0s 64us/step - loss: 0.5436 - acc: 0.7273\n",
            "Epoch 98/150\n",
            "242/242 [==============================] - 0s 63us/step - loss: 0.5460 - acc: 0.7231\n",
            "Epoch 99/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.5441 - acc: 0.7355\n",
            "Epoch 100/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.5433 - acc: 0.7314\n",
            "Epoch 101/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.5414 - acc: 0.7190\n",
            "Epoch 102/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.5432 - acc: 0.7521\n",
            "Epoch 103/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.5664 - acc: 0.6777\n",
            "Epoch 104/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.5382 - acc: 0.7355\n",
            "Epoch 105/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.5437 - acc: 0.7479\n",
            "Epoch 106/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.5494 - acc: 0.7066\n",
            "Epoch 107/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.5320 - acc: 0.7355\n",
            "Epoch 108/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.5454 - acc: 0.7355\n",
            "Epoch 109/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.5347 - acc: 0.7355\n",
            "Epoch 110/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.5357 - acc: 0.7397\n",
            "Epoch 111/150\n",
            "242/242 [==============================] - 0s 53us/step - loss: 0.5363 - acc: 0.7190\n",
            "Epoch 112/150\n",
            "242/242 [==============================] - 0s 66us/step - loss: 0.5468 - acc: 0.7479\n",
            "Epoch 113/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.5383 - acc: 0.7231\n",
            "Epoch 114/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.5435 - acc: 0.7355\n",
            "Epoch 115/150\n",
            "242/242 [==============================] - 0s 64us/step - loss: 0.5316 - acc: 0.7314\n",
            "Epoch 116/150\n",
            "242/242 [==============================] - 0s 70us/step - loss: 0.5399 - acc: 0.7107\n",
            "Epoch 117/150\n",
            "242/242 [==============================] - 0s 76us/step - loss: 0.5310 - acc: 0.7355\n",
            "Epoch 118/150\n",
            "242/242 [==============================] - 0s 72us/step - loss: 0.5330 - acc: 0.7231\n",
            "Epoch 119/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.5422 - acc: 0.7521\n",
            "Epoch 120/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.5343 - acc: 0.7231\n",
            "Epoch 121/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.5331 - acc: 0.7314\n",
            "Epoch 122/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.5302 - acc: 0.7438\n",
            "Epoch 123/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.5437 - acc: 0.7438\n",
            "Epoch 124/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.5409 - acc: 0.7231\n",
            "Epoch 125/150\n",
            "242/242 [==============================] - 0s 55us/step - loss: 0.5292 - acc: 0.7479\n",
            "Epoch 126/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.5301 - acc: 0.7521\n",
            "Epoch 127/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.5296 - acc: 0.7149\n",
            "Epoch 128/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.5273 - acc: 0.7438\n",
            "Epoch 129/150\n",
            "242/242 [==============================] - 0s 65us/step - loss: 0.5221 - acc: 0.7562\n",
            "Epoch 130/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.5281 - acc: 0.7273\n",
            "Epoch 131/150\n",
            "242/242 [==============================] - 0s 65us/step - loss: 0.5217 - acc: 0.7314\n",
            "Epoch 132/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.5317 - acc: 0.7479\n",
            "Epoch 133/150\n",
            "242/242 [==============================] - 0s 64us/step - loss: 0.5170 - acc: 0.7314\n",
            "Epoch 134/150\n",
            "242/242 [==============================] - 0s 64us/step - loss: 0.5332 - acc: 0.7107\n",
            "Epoch 135/150\n",
            "242/242 [==============================] - 0s 64us/step - loss: 0.5338 - acc: 0.7397\n",
            "Epoch 136/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.5185 - acc: 0.7355\n",
            "Epoch 137/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.5208 - acc: 0.7314\n",
            "Epoch 138/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.5186 - acc: 0.7479\n",
            "Epoch 139/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.5174 - acc: 0.7438\n",
            "Epoch 140/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.5174 - acc: 0.7562\n",
            "Epoch 141/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.5179 - acc: 0.7521\n",
            "Epoch 142/150\n",
            "242/242 [==============================] - 0s 63us/step - loss: 0.5235 - acc: 0.7355\n",
            "Epoch 143/150\n",
            "242/242 [==============================] - 0s 64us/step - loss: 0.5132 - acc: 0.7562\n",
            "Epoch 144/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.5215 - acc: 0.7438\n",
            "Epoch 145/150\n",
            "242/242 [==============================] - 0s 64us/step - loss: 0.5114 - acc: 0.7562\n",
            "Epoch 146/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.5153 - acc: 0.7603\n",
            "Epoch 147/150\n",
            "242/242 [==============================] - 0s 70us/step - loss: 0.5124 - acc: 0.7397\n",
            "Epoch 148/150\n",
            "242/242 [==============================] - 0s 68us/step - loss: 0.5111 - acc: 0.7438\n",
            "Epoch 149/150\n",
            "242/242 [==============================] - 0s 63us/step - loss: 0.5110 - acc: 0.7562\n",
            "Epoch 150/150\n",
            "242/242 [==============================] - 0s 55us/step - loss: 0.5094 - acc: 0.7397\n",
            "61/61 [==============================] - 0s 6ms/step\n",
            "Epoch 1/150\n",
            "242/242 [==============================] - 1s 5ms/step - loss: 4.8173 - acc: 0.4587\n",
            "Epoch 2/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 1.8192 - acc: 0.6157\n",
            "Epoch 3/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 1.6701 - acc: 0.6364\n",
            "Epoch 4/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 1.4092 - acc: 0.6488\n",
            "Epoch 5/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 1.1453 - acc: 0.6570\n",
            "Epoch 6/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.9976 - acc: 0.6777\n",
            "Epoch 7/150\n",
            "242/242 [==============================] - 0s 53us/step - loss: 0.9469 - acc: 0.6694\n",
            "Epoch 8/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.8395 - acc: 0.6860\n",
            "Epoch 9/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.7912 - acc: 0.6901\n",
            "Epoch 10/150\n",
            "242/242 [==============================] - 0s 53us/step - loss: 0.7640 - acc: 0.6942\n",
            "Epoch 11/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.7236 - acc: 0.7273\n",
            "Epoch 12/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.7083 - acc: 0.7066\n",
            "Epoch 13/150\n",
            "242/242 [==============================] - 0s 54us/step - loss: 0.6997 - acc: 0.6983\n",
            "Epoch 14/150\n",
            "242/242 [==============================] - 0s 54us/step - loss: 0.6987 - acc: 0.7149\n",
            "Epoch 15/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.6752 - acc: 0.7066\n",
            "Epoch 16/150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 0s 58us/step - loss: 0.6753 - acc: 0.6942\n",
            "Epoch 17/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.6606 - acc: 0.7149\n",
            "Epoch 18/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.6591 - acc: 0.7273\n",
            "Epoch 19/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.6528 - acc: 0.7107\n",
            "Epoch 20/150\n",
            "242/242 [==============================] - 0s 68us/step - loss: 0.6487 - acc: 0.7025\n",
            "Epoch 21/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.6496 - acc: 0.7107\n",
            "Epoch 22/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.6393 - acc: 0.7231\n",
            "Epoch 23/150\n",
            "242/242 [==============================] - 0s 54us/step - loss: 0.6363 - acc: 0.7231\n",
            "Epoch 24/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.6267 - acc: 0.7273\n",
            "Epoch 25/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.6415 - acc: 0.7025\n",
            "Epoch 26/150\n",
            "242/242 [==============================] - 0s 63us/step - loss: 0.6264 - acc: 0.7231\n",
            "Epoch 27/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.6157 - acc: 0.7438\n",
            "Epoch 28/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.6291 - acc: 0.7107\n",
            "Epoch 29/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.6284 - acc: 0.7149\n",
            "Epoch 30/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.6362 - acc: 0.7066\n",
            "Epoch 31/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.6209 - acc: 0.7107\n",
            "Epoch 32/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.6019 - acc: 0.7231\n",
            "Epoch 33/150\n",
            "242/242 [==============================] - 0s 55us/step - loss: 0.5828 - acc: 0.7521\n",
            "Epoch 34/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.5668 - acc: 0.7438\n",
            "Epoch 35/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.5553 - acc: 0.7521\n",
            "Epoch 36/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.5729 - acc: 0.7438\n",
            "Epoch 37/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.5736 - acc: 0.7190\n",
            "Epoch 38/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.5529 - acc: 0.7314\n",
            "Epoch 39/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.5298 - acc: 0.7479\n",
            "Epoch 40/150\n",
            "242/242 [==============================] - 0s 55us/step - loss: 0.5210 - acc: 0.7645\n",
            "Epoch 41/150\n",
            "242/242 [==============================] - 0s 55us/step - loss: 0.5183 - acc: 0.7603\n",
            "Epoch 42/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.5182 - acc: 0.7479\n",
            "Epoch 43/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.5015 - acc: 0.7645\n",
            "Epoch 44/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.5000 - acc: 0.7521\n",
            "Epoch 45/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4994 - acc: 0.7603\n",
            "Epoch 46/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.5204 - acc: 0.7231\n",
            "Epoch 47/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.5202 - acc: 0.7521\n",
            "Epoch 48/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.5497 - acc: 0.7190\n",
            "Epoch 49/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.4805 - acc: 0.7686\n",
            "Epoch 50/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4759 - acc: 0.7810\n",
            "Epoch 51/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.4721 - acc: 0.7893\n",
            "Epoch 52/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4673 - acc: 0.7769\n",
            "Epoch 53/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.4743 - acc: 0.7851\n",
            "Epoch 54/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4909 - acc: 0.7479\n",
            "Epoch 55/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.4737 - acc: 0.7769\n",
            "Epoch 56/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4645 - acc: 0.7893\n",
            "Epoch 57/150\n",
            "242/242 [==============================] - 0s 63us/step - loss: 0.4582 - acc: 0.8017\n",
            "Epoch 58/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4593 - acc: 0.7893\n",
            "Epoch 59/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4569 - acc: 0.7851\n",
            "Epoch 60/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4540 - acc: 0.7975\n",
            "Epoch 61/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.4547 - acc: 0.7893\n",
            "Epoch 62/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4500 - acc: 0.7934\n",
            "Epoch 63/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.4538 - acc: 0.7893\n",
            "Epoch 64/150\n",
            "242/242 [==============================] - 0s 55us/step - loss: 0.4575 - acc: 0.7810\n",
            "Epoch 65/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4465 - acc: 0.7934\n",
            "Epoch 66/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4558 - acc: 0.7934\n",
            "Epoch 67/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.4464 - acc: 0.8058\n",
            "Epoch 68/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4485 - acc: 0.7975\n",
            "Epoch 69/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.4458 - acc: 0.7810\n",
            "Epoch 70/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4615 - acc: 0.8017\n",
            "Epoch 71/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4552 - acc: 0.7851\n",
            "Epoch 72/150\n",
            "242/242 [==============================] - 0s 54us/step - loss: 0.4380 - acc: 0.8099\n",
            "Epoch 73/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4336 - acc: 0.8058\n",
            "Epoch 74/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4344 - acc: 0.7975\n",
            "Epoch 75/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4382 - acc: 0.8017\n",
            "Epoch 76/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4640 - acc: 0.7603\n",
            "Epoch 77/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.4537 - acc: 0.7727\n",
            "Epoch 78/150\n",
            "242/242 [==============================] - 0s 68us/step - loss: 0.4267 - acc: 0.8140\n",
            "Epoch 79/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.4489 - acc: 0.8058\n",
            "Epoch 80/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.4627 - acc: 0.7810\n",
            "Epoch 81/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.4437 - acc: 0.7851\n",
            "Epoch 82/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4649 - acc: 0.7810\n",
            "Epoch 83/150\n",
            "242/242 [==============================] - 0s 54us/step - loss: 0.4486 - acc: 0.7686\n",
            "Epoch 84/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4352 - acc: 0.7851\n",
            "Epoch 85/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4276 - acc: 0.8140\n",
            "Epoch 86/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4185 - acc: 0.8058\n",
            "Epoch 87/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4231 - acc: 0.7893\n",
            "Epoch 88/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4263 - acc: 0.8058\n",
            "Epoch 89/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.4258 - acc: 0.8058\n",
            "Epoch 90/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4184 - acc: 0.8182\n",
            "Epoch 91/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4180 - acc: 0.8058\n",
            "Epoch 92/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4223 - acc: 0.8182\n",
            "Epoch 93/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4167 - acc: 0.8099\n",
            "Epoch 94/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.4177 - acc: 0.8017\n",
            "Epoch 95/150\n",
            "242/242 [==============================] - 0s 55us/step - loss: 0.4220 - acc: 0.8223\n",
            "Epoch 96/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4125 - acc: 0.8099\n",
            "Epoch 97/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4178 - acc: 0.8264\n",
            "Epoch 98/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4131 - acc: 0.8099\n",
            "Epoch 99/150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 0s 59us/step - loss: 0.4110 - acc: 0.8099\n",
            "Epoch 100/150\n",
            "242/242 [==============================] - 0s 52us/step - loss: 0.4169 - acc: 0.8182\n",
            "Epoch 101/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4080 - acc: 0.8347\n",
            "Epoch 102/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4089 - acc: 0.8223\n",
            "Epoch 103/150\n",
            "242/242 [==============================] - 0s 64us/step - loss: 0.4112 - acc: 0.8264\n",
            "Epoch 104/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4336 - acc: 0.7975\n",
            "Epoch 105/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.4037 - acc: 0.8223\n",
            "Epoch 106/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4052 - acc: 0.8347\n",
            "Epoch 107/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4039 - acc: 0.8264\n",
            "Epoch 108/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4056 - acc: 0.8306\n",
            "Epoch 109/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4058 - acc: 0.8223\n",
            "Epoch 110/150\n",
            "242/242 [==============================] - 0s 52us/step - loss: 0.4010 - acc: 0.8223\n",
            "Epoch 111/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4076 - acc: 0.8264\n",
            "Epoch 112/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.3987 - acc: 0.8347\n",
            "Epoch 113/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.3986 - acc: 0.8306\n",
            "Epoch 114/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4033 - acc: 0.8264\n",
            "Epoch 115/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4137 - acc: 0.8347\n",
            "Epoch 116/150\n",
            "242/242 [==============================] - 0s 54us/step - loss: 0.4135 - acc: 0.8099\n",
            "Epoch 117/150\n",
            "242/242 [==============================] - 0s 64us/step - loss: 0.4416 - acc: 0.7975\n",
            "Epoch 118/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4279 - acc: 0.8140\n",
            "Epoch 119/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.3952 - acc: 0.8264\n",
            "Epoch 120/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4206 - acc: 0.8058\n",
            "Epoch 121/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.3999 - acc: 0.8264\n",
            "Epoch 122/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.4055 - acc: 0.8140\n",
            "Epoch 123/150\n",
            "242/242 [==============================] - 0s 63us/step - loss: 0.4269 - acc: 0.7893\n",
            "Epoch 124/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4170 - acc: 0.7810\n",
            "Epoch 125/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4404 - acc: 0.7727\n",
            "Epoch 126/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4137 - acc: 0.8347\n",
            "Epoch 127/150\n",
            "242/242 [==============================] - 0s 55us/step - loss: 0.3938 - acc: 0.8306\n",
            "Epoch 128/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.3915 - acc: 0.8306\n",
            "Epoch 129/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4079 - acc: 0.8140\n",
            "Epoch 130/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4060 - acc: 0.8182\n",
            "Epoch 131/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.3912 - acc: 0.8264\n",
            "Epoch 132/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.3952 - acc: 0.8306\n",
            "Epoch 133/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.3941 - acc: 0.8264\n",
            "Epoch 134/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.3950 - acc: 0.8306\n",
            "Epoch 135/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4078 - acc: 0.8140\n",
            "Epoch 136/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.3952 - acc: 0.8306\n",
            "Epoch 137/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.3924 - acc: 0.8264\n",
            "Epoch 138/150\n",
            "242/242 [==============================] - 0s 54us/step - loss: 0.3878 - acc: 0.8430\n",
            "Epoch 139/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.3869 - acc: 0.8347\n",
            "Epoch 140/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.3885 - acc: 0.8306\n",
            "Epoch 141/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.3903 - acc: 0.8306\n",
            "Epoch 142/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.3888 - acc: 0.8182\n",
            "Epoch 143/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.3930 - acc: 0.8264\n",
            "Epoch 144/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.3904 - acc: 0.8347\n",
            "Epoch 145/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.3886 - acc: 0.8140\n",
            "Epoch 146/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.3833 - acc: 0.8347\n",
            "Epoch 147/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.3812 - acc: 0.8347\n",
            "Epoch 148/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.3921 - acc: 0.8347\n",
            "Epoch 149/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4063 - acc: 0.8017\n",
            "Epoch 150/150\n",
            "242/242 [==============================] - 0s 64us/step - loss: 0.4052 - acc: 0.8058\n",
            "61/61 [==============================] - 0s 6ms/step\n",
            "Epoch 1/150\n",
            "242/242 [==============================] - 1s 5ms/step - loss: 3.8059 - acc: 0.5455\n",
            "Epoch 2/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 1.0800 - acc: 0.6364\n",
            "Epoch 3/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 1.1337 - acc: 0.5455\n",
            "Epoch 4/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.7317 - acc: 0.6488\n",
            "Epoch 5/150\n",
            "242/242 [==============================] - 0s 55us/step - loss: 0.8213 - acc: 0.6322\n",
            "Epoch 6/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.7179 - acc: 0.6694\n",
            "Epoch 7/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.7221 - acc: 0.6570\n",
            "Epoch 8/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.6900 - acc: 0.6364\n",
            "Epoch 9/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.6639 - acc: 0.6529\n",
            "Epoch 10/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.6540 - acc: 0.6777\n",
            "Epoch 11/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.6451 - acc: 0.6736\n",
            "Epoch 12/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.6411 - acc: 0.6694\n",
            "Epoch 13/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.6368 - acc: 0.6694\n",
            "Epoch 14/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.6273 - acc: 0.6901\n",
            "Epoch 15/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.6276 - acc: 0.6860\n",
            "Epoch 16/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.6285 - acc: 0.6694\n",
            "Epoch 17/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.6183 - acc: 0.6983\n",
            "Epoch 18/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.6177 - acc: 0.6942\n",
            "Epoch 19/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.6216 - acc: 0.6529\n",
            "Epoch 20/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.6134 - acc: 0.7149\n",
            "Epoch 21/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.6107 - acc: 0.7107\n",
            "Epoch 22/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.6075 - acc: 0.6942\n",
            "Epoch 23/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.6036 - acc: 0.6942\n",
            "Epoch 24/150\n",
            "242/242 [==============================] - 0s 54us/step - loss: 0.6010 - acc: 0.6983\n",
            "Epoch 25/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.5948 - acc: 0.7066\n",
            "Epoch 26/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.5969 - acc: 0.6694\n",
            "Epoch 27/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.6014 - acc: 0.7107\n",
            "Epoch 28/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.5889 - acc: 0.7066\n",
            "Epoch 29/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.5981 - acc: 0.7066\n",
            "Epoch 30/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.5929 - acc: 0.6860\n",
            "Epoch 31/150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 0s 56us/step - loss: 0.5872 - acc: 0.7190\n",
            "Epoch 32/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.5817 - acc: 0.7107\n",
            "Epoch 33/150\n",
            "242/242 [==============================] - 0s 63us/step - loss: 0.5822 - acc: 0.7107\n",
            "Epoch 34/150\n",
            "242/242 [==============================] - 0s 68us/step - loss: 0.5765 - acc: 0.7107\n",
            "Epoch 35/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.5964 - acc: 0.7107\n",
            "Epoch 36/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.5974 - acc: 0.7066\n",
            "Epoch 37/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.5719 - acc: 0.7149\n",
            "Epoch 38/150\n",
            "242/242 [==============================] - 0s 64us/step - loss: 0.5790 - acc: 0.7066\n",
            "Epoch 39/150\n",
            "242/242 [==============================] - 0s 64us/step - loss: 0.5835 - acc: 0.7190\n",
            "Epoch 40/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.5640 - acc: 0.7107\n",
            "Epoch 41/150\n",
            "242/242 [==============================] - 0s 54us/step - loss: 0.5665 - acc: 0.7025\n",
            "Epoch 42/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.5675 - acc: 0.7231\n",
            "Epoch 43/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.5585 - acc: 0.7190\n",
            "Epoch 44/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.5571 - acc: 0.7107\n",
            "Epoch 45/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.5655 - acc: 0.7190\n",
            "Epoch 46/150\n",
            "242/242 [==============================] - 0s 65us/step - loss: 0.5554 - acc: 0.7273\n",
            "Epoch 47/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.5515 - acc: 0.7355\n",
            "Epoch 48/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.5490 - acc: 0.7231\n",
            "Epoch 49/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.5645 - acc: 0.7107\n",
            "Epoch 50/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.5537 - acc: 0.7190\n",
            "Epoch 51/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.5502 - acc: 0.7397\n",
            "Epoch 52/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.5476 - acc: 0.7397\n",
            "Epoch 53/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.5421 - acc: 0.7438\n",
            "Epoch 54/150\n",
            "242/242 [==============================] - 0s 64us/step - loss: 0.5578 - acc: 0.7231\n",
            "Epoch 55/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.5523 - acc: 0.7314\n",
            "Epoch 56/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.5450 - acc: 0.7314\n",
            "Epoch 57/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.5364 - acc: 0.7314\n",
            "Epoch 58/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.5366 - acc: 0.7273\n",
            "Epoch 59/150\n",
            "242/242 [==============================] - 0s 66us/step - loss: 0.5680 - acc: 0.7231\n",
            "Epoch 60/150\n",
            "242/242 [==============================] - 0s 63us/step - loss: 0.5318 - acc: 0.7397\n",
            "Epoch 61/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.5332 - acc: 0.7479\n",
            "Epoch 62/150\n",
            "242/242 [==============================] - 0s 63us/step - loss: 0.5328 - acc: 0.7438\n",
            "Epoch 63/150\n",
            "242/242 [==============================] - 0s 68us/step - loss: 0.5418 - acc: 0.7479\n",
            "Epoch 64/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.5411 - acc: 0.7397\n",
            "Epoch 65/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.5165 - acc: 0.7438\n",
            "Epoch 66/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.5231 - acc: 0.7438\n",
            "Epoch 67/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.5239 - acc: 0.7521\n",
            "Epoch 68/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.5367 - acc: 0.7066\n",
            "Epoch 69/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.5103 - acc: 0.7479\n",
            "Epoch 70/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.5121 - acc: 0.7562\n",
            "Epoch 71/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.5132 - acc: 0.7851\n",
            "Epoch 72/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.5132 - acc: 0.7727\n",
            "Epoch 73/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.5521 - acc: 0.7107\n",
            "Epoch 74/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.5168 - acc: 0.7521\n",
            "Epoch 75/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.5175 - acc: 0.7438\n",
            "Epoch 76/150\n",
            "242/242 [==============================] - 0s 64us/step - loss: 0.5066 - acc: 0.7562\n",
            "Epoch 77/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.5012 - acc: 0.7438\n",
            "Epoch 78/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4976 - acc: 0.7727\n",
            "Epoch 79/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4928 - acc: 0.7686\n",
            "Epoch 80/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4930 - acc: 0.7727\n",
            "Epoch 81/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.4902 - acc: 0.7851\n",
            "Epoch 82/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4894 - acc: 0.7603\n",
            "Epoch 83/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.4889 - acc: 0.7479\n",
            "Epoch 84/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4873 - acc: 0.7562\n",
            "Epoch 85/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4833 - acc: 0.7562\n",
            "Epoch 86/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4880 - acc: 0.7727\n",
            "Epoch 87/150\n",
            "242/242 [==============================] - 0s 64us/step - loss: 0.5136 - acc: 0.7355\n",
            "Epoch 88/150\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.5183 - acc: 0.7603\n",
            "Epoch 89/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.5147 - acc: 0.7521\n",
            "Epoch 90/150\n",
            "242/242 [==============================] - 0s 63us/step - loss: 0.4854 - acc: 0.7562\n",
            "Epoch 91/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4891 - acc: 0.7851\n",
            "Epoch 92/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4785 - acc: 0.7727\n",
            "Epoch 93/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4720 - acc: 0.7810\n",
            "Epoch 94/150\n",
            "242/242 [==============================] - 0s 70us/step - loss: 0.4656 - acc: 0.7686\n",
            "Epoch 95/150\n",
            "242/242 [==============================] - 0s 63us/step - loss: 0.4632 - acc: 0.7810\n",
            "Epoch 96/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.4626 - acc: 0.7727\n",
            "Epoch 97/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4618 - acc: 0.7934\n",
            "Epoch 98/150\n",
            "242/242 [==============================] - 0s 50us/step - loss: 0.4643 - acc: 0.7934\n",
            "Epoch 99/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4595 - acc: 0.7769\n",
            "Epoch 100/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4603 - acc: 0.7893\n",
            "Epoch 101/150\n",
            "242/242 [==============================] - 0s 54us/step - loss: 0.4596 - acc: 0.7810\n",
            "Epoch 102/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.4641 - acc: 0.7769\n",
            "Epoch 103/150\n",
            "242/242 [==============================] - 0s 51us/step - loss: 0.4591 - acc: 0.7810\n",
            "Epoch 104/150\n",
            "242/242 [==============================] - 0s 52us/step - loss: 0.4780 - acc: 0.7934\n",
            "Epoch 105/150\n",
            "242/242 [==============================] - 0s 54us/step - loss: 0.4920 - acc: 0.7727\n",
            "Epoch 106/150\n",
            "242/242 [==============================] - 0s 53us/step - loss: 0.4716 - acc: 0.8140\n",
            "Epoch 107/150\n",
            "242/242 [==============================] - 0s 55us/step - loss: 0.4884 - acc: 0.7686\n",
            "Epoch 108/150\n",
            "242/242 [==============================] - 0s 50us/step - loss: 0.4676 - acc: 0.7810\n",
            "Epoch 109/150\n",
            "242/242 [==============================] - 0s 50us/step - loss: 0.4810 - acc: 0.7686\n",
            "Epoch 110/150\n",
            "242/242 [==============================] - 0s 52us/step - loss: 0.4697 - acc: 0.7686\n",
            "Epoch 111/150\n",
            "242/242 [==============================] - 0s 51us/step - loss: 0.4563 - acc: 0.8058\n",
            "Epoch 112/150\n",
            "242/242 [==============================] - 0s 50us/step - loss: 0.4430 - acc: 0.7893\n",
            "Epoch 113/150\n",
            "242/242 [==============================] - 0s 50us/step - loss: 0.4395 - acc: 0.7975\n",
            "Epoch 114/150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 0s 55us/step - loss: 0.4370 - acc: 0.8058\n",
            "Epoch 115/150\n",
            "242/242 [==============================] - 0s 52us/step - loss: 0.4346 - acc: 0.8017\n",
            "Epoch 116/150\n",
            "242/242 [==============================] - 0s 50us/step - loss: 0.4575 - acc: 0.7893\n",
            "Epoch 117/150\n",
            "242/242 [==============================] - 0s 63us/step - loss: 0.4418 - acc: 0.8058\n",
            "Epoch 118/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4407 - acc: 0.8017\n",
            "Epoch 119/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4325 - acc: 0.8058\n",
            "Epoch 120/150\n",
            "242/242 [==============================] - 0s 52us/step - loss: 0.4335 - acc: 0.8058\n",
            "Epoch 121/150\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.4287 - acc: 0.8140\n",
            "Epoch 122/150\n",
            "242/242 [==============================] - 0s 54us/step - loss: 0.4291 - acc: 0.8099\n",
            "Epoch 123/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4279 - acc: 0.8017\n",
            "Epoch 124/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4361 - acc: 0.7686\n",
            "Epoch 125/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4404 - acc: 0.8058\n",
            "Epoch 126/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4386 - acc: 0.7934\n",
            "Epoch 127/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.4662 - acc: 0.7851\n",
            "Epoch 128/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4274 - acc: 0.8058\n",
            "Epoch 129/150\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.4227 - acc: 0.8223\n",
            "Epoch 130/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4223 - acc: 0.8017\n",
            "Epoch 131/150\n",
            "242/242 [==============================] - 0s 55us/step - loss: 0.4244 - acc: 0.8017\n",
            "Epoch 132/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4201 - acc: 0.8223\n",
            "Epoch 133/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4245 - acc: 0.8182\n",
            "Epoch 134/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4263 - acc: 0.7975\n",
            "Epoch 135/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4178 - acc: 0.8099\n",
            "Epoch 136/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4188 - acc: 0.8058\n",
            "Epoch 137/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4148 - acc: 0.8182\n",
            "Epoch 138/150\n",
            "242/242 [==============================] - 0s 61us/step - loss: 0.4157 - acc: 0.8140\n",
            "Epoch 139/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4215 - acc: 0.7893\n",
            "Epoch 140/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4092 - acc: 0.8223\n",
            "Epoch 141/150\n",
            "242/242 [==============================] - 0s 54us/step - loss: 0.4166 - acc: 0.8099\n",
            "Epoch 142/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4177 - acc: 0.8140\n",
            "Epoch 143/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4300 - acc: 0.8017\n",
            "Epoch 144/150\n",
            "242/242 [==============================] - 0s 55us/step - loss: 0.4116 - acc: 0.8140\n",
            "Epoch 145/150\n",
            "242/242 [==============================] - 0s 57us/step - loss: 0.4111 - acc: 0.8264\n",
            "Epoch 146/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4105 - acc: 0.8182\n",
            "Epoch 147/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4391 - acc: 0.7934\n",
            "Epoch 148/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4259 - acc: 0.8264\n",
            "Epoch 149/150\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.4009 - acc: 0.8264\n",
            "Epoch 150/150\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4059 - acc: 0.8223\n",
            "61/61 [==============================] - 0s 7ms/step\n",
            "Epoch 1/150\n",
            "243/243 [==============================] - 1s 5ms/step - loss: 3.4014 - acc: 0.5802\n",
            "Epoch 2/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 1.5210 - acc: 0.6543\n",
            "Epoch 3/150\n",
            "243/243 [==============================] - 0s 57us/step - loss: 1.3790 - acc: 0.6214\n",
            "Epoch 4/150\n",
            "243/243 [==============================] - 0s 58us/step - loss: 1.0222 - acc: 0.6502\n",
            "Epoch 5/150\n",
            "243/243 [==============================] - 0s 57us/step - loss: 0.9381 - acc: 0.6996\n",
            "Epoch 6/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.9053 - acc: 0.6872\n",
            "Epoch 7/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.8516 - acc: 0.7078\n",
            "Epoch 8/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.8370 - acc: 0.7119\n",
            "Epoch 9/150\n",
            "243/243 [==============================] - 0s 57us/step - loss: 0.8049 - acc: 0.7037\n",
            "Epoch 10/150\n",
            "243/243 [==============================] - 0s 58us/step - loss: 0.7849 - acc: 0.7119\n",
            "Epoch 11/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.7743 - acc: 0.7202\n",
            "Epoch 12/150\n",
            "243/243 [==============================] - 0s 58us/step - loss: 0.7610 - acc: 0.7243\n",
            "Epoch 13/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.7427 - acc: 0.7243\n",
            "Epoch 14/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.7336 - acc: 0.7119\n",
            "Epoch 15/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.7146 - acc: 0.7284\n",
            "Epoch 16/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.7050 - acc: 0.7119\n",
            "Epoch 17/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.7093 - acc: 0.7243\n",
            "Epoch 18/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.7090 - acc: 0.6996\n",
            "Epoch 19/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.6754 - acc: 0.7037\n",
            "Epoch 20/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.6733 - acc: 0.7037\n",
            "Epoch 21/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.6706 - acc: 0.7078\n",
            "Epoch 22/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.6475 - acc: 0.7407\n",
            "Epoch 23/150\n",
            "243/243 [==============================] - 0s 66us/step - loss: 0.6555 - acc: 0.7078\n",
            "Epoch 24/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.6384 - acc: 0.7366\n",
            "Epoch 25/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.6395 - acc: 0.7202\n",
            "Epoch 26/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.6230 - acc: 0.7531\n",
            "Epoch 27/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.6082 - acc: 0.7490\n",
            "Epoch 28/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.6199 - acc: 0.7366\n",
            "Epoch 29/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.6054 - acc: 0.7407\n",
            "Epoch 30/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.5938 - acc: 0.7531\n",
            "Epoch 31/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.5918 - acc: 0.7613\n",
            "Epoch 32/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.5891 - acc: 0.7572\n",
            "Epoch 33/150\n",
            "243/243 [==============================] - 0s 66us/step - loss: 0.5935 - acc: 0.7654\n",
            "Epoch 34/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.5870 - acc: 0.7284\n",
            "Epoch 35/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.5915 - acc: 0.7695\n",
            "Epoch 36/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.6180 - acc: 0.7325\n",
            "Epoch 37/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.6121 - acc: 0.7572\n",
            "Epoch 38/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.5725 - acc: 0.7407\n",
            "Epoch 39/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.5682 - acc: 0.7737\n",
            "Epoch 40/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.5615 - acc: 0.7531\n",
            "Epoch 41/150\n",
            "243/243 [==============================] - 0s 57us/step - loss: 0.5580 - acc: 0.7572\n",
            "Epoch 42/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.5565 - acc: 0.7819\n",
            "Epoch 43/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.5675 - acc: 0.7490\n",
            "Epoch 44/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.5764 - acc: 0.7449\n",
            "Epoch 45/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.5715 - acc: 0.7531\n",
            "Epoch 46/150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "243/243 [==============================] - 0s 61us/step - loss: 0.5437 - acc: 0.7942\n",
            "Epoch 47/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.5465 - acc: 0.7654\n",
            "Epoch 48/150\n",
            "243/243 [==============================] - 0s 57us/step - loss: 0.5574 - acc: 0.7737\n",
            "Epoch 49/150\n",
            "243/243 [==============================] - 0s 74us/step - loss: 0.5363 - acc: 0.7819\n",
            "Epoch 50/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.5405 - acc: 0.7819\n",
            "Epoch 51/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.5380 - acc: 0.7819\n",
            "Epoch 52/150\n",
            "243/243 [==============================] - 0s 58us/step - loss: 0.5271 - acc: 0.7737\n",
            "Epoch 53/150\n",
            "243/243 [==============================] - 0s 58us/step - loss: 0.5240 - acc: 0.7860\n",
            "Epoch 54/150\n",
            "243/243 [==============================] - 0s 57us/step - loss: 0.5306 - acc: 0.7901\n",
            "Epoch 55/150\n",
            "243/243 [==============================] - 0s 57us/step - loss: 0.5293 - acc: 0.7778\n",
            "Epoch 56/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.5188 - acc: 0.7778\n",
            "Epoch 57/150\n",
            "243/243 [==============================] - 0s 64us/step - loss: 0.5162 - acc: 0.8025\n",
            "Epoch 58/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.5109 - acc: 0.7860\n",
            "Epoch 59/150\n",
            "243/243 [==============================] - 0s 72us/step - loss: 0.5081 - acc: 0.7819\n",
            "Epoch 60/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.5075 - acc: 0.7984\n",
            "Epoch 61/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.5199 - acc: 0.7819\n",
            "Epoch 62/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.5422 - acc: 0.7901\n",
            "Epoch 63/150\n",
            "243/243 [==============================] - 0s 56us/step - loss: 0.5496 - acc: 0.7860\n",
            "Epoch 64/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.5101 - acc: 0.7984\n",
            "Epoch 65/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.5068 - acc: 0.8025\n",
            "Epoch 66/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.5010 - acc: 0.7737\n",
            "Epoch 67/150\n",
            "243/243 [==============================] - 0s 54us/step - loss: 0.4943 - acc: 0.8025\n",
            "Epoch 68/150\n",
            "243/243 [==============================] - 0s 53us/step - loss: 0.4952 - acc: 0.7984\n",
            "Epoch 69/150\n",
            "243/243 [==============================] - 0s 57us/step - loss: 0.5031 - acc: 0.8107\n",
            "Epoch 70/150\n",
            "243/243 [==============================] - 0s 57us/step - loss: 0.5029 - acc: 0.7984\n",
            "Epoch 71/150\n",
            "243/243 [==============================] - 0s 54us/step - loss: 0.4923 - acc: 0.8066\n",
            "Epoch 72/150\n",
            "243/243 [==============================] - 0s 54us/step - loss: 0.4885 - acc: 0.8189\n",
            "Epoch 73/150\n",
            "243/243 [==============================] - 0s 53us/step - loss: 0.4869 - acc: 0.8025\n",
            "Epoch 74/150\n",
            "243/243 [==============================] - 0s 58us/step - loss: 0.4858 - acc: 0.8066\n",
            "Epoch 75/150\n",
            "243/243 [==============================] - 0s 51us/step - loss: 0.5001 - acc: 0.7819\n",
            "Epoch 76/150\n",
            "243/243 [==============================] - 0s 55us/step - loss: 0.5042 - acc: 0.8148\n",
            "Epoch 77/150\n",
            "243/243 [==============================] - 0s 50us/step - loss: 0.5518 - acc: 0.7490\n",
            "Epoch 78/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.5031 - acc: 0.8148\n",
            "Epoch 79/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.4851 - acc: 0.8025\n",
            "Epoch 80/150\n",
            "243/243 [==============================] - 0s 69us/step - loss: 0.4980 - acc: 0.8025\n",
            "Epoch 81/150\n",
            "243/243 [==============================] - 0s 76us/step - loss: 0.5149 - acc: 0.7984\n",
            "Epoch 82/150\n",
            "243/243 [==============================] - 0s 66us/step - loss: 0.5051 - acc: 0.8025\n",
            "Epoch 83/150\n",
            "243/243 [==============================] - 0s 64us/step - loss: 0.4756 - acc: 0.8313\n",
            "Epoch 84/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.4804 - acc: 0.8148\n",
            "Epoch 85/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.4587 - acc: 0.8025\n",
            "Epoch 86/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.4295 - acc: 0.8272\n",
            "Epoch 87/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.4171 - acc: 0.8148\n",
            "Epoch 88/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.4117 - acc: 0.7984\n",
            "Epoch 89/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.4057 - acc: 0.8230\n",
            "Epoch 90/150\n",
            "243/243 [==============================] - 0s 57us/step - loss: 0.4078 - acc: 0.8395\n",
            "Epoch 91/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.4137 - acc: 0.8025\n",
            "Epoch 92/150\n",
            "243/243 [==============================] - 0s 58us/step - loss: 0.4220 - acc: 0.7984\n",
            "Epoch 93/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.4078 - acc: 0.8189\n",
            "Epoch 94/150\n",
            "243/243 [==============================] - 0s 57us/step - loss: 0.3995 - acc: 0.7984\n",
            "Epoch 95/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.4009 - acc: 0.8066\n",
            "Epoch 96/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.4004 - acc: 0.8189\n",
            "Epoch 97/150\n",
            "243/243 [==============================] - 0s 65us/step - loss: 0.4395 - acc: 0.7942\n",
            "Epoch 98/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.4243 - acc: 0.8189\n",
            "Epoch 99/150\n",
            "243/243 [==============================] - 0s 66us/step - loss: 0.4135 - acc: 0.8148\n",
            "Epoch 100/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.4203 - acc: 0.8066\n",
            "Epoch 101/150\n",
            "243/243 [==============================] - 0s 66us/step - loss: 0.3992 - acc: 0.8272\n",
            "Epoch 102/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.4005 - acc: 0.8189\n",
            "Epoch 103/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.4002 - acc: 0.8354\n",
            "Epoch 104/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.4027 - acc: 0.8025\n",
            "Epoch 105/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.4062 - acc: 0.8148\n",
            "Epoch 106/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.3988 - acc: 0.8107\n",
            "Epoch 107/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.3911 - acc: 0.8230\n",
            "Epoch 108/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.4026 - acc: 0.8230\n",
            "Epoch 109/150\n",
            "243/243 [==============================] - 0s 58us/step - loss: 0.3904 - acc: 0.8189\n",
            "Epoch 110/150\n",
            "243/243 [==============================] - 0s 57us/step - loss: 0.3984 - acc: 0.8189\n",
            "Epoch 111/150\n",
            "243/243 [==============================] - 0s 58us/step - loss: 0.4013 - acc: 0.8107\n",
            "Epoch 112/150\n",
            "243/243 [==============================] - 0s 58us/step - loss: 0.3975 - acc: 0.8107\n",
            "Epoch 113/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.3921 - acc: 0.8272\n",
            "Epoch 114/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.3991 - acc: 0.8189\n",
            "Epoch 115/150\n",
            "243/243 [==============================] - 0s 64us/step - loss: 0.4012 - acc: 0.8189\n",
            "Epoch 116/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.4008 - acc: 0.8066\n",
            "Epoch 117/150\n",
            "243/243 [==============================] - 0s 58us/step - loss: 0.3994 - acc: 0.8230\n",
            "Epoch 118/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.3989 - acc: 0.8230\n",
            "Epoch 119/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.3887 - acc: 0.8189\n",
            "Epoch 120/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.3856 - acc: 0.8313\n",
            "Epoch 121/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.3872 - acc: 0.8148\n",
            "Epoch 122/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.3835 - acc: 0.8313\n",
            "Epoch 123/150\n",
            "243/243 [==============================] - 0s 58us/step - loss: 0.3928 - acc: 0.8354\n",
            "Epoch 124/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.3877 - acc: 0.8230\n",
            "Epoch 125/150\n",
            "243/243 [==============================] - 0s 66us/step - loss: 0.3924 - acc: 0.8313\n",
            "Epoch 126/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.3821 - acc: 0.8272\n",
            "Epoch 127/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.3908 - acc: 0.8230\n",
            "Epoch 128/150\n",
            "243/243 [==============================] - 0s 65us/step - loss: 0.3834 - acc: 0.8272\n",
            "Epoch 129/150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "243/243 [==============================] - 0s 60us/step - loss: 0.3943 - acc: 0.8107\n",
            "Epoch 130/150\n",
            "243/243 [==============================] - 0s 56us/step - loss: 0.3843 - acc: 0.8230\n",
            "Epoch 131/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.3806 - acc: 0.8313\n",
            "Epoch 132/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.3807 - acc: 0.8230\n",
            "Epoch 133/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.3922 - acc: 0.8395\n",
            "Epoch 134/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.3837 - acc: 0.8189\n",
            "Epoch 135/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.3896 - acc: 0.8230\n",
            "Epoch 136/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.3798 - acc: 0.8272\n",
            "Epoch 137/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.3834 - acc: 0.8107\n",
            "Epoch 138/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.3787 - acc: 0.8395\n",
            "Epoch 139/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.3814 - acc: 0.8395\n",
            "Epoch 140/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.3886 - acc: 0.8066\n",
            "Epoch 141/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.4032 - acc: 0.8230\n",
            "Epoch 142/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.4033 - acc: 0.8230\n",
            "Epoch 143/150\n",
            "243/243 [==============================] - 0s 65us/step - loss: 0.3956 - acc: 0.8148\n",
            "Epoch 144/150\n",
            "243/243 [==============================] - 0s 67us/step - loss: 0.4081 - acc: 0.7901\n",
            "Epoch 145/150\n",
            "243/243 [==============================] - 0s 64us/step - loss: 0.4040 - acc: 0.7860\n",
            "Epoch 146/150\n",
            "243/243 [==============================] - 0s 67us/step - loss: 0.3825 - acc: 0.8354\n",
            "Epoch 147/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.3865 - acc: 0.8436\n",
            "Epoch 148/150\n",
            "243/243 [==============================] - 0s 65us/step - loss: 0.3884 - acc: 0.8107\n",
            "Epoch 149/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.4015 - acc: 0.8189\n",
            "Epoch 150/150\n",
            "243/243 [==============================] - 0s 65us/step - loss: 0.3760 - acc: 0.8354\n",
            "60/60 [==============================] - 0s 7ms/step\n",
            "Epoch 1/150\n",
            "243/243 [==============================] - 1s 5ms/step - loss: 0.8777 - acc: 0.6584\n",
            "Epoch 2/150\n",
            "243/243 [==============================] - 0s 64us/step - loss: 0.8087 - acc: 0.6461\n",
            "Epoch 3/150\n",
            "243/243 [==============================] - 0s 67us/step - loss: 0.7612 - acc: 0.6626\n",
            "Epoch 4/150\n",
            "243/243 [==============================] - 0s 75us/step - loss: 0.7421 - acc: 0.6502\n",
            "Epoch 5/150\n",
            "243/243 [==============================] - 0s 75us/step - loss: 0.7152 - acc: 0.6502\n",
            "Epoch 6/150\n",
            "243/243 [==============================] - 0s 68us/step - loss: 0.7027 - acc: 0.6543\n",
            "Epoch 7/150\n",
            "243/243 [==============================] - 0s 67us/step - loss: 0.6932 - acc: 0.6379\n",
            "Epoch 8/150\n",
            "243/243 [==============================] - 0s 70us/step - loss: 0.6763 - acc: 0.6584\n",
            "Epoch 9/150\n",
            "243/243 [==============================] - 0s 67us/step - loss: 0.6738 - acc: 0.6461\n",
            "Epoch 10/150\n",
            "243/243 [==============================] - 0s 69us/step - loss: 0.6580 - acc: 0.6584\n",
            "Epoch 11/150\n",
            "243/243 [==============================] - 0s 77us/step - loss: 0.6601 - acc: 0.6420\n",
            "Epoch 12/150\n",
            "243/243 [==============================] - 0s 96us/step - loss: 0.6478 - acc: 0.6502\n",
            "Epoch 13/150\n",
            "243/243 [==============================] - 0s 92us/step - loss: 0.6403 - acc: 0.6543\n",
            "Epoch 14/150\n",
            "243/243 [==============================] - 0s 81us/step - loss: 0.6380 - acc: 0.6667\n",
            "Epoch 15/150\n",
            "243/243 [==============================] - 0s 80us/step - loss: 0.6470 - acc: 0.6543\n",
            "Epoch 16/150\n",
            "243/243 [==============================] - 0s 79us/step - loss: 0.6276 - acc: 0.6667\n",
            "Epoch 17/150\n",
            "243/243 [==============================] - 0s 81us/step - loss: 0.6248 - acc: 0.6749\n",
            "Epoch 18/150\n",
            "243/243 [==============================] - 0s 81us/step - loss: 0.6234 - acc: 0.6543\n",
            "Epoch 19/150\n",
            "243/243 [==============================] - 0s 93us/step - loss: 0.6166 - acc: 0.6749\n",
            "Epoch 20/150\n",
            "243/243 [==============================] - 0s 79us/step - loss: 0.6147 - acc: 0.6667\n",
            "Epoch 21/150\n",
            "243/243 [==============================] - 0s 72us/step - loss: 0.6086 - acc: 0.6790\n",
            "Epoch 22/150\n",
            "243/243 [==============================] - 0s 76us/step - loss: 0.6069 - acc: 0.6790\n",
            "Epoch 23/150\n",
            "243/243 [==============================] - 0s 81us/step - loss: 0.6037 - acc: 0.6872\n",
            "Epoch 24/150\n",
            "243/243 [==============================] - 0s 75us/step - loss: 0.6009 - acc: 0.6626\n",
            "Epoch 25/150\n",
            "243/243 [==============================] - 0s 81us/step - loss: 0.5999 - acc: 0.6667\n",
            "Epoch 26/150\n",
            "243/243 [==============================] - 0s 83us/step - loss: 0.5915 - acc: 0.6790\n",
            "Epoch 27/150\n",
            "243/243 [==============================] - 0s 72us/step - loss: 0.5948 - acc: 0.6749\n",
            "Epoch 28/150\n",
            "243/243 [==============================] - 0s 87us/step - loss: 0.5876 - acc: 0.6872\n",
            "Epoch 29/150\n",
            "243/243 [==============================] - 0s 68us/step - loss: 0.5921 - acc: 0.6872\n",
            "Epoch 30/150\n",
            "243/243 [==============================] - 0s 69us/step - loss: 0.5840 - acc: 0.6708\n",
            "Epoch 31/150\n",
            "243/243 [==============================] - 0s 73us/step - loss: 0.5793 - acc: 0.6831\n",
            "Epoch 32/150\n",
            "243/243 [==============================] - 0s 68us/step - loss: 0.5832 - acc: 0.6790\n",
            "Epoch 33/150\n",
            "243/243 [==============================] - 0s 65us/step - loss: 0.5810 - acc: 0.6790\n",
            "Epoch 34/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.5827 - acc: 0.6708\n",
            "Epoch 35/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.6066 - acc: 0.6708\n",
            "Epoch 36/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.5763 - acc: 0.7037\n",
            "Epoch 37/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.5668 - acc: 0.6667\n",
            "Epoch 38/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.5690 - acc: 0.6831\n",
            "Epoch 39/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.5635 - acc: 0.6996\n",
            "Epoch 40/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.5670 - acc: 0.6872\n",
            "Epoch 41/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.5587 - acc: 0.6955\n",
            "Epoch 42/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.5609 - acc: 0.6872\n",
            "Epoch 43/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.5569 - acc: 0.7037\n",
            "Epoch 44/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.5545 - acc: 0.7078\n",
            "Epoch 45/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.5522 - acc: 0.6955\n",
            "Epoch 46/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.5546 - acc: 0.6914\n",
            "Epoch 47/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.5574 - acc: 0.7037\n",
            "Epoch 48/150\n",
            "243/243 [==============================] - 0s 64us/step - loss: 0.5473 - acc: 0.7078\n",
            "Epoch 49/150\n",
            "243/243 [==============================] - 0s 65us/step - loss: 0.5480 - acc: 0.6996\n",
            "Epoch 50/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.5477 - acc: 0.7119\n",
            "Epoch 51/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.5527 - acc: 0.7243\n",
            "Epoch 52/150\n",
            "243/243 [==============================] - 0s 64us/step - loss: 0.5478 - acc: 0.6872\n",
            "Epoch 53/150\n",
            "243/243 [==============================] - 0s 65us/step - loss: 0.5502 - acc: 0.7160\n",
            "Epoch 54/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.5338 - acc: 0.7243\n",
            "Epoch 55/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.5394 - acc: 0.7078\n",
            "Epoch 56/150\n",
            "243/243 [==============================] - 0s 65us/step - loss: 0.5337 - acc: 0.7243\n",
            "Epoch 57/150\n",
            "243/243 [==============================] - 0s 73us/step - loss: 0.5319 - acc: 0.7366\n",
            "Epoch 58/150\n",
            "243/243 [==============================] - 0s 76us/step - loss: 0.5327 - acc: 0.7202\n",
            "Epoch 59/150\n",
            "243/243 [==============================] - 0s 78us/step - loss: 0.5309 - acc: 0.7202\n",
            "Epoch 60/150\n",
            "243/243 [==============================] - 0s 73us/step - loss: 0.5279 - acc: 0.7284\n",
            "Epoch 61/150\n",
            "243/243 [==============================] - 0s 82us/step - loss: 0.5330 - acc: 0.7366\n",
            "Epoch 62/150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "243/243 [==============================] - 0s 70us/step - loss: 0.5381 - acc: 0.7119\n",
            "Epoch 63/150\n",
            "243/243 [==============================] - 0s 80us/step - loss: 0.5456 - acc: 0.7449\n",
            "Epoch 64/150\n",
            "243/243 [==============================] - 0s 76us/step - loss: 0.5425 - acc: 0.7202\n",
            "Epoch 65/150\n",
            "243/243 [==============================] - 0s 77us/step - loss: 0.5299 - acc: 0.7490\n",
            "Epoch 66/150\n",
            "243/243 [==============================] - 0s 81us/step - loss: 0.5178 - acc: 0.7284\n",
            "Epoch 67/150\n",
            "243/243 [==============================] - 0s 86us/step - loss: 0.5254 - acc: 0.7119\n",
            "Epoch 68/150\n",
            "243/243 [==============================] - 0s 79us/step - loss: 0.5283 - acc: 0.7407\n",
            "Epoch 69/150\n",
            "243/243 [==============================] - 0s 82us/step - loss: 0.5130 - acc: 0.7325\n",
            "Epoch 70/150\n",
            "243/243 [==============================] - 0s 81us/step - loss: 0.5143 - acc: 0.7407\n",
            "Epoch 71/150\n",
            "243/243 [==============================] - 0s 72us/step - loss: 0.5085 - acc: 0.7407\n",
            "Epoch 72/150\n",
            "243/243 [==============================] - 0s 67us/step - loss: 0.5068 - acc: 0.7325\n",
            "Epoch 73/150\n",
            "243/243 [==============================] - 0s 67us/step - loss: 0.5077 - acc: 0.7449\n",
            "Epoch 74/150\n",
            "243/243 [==============================] - 0s 67us/step - loss: 0.5048 - acc: 0.7449\n",
            "Epoch 75/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.5082 - acc: 0.7407\n",
            "Epoch 76/150\n",
            "243/243 [==============================] - 0s 69us/step - loss: 0.5066 - acc: 0.7490\n",
            "Epoch 77/150\n",
            "243/243 [==============================] - 0s 75us/step - loss: 0.5057 - acc: 0.7284\n",
            "Epoch 78/150\n",
            "243/243 [==============================] - 0s 68us/step - loss: 0.4982 - acc: 0.7490\n",
            "Epoch 79/150\n",
            "243/243 [==============================] - 0s 65us/step - loss: 0.4956 - acc: 0.7449\n",
            "Epoch 80/150\n",
            "243/243 [==============================] - 0s 64us/step - loss: 0.4954 - acc: 0.7531\n",
            "Epoch 81/150\n",
            "243/243 [==============================] - 0s 64us/step - loss: 0.4985 - acc: 0.7449\n",
            "Epoch 82/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.5125 - acc: 0.7366\n",
            "Epoch 83/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.5159 - acc: 0.7490\n",
            "Epoch 84/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.4846 - acc: 0.7449\n",
            "Epoch 85/150\n",
            "243/243 [==============================] - 0s 64us/step - loss: 0.4911 - acc: 0.7572\n",
            "Epoch 86/150\n",
            "243/243 [==============================] - 0s 65us/step - loss: 0.4904 - acc: 0.7531\n",
            "Epoch 87/150\n",
            "243/243 [==============================] - 0s 64us/step - loss: 0.4857 - acc: 0.7654\n",
            "Epoch 88/150\n",
            "243/243 [==============================] - 0s 64us/step - loss: 0.4847 - acc: 0.7572\n",
            "Epoch 89/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.4829 - acc: 0.7572\n",
            "Epoch 90/150\n",
            "243/243 [==============================] - 0s 64us/step - loss: 0.4888 - acc: 0.7613\n",
            "Epoch 91/150\n",
            "243/243 [==============================] - 0s 66us/step - loss: 0.4991 - acc: 0.7531\n",
            "Epoch 92/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.4947 - acc: 0.7654\n",
            "Epoch 93/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.4772 - acc: 0.7490\n",
            "Epoch 94/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.4847 - acc: 0.7449\n",
            "Epoch 95/150\n",
            "243/243 [==============================] - 0s 58us/step - loss: 0.4736 - acc: 0.7778\n",
            "Epoch 96/150\n",
            "243/243 [==============================] - 0s 57us/step - loss: 0.4740 - acc: 0.7572\n",
            "Epoch 97/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.4718 - acc: 0.7654\n",
            "Epoch 98/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.4727 - acc: 0.7778\n",
            "Epoch 99/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.4701 - acc: 0.7737\n",
            "Epoch 100/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.4698 - acc: 0.7695\n",
            "Epoch 101/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.4666 - acc: 0.7654\n",
            "Epoch 102/150\n",
            "243/243 [==============================] - 0s 64us/step - loss: 0.4672 - acc: 0.7695\n",
            "Epoch 103/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.4669 - acc: 0.7654\n",
            "Epoch 104/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.4660 - acc: 0.7737\n",
            "Epoch 105/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.4625 - acc: 0.7737\n",
            "Epoch 106/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.4593 - acc: 0.7778\n",
            "Epoch 107/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.4636 - acc: 0.7654\n",
            "Epoch 108/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.4811 - acc: 0.7778\n",
            "Epoch 109/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.4730 - acc: 0.7572\n",
            "Epoch 110/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.4630 - acc: 0.7901\n",
            "Epoch 111/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.4579 - acc: 0.7778\n",
            "Epoch 112/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.4537 - acc: 0.7778\n",
            "Epoch 113/150\n",
            "243/243 [==============================] - 0s 58us/step - loss: 0.4511 - acc: 0.7860\n",
            "Epoch 114/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.4509 - acc: 0.7860\n",
            "Epoch 115/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.4496 - acc: 0.7778\n",
            "Epoch 116/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.4490 - acc: 0.7860\n",
            "Epoch 117/150\n",
            "243/243 [==============================] - 0s 60us/step - loss: 0.4465 - acc: 0.7819\n",
            "Epoch 118/150\n",
            "243/243 [==============================] - 0s 63us/step - loss: 0.4518 - acc: 0.7695\n",
            "Epoch 119/150\n",
            "243/243 [==============================] - 0s 64us/step - loss: 0.4432 - acc: 0.7860\n",
            "Epoch 120/150\n",
            "243/243 [==============================] - 0s 61us/step - loss: 0.4421 - acc: 0.7984\n",
            "Epoch 121/150\n",
            "243/243 [==============================] - 0s 69us/step - loss: 0.4480 - acc: 0.7778\n",
            "Epoch 122/150\n",
            "243/243 [==============================] - 0s 67us/step - loss: 0.4374 - acc: 0.7942\n",
            "Epoch 123/150\n",
            "243/243 [==============================] - 0s 69us/step - loss: 0.4475 - acc: 0.7819\n",
            "Epoch 124/150\n",
            "243/243 [==============================] - 0s 68us/step - loss: 0.4559 - acc: 0.7737\n",
            "Epoch 125/150\n",
            "243/243 [==============================] - 0s 71us/step - loss: 0.4345 - acc: 0.7942\n",
            "Epoch 126/150\n",
            "243/243 [==============================] - 0s 72us/step - loss: 0.4306 - acc: 0.8066\n",
            "Epoch 127/150\n",
            "243/243 [==============================] - 0s 77us/step - loss: 0.4275 - acc: 0.8066\n",
            "Epoch 128/150\n",
            "243/243 [==============================] - 0s 72us/step - loss: 0.4337 - acc: 0.7984\n",
            "Epoch 129/150\n",
            "243/243 [==============================] - 0s 73us/step - loss: 0.4345 - acc: 0.7942\n",
            "Epoch 130/150\n",
            "243/243 [==============================] - 0s 72us/step - loss: 0.4237 - acc: 0.8066\n",
            "Epoch 131/150\n",
            "243/243 [==============================] - 0s 69us/step - loss: 0.4208 - acc: 0.8025\n",
            "Epoch 132/150\n",
            "243/243 [==============================] - 0s 71us/step - loss: 0.4228 - acc: 0.8025\n",
            "Epoch 133/150\n",
            "243/243 [==============================] - 0s 68us/step - loss: 0.4204 - acc: 0.8107\n",
            "Epoch 134/150\n",
            "243/243 [==============================] - 0s 75us/step - loss: 0.4180 - acc: 0.8189\n",
            "Epoch 135/150\n",
            "243/243 [==============================] - 0s 68us/step - loss: 0.4140 - acc: 0.8066\n",
            "Epoch 136/150\n",
            "243/243 [==============================] - 0s 70us/step - loss: 0.4226 - acc: 0.8025\n",
            "Epoch 137/150\n",
            "243/243 [==============================] - 0s 66us/step - loss: 0.4147 - acc: 0.8107\n",
            "Epoch 138/150\n",
            "243/243 [==============================] - 0s 65us/step - loss: 0.4138 - acc: 0.8148\n",
            "Epoch 139/150\n",
            "243/243 [==============================] - 0s 67us/step - loss: 0.4111 - acc: 0.7942\n",
            "Epoch 140/150\n",
            "243/243 [==============================] - 0s 69us/step - loss: 0.4110 - acc: 0.8107\n",
            "Epoch 141/150\n",
            "243/243 [==============================] - 0s 67us/step - loss: 0.4161 - acc: 0.8107\n",
            "Epoch 142/150\n",
            "243/243 [==============================] - 0s 58us/step - loss: 0.4080 - acc: 0.8148\n",
            "Epoch 143/150\n",
            "243/243 [==============================] - 0s 69us/step - loss: 0.4062 - acc: 0.8148\n",
            "Epoch 144/150\n",
            "243/243 [==============================] - 0s 62us/step - loss: 0.4061 - acc: 0.8066\n",
            "Epoch 145/150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "243/243 [==============================] - 0s 60us/step - loss: 0.4013 - acc: 0.8066\n",
            "Epoch 146/150\n",
            "243/243 [==============================] - 0s 58us/step - loss: 0.4111 - acc: 0.8189\n",
            "Epoch 147/150\n",
            "243/243 [==============================] - 0s 65us/step - loss: 0.3992 - acc: 0.8107\n",
            "Epoch 148/150\n",
            "243/243 [==============================] - 0s 65us/step - loss: 0.4063 - acc: 0.8148\n",
            "Epoch 149/150\n",
            "243/243 [==============================] - 0s 68us/step - loss: 0.3986 - acc: 0.8066\n",
            "Epoch 150/150\n",
            "243/243 [==============================] - 0s 59us/step - loss: 0.4021 - acc: 0.8066\n",
            "60/60 [==============================] - 0s 7ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XmVSDdlBpuUq",
        "colab_type": "code",
        "colab": {},
        "outputId": "4dcbe8b4-8463-465f-992e-d9fb67f9c481"
      },
      "cell_type": "code",
      "source": [
        "print(f'Results: {results.mean():.2f}, {results.std():.2f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results: 0.77, 0.09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "anpIl6ENpuUt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Our mean accuracy is 0.77 with a std-dev of 0.09. That's not bad for a first shot"
      ]
    },
    {
      "metadata": {
        "id": "I1NvLqKGpuUv",
        "colab_type": "code",
        "colab": {},
        "outputId": "828ccbeb-5f39-48eb-b553-8b34ceb04ea0"
      },
      "cell_type": "code",
      "source": [
        "# fix random seed for reproducibility\n",
        "seed = 101\n",
        "np.random.seed(seed)\n",
        "\n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(13, input_dim=X.shape[1], activation='relu'))\n",
        "  model.add(Dense(8, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "param_grid = {'batch_size': range(32, 128, 32),\n",
        "              'epochs':     range(20, 200, 20)}\n",
        "\n",
        "print('Creating grid search')\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\n",
        "\n",
        "print('Fitting')\n",
        "grid_result = grid.fit(X, y)\n",
        "\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating grid search\n",
            "Fitting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.8151815132339402 using {'batch_size': 64, 'epochs': 180}\n",
            "Means: 0.5445544549537571, Stdev: 0.32498273026560126 with: {'batch_size': 32, 'epochs': 20}\n",
            "Means: 0.6633663321092184, Stdev: 0.3000055946246005 with: {'batch_size': 32, 'epochs': 40}\n",
            "Means: 0.570957094922711, Stdev: 0.3273572743345168 with: {'batch_size': 32, 'epochs': 60}\n",
            "Means: 0.6666666638142992, Stdev: 0.3654260837339631 with: {'batch_size': 32, 'epochs': 80}\n",
            "Means: 0.21122111876805624, Stdev: 0.2956329437494221 with: {'batch_size': 32, 'epochs': 100}\n",
            "Means: 0.6105610582694756, Stdev: 0.34514733472950465 with: {'batch_size': 32, 'epochs': 120}\n",
            "Means: 0.45874587409567125, Stdev: 0.39785922778490446 with: {'batch_size': 32, 'epochs': 140}\n",
            "Means: 0.39273927422246524, Stdev: 0.32194238291140614 with: {'batch_size': 32, 'epochs': 160}\n",
            "Means: 0.7458745815572959, Stdev: 0.08978753422069291 with: {'batch_size': 32, 'epochs': 180}\n",
            "Means: 0.5841584187923091, Stdev: 0.27156821591288166 with: {'batch_size': 64, 'epochs': 20}\n",
            "Means: 0.2640264036361337, Stdev: 0.38625628340825996 with: {'batch_size': 64, 'epochs': 40}\n",
            "Means: 0.4983498277050434, Stdev: 0.3460004318320958 with: {'batch_size': 64, 'epochs': 60}\n",
            "Means: 0.5577557747906977, Stdev: 0.3930285538257894 with: {'batch_size': 64, 'epochs': 80}\n",
            "Means: 0.6072607302036223, Stdev: 0.32714365867878714 with: {'batch_size': 64, 'epochs': 100}\n",
            "Means: 0.5973597359735974, Stdev: 0.34450005828793745 with: {'batch_size': 64, 'epochs': 120}\n",
            "Means: 0.5379537914452379, Stdev: 0.33155469414138 with: {'batch_size': 64, 'epochs': 140}\n",
            "Means: 0.5412541232486763, Stdev: 0.27859653319484795 with: {'batch_size': 64, 'epochs': 160}\n",
            "Means: 0.8151815132339402, Stdev: 0.17906995353507357 with: {'batch_size': 64, 'epochs': 180}\n",
            "Means: 0.6732673208312233, Stdev: 0.36476377273789495 with: {'batch_size': 96, 'epochs': 20}\n",
            "Means: 0.3597359665156198, Stdev: 0.29362577634238013 with: {'batch_size': 96, 'epochs': 40}\n",
            "Means: 0.34653464677703655, Stdev: 0.18991823771080094 with: {'batch_size': 96, 'epochs': 60}\n",
            "Means: 0.34983498230576515, Stdev: 0.42318151828182166 with: {'batch_size': 96, 'epochs': 80}\n",
            "Means: 0.6435643584027936, Stdev: 0.24369100679491376 with: {'batch_size': 96, 'epochs': 100}\n",
            "Means: 0.4686468589817337, Stdev: 0.28467384043978783 with: {'batch_size': 96, 'epochs': 120}\n",
            "Means: 0.42904290409371404, Stdev: 0.26210592086856094 with: {'batch_size': 96, 'epochs': 140}\n",
            "Means: 0.6303630321726154, Stdev: 0.3555586475338533 with: {'batch_size': 96, 'epochs': 160}\n",
            "Means: 0.4521452155050271, Stdev: 0.36795446864800235 with: {'batch_size': 96, 'epochs': 180}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b2AvWl1IpuUz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### It's getting better - 0.81 accuracy with 180 epochs and batch_size of 64"
      ]
    },
    {
      "metadata": {
        "id": "EsF0fdLdpuU0",
        "colab_type": "code",
        "colab": {},
        "outputId": "71d11379-4909-45b4-d653-e21bfc37db26"
      },
      "cell_type": "code",
      "source": [
        "# fix random seed for reproducibility\n",
        "seed = 101\n",
        "np.random.seed(seed)\n",
        "\n",
        "def create_model(learn_rate=0.01, momentum=0):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(13, input_dim=X.shape[1], activation='relu'))\n",
        "  model.add(Dense(8, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "param_grid = {'batch_size': [64],\n",
        "              'epochs':     [180],\n",
        "              'learn_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n",
        "              'momentum':   [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]}\n",
        "\n",
        "print('Creating grid search')\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\n",
        "\n",
        "print('Fitting')\n",
        "grid_result = grid.fit(X, y)\n",
        "\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating grid search\n",
            "Fitting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.8580858097611481 using {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.2, 'momentum': 0.8}\n",
            "Means: 0.528052807837823, Stdev: 0.4503243332742807 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.001, 'momentum': 0.0}\n",
            "Means: 0.1782178276836282, Stdev: 0.23886222553010986 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.001, 'momentum': 0.2}\n",
            "Means: 0.34323432225205325, Stdev: 0.42842525954602934 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.001, 'momentum': 0.4}\n",
            "Means: 0.4587458757677487, Stdev: 0.45433663507435285 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.001, 'momentum': 0.6}\n",
            "Means: 0.23102310290037603, Stdev: 0.2844536638898839 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.001, 'momentum': 0.8}\n",
            "Means: 0.11881188472898879, Stdev: 0.14572282646488985 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.001, 'momentum': 0.9}\n",
            "Means: 0.5412541242322513, Stdev: 0.45433663507435285 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.01, 'momentum': 0.0}\n",
            "Means: 0.8547854716628298, Stdev: 0.1817044780743494 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.01, 'momentum': 0.2}\n",
            "Means: 0.5445544542652545, Stdev: 0.4540248686382509 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.01, 'momentum': 0.4}\n",
            "Means: 0.33993399221904996, Stdev: 0.4272031763574851 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.01, 'momentum': 0.6}\n",
            "Means: 0.4587458757677487, Stdev: 0.45433663507435285 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.01, 'momentum': 0.8}\n",
            "Means: 0.66006600778095, Stdev: 0.4272031763574851 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.01, 'momentum': 0.9}\n",
            "Means: 0.2607260737875507, Stdev: 0.38842202431342 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.1, 'momentum': 0.0}\n",
            "Means: 0.66006600778095, Stdev: 0.4272031763574851 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.1, 'momentum': 0.2}\n",
            "Means: 0.6567656777479468, Stdev: 0.4284252595460293 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.1, 'momentum': 0.4}\n",
            "Means: 0.34323432225205325, Stdev: 0.42842525954602934 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.1, 'momentum': 0.6}\n",
            "Means: 0.46204620580075206, Stdev: 0.45462422971365796 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.1, 'momentum': 0.8}\n",
            "Means: 0.66006600778095, Stdev: 0.4272031763574851 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.1, 'momentum': 0.9}\n",
            "Means: 0.4587458757677487, Stdev: 0.45433663507435285 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.2, 'momentum': 0.0}\n",
            "Means: 0.5445544542652545, Stdev: 0.4540248686382509 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.2, 'momentum': 0.2}\n",
            "Means: 0.4587458757677487, Stdev: 0.45433663507435285 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.2, 'momentum': 0.4}\n",
            "Means: 0.0594059417743494, Stdev: 0.11832394767378107 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.2, 'momentum': 0.6}\n",
            "Means: 0.8580858097611481, Stdev: 0.2826627559204759 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.2, 'momentum': 0.8}\n",
            "Means: 0.1419141902388519, Stdev: 0.2826627559204759 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.2, 'momentum': 0.9}\n",
            "Means: 0.6567656777479468, Stdev: 0.4284252595460293 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.3, 'momentum': 0.0}\n",
            "Means: 0.5445544542652545, Stdev: 0.4540248686382509 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.3, 'momentum': 0.2}\n",
            "Means: 0.33993399221904996, Stdev: 0.4272031763574851 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.3, 'momentum': 0.4}\n",
            "Means: 0.4587458757677487, Stdev: 0.45433663507435285 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.3, 'momentum': 0.6}\n",
            "Means: 0.33993399221904996, Stdev: 0.4272031763574851 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.3, 'momentum': 0.8}\n",
            "Means: 0.0594059417743494, Stdev: 0.11832394767378107 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.3, 'momentum': 0.9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n8MrkL6rpuU3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Best: 0.8580858097611481 using {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.2, 'momentum': 0.8}"
      ]
    },
    {
      "metadata": {
        "id": "oR8wufczpuU4",
        "colab_type": "code",
        "colab": {},
        "outputId": "cdab2f23-e365-4374-8979-9315425d9b09"
      },
      "cell_type": "code",
      "source": [
        "# fix random seed for reproducibility\n",
        "seed = 101\n",
        "np.random.seed(seed)\n",
        "\n",
        "def create_model(learn_rate=0.01, momentum=0, init_mode='uniform'):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(13, input_dim=X.shape[1], kernel_initializer=init_mode, activation='relu'))\n",
        "  model.add(Dense(8, kernel_initializer=init_mode, activation='relu'))\n",
        "  model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))\n",
        "  optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "param_grid = {'batch_size': [32, 64, 128],\n",
        "              'epochs':     [160, 180, 200],\n",
        "              'learn_rate': [0.1, 0.2, 0.3],\n",
        "              'momentum':   [0.7, 0.8, 0.9],\n",
        "              'init_mode': ['uniform', 'lecun_uniform', 'normal', 'zero', \n",
        "                             'glorot_normal', 'glorot_uniform', 'he_normal', \n",
        "                             'he_uniform']}\n",
        "\n",
        "print('Creating grid search')\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\n",
        "\n",
        "print('Fitting')\n",
        "grid_result = grid.fit(X, y)\n",
        "\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating grid search\n",
            "Fitting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-183-591a98b96cda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fitting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best: {grid_result.best_score_} using {grid_result.best_params_}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "BqNgwrFNpuU8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Too late - couldn't finish this run - took too long"
      ]
    },
    {
      "metadata": {
        "id": "NHULG3HkpuU9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gEXoxp7tpuU_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mY4UscAfpuVB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}