{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6SKlgYrpcym"
   },
   "source": [
    "# Neural Networks Sprint Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrEbRrjVphPM"
   },
   "source": [
    "## 1) Define the following terms:\n",
    "\n",
    "- Neuron\n",
    "- Input Layer\n",
    "- Hidden Layer\n",
    "- Output Layer\n",
    "- Activation\n",
    "- Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5EksLqnp4oB"
   },
   "source": [
    "Neuron: a container that holds a single scalar (usually a floating-point number) called an activation; neurons combine to form columns that make up either the input data or layers in a neural network. Also called 'node'.   \n",
    "Input Layer: Ryan Allred mentioned during lecture that sometimes 'layer' is a misnomer with regard to input. In any case, this is the left-most information in a NN and is in the form of an array of rows by columns.  \n",
    "Hidden Layer: this is a true layer, comprised of neurons, of which at least 1 is required to form a non-perceptron NN. Each hidden layer in a NN receives activations from the input or previous hidden layer, applies a weight to each activation then a bias, and forwards the new activations to each neuron in the next hidden layer or output layer.  \n",
    "Output Layer: the right-most layer in a NN, this receives activations from the previous layer then returns final scalars, either integer or float, that provide an information array about the question of interest, eg, classification or regression.  \n",
    "Activation: a single scalar found in a neuron.  \n",
    "Backpropagation: an algorithmic process by which weights in a NN are revised in a backwards propagated fashion after each training epoch, ie last weight is revised, then one just prior, then one before that, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ri_gRA2Jp728"
   },
   "source": [
    "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 1  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ig6ZTH8tpQ19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 4), (4, 1))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define x1, x2, x3\n",
    "\n",
    "x1 = [1, 1, 0, 1]\n",
    "x2 = [1, 0, 1, 0]\n",
    "x3 = [1, 1, 1, 1]\n",
    "y_list = [1, 0, 0, 0]\n",
    "\n",
    "X = np.array(list(zip(x1, x2, x3, np.ones(4))))\n",
    "y = np.array([[val] for val in y_list])\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write perceptron class\n",
    "\n",
    "\n",
    "class ANDPerceptron():\n",
    "    def __init__(self, X, y, niter=100):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.niter = niter\n",
    "       \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_prime(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "    \n",
    "    def fit(self):\n",
    "        # Create weights\n",
    "        weights = 2 * np.random.random((self.X.shape[0], self.X.shape[1])) - 1\n",
    "\n",
    "        for iteration in range(self.niter):\n",
    "            # Weighted sum of inputs and weights\n",
    "            weighted_sum = np.dot(self.X, weights)\n",
    "\n",
    "            # Activate with sigmoid function\n",
    "            activated_output = self.sigmoid(weighted_sum)\n",
    "\n",
    "            # Calculate Error\n",
    "            error = self.y - activated_output\n",
    "\n",
    "            # Calculate weight adjustments with sigmoid_derivative\n",
    "            adjustments = error * self.sigmoid_prime(activated_output)\n",
    "\n",
    "            # Update weights\n",
    "            weights += np.dot(self.X.T, adjustments)\n",
    "        \n",
    "        print('optimized weights after training: ')\n",
    "        print(weights)\n",
    "        print('\\ny:', y)\n",
    "        print(\"\\noutputs after training:\")\n",
    "        print(activated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP = ANDPerceptron(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized weights after training: \n",
      "[[ 2.92611756  2.89688383  3.16407008  2.95721367]\n",
      " [ 3.74074392  3.70016481  3.93126604  3.76444709]\n",
      " [-3.15047259 -3.19682681 -2.93397234 -1.87306245]\n",
      " [-2.40487168 -2.30702194 -2.93880274 -3.7228538 ]]\n",
      "\n",
      "y: [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "outputs after training:\n",
      "[[0.75048007 0.74700151 0.77085915 0.75315822]\n",
      " [0.06783945 0.06928197 0.06297163 0.06724257]\n",
      " [0.14130851 0.14263029 0.12644057 0.13924864]\n",
      " [0.06783945 0.06928197 0.06297163 0.06724257]]\n"
     ]
    }
   ],
   "source": [
    "AP.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86HyRi8Osr3U"
   },
   "source": [
    "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
    "- Your network must have one hidden layer. \n",
    "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "- Train your model on the Heart Disease dataset from UCI:\n",
    "\n",
    "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
    "\n",
    "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNfiajv3v4Ed"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       "    age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       " 0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       " 1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       " 2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       " 3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       " 4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       " \n",
       "    ca  thal  target  \n",
       " 0   0     1       1  \n",
       " 1   0     2       1  \n",
       " 2   0     2       1  \n",
       " 3   0     2       1  \n",
       " 4   0     2       1  )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sum(), df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((303, 13), (303, 1))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop('target', axis=1)\n",
    "y = df.target\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Define node size of input, hidden layer (one hidden layer only here), and output layer;\n",
    "        plus weights (two). These values are all fixed\n",
    "        '''\n",
    "        self.input_size = 13\n",
    "        self.hidden_layer_size = 4\n",
    "        self.output_layer_size = 1\n",
    "        \n",
    "        # Weights (parameters)\n",
    "        self.L1_weights = np.random.randn(self.input_size, self.hidden_layer_size)\n",
    "        self.L2_weights = np.random.randn(self.hidden_layer_size, self.output_layer_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''Propagate inputs forward through network'''\n",
    "        # Weighted sum between inputs and hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.L1_weights)  # WL calls this self.z2; summation is the idea\n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)  # WL calls this self.a2\n",
    "        # Weighted sum between hidden layer and output layer\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.L2_weights)  # WL calls this self.z3\n",
    "        y_hat = self.sigmoid(self.output_sum)  # called y_hat because is an estimate of output data \n",
    "        return y_hat\n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "        '''Apply sigmoid activation function to scalar, vector, or matrix'''\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def sigmoid_prime(self, s):\n",
    "        '''Calculate gradient of sigmoid'''\n",
    "        return np.exp(-s) / ((1 + np.exp(-s))**2)\n",
    "            \n",
    "    def cost_function(self, X, y):\n",
    "        '''\n",
    "        Compute cost for given X, y, using weights already stored in class. Cost is a\n",
    "        measure of how incorrect model is after at least one complete forward propagation\n",
    "        '''\n",
    "        self.y_hat = self.forward(X)\n",
    "        J = 0.5 * sum((y - self.y_hat)**2)  # J is term for cost output unit\n",
    "        return J\n",
    "        \n",
    "    def cost_function_prime(self, X, y):\n",
    "        '''Compute derivative with respect to L1_weights and L2_weights for a given X and y'''\n",
    "        self.y_hat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y - self.y_hat), self.sigmoid_prime(self.output_sum))\n",
    "        dJdL2 = np.dot(self.activated_hidden.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.L2_weights.T) * self.sigmoid_prime(self.hidden_sum)\n",
    "        dJdL1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdL1, dJdL2\n",
    "    \n",
    "    # Helper Functions for interacting with other classes\n",
    "    def get_params(self):\n",
    "        '''Get L1_weights and L2_weights unrolled into vector'''\n",
    "        params = np.concatenate((self.L1_weights.ravel(), self.L2_weights.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        '''Set L1 and L2 using single parameter vector'''\n",
    "        L1_start = 0\n",
    "        L1_end = self.hidden_layer_size * self.input_size\n",
    "        self.L1_weights = np.reshape(params[L1_start: L1_end], (self.input_size, self.hidden_layer_size))\n",
    "        L2_end = L1_end + self.hidden_layer_size * self.output_layer_size\n",
    "        self.L2_weights = np.reshape(params[L1_end: L2_end], (self.hidden_layer_size, self.output_layer_size))\n",
    "    \n",
    "    def compute_gradients(self, X, y):\n",
    "        '''\n",
    "        Returns the vector that takes us in the most downward direction along some function\n",
    "        in hyperspace that has as many dimensions as we have weights--2, in this case\n",
    "        '''\n",
    "        dJdL1, dJdL2 = self.cost_function_prime(X, y)\n",
    "        return np.concatenate((dJdL1.ravel(), dJdL2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "# Make trainer class - credit to Welch Labs\n",
    "\n",
    "class trainer():\n",
    "    def __init__(self, N):\n",
    "        # Make Local reference to network\n",
    "        self.N = N\n",
    "    \n",
    "    def callback_func(self, params):\n",
    "        self.N.set_params(params)\n",
    "        self.J.append(self.N.cost_function(self.X, self.y))   \n",
    "        \n",
    "    def cost_function_wrapper(self, params, X, y):\n",
    "        self.N.set_params(params)\n",
    "        cost = self.N.cost_function(X, y)\n",
    "        grad = self.N.compute_gradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        # Make an internal variable for the callback function\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        # Make empty list to store costs\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.get_params()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.cost_function_wrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callback_func)\n",
    "\n",
    "        self.N.set_params(_res.x)\n",
    "        self.optimization_results = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = MultilayerPerceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = trainer(MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 38.796519\n",
      "         Iterations: 0\n",
      "         Function evaluations: 18\n",
      "         Gradient evaluations: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhump\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: RuntimeWarning: overflow encountered in square\n",
      "C:\\Users\\jhump\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\jhump\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: RuntimeWarning: overflow encountered in square\n",
      "C:\\Users\\jhump\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "T.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGT1oRzXw3H9"
   },
   "source": [
    "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
    "\n",
    "- Use the Heart Disease Dataset (binary classification)\n",
    "- Use an appropriate loss function for a binary classification task\n",
    "- Use an appropriate activation function on the final layer of your network. \n",
    "- Train your model using verbose output for ease of grading.\n",
    "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
    "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWw4IYxLxKwH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jhump\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 24)                336       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 753\n",
      "Trainable params: 753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\jhump\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 242 samples, validate on 61 samples\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 50us/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 70us/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 70us/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 79us/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 66us/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 74us/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 80us/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 81us/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 70us/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 66us/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 85us/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 89us/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 60us/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 5.0726 - acc: 0.6818 - val_loss: 15.9424 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x248c6ef2cf8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Will continue to work with UCI Heart Disease dataset [domain favorite: health binary classification]\n",
    "# Train baseline model with 24 input nodes, 1 hidden layer of sixteen nodes, and 1 output node\n",
    "\n",
    "# Global hyperparameters\n",
    "inputs = X.shape[1]\n",
    "epochs = 20\n",
    "batch_size = 80\n",
    "\n",
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(Dense(24, activation='relu', input_shape=(inputs,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "model.summary()\n",
    "model.fit(X, y, validation_split=0.2, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 6.8042 - acc: 0.5702\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 134us/step - loss: 4.4126 - acc: 0.5744\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 145us/step - loss: 3.0221 - acc: 0.6322\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 163us/step - loss: 2.0247 - acc: 0.6364\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 171us/step - loss: 1.3814 - acc: 0.6240\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 153us/step - loss: 0.7759 - acc: 0.6694\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 169us/step - loss: 0.6909 - acc: 0.6983\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 355us/step - loss: 0.6555 - acc: 0.7025\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 192us/step - loss: 0.6909 - acc: 0.6529\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 143us/step - loss: 0.7191 - acc: 0.6364\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 145us/step - loss: 0.6053 - acc: 0.7107\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 298us/step - loss: 0.7115 - acc: 0.6074\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 205us/step - loss: 0.5768 - acc: 0.7190\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 211us/step - loss: 0.5463 - acc: 0.7314\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 196us/step - loss: 0.5174 - acc: 0.7314\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 198us/step - loss: 0.5513 - acc: 0.7397\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 221us/step - loss: 0.5214 - acc: 0.7479\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 167us/step - loss: 0.5214 - acc: 0.7727\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 171us/step - loss: 0.6053 - acc: 0.7314\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 182us/step - loss: 0.5078 - acc: 0.7686\n",
      "61/61 [==============================] - 0s 3ms/step\n",
      "242/242 [==============================] - 0s 97us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 159us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 161us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 132us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 151us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 172us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 211us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 207us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 171us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 143us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 174us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 153us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 171us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 167us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 159us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 198us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 182us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 169us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 167us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 149us/step - loss: 9.0911 - acc: 0.4298\n",
      "61/61 [==============================] - 0s 2ms/step\n",
      "242/242 [==============================] - ETA:  - 0s 50us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 3.2262 - acc: 0.5579\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 159us/step - loss: 1.4822 - acc: 0.5909\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 155us/step - loss: 0.8327 - acc: 0.6116\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 140us/step - loss: 0.7444 - acc: 0.5868\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 157us/step - loss: 0.6401 - acc: 0.6529\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 163us/step - loss: 0.6311 - acc: 0.6694\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 159us/step - loss: 0.6229 - acc: 0.6653\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 188us/step - loss: 0.6048 - acc: 0.6860\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 174us/step - loss: 0.6361 - acc: 0.6529\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 192us/step - loss: 0.5772 - acc: 0.7149\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 171us/step - loss: 0.5628 - acc: 0.7190\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 132us/step - loss: 0.5790 - acc: 0.7149\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 165us/step - loss: 0.5563 - acc: 0.7273\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 155us/step - loss: 0.6015 - acc: 0.6942\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 143us/step - loss: 0.5407 - acc: 0.7355\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 136us/step - loss: 0.5570 - acc: 0.6653\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 126us/step - loss: 0.6684 - acc: 0.6694\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 128us/step - loss: 0.6080 - acc: 0.6942\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 116us/step - loss: 0.8065 - acc: 0.6405\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 128us/step - loss: 0.5356 - acc: 0.7521\n",
      "61/61 [==============================] - 0s 2ms/step\n",
      "242/242 [==============================] - 0s 47us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 117us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 126us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 132us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 119us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 126us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 138us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 142us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 154us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 173us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 136us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 146us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 136us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 158us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 142us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 152us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 160us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 140us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 134us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 167us/step - loss: 10.9444 - acc: 0.3210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 2ms/step\n",
      "243/243 [==============================] - 0s 43us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 1s 3ms/step - loss: 9.4182 - acc: 0.3169\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 132us/step - loss: 2.4989 - acc: 0.5885\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 121us/step - loss: 1.6997 - acc: 0.6584\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 130us/step - loss: 1.4337 - acc: 0.6296\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 132us/step - loss: 1.3768 - acc: 0.6255\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 134us/step - loss: 1.0702 - acc: 0.6749\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 126us/step - loss: 0.9581 - acc: 0.6708\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 126us/step - loss: 0.8957 - acc: 0.6955\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 136us/step - loss: 0.7775 - acc: 0.6955\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 144us/step - loss: 0.7605 - acc: 0.6790\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 113us/step - loss: 0.8770 - acc: 0.6749\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 148us/step - loss: 0.7758 - acc: 0.6667\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 167us/step - loss: 0.7635 - acc: 0.6790\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 156us/step - loss: 0.6480 - acc: 0.7449\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 167us/step - loss: 0.5875 - acc: 0.7202\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 163us/step - loss: 0.5796 - acc: 0.7243\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 173us/step - loss: 0.5647 - acc: 0.7325\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 185us/step - loss: 0.5733 - acc: 0.7119\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - ETA: 0s - loss: 0.5691 - acc: 0.750 - 0s 167us/step - loss: 0.6675 - acc: 0.6872\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 216us/step - loss: 0.5270 - acc: 0.7366\n",
      "60/60 [==============================] - 0s 3ms/step\n",
      "243/243 [==============================] - 0s 58us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 89us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 103us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 97us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 99us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 87us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 99us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 103us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 95us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 95us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 91us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 79us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 107us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 87us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 89us/step - loss: 9.0911 - acc: 0.4298\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 97us/step - loss: 9.0911 - acc: 0.4298\n",
      "61/61 [==============================] - 0s 3ms/step\n",
      "242/242 [==============================] - 0s 37us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 85us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 91us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 101us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 87us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 105us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 99us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 97us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 95us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 116us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 95us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 103us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 76us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 99us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 91us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 76us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 85us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 79us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 97us/step - loss: 6.9268 - acc: 0.5702\n",
      "61/61 [==============================] - 0s 3ms/step\n",
      "242/242 [==============================] - 0s 33us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 3.2528 - acc: 0.5207\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 95us/step - loss: 2.5055 - acc: 0.5744\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 91us/step - loss: 1.6459 - acc: 0.5785\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 99us/step - loss: 1.2996 - acc: 0.5661\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.9740 - acc: 0.5868\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 89us/step - loss: 0.9339 - acc: 0.5579\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 118us/step - loss: 0.9516 - acc: 0.5909\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 0.7853 - acc: 0.6281\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.7931 - acc: 0.5826\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 87us/step - loss: 0.7563 - acc: 0.6240\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 99us/step - loss: 0.6756 - acc: 0.6570\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 112us/step - loss: 0.6914 - acc: 0.6694\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 87us/step - loss: 0.7436 - acc: 0.6694\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 0.6769 - acc: 0.6694\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 91us/step - loss: 0.6905 - acc: 0.6364\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.7843 - acc: 0.6405\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.6359 - acc: 0.6901\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.6940 - acc: 0.6860\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 99us/step - loss: 0.7110 - acc: 0.6736\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 107us/step - loss: 0.6229 - acc: 0.7231\n",
      "61/61 [==============================] - 0s 3ms/step\n",
      "242/242 [==============================] - 0s 50us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 82us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 76us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 95us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 88us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 84us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 80us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 88us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 78us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 97us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 146us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 105us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 84us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 88us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 86us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 84us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 101us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 91us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 119us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 86us/step - loss: 10.9444 - acc: 0.3210\n",
      "60/60 [==============================] - 0s 4ms/step\n",
      "243/243 [==============================] - 0s 39us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 78us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 78us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 84us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 82us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 109us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 86us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 84us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 107us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 88us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 105us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 99us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 95us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 84us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 95us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 95us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 86us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 107us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 97us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 95us/step - loss: 10.9444 - acc: 0.3210\n",
      "60/60 [==============================] - 0s 4ms/step\n",
      "243/243 [==============================] - 0s 60us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 8.9084 - acc: 0.4298\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 60us/step - loss: 6.2308 - acc: 0.4298\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 70us/step - loss: 2.4372 - acc: 0.4504\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 0.8672 - acc: 0.6446\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 1.4837 - acc: 0.5826\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 1.3020 - acc: 0.6116\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.8201 - acc: 0.6818\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.7384 - acc: 0.6198\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 85us/step - loss: 0.8316 - acc: 0.5992\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.7335 - acc: 0.6074\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.6788 - acc: 0.6818\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.7004 - acc: 0.6777\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.6038 - acc: 0.7066\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 0.7281 - acc: 0.6240\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.6525 - acc: 0.6777\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.5886 - acc: 0.7025\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 70us/step - loss: 0.5863 - acc: 0.6983\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 0.5827 - acc: 0.6942\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.5793 - acc: 0.6901\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.5787 - acc: 0.6901\n",
      "61/61 [==============================] - 0s 4ms/step\n",
      "242/242 [==============================] - 0s 37us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 8.7050 - acc: 0.4298\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 6.3846 - acc: 0.4876\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 66us/step - loss: 4.3471 - acc: 0.5331\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 74us/step - loss: 3.1993 - acc: 0.5826\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 2.6989 - acc: 0.5785\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 2.3044 - acc: 0.5620\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 70us/step - loss: 1.7954 - acc: 0.5661\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 1.3807 - acc: 0.5909\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 70us/step - loss: 1.0665 - acc: 0.6157\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.8813 - acc: 0.6198\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 0.8193 - acc: 0.6033\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 0.7532 - acc: 0.6281\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 70us/step - loss: 0.7635 - acc: 0.6033\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 66us/step - loss: 0.6928 - acc: 0.6405\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 66us/step - loss: 0.6763 - acc: 0.6653\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.6618 - acc: 0.6240\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.6786 - acc: 0.6364\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.6683 - acc: 0.6529\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 0.6953 - acc: 0.6198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.8728 - acc: 0.5579\n",
      "61/61 [==============================] - 0s 5ms/step\n",
      "242/242 [==============================] - 0s 35us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 91us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 74us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 66us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 70us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 89us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 87us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 107us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 81us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 70us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 91us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 79us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 74us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 79us/step - loss: 8.1257 - acc: 0.4959\n",
      "61/61 [==============================] - 0s 5ms/step\n",
      "242/242 [==============================] - 0s 43us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 1.6568 - acc: 0.7407\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 80us/step - loss: 1.7703 - acc: 0.5267\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 84us/step - loss: 1.2847 - acc: 0.7202\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 68us/step - loss: 0.9336 - acc: 0.7202\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 76us/step - loss: 1.1465 - acc: 0.6091\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 84us/step - loss: 0.9529 - acc: 0.6790\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 76us/step - loss: 0.9234 - acc: 0.6790\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 68us/step - loss: 0.8088 - acc: 0.6749\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 76us/step - loss: 1.0239 - acc: 0.7325\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 80us/step - loss: 0.7627 - acc: 0.6831\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 95us/step - loss: 0.9420 - acc: 0.6337\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 91us/step - loss: 0.9463 - acc: 0.7284\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 95us/step - loss: 0.8182 - acc: 0.7366\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 82us/step - loss: 0.6748 - acc: 0.7160\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 95us/step - loss: 0.6985 - acc: 0.7119\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 88us/step - loss: 0.6464 - acc: 0.7119\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 88us/step - loss: 0.6722 - acc: 0.6790\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 84us/step - loss: 0.6959 - acc: 0.7284\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 78us/step - loss: 0.6367 - acc: 0.7366\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 84us/step - loss: 0.6203 - acc: 0.6914\n",
      "60/60 [==============================] - 0s 6ms/step\n",
      "243/243 [==============================] - 0s 41us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 74us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 68us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 70us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 74us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 74us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 70us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 76us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 72us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 84us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 80us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 97us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 78us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 82us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 76us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 80us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 80us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 82us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 78us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 72us/step - loss: 10.9444 - acc: 0.3210\n",
      "60/60 [==============================] - 0s 7ms/step\n",
      "243/243 [==============================] - 0s 35us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 2.1818 - acc: 0.6446\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 54us/step - loss: 1.9118 - acc: 0.6777\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 62us/step - loss: 1.7489 - acc: 0.6942\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 64us/step - loss: 1.3587 - acc: 0.7149\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 1.6744 - acc: 0.5992\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 74us/step - loss: 1.6289 - acc: 0.5620\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 70us/step - loss: 1.0906 - acc: 0.6983\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 70us/step - loss: 1.5927 - acc: 0.6322\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 1.1771 - acc: 0.6818\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 78us/step - loss: 1.0434 - acc: 0.6570\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 76us/step - loss: 1.0085 - acc: 0.6818\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 74us/step - loss: 1.0971 - acc: 0.6818\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 78us/step - loss: 0.9356 - acc: 0.7025\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.9057 - acc: 0.6942\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.9756 - acc: 0.7231\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 1.0328 - acc: 0.7025\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.9590 - acc: 0.6694\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 64us/step - loss: 1.2656 - acc: 0.5785\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.8981 - acc: 0.6942\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 64us/step - loss: 0.9285 - acc: 0.7107\n",
      "61/61 [==============================] - 0s 6ms/step\n",
      "242/242 [==============================] - 0s 33us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 116us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 510us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 85us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 62us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 76us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 79us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 60us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 60us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 70us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 58us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 64us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 70us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 66us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 64us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 70us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 60us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 78us/step - loss: 6.9268 - acc: 0.5702\n",
      "61/61 [==============================] - 0s 6ms/step\n",
      "242/242 [==============================] - 0s 31us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 1.4735 - acc: 0.5785\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 66us/step - loss: 1.3140 - acc: 0.5455\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 58us/step - loss: 1.1132 - acc: 0.5992\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.8266 - acc: 0.6364\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.7740 - acc: 0.6446\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 66us/step - loss: 0.8107 - acc: 0.6405\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.7271 - acc: 0.6446\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.7097 - acc: 0.6364\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.8433 - acc: 0.5826\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.7501 - acc: 0.6116\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.7530 - acc: 0.6488\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.7510 - acc: 0.6364\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 70us/step - loss: 0.7385 - acc: 0.6240\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 68us/step - loss: 0.6829 - acc: 0.6529\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.7354 - acc: 0.6612\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.6849 - acc: 0.6570\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.7744 - acc: 0.6322\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.7287 - acc: 0.6446\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 97us/step - loss: 0.7353 - acc: 0.6157\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.6699 - acc: 0.6612\n",
      "61/61 [==============================] - 0s 7ms/step\n",
      "242/242 [==============================] - 0s 35us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 2s 9ms/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 55us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 72us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 101us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 62us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 78us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 74us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 60us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 58us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 101us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 80us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 121us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 80us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 88us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 86us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 105us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 148us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 95us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 97us/step - loss: 10.9444 - acc: 0.3210\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 119us/step - loss: 10.9444 - acc: 0.3210\n",
      "60/60 [==============================] - 0s 8ms/step\n",
      "243/243 [==============================] - 0s 49us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 2s 9ms/step - loss: 8.5803 - acc: 0.3210\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 101us/step - loss: 5.8188 - acc: 0.3786\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 78us/step - loss: 3.3819 - acc: 0.5844\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 60us/step - loss: 1.7753 - acc: 0.6667\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 84us/step - loss: 1.7219 - acc: 0.7119\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 78us/step - loss: 2.2145 - acc: 0.7243\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - ETA: 0s - loss: 1.5590 - acc: 0.812 - 0s 97us/step - loss: 2.2801 - acc: 0.7160\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 93us/step - loss: 2.0306 - acc: 0.7284\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 107us/step - loss: 1.5929 - acc: 0.7202\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 121us/step - loss: 1.4085 - acc: 0.6790\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 93us/step - loss: 1.3718 - acc: 0.6461\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 80us/step - loss: 1.2630 - acc: 0.6543\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 70us/step - loss: 1.1274 - acc: 0.7037\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 84us/step - loss: 1.0563 - acc: 0.6872\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 99us/step - loss: 0.9966 - acc: 0.6790\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 82us/step - loss: 0.9502 - acc: 0.6831\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 86us/step - loss: 0.9014 - acc: 0.6708\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 88us/step - loss: 0.8680 - acc: 0.6790\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 0s 95us/step - loss: 0.8426 - acc: 0.6955\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 103us/step - loss: 0.8256 - acc: 0.6914\n",
      "60/60 [==============================] - 1s 10ms/step\n",
      "243/243 [==============================] - 0s 33us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhump\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "303/303 [==============================] - 1s 5ms/step - loss: 6.9331 - acc: 0.3960\n",
      "Epoch 2/20\n",
      "303/303 [==============================] - 0s 150us/step - loss: 4.5054 - acc: 0.4059\n",
      "Epoch 3/20\n",
      "303/303 [==============================] - 0s 142us/step - loss: 3.2680 - acc: 0.3927\n",
      "Epoch 4/20\n",
      "303/303 [==============================] - 0s 135us/step - loss: 2.2994 - acc: 0.3795\n",
      "Epoch 5/20\n",
      "303/303 [==============================] - 0s 152us/step - loss: 1.5262 - acc: 0.3696\n",
      "Epoch 6/20\n",
      "303/303 [==============================] - 0s 134us/step - loss: 1.1089 - acc: 0.4158\n",
      "Epoch 7/20\n",
      "303/303 [==============================] - 0s 144us/step - loss: 0.9067 - acc: 0.5215\n",
      "Epoch 8/20\n",
      "303/303 [==============================] - 0s 152us/step - loss: 0.8014 - acc: 0.5248\n",
      "Epoch 9/20\n",
      "303/303 [==============================] - 0s 145us/step - loss: 0.7650 - acc: 0.5908\n",
      "Epoch 10/20\n",
      "303/303 [==============================] - 0s 137us/step - loss: 0.9241 - acc: 0.5446\n",
      "Epoch 11/20\n",
      "303/303 [==============================] - 0s 262us/step - loss: 0.8192 - acc: 0.6007\n",
      "Epoch 12/20\n",
      "303/303 [==============================] - 0s 172us/step - loss: 0.6925 - acc: 0.6436\n",
      "Epoch 13/20\n",
      "303/303 [==============================] - 0s 137us/step - loss: 0.7046 - acc: 0.6139\n",
      "Epoch 14/20\n",
      "303/303 [==============================] - 0s 152us/step - loss: 0.6905 - acc: 0.6403\n",
      "Epoch 15/20\n",
      "303/303 [==============================] - 0s 177us/step - loss: 0.6133 - acc: 0.6865\n",
      "Epoch 16/20\n",
      "303/303 [==============================] - 0s 134us/step - loss: 0.6015 - acc: 0.7030\n",
      "Epoch 17/20\n",
      "303/303 [==============================] - 0s 152us/step - loss: 0.6060 - acc: 0.7063\n",
      "Epoch 18/20\n",
      "303/303 [==============================] - 0s 158us/step - loss: 0.6314 - acc: 0.6799\n",
      "Epoch 19/20\n",
      "303/303 [==============================] - 0s 150us/step - loss: 0.5589 - acc: 0.7030\n",
      "Epoch 20/20\n",
      "303/303 [==============================] - 0s 152us/step - loss: 0.5341 - acc: 0.7393\n",
      "Best: 0.7854785478547854 using {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.7854785478547854, Stdev: 0.17873916063890743 with: {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.7128712871287128, Stdev: 0.3942732345420873 with: {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.6138613861386139, Stdev: 0.2612277448698444 with: {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.4884488448844885, Stdev: 0.33263741359520604 with: {'batch_size': 80, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "# Bring in GridSearchCV, which requires KerasClassifier\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "kc_model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# Define the grid search parameters\n",
    "param_grid = {'batch_size': [20, 40, 60, 80],\n",
    "              'epochs': [20]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=kc_model, param_grid=param_grid, scoring='accuracy', n_jobs=1, cv=5)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")\n",
    "\n",
    "# Comment: This model achieved highest performance in my experiments in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 9ms/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 244us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 227us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 298us/step - loss: 9.0911 - acc: 0.0000e+00 0s - loss: 9.1004 - acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 318us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 256us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 200us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 171us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 192us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 184us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 171us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 198us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 254us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 248us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 238us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 207us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 190us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 188us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 186us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 217us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "61/61 [==============================] - 0s 8ms/step\n",
      "242/242 [==============================] - 0s 157us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 9ms/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 238us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 209us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 205us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 176us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 149us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 188us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 271us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 275us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 244us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 194us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 217us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 225us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 331us/step - loss: 6.9268 - acc: 0.5702 0s - loss: 6.2681 - acc: 0.611\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 267us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 236us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 275us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 192us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - ETA: 0s - loss: 4.8354 - acc: 0.700 - 0s 213us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 289us/step - loss: 6.9268 - acc: 0.5702\n",
      "61/61 [==============================] - 1s 10ms/step\n",
      "242/242 [==============================] - 0s 89us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 8.1249 - acc: 0.4917\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 236us/step - loss: 8.1249 - acc: 0.4917\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 225us/step - loss: 8.1249 - acc: 0.4917\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 215us/step - loss: 8.1249 - acc: 0.4917\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 283us/step - loss: 8.1249 - acc: 0.4917\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 401us/step - loss: 8.1249 - acc: 0.4917\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 287us/step - loss: 8.1249 - acc: 0.4917\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 246us/step - loss: 8.1249 - acc: 0.4917\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 291us/step - loss: 8.1249 - acc: 0.4917\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 227us/step - loss: 8.1249 - acc: 0.4917\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 248us/step - loss: 8.1249 - acc: 0.4917\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 182us/step - loss: 8.1249 - acc: 0.4917\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 188us/step - loss: 8.1249 - acc: 0.4917\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 167us/step - loss: 8.1249 - acc: 0.4917\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 207us/step - loss: 8.1249 - acc: 0.4917\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 186us/step - loss: 8.1249 - acc: 0.4917\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 194us/step - loss: 8.1249 - acc: 0.4917\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 211us/step - loss: 8.1249 - acc: 0.4917\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 202us/step - loss: 8.1249 - acc: 0.4917\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 186us/step - loss: 8.1249 - acc: 0.4917\n",
      "61/61 [==============================] - 1s 9ms/step\n",
      "242/242 [==============================] - 0s 91us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 2s 7ms/step - loss: 5.0576 - acc: 0.0041 \n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 193us/step - loss: 4.8142 - acc: 0.1687\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 200us/step - loss: 4.7968 - acc: 0.1605\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 193us/step - loss: 5.1175 - acc: 0.0041\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 195us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 198us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 200us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 200us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 202us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 191us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 210us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 191us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 195us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 175us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 259us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 177us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 200us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 233us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 0s 274us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 247us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "60/60 [==============================] - 1s 11ms/step\n",
      "243/243 [==============================] - 0s 152us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 2s 9ms/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 181us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 175us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 160us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 167us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 198us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 265us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 220us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 257us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 300us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 290us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 210us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 206us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 208us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 173us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 165us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 198us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 189us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 202us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 206us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "60/60 [==============================] - 1s 11ms/step\n",
      "243/243 [==============================] - 0s 80us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 128us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 116us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 112us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 130us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 128us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 138us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 140us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 145us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 159us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 184us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 140us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 136us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 143us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 116us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 116us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 109us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 116us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 128us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 122us/step - loss: 6.9268 - acc: 0.5702\n",
      "61/61 [==============================] - 1s 12ms/step\n",
      "242/242 [==============================] - 0s 60us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 10ms/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 120us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 151us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 149us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 178us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 176us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 151us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 157us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 157us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 151us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 192us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 188us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 198us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 194us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 126us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 171us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 169us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 198us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 180us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 207us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "61/61 [==============================] - 1s 11ms/step\n",
      "242/242 [==============================] - 0s 62us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 132us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 112us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 147us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 120us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 153us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 149us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 143us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 178us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 188us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 143us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 145us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 163us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 157us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 147us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 176us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 138us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 132us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 132us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 138us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "61/61 [==============================] - 1s 10ms/step\n",
      "242/242 [==============================] - 0s 56us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 2s 8ms/step - loss: 4.8632 - acc: 0.0329\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 93us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 99us/step - loss: 5.0527 - acc: 0.0041\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 115us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 101us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 99us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 111us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 128us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 134us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 138us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 132us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 115us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 121us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 123us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 128us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 140us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 111us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 109us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 212us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 401us/step - loss: 5.0517 - acc: 0.0041\n",
      "60/60 [==============================] - 1s 12ms/step\n",
      "243/243 [==============================] - 0s 68us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 2s 9ms/step - loss: 10.3235 - acc: 0.2222\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 107us/step - loss: 11.3577 - acc: 0.2551\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 117us/step - loss: 9.8004 - acc: 0.1934\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 121us/step - loss: 5.1461 - acc: 0.0412\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 132us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 156us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 191us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 294us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 189us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 165us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 220us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 222us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 195us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 175us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 251us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 333us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 163us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 214us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 125us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 132us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "60/60 [==============================] - 1s 12ms/step\n",
      "243/243 [==============================] - 0s 80us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 10ms/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 112us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 87us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 72us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 87us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 89us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 107us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 89us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 89us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 85us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 95us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 95us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 97us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 95us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 97us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 85us/step - loss: 6.9268 - acc: 0.5702\n",
      "61/61 [==============================] - 1s 12ms/step\n",
      "242/242 [==============================] - 0s 41us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 9ms/step - loss: 8.8244 - acc: 0.1570\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 9.0472 - acc: 0.0413\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 99us/step - loss: 8.9045 - acc: 0.0207\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 8.8998 - acc: 0.0083\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 120us/step - loss: 9.0304 - acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 105us/step - loss: 9.0356 - acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 85us/step - loss: 9.0268 - acc: 0.0041\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 89us/step - loss: 9.0252 - acc: 0.0041\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 103us/step - loss: 9.0252 - acc: 0.0041\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 89us/step - loss: 9.0252 - acc: 0.0041\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 99us/step - loss: 9.0252 - acc: 0.0041\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 89us/step - loss: 9.0252 - acc: 0.0041\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 9.0252 - acc: 0.0041\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 87us/step - loss: 9.0252 - acc: 0.0041\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 99us/step - loss: 9.0252 - acc: 0.0041\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 118us/step - loss: 9.0252 - acc: 0.0041\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 87us/step - loss: 9.0252 - acc: 0.0041\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 105us/step - loss: 9.0252 - acc: 0.0041\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 87us/step - loss: 9.0252 - acc: 0.0041\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 99us/step - loss: 9.0252 - acc: 0.0041\n",
      "61/61 [==============================] - 1s 12ms/step\n",
      "242/242 [==============================] - 0s 58us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 9ms/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 78us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 95us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 91us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 91us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 85us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 95us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 81us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 116us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 103us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 97us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 101us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 95us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 105us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 105us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 118us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 112us/step - loss: 8.1257 - acc: 0.4959\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 112us/step - loss: 8.1257 - acc: 0.4959\n",
      "61/61 [==============================] - 1s 12ms/step\n",
      "242/242 [==============================] - 0s 64us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 2s 9ms/step - loss: 4.6698 - acc: 0.1358\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 93us/step - loss: 4.9401 - acc: 0.0123\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 78us/step - loss: 4.9861 - acc: 0.0082\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 95us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 95us/step - loss: 5.0600 - acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 105us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 84us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 101us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 78us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 95us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 97us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 93us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 101us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 103us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 93us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 109us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 99us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 101us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 103us/step - loss: 5.0517 - acc: 0.0041\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 111us/step - loss: 5.0517 - acc: 0.0041\n",
      "60/60 [==============================] - 1s 15ms/step\n",
      "243/243 [==============================] - 0s 43us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 3s 13ms/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 204us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 113us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 99us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 103us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 128us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 158us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 132us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 117us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 128us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 117us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 136us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 115us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 132us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 109us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 101us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 140us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 107us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 103us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 126us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "60/60 [==============================] - 1s 13ms/step\n",
      "243/243 [==============================] - 0s 43us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 10ms/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 105us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 97us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 101us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 74us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 110us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 120us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 76us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 97us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 87us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 107us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 120us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 110us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 97us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 89us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 97us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 99us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 9.0911 - acc: 0.0000e+00\n",
      "61/61 [==============================] - 1s 13ms/step\n",
      "242/242 [==============================] - 0s 37us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 10ms/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 74us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 76us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 74us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 99us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 81us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 105us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 112us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 97us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 95us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 91us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 89us/step - loss: 6.9268 - acc: 0.5702\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 99us/step - loss: 6.9268 - acc: 0.5702\n",
      "61/61 [==============================] - 1s 15ms/step\n",
      "242/242 [==============================] - 0s 43us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 2s 10ms/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 85us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 79us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 81us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 95us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 91us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 99us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 99us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 97us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 91us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 105us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 79us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 95us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 89us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 95us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 93us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 107us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 83us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 87us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 101us/step - loss: 7.9053 - acc: 0.0000e+00\n",
      "61/61 [==============================] - 1s 14ms/step\n",
      "242/242 [==============================] - 0s 52us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 2s 10ms/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 86us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 101us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 84us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 86us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 78us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 80us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 84us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 93us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 84us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 97us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 84us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 97us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 88us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 101us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 86us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 103us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - ETA: 0s - loss: 4.3842 - acc: 0.0000e+0 - 0s 93us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 93us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 119us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "60/60 [==============================] - 1s 14ms/step\n",
      "243/243 [==============================] - 0s 43us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 3s 11ms/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 82us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 80us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 91us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 82us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 91us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 88us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 82us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 101us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 0s 93us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 103us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 88us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 76us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 91us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 121us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 95us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 99us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 91us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 113us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 91us/step - loss: 5.1173 - acc: 0.0000e+00\n",
      "60/60 [==============================] - 1s 15ms/step\n",
      "243/243 [==============================] - 0s 39us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhump\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "303/303 [==============================] - 3s 9ms/step - loss: 7.2101 - acc: 0.0066\n",
      "Epoch 2/20\n",
      "303/303 [==============================] - 0s 83us/step - loss: 7.2090 - acc: 0.0066\n",
      "Epoch 3/20\n",
      "303/303 [==============================] - 0s 106us/step - loss: 7.2123 - acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "303/303 [==============================] - 0s 84us/step - loss: 7.2083 - acc: 0.0033\n",
      "Epoch 5/20\n",
      "303/303 [==============================] - 0s 112us/step - loss: 7.2083 - acc: 0.0033\n",
      "Epoch 6/20\n",
      "303/303 [==============================] - 0s 101us/step - loss: 7.2083 - acc: 0.0033\n",
      "Epoch 7/20\n",
      "303/303 [==============================] - 0s 94us/step - loss: 7.1195 - acc: 0.0099\n",
      "Epoch 8/20\n",
      "303/303 [==============================] - 0s 101us/step - loss: 7.1123 - acc: 0.0165\n",
      "Epoch 9/20\n",
      "303/303 [==============================] - 0s 94us/step - loss: 7.2083 - acc: 0.0033\n",
      "Epoch 10/20\n",
      "303/303 [==============================] - 0s 82us/step - loss: 7.2083 - acc: 0.0033\n",
      "Epoch 11/20\n",
      "303/303 [==============================] - 0s 94us/step - loss: 7.2083 - acc: 0.0033\n",
      "Epoch 12/20\n",
      "303/303 [==============================] - 0s 91us/step - loss: 7.2083 - acc: 0.0033\n",
      "Epoch 13/20\n",
      "303/303 [==============================] - 0s 87us/step - loss: 7.2083 - acc: 0.0033\n",
      "Epoch 14/20\n",
      "303/303 [==============================] - 0s 76us/step - loss: 7.2083 - acc: 0.0033\n",
      "Epoch 15/20\n",
      "303/303 [==============================] - 0s 81us/step - loss: 7.2083 - acc: 0.0033\n",
      "Epoch 16/20\n",
      "303/303 [==============================] - ETA: 0s - loss: 8.1705 - acc: 0.0000e+0 - 0s 63us/step - loss: 7.2083 - acc: 0.0033\n",
      "Epoch 17/20\n",
      "303/303 [==============================] - 0s 73us/step - loss: 7.2083 - acc: 0.0033\n",
      "Epoch 18/20\n",
      "303/303 [==============================] - 0s 83us/step - loss: 7.2083 - acc: 0.0033\n",
      "Epoch 19/20\n",
      "303/303 [==============================] - 0s 94us/step - loss: 7.2083 - acc: 0.0033\n",
      "Epoch 20/20\n",
      "303/303 [==============================] - 0s 79us/step - loss: 7.2083 - acc: 0.0033\n",
      "Best: 0.3432343234323432 using {'batch_size': 80, 'epochs': 20}\n",
      "Means: 0.2607260726072607, Stdev: 0.38842202420902344 with: {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.33993399339933994, Stdev: 0.4233867363174336 with: {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.25742574257425743, Stdev: 0.38214550186147705 with: {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.3432343234323432, Stdev: 0.42842526054244967 with: {'batch_size': 80, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "# Repeat experiment, changing only activation function in output layer from sigmoid to relu\n",
    "\n",
    "def create_model():\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='relu'))  # changed from sigmoid in previous experiment\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "kc_model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=kc_model, param_grid=param_grid, scoring='accuracy', n_jobs=1, cv=5)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")\n",
    "\n",
    "# Comment: Changing output layer activation function from sigmoid to relu significantly decreased performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_133 (Dense)            (None, 36)                504       \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 16)                592       \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,113\n",
      "Trainable params: 1,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 242 samples, validate on 61 samples\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 4s 15ms/step - loss: 3.5429 - acc: 0.3636 - val_loss: 9.3469 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 132us/step - loss: 3.1809 - acc: 0.6818 - val_loss: 9.5680 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 172us/step - loss: 2.3019 - acc: 0.6777 - val_loss: 2.2947 - val_acc: 0.3443\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 202us/step - loss: 1.7936 - acc: 0.3884 - val_loss: 1.1048 - val_acc: 0.6557\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 157us/step - loss: 1.7050 - acc: 0.4256 - val_loss: 2.7403 - val_acc: 0.2131\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 161us/step - loss: 1.2175 - acc: 0.6901 - val_loss: 4.7128 - val_acc: 0.0492\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 134us/step - loss: 1.2272 - acc: 0.7025 - val_loss: 2.4020 - val_acc: 0.3115\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 161us/step - loss: 0.8597 - acc: 0.7066 - val_loss: 2.0746 - val_acc: 0.3443\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 163us/step - loss: 0.8674 - acc: 0.7066 - val_loss: 2.6945 - val_acc: 0.2951\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 207us/step - loss: 0.9080 - acc: 0.7107 - val_loss: 2.4140 - val_acc: 0.3607\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 130us/step - loss: 0.8857 - acc: 0.6983 - val_loss: 1.5086 - val_acc: 0.5410\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 149us/step - loss: 1.1536 - acc: 0.6364 - val_loss: 0.8058 - val_acc: 0.6557\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 165us/step - loss: 1.1923 - acc: 0.5950 - val_loss: 1.5529 - val_acc: 0.4918\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 140us/step - loss: 0.8583 - acc: 0.7190 - val_loss: 1.7984 - val_acc: 0.4098\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 167us/step - loss: 0.8347 - acc: 0.7314 - val_loss: 1.5930 - val_acc: 0.4590\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 161us/step - loss: 0.8336 - acc: 0.7149 - val_loss: 1.5708 - val_acc: 0.4590\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 182us/step - loss: 0.9427 - acc: 0.6653 - val_loss: 1.3998 - val_acc: 0.5410\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 169us/step - loss: 0.7986 - acc: 0.7066 - val_loss: 3.0371 - val_acc: 0.2623\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 143us/step - loss: 0.9053 - acc: 0.7314 - val_loss: 2.9004 - val_acc: 0.2787\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 159us/step - loss: 0.8081 - acc: 0.7025 - val_loss: 2.2574 - val_acc: 0.3443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x248c6e26da0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train new model with 36 input nodes, 1 hidden layer of sixteen nodes, and 1 output node\n",
    "\n",
    "# Global hyperparameters\n",
    "inputs = X.shape[1]\n",
    "epochs = 20\n",
    "batch_size = 80\n",
    "\n",
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(Dense(36, activation='relu', input_shape=(inputs,)))  # was 24 input nodes in baseline\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "model.summary()\n",
    "model.fit(X, y, validation_split=0.2, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# Comment: Increasing input nodes from 24 to 36 worsened the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_136 (Dense)            (None, 24)                336       \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 961\n",
      "Trainable params: 961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 242 samples, validate on 61 samples\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 4s 15ms/step - loss: 4.5166 - acc: 0.6818 - val_loss: 9.4226 - val_acc: 0.0164\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 101us/step - loss: 2.5608 - acc: 0.5331 - val_loss: 2.1147 - val_acc: 0.4918\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 101us/step - loss: 2.8987 - acc: 0.3223 - val_loss: 3.5316 - val_acc: 0.2459\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 136us/step - loss: 1.9007 - acc: 0.4421 - val_loss: 4.5413 - val_acc: 0.1475\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 134us/step - loss: 1.6272 - acc: 0.4917 - val_loss: 3.5689 - val_acc: 0.2131\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 151us/step - loss: 1.6202 - acc: 0.4669 - val_loss: 3.1862 - val_acc: 0.2131\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 167us/step - loss: 1.4324 - acc: 0.5207 - val_loss: 5.1399 - val_acc: 0.0820\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 231us/step - loss: 1.4292 - acc: 0.6364 - val_loss: 5.0367 - val_acc: 0.0820\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 182us/step - loss: 1.3131 - acc: 0.6364 - val_loss: 3.7274 - val_acc: 0.1639\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 169us/step - loss: 1.2141 - acc: 0.5661 - val_loss: 3.5463 - val_acc: 0.1803\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 153us/step - loss: 1.1707 - acc: 0.5785 - val_loss: 3.2335 - val_acc: 0.2295\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 174us/step - loss: 1.1770 - acc: 0.5537 - val_loss: 2.7922 - val_acc: 0.3115\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 155us/step - loss: 1.1079 - acc: 0.5785 - val_loss: 3.3535 - val_acc: 0.1803\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 151us/step - loss: 1.0363 - acc: 0.5868 - val_loss: 3.4843 - val_acc: 0.1475\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 196us/step - loss: 1.0396 - acc: 0.6281 - val_loss: 3.7750 - val_acc: 0.0984\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 157us/step - loss: 0.9990 - acc: 0.6446 - val_loss: 2.6005 - val_acc: 0.2623\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 200us/step - loss: 0.9450 - acc: 0.5579 - val_loss: 3.0791 - val_acc: 0.1311\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 143us/step - loss: 1.0143 - acc: 0.6488 - val_loss: 2.8947 - val_acc: 0.1311\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 234us/step - loss: 0.9361 - acc: 0.5661 - val_loss: 0.7406 - val_acc: 0.6066\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 196us/step - loss: 1.3641 - acc: 0.4421 - val_loss: 1.9243 - val_acc: 0.2951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x248f3b03e80>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train new model with 24 input nodes [same as baseline], 1 hidden layer of 24 nodes [up from baseline],\n",
    "# and 1 output node\n",
    "\n",
    "# Global hyperparameters\n",
    "inputs = X.shape[1]\n",
    "epochs = 20\n",
    "batch_size = 80\n",
    "\n",
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(Dense(24, activation='relu', input_shape=(inputs,)))\n",
    "model.add(Dense(24, activation='relu'))  # was 16 nodes at baseline\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "model.summary()\n",
    "model.fit(X, y, validation_split=0.2, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# Comment: Increasing hidden layer nodes from 16 to 24 decreased the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DS43SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
