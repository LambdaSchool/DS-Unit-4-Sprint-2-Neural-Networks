{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6SKlgYrpcym"
   },
   "source": [
    "# Neural Networks Sprint Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrEbRrjVphPM"
   },
   "source": [
    "## 1) Define the following terms:\n",
    "\n",
    "- Neuron\n",
    "- Input Layer\n",
    "- Hidden Layer\n",
    "- Output Layer\n",
    "- Activation\n",
    "- Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5EksLqnp4oB"
   },
   "source": [
    "Neuron: a container that holds a single scalar (usually a floating-point number) called an activation; neurons combine to form columns that make up either the input data or layers in a neural network. Also called 'node'.   \n",
    "Input Layer: Ryan Allred mentioned during lecture that sometimes 'layer' is a misnomer with regard to input. In any case, this is the left-most information in a NN and is in the form of an array of rows by columns.  \n",
    "Hidden Layer: this is a true layer, comprised of neurons, of which at least 1 is required to form a non-perceptron NN. Each hidden layer in a NN receives activations from the input or previous hidden layer, applies a weight to each activation then a bias, and forwards the new activations to each neuron in the next hidden layer or output layer.  \n",
    "Output Layer: the right-most layer in a NN, this receives activations from the previous layer then returns final scalars, either integer or float, that provide an information array about the question of interest, eg, classification or regression.  \n",
    "Activation: a single scalar found in a neuron.  \n",
    "Backpropagation: an algorithmic process by which weights in a NN are revised in a backwards propagated fashion after each training epoch, ie last weight is revised, then one just prior, then one before that, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ri_gRA2Jp728"
   },
   "source": [
    "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 1  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ig6ZTH8tpQ19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 4), (4, 1))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define x1, x2, x3\n",
    "\n",
    "x1 = [1, 1, 0, 1]\n",
    "x2 = [1, 0, 1, 0]\n",
    "x3 = [1, 1, 1, 1]\n",
    "y_list = [1, 0, 0, 0]\n",
    "\n",
    "X = np.array(list(zip(x1, x2, x3, np.ones(4))))\n",
    "y = np.array([[val] for val in y_list])\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write perceptron class\n",
    "\n",
    "\n",
    "class ANDPerceptron():\n",
    "    def __init__(self, X, y, niter=100):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.niter = niter\n",
    "       \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_prime(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "    \n",
    "    def fit(self):\n",
    "        # Create weights\n",
    "        weights = 2 * np.random.random((self.X.shape[0], self.X.shape[1])) - 1\n",
    "\n",
    "        for iteration in range(self.niter):\n",
    "            # Weighted sum of inputs and weights\n",
    "            weighted_sum = np.dot(self.X, weights)\n",
    "\n",
    "            # Activate with sigmoid function\n",
    "            activated_output = self.sigmoid(weighted_sum)\n",
    "\n",
    "            # Calculate Error\n",
    "            error = self.y - activated_output\n",
    "\n",
    "            # Calculate weight adjustments with sigmoid_derivative\n",
    "            adjustments = error * self.sigmoid_prime(activated_output)\n",
    "\n",
    "            # Update weights\n",
    "            weights += np.dot(self.X.T, adjustments)\n",
    "        \n",
    "        print('optimized weights after training: ')\n",
    "        print(weights)\n",
    "        print('\\ny:', y)\n",
    "        print(\"\\noutputs after training:\")\n",
    "        print(activated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP = ANDPerceptron(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized weights after training: \n",
      "[[ 3.07459478  2.99438472  3.08288513  2.90912249]\n",
      " [ 3.86541602  3.78626125  3.88584973  3.72090025]\n",
      " [-3.20702151 -1.9927788  -3.59795866 -3.60748039]\n",
      " [-2.55054592 -3.64689253 -2.18076543 -1.92065838]]\n",
      "\n",
      "y: [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "outputs after training:\n",
      "[[0.76363645 0.75600267 0.7650122  0.74865501]\n",
      " [0.0645059  0.0668376  0.06371967 0.06849584]\n",
      " [0.13206253 0.13659925 0.13199551 0.14220624]\n",
      " [0.0645059  0.0668376  0.06371967 0.06849584]]\n"
     ]
    }
   ],
   "source": [
    "AP.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86HyRi8Osr3U"
   },
   "source": [
    "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
    "- Your network must have one hidden layer. \n",
    "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "- Train your model on the Heart Disease dataset from UCI:\n",
    "\n",
    "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
    "\n",
    "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNfiajv3v4Ed"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGT1oRzXw3H9"
   },
   "source": [
    "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
    "\n",
    "- Use the Heart Disease Dataset (binary classification)\n",
    "- Use an appropriate loss function for a binary classification task\n",
    "- Use an appropriate activation function on the final layer of your network. \n",
    "- Train your model using verbose output for ease of grading.\n",
    "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
    "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWw4IYxLxKwH"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DS43SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
