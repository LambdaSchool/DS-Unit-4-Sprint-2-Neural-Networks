{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FVwDfNcdqlbZ"
   },
   "source": [
    "This study guide should reinforce and provide practice for all of the concepts you have seen in the past week. There are a mix of written questions and coding exercises, both are equally important to prepare you for the sprint challenge as well as to be able to speak on these topics comfortably in interviews and on the job.\n",
    "\n",
    "If you get stuck or are unsure of something remember the 20 minute rule. If that doesn't help, then research a solution with google and stackoverflow. Only once you have exausted these methods should you turn to your Team Lead - they won't be there on your SC or during an interview. That being said, don't hesitate to ask for help if you truly are stuck.\n",
    "\n",
    "Have fun studying!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zHl_qn9TpggG"
   },
   "source": [
    "# Neural Networks by Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F3LTw7Csr53V"
   },
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ssxAniIZqxxU"
   },
   "source": [
    "Define the following terms in your own words, do not simply copy and paste a definition found elsewhere but reword it to be understandable and memorable to you. *Double click the markdown to add your definitions.*\n",
    "\n",
    "**Input Layer:** is what receives input from our dataset. Sometimes it is called the visible layer because it's the only part that is exposed to our data and that our data interacts with directly. Typically node maps are drawn with one input node for each of the different inputs/features/columns of our dataset that will be passed to the network.\n",
    "\n",
    "**Hidden Layer:** This is because they cannot be accessed except through the input layer. They're inside of the network and they perform their functions, but we don't directly interact with them. The simplest possible network is to have a single neuron in the hidden layer that just outputs the value. \n",
    "\n",
    "**Output Layer:** The final layer is called the Output Layer. The purpose of the output layer is to output a vector of values that is in a format that is suitable for the type of problem that we're trying to address. Typically the output value is modified by an \"activation function\" to transform it into a format that makes sense for our context\n",
    "\n",
    "**Neuron:** the neurons or \"nodes\" are similar in that they receive inputs and pass on their signal to the next layer of nodes if a certain threshold is reached. The goal with ANNs is not to create a realistic model of the brain but to craft robust algorithms and data structures that can model the complex relationships found in data.\n",
    "\n",
    "**Weight:** A weight represent the strength of the connection between units. If the weight from node 1 to node 2 has greater magnitude, it means that neuron 1 has greater influence over neuron 2. A weight brings down the importance of the input value.\n",
    "\n",
    "**Bias:** Bias is a constant which helps the model in a way that it can fit best for the given data\n",
    "\n",
    "**Activation Function:** In Neural Networks, each node has an activation function. The activation function decides whether a cell \"fires\" or not. Sometimes it is said that the cell is \"activated\" or not. In Artificial Neural Networks activation functions decide how much signal to pass onto the next layer. This is why they are sometimes referred to as transfer functions because they determine how much signal is transferred to the next layer.\n",
    "\n",
    "**Node Map:** it's a visual diagram of the architecture or \"topology\" of our neural network. Typically node maps are drawn with one input node for each of the different inputs/features/columns of our dataset that will be passed to the network.\n",
    "\n",
    "**Perceptron:** A perceptron is just a single node or neuron of a neural network with nothing else.\n",
    "\n",
    "**Epoch:** Same as iteration\n",
    "\n",
    "**Feed Forward Neural Network:** feed forward neural networks in which the data flows in one direction (forward propagation) and the error flows in the opposite direction (backwards propagation). \n",
    "\n",
    "**Back Propagation:** Backpropagation is short for \"Backwards Propagation of errors\" and refers to a specific (rather calculus intensive) algorithm for how weights in a neural network are updated in reverse order at the end of each training epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZgO6aE9br9N9"
   },
   "source": [
    "## Questions of Understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SwG5JUtEzMGh"
   },
   "source": [
    "1. Name 2 activation functions and when they might be used\n",
    "```\n",
    "#https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/\n",
    " - NNS applied to a binary classification problem might use a sigmoid function as its activation function in order to squishify values down to represent a probability. \n",
    " - NNS applied to multiclass classification problems might have multiple output nodes in the output layer, one for each class that we're trying to predict. This output layer would probably employ what's called a \"softmax function\" for accomplishing this\n",
    " ```\n",
    "\n",
    "\n",
    "2. What types of machine learning problems are neural networks best suited for?\n",
    "```\n",
    "Neural networks are best for situations where the data is “high-dimensional.” For example, a medium-size image file may have 1024 x 768 pixels. Each pixel contains 3 values for the intensity of red, green, and blue at that point in the image. All told, this is 1024 x 768 x 3 = 2,359,296 values. Each one of these values is a separate dimension and a separate input to a neuron at the start of the network.\n",
    "```\n",
    "\n",
    "\n",
    "3. In a linear regression problem, we can attempt to account for nonlinear features with polynomial features. What problem do we encounter as our feature size increases? How does a neural network avoid/address this issue?\n",
    "```\n",
    "The reality is that in order to fit really curvy nonlinear patterns in data in really complex high dimensional features spaces, the number of polynomial terms that we would have to include in a linear or logistic regression model faces a problem of combinatorial explosion in terms of the number of features that would be required. The interactions between layers of neurons in neural networks in a way accounts for that combinatorial explosion within the structure of the algorithm as needed instead of us having to provide it beforehand.\n",
    "```\n",
    "\n",
    "\n",
    "4. What are some of the tradeoffs of using a neural network versus a traditional machine learning algorithm like linear regression or a decision tree?\n",
    "```\n",
    "STRENGTHS: Linear regression is straightforward to understand and explain, and can be regularized to avoid overfitting. In addition, linear models can be updated easily with new data using stochastic gradient descent.\n",
    "WEAKNESSES: Linear regression performs poorly when there are non-linear relationships. They are not naturally flexible enough to capture more complex patterns, and adding the right interaction terms or polynomials can be tricky and time-consuming.\n",
    "STRENGTHS: Deep neural networks perform very well on image, audio, and text data, and they can be easily updated with new data using batch propagation. Their architectures (i.e. number and structure of layers) can be adapted to many types of problems, and their hidden layers reduce the need for feature engineering.\n",
    "WEAKNESSES:Deep learning algorithms are usually not suitable as general-purpose algorithms because they require a very large amount of data. In fact, they are usually outperformed by tree ensembles for classical machine learning problems. In addition, they are computationally intensive to train, and they require much more expertise to tune (i.e. set the architecture and hyperparameters).\n",
    "```\n",
    "\n",
    "\n",
    "5. What determines the size of the input layer?\n",
    "```\n",
    "Input layers/input node for each of the different inputs/features/columns of our dataset that will be passed to the network.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QiNp8dSK8rcx"
   },
   "source": [
    "## Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nDGlUI2A85tS"
   },
   "source": [
    "Use the starter code below to build a perceptron, with just numpy, to predict whether a passenger survived or not. You may reduce the number of features for X to fit code you have already worked on throughout the week, but it is recommended that you modify the code instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "GkUhPR-HpieB",
    "outputId": "12e62f92-90f5-4ead-d02f-40e4200f662f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (887, 7) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses Aboard</th>\n",
       "      <th>Parents/Children Aboard</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  ...  Parents/Children Aboard     Fare\n",
       "0         0       3  ...                        0   7.2500\n",
       "1         1       1  ...                        0  71.2833\n",
       "2         1       3  ...                        0   7.9250\n",
       "3         1       1  ...                        0  53.1000\n",
       "4         0       3  ...                        0   8.0500\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/bundickm/Study-Guides/master/data/titanic.csv')\n",
    "print('Shape:', df.shape, '\\n')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "95aCFxje_XD5"
   },
   "outputs": [],
   "source": [
    "X = np.array(df.drop(columns='Survived'))\n",
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "umUm9VbKHzky",
    "outputId": "1b5dfd9e-3396-4171-957e-c5678bcf8741"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.614431\n",
       "1    0.385569\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Survived'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "waTLtf7TQC3N",
    "outputId": "251dd7e6-ba5f-4ea1-a35a-ea225af99885"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived                   0\n",
       "Pclass                     0\n",
       "Sex                        0\n",
       "Age                        0\n",
       "Siblings/Spouses Aboard    0\n",
       "Parents/Children Aboard    0\n",
       "Fare                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPHva8JJABGZ"
   },
   "source": [
    "Create a multilayer perceptron with back propagation, with just numpy, and apply it to the same data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0CEOwQzH95a"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iaEElIwfK_BK"
   },
   "outputs": [],
   "source": [
    "#Multilayer Perceptron\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=6, activation = \"relu\"))\n",
    "model.add(Dense(1, activation = \"sigmoid\")) # binary output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w8E-L-ZpK_Y8"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\",  metrics = [\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "Ksot48KpK_hs",
    "outputId": "04602d8f-a20a-4a49-972d-d54755257c0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                70        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 191\n",
      "Trainable params: 191\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SxqrH17HNpMh"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YiFtGbs7N518"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y, \n",
    "    test_size = 0.20, \n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1IlQywLMORHq",
    "outputId": "af855832-278b-4261-d1e7-933c472325a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((709, 6), (178, 6), (709,), (178,))"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "d6fIMsJqK_o8",
    "outputId": "16cb5b44-d348-48fa-ea8d-32f3d23c7506"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/99\n",
      "71/71 [==============================] - 0s 4ms/step - loss: 1.1946 - accuracy: 0.6544 - val_loss: 0.7453 - val_accuracy: 0.6461\n",
      "Epoch 2/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6584 - accuracy: 0.6911 - val_loss: 0.6346 - val_accuracy: 0.6798\n",
      "Epoch 3/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6059 - accuracy: 0.6953 - val_loss: 0.6289 - val_accuracy: 0.6742\n",
      "Epoch 4/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5971 - accuracy: 0.7109 - val_loss: 0.6186 - val_accuracy: 0.6798\n",
      "Epoch 5/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5938 - accuracy: 0.7094 - val_loss: 0.6031 - val_accuracy: 0.6798\n",
      "Epoch 6/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5805 - accuracy: 0.7137 - val_loss: 0.5991 - val_accuracy: 0.6798\n",
      "Epoch 7/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5780 - accuracy: 0.7250 - val_loss: 0.5992 - val_accuracy: 0.6798\n",
      "Epoch 8/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5821 - accuracy: 0.7179 - val_loss: 0.5907 - val_accuracy: 0.6685\n",
      "Epoch 9/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5659 - accuracy: 0.7221 - val_loss: 0.5883 - val_accuracy: 0.6742\n",
      "Epoch 10/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5848 - accuracy: 0.7109 - val_loss: 0.5913 - val_accuracy: 0.6798\n",
      "Epoch 11/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5733 - accuracy: 0.7179 - val_loss: 0.5878 - val_accuracy: 0.6854\n",
      "Epoch 12/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5629 - accuracy: 0.7193 - val_loss: 0.5897 - val_accuracy: 0.6854\n",
      "Epoch 13/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5601 - accuracy: 0.7207 - val_loss: 0.5714 - val_accuracy: 0.6742\n",
      "Epoch 14/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5576 - accuracy: 0.7109 - val_loss: 0.5717 - val_accuracy: 0.6910\n",
      "Epoch 15/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5539 - accuracy: 0.7123 - val_loss: 0.5713 - val_accuracy: 0.6910\n",
      "Epoch 16/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5596 - accuracy: 0.7179 - val_loss: 0.5734 - val_accuracy: 0.6910\n",
      "Epoch 17/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5392 - accuracy: 0.7193 - val_loss: 0.5598 - val_accuracy: 0.6742\n",
      "Epoch 18/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5373 - accuracy: 0.7221 - val_loss: 0.5567 - val_accuracy: 0.6798\n",
      "Epoch 19/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5310 - accuracy: 0.7278 - val_loss: 0.5561 - val_accuracy: 0.6966\n",
      "Epoch 20/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5520 - accuracy: 0.7193 - val_loss: 0.5499 - val_accuracy: 0.6798\n",
      "Epoch 21/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5426 - accuracy: 0.7264 - val_loss: 0.5475 - val_accuracy: 0.6854\n",
      "Epoch 22/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5307 - accuracy: 0.7264 - val_loss: 0.5498 - val_accuracy: 0.7191\n",
      "Epoch 23/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5383 - accuracy: 0.7221 - val_loss: 0.5443 - val_accuracy: 0.7022\n",
      "Epoch 24/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.7405 - val_loss: 0.5339 - val_accuracy: 0.6966\n",
      "Epoch 25/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7433 - val_loss: 0.5800 - val_accuracy: 0.6742\n",
      "Epoch 26/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5287 - accuracy: 0.7278 - val_loss: 0.5329 - val_accuracy: 0.6966\n",
      "Epoch 27/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5026 - accuracy: 0.7405 - val_loss: 0.5342 - val_accuracy: 0.6854\n",
      "Epoch 28/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4963 - accuracy: 0.7532 - val_loss: 0.5283 - val_accuracy: 0.6854\n",
      "Epoch 29/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4930 - accuracy: 0.7377 - val_loss: 0.5179 - val_accuracy: 0.7022\n",
      "Epoch 30/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4907 - accuracy: 0.7489 - val_loss: 0.5143 - val_accuracy: 0.6910\n",
      "Epoch 31/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5017 - accuracy: 0.7546 - val_loss: 0.5259 - val_accuracy: 0.7247\n",
      "Epoch 32/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4783 - accuracy: 0.7687 - val_loss: 0.5121 - val_accuracy: 0.6966\n",
      "Epoch 33/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4831 - accuracy: 0.7715 - val_loss: 0.5242 - val_accuracy: 0.6910\n",
      "Epoch 34/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4837 - accuracy: 0.7588 - val_loss: 0.5114 - val_accuracy: 0.7079\n",
      "Epoch 35/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4688 - accuracy: 0.7757 - val_loss: 0.5064 - val_accuracy: 0.6966\n",
      "Epoch 36/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4668 - accuracy: 0.7828 - val_loss: 0.5099 - val_accuracy: 0.7247\n",
      "Epoch 37/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4619 - accuracy: 0.7870 - val_loss: 0.5031 - val_accuracy: 0.7135\n",
      "Epoch 38/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4598 - accuracy: 0.7828 - val_loss: 0.4981 - val_accuracy: 0.7247\n",
      "Epoch 39/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4636 - accuracy: 0.7913 - val_loss: 0.5062 - val_accuracy: 0.7472\n",
      "Epoch 40/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4567 - accuracy: 0.7800 - val_loss: 0.5016 - val_accuracy: 0.7303\n",
      "Epoch 41/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4492 - accuracy: 0.8096 - val_loss: 0.4952 - val_accuracy: 0.7528\n",
      "Epoch 42/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4455 - accuracy: 0.8138 - val_loss: 0.4921 - val_accuracy: 0.7247\n",
      "Epoch 43/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4480 - accuracy: 0.8011 - val_loss: 0.4880 - val_accuracy: 0.7528\n",
      "Epoch 44/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4536 - accuracy: 0.7913 - val_loss: 0.4864 - val_accuracy: 0.7247\n",
      "Epoch 45/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4741 - accuracy: 0.7856 - val_loss: 0.4950 - val_accuracy: 0.7416\n",
      "Epoch 46/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4414 - accuracy: 0.8152 - val_loss: 0.4847 - val_accuracy: 0.7360\n",
      "Epoch 47/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4508 - accuracy: 0.8124 - val_loss: 0.5046 - val_accuracy: 0.7640\n",
      "Epoch 48/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4537 - accuracy: 0.8054 - val_loss: 0.4893 - val_accuracy: 0.7416\n",
      "Epoch 49/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4659 - accuracy: 0.7927 - val_loss: 0.4957 - val_accuracy: 0.7360\n",
      "Epoch 50/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4344 - accuracy: 0.8096 - val_loss: 0.4821 - val_accuracy: 0.7303\n",
      "Epoch 51/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4402 - accuracy: 0.8011 - val_loss: 0.5028 - val_accuracy: 0.7584\n",
      "Epoch 52/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4366 - accuracy: 0.8124 - val_loss: 0.4863 - val_accuracy: 0.7472\n",
      "Epoch 53/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4391 - accuracy: 0.8082 - val_loss: 0.4997 - val_accuracy: 0.7472\n",
      "Epoch 54/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4417 - accuracy: 0.8223 - val_loss: 0.4811 - val_accuracy: 0.7360\n",
      "Epoch 55/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4307 - accuracy: 0.8181 - val_loss: 0.4823 - val_accuracy: 0.7472\n",
      "Epoch 56/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4302 - accuracy: 0.8110 - val_loss: 0.4824 - val_accuracy: 0.7528\n",
      "Epoch 57/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4451 - accuracy: 0.8096 - val_loss: 0.4891 - val_accuracy: 0.7640\n",
      "Epoch 58/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4321 - accuracy: 0.8039 - val_loss: 0.4947 - val_accuracy: 0.7584\n",
      "Epoch 59/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4454 - accuracy: 0.8054 - val_loss: 0.4923 - val_accuracy: 0.7697\n",
      "Epoch 60/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4386 - accuracy: 0.7983 - val_loss: 0.4785 - val_accuracy: 0.7697\n",
      "Epoch 61/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4466 - accuracy: 0.8054 - val_loss: 0.4804 - val_accuracy: 0.7584\n",
      "Epoch 62/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4313 - accuracy: 0.7941 - val_loss: 0.4863 - val_accuracy: 0.7416\n",
      "Epoch 63/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4275 - accuracy: 0.8110 - val_loss: 0.4820 - val_accuracy: 0.7416\n",
      "Epoch 64/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4255 - accuracy: 0.8068 - val_loss: 0.4742 - val_accuracy: 0.7416\n",
      "Epoch 65/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4351 - accuracy: 0.8054 - val_loss: 0.4826 - val_accuracy: 0.7528\n",
      "Epoch 66/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4296 - accuracy: 0.8068 - val_loss: 0.4787 - val_accuracy: 0.7472\n",
      "Epoch 67/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4303 - accuracy: 0.8039 - val_loss: 0.4763 - val_accuracy: 0.7528\n",
      "Epoch 68/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4264 - accuracy: 0.8082 - val_loss: 0.4818 - val_accuracy: 0.7472\n",
      "Epoch 69/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4295 - accuracy: 0.8039 - val_loss: 0.4720 - val_accuracy: 0.7472\n",
      "Epoch 70/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4285 - accuracy: 0.8124 - val_loss: 0.4836 - val_accuracy: 0.7640\n",
      "Epoch 71/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4251 - accuracy: 0.8011 - val_loss: 0.4854 - val_accuracy: 0.7584\n",
      "Epoch 72/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4187 - accuracy: 0.8124 - val_loss: 0.5022 - val_accuracy: 0.7528\n",
      "Epoch 73/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4243 - accuracy: 0.8054 - val_loss: 0.4797 - val_accuracy: 0.7697\n",
      "Epoch 74/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4349 - accuracy: 0.8110 - val_loss: 0.4935 - val_accuracy: 0.7472\n",
      "Epoch 75/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4433 - accuracy: 0.8068 - val_loss: 0.4762 - val_accuracy: 0.7472\n",
      "Epoch 76/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4230 - accuracy: 0.7997 - val_loss: 0.4926 - val_accuracy: 0.7528\n",
      "Epoch 77/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4347 - accuracy: 0.8068 - val_loss: 0.5113 - val_accuracy: 0.7472\n",
      "Epoch 78/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4271 - accuracy: 0.8181 - val_loss: 0.4858 - val_accuracy: 0.7640\n",
      "Epoch 79/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4324 - accuracy: 0.8068 - val_loss: 0.4728 - val_accuracy: 0.7584\n",
      "Epoch 80/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.8039 - val_loss: 0.4789 - val_accuracy: 0.7472\n",
      "Epoch 81/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4176 - accuracy: 0.8181 - val_loss: 0.4890 - val_accuracy: 0.7584\n",
      "Epoch 82/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4217 - accuracy: 0.8124 - val_loss: 0.4775 - val_accuracy: 0.7584\n",
      "Epoch 83/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4286 - accuracy: 0.8124 - val_loss: 0.4778 - val_accuracy: 0.7472\n",
      "Epoch 84/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4207 - accuracy: 0.8124 - val_loss: 0.4722 - val_accuracy: 0.7472\n",
      "Epoch 85/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4219 - accuracy: 0.8152 - val_loss: 0.4836 - val_accuracy: 0.7528\n",
      "Epoch 86/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4591 - accuracy: 0.8054 - val_loss: 0.4880 - val_accuracy: 0.7640\n",
      "Epoch 87/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4129 - accuracy: 0.8068 - val_loss: 0.4943 - val_accuracy: 0.7640\n",
      "Epoch 88/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4250 - accuracy: 0.8124 - val_loss: 0.4771 - val_accuracy: 0.7697\n",
      "Epoch 89/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4165 - accuracy: 0.8166 - val_loss: 0.4836 - val_accuracy: 0.7640\n",
      "Epoch 90/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4176 - accuracy: 0.8152 - val_loss: 0.4729 - val_accuracy: 0.7584\n",
      "Epoch 91/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4511 - accuracy: 0.8054 - val_loss: 0.4846 - val_accuracy: 0.7584\n",
      "Epoch 92/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4236 - accuracy: 0.8068 - val_loss: 0.4768 - val_accuracy: 0.7640\n",
      "Epoch 93/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4121 - accuracy: 0.8039 - val_loss: 0.4858 - val_accuracy: 0.7584\n",
      "Epoch 94/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4227 - accuracy: 0.8054 - val_loss: 0.4684 - val_accuracy: 0.7584\n",
      "Epoch 95/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4147 - accuracy: 0.8138 - val_loss: 0.4734 - val_accuracy: 0.7640\n",
      "Epoch 96/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4209 - accuracy: 0.8068 - val_loss: 0.4769 - val_accuracy: 0.7753\n",
      "Epoch 97/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4130 - accuracy: 0.8124 - val_loss: 0.4819 - val_accuracy: 0.7809\n",
      "Epoch 98/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4146 - accuracy: 0.8166 - val_loss: 0.4736 - val_accuracy: 0.7528\n",
      "Epoch 99/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4151 - accuracy: 0.8068 - val_loss: 0.4740 - val_accuracy: 0.7753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff987542940>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs = 99, \n",
    "          batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CyxTtBQlRLen"
   },
   "outputs": [],
   "source": [
    "#Single Layer Perceptron with Keras\n",
    "\n",
    "model_single = Sequential()\n",
    "model_single.add(Dense(1, input_dim=6, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kI9Tt34pR7UP"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\",  metrics = [\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "eyrG2kxmSRCX",
    "outputId": "999fb2bb-a14d-4bb7-c22d-fbebe16ce7c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 7\n",
      "Trainable params: 7\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_single.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pOOQDph9SEX6",
    "outputId": "c1d20165-24c9-4375-d5db-bf745c619f46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/99\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 0.4268 - accuracy: 0.8068 - val_loss: 0.4727 - val_accuracy: 0.7584\n",
      "Epoch 2/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4167 - accuracy: 0.8209 - val_loss: 0.4738 - val_accuracy: 0.7584\n",
      "Epoch 3/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4368 - accuracy: 0.8096 - val_loss: 0.5351 - val_accuracy: 0.7697\n",
      "Epoch 4/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4332 - accuracy: 0.8068 - val_loss: 0.4748 - val_accuracy: 0.7584\n",
      "Epoch 5/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4140 - accuracy: 0.8110 - val_loss: 0.4728 - val_accuracy: 0.7584\n",
      "Epoch 6/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4091 - accuracy: 0.8209 - val_loss: 0.4701 - val_accuracy: 0.7584\n",
      "Epoch 7/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4332 - accuracy: 0.8096 - val_loss: 0.4704 - val_accuracy: 0.7584\n",
      "Epoch 8/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4176 - accuracy: 0.8152 - val_loss: 0.4788 - val_accuracy: 0.7753\n",
      "Epoch 9/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4137 - accuracy: 0.8110 - val_loss: 0.4709 - val_accuracy: 0.7640\n",
      "Epoch 10/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4086 - accuracy: 0.8209 - val_loss: 0.4884 - val_accuracy: 0.7697\n",
      "Epoch 11/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4150 - accuracy: 0.8025 - val_loss: 0.4805 - val_accuracy: 0.7865\n",
      "Epoch 12/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4111 - accuracy: 0.8138 - val_loss: 0.4825 - val_accuracy: 0.7809\n",
      "Epoch 13/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4205 - accuracy: 0.8110 - val_loss: 0.4769 - val_accuracy: 0.7753\n",
      "Epoch 14/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4183 - accuracy: 0.8237 - val_loss: 0.4797 - val_accuracy: 0.7809\n",
      "Epoch 15/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4133 - accuracy: 0.8110 - val_loss: 0.4753 - val_accuracy: 0.7697\n",
      "Epoch 16/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4128 - accuracy: 0.8223 - val_loss: 0.4704 - val_accuracy: 0.7753\n",
      "Epoch 17/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4078 - accuracy: 0.8138 - val_loss: 0.4805 - val_accuracy: 0.7809\n",
      "Epoch 18/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4199 - accuracy: 0.8152 - val_loss: 0.4840 - val_accuracy: 0.7640\n",
      "Epoch 19/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4097 - accuracy: 0.8166 - val_loss: 0.4797 - val_accuracy: 0.7809\n",
      "Epoch 20/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4085 - accuracy: 0.8195 - val_loss: 0.4797 - val_accuracy: 0.7921\n",
      "Epoch 21/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4075 - accuracy: 0.8152 - val_loss: 0.4678 - val_accuracy: 0.7865\n",
      "Epoch 22/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4201 - accuracy: 0.8181 - val_loss: 0.4807 - val_accuracy: 0.7640\n",
      "Epoch 23/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4092 - accuracy: 0.8195 - val_loss: 0.4662 - val_accuracy: 0.7865\n",
      "Epoch 24/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4073 - accuracy: 0.8166 - val_loss: 0.4914 - val_accuracy: 0.7640\n",
      "Epoch 25/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4183 - accuracy: 0.8096 - val_loss: 0.4774 - val_accuracy: 0.7697\n",
      "Epoch 26/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4156 - accuracy: 0.8195 - val_loss: 0.4718 - val_accuracy: 0.7921\n",
      "Epoch 27/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4169 - accuracy: 0.8279 - val_loss: 0.4780 - val_accuracy: 0.7921\n",
      "Epoch 28/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4126 - accuracy: 0.8223 - val_loss: 0.4748 - val_accuracy: 0.7753\n",
      "Epoch 29/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4107 - accuracy: 0.8223 - val_loss: 0.4694 - val_accuracy: 0.7809\n",
      "Epoch 30/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4186 - accuracy: 0.8181 - val_loss: 0.4794 - val_accuracy: 0.7809\n",
      "Epoch 31/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4076 - accuracy: 0.8138 - val_loss: 0.4810 - val_accuracy: 0.7640\n",
      "Epoch 32/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4102 - accuracy: 0.8138 - val_loss: 0.4878 - val_accuracy: 0.7753\n",
      "Epoch 33/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4083 - accuracy: 0.8195 - val_loss: 0.4711 - val_accuracy: 0.7640\n",
      "Epoch 34/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4177 - accuracy: 0.8124 - val_loss: 0.4777 - val_accuracy: 0.7697\n",
      "Epoch 35/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4109 - accuracy: 0.8166 - val_loss: 0.4721 - val_accuracy: 0.7809\n",
      "Epoch 36/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4195 - accuracy: 0.8237 - val_loss: 0.4681 - val_accuracy: 0.7978\n",
      "Epoch 37/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4047 - accuracy: 0.8265 - val_loss: 0.4688 - val_accuracy: 0.7865\n",
      "Epoch 38/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4082 - accuracy: 0.8166 - val_loss: 0.4694 - val_accuracy: 0.7809\n",
      "Epoch 39/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4045 - accuracy: 0.8152 - val_loss: 0.4758 - val_accuracy: 0.7921\n",
      "Epoch 40/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4076 - accuracy: 0.8209 - val_loss: 0.4817 - val_accuracy: 0.7809\n",
      "Epoch 41/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4053 - accuracy: 0.8237 - val_loss: 0.4693 - val_accuracy: 0.7865\n",
      "Epoch 42/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4110 - accuracy: 0.8195 - val_loss: 0.4617 - val_accuracy: 0.7809\n",
      "Epoch 43/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4104 - accuracy: 0.8195 - val_loss: 0.4697 - val_accuracy: 0.7753\n",
      "Epoch 44/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4043 - accuracy: 0.8223 - val_loss: 0.4680 - val_accuracy: 0.7809\n",
      "Epoch 45/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4154 - accuracy: 0.8152 - val_loss: 0.4685 - val_accuracy: 0.7865\n",
      "Epoch 46/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4049 - accuracy: 0.8279 - val_loss: 0.4863 - val_accuracy: 0.7640\n",
      "Epoch 47/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4172 - accuracy: 0.8152 - val_loss: 0.4709 - val_accuracy: 0.7978\n",
      "Epoch 48/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4002 - accuracy: 0.8251 - val_loss: 0.4653 - val_accuracy: 0.7753\n",
      "Epoch 49/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4115 - accuracy: 0.8138 - val_loss: 0.4655 - val_accuracy: 0.7809\n",
      "Epoch 50/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4060 - accuracy: 0.8350 - val_loss: 0.4633 - val_accuracy: 0.7584\n",
      "Epoch 51/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4142 - accuracy: 0.8181 - val_loss: 0.5265 - val_accuracy: 0.7809\n",
      "Epoch 52/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4064 - accuracy: 0.8293 - val_loss: 0.4626 - val_accuracy: 0.7697\n",
      "Epoch 53/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4279 - accuracy: 0.8251 - val_loss: 0.4770 - val_accuracy: 0.7584\n",
      "Epoch 54/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4088 - accuracy: 0.8195 - val_loss: 0.4669 - val_accuracy: 0.7865\n",
      "Epoch 55/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4105 - accuracy: 0.8138 - val_loss: 0.4592 - val_accuracy: 0.7640\n",
      "Epoch 56/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4082 - accuracy: 0.8195 - val_loss: 0.4646 - val_accuracy: 0.7809\n",
      "Epoch 57/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4065 - accuracy: 0.8251 - val_loss: 0.4676 - val_accuracy: 0.7865\n",
      "Epoch 58/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4026 - accuracy: 0.8237 - val_loss: 0.4690 - val_accuracy: 0.7697\n",
      "Epoch 59/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4088 - accuracy: 0.8181 - val_loss: 0.4676 - val_accuracy: 0.7753\n",
      "Epoch 60/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4072 - accuracy: 0.8138 - val_loss: 0.4785 - val_accuracy: 0.7697\n",
      "Epoch 61/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4107 - accuracy: 0.8209 - val_loss: 0.4663 - val_accuracy: 0.7753\n",
      "Epoch 62/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4033 - accuracy: 0.8124 - val_loss: 0.4740 - val_accuracy: 0.7865\n",
      "Epoch 63/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4053 - accuracy: 0.8237 - val_loss: 0.4665 - val_accuracy: 0.7640\n",
      "Epoch 64/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4014 - accuracy: 0.8237 - val_loss: 0.4649 - val_accuracy: 0.7865\n",
      "Epoch 65/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3990 - accuracy: 0.8293 - val_loss: 0.4679 - val_accuracy: 0.7921\n",
      "Epoch 66/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4031 - accuracy: 0.8223 - val_loss: 0.4635 - val_accuracy: 0.7865\n",
      "Epoch 67/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4089 - accuracy: 0.8293 - val_loss: 0.4637 - val_accuracy: 0.7753\n",
      "Epoch 68/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4060 - accuracy: 0.8223 - val_loss: 0.4818 - val_accuracy: 0.7697\n",
      "Epoch 69/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4084 - accuracy: 0.8195 - val_loss: 0.4747 - val_accuracy: 0.7753\n",
      "Epoch 70/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4017 - accuracy: 0.8237 - val_loss: 0.4620 - val_accuracy: 0.7809\n",
      "Epoch 71/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3996 - accuracy: 0.8195 - val_loss: 0.4612 - val_accuracy: 0.7753\n",
      "Epoch 72/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3985 - accuracy: 0.8322 - val_loss: 0.4733 - val_accuracy: 0.7640\n",
      "Epoch 73/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4031 - accuracy: 0.8195 - val_loss: 0.4585 - val_accuracy: 0.7697\n",
      "Epoch 74/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3985 - accuracy: 0.8279 - val_loss: 0.4747 - val_accuracy: 0.7865\n",
      "Epoch 75/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3951 - accuracy: 0.8293 - val_loss: 0.4766 - val_accuracy: 0.7865\n",
      "Epoch 76/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4062 - accuracy: 0.8307 - val_loss: 0.4545 - val_accuracy: 0.7753\n",
      "Epoch 77/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.8307 - val_loss: 0.4575 - val_accuracy: 0.7865\n",
      "Epoch 78/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4031 - accuracy: 0.8265 - val_loss: 0.4696 - val_accuracy: 0.7640\n",
      "Epoch 79/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3957 - accuracy: 0.8279 - val_loss: 0.4645 - val_accuracy: 0.7865\n",
      "Epoch 80/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4010 - accuracy: 0.8209 - val_loss: 0.4618 - val_accuracy: 0.7697\n",
      "Epoch 81/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4057 - accuracy: 0.8209 - val_loss: 0.4777 - val_accuracy: 0.7584\n",
      "Epoch 82/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4050 - accuracy: 0.8265 - val_loss: 0.4676 - val_accuracy: 0.7809\n",
      "Epoch 83/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3931 - accuracy: 0.8350 - val_loss: 0.4651 - val_accuracy: 0.7921\n",
      "Epoch 84/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3962 - accuracy: 0.8307 - val_loss: 0.4722 - val_accuracy: 0.7865\n",
      "Epoch 85/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4040 - accuracy: 0.8293 - val_loss: 0.4992 - val_accuracy: 0.7753\n",
      "Epoch 86/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4068 - accuracy: 0.8293 - val_loss: 0.4678 - val_accuracy: 0.7697\n",
      "Epoch 87/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3982 - accuracy: 0.8293 - val_loss: 0.4664 - val_accuracy: 0.7921\n",
      "Epoch 88/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3908 - accuracy: 0.8293 - val_loss: 0.4894 - val_accuracy: 0.7584\n",
      "Epoch 89/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4095 - accuracy: 0.8195 - val_loss: 0.4930 - val_accuracy: 0.7753\n",
      "Epoch 90/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3952 - accuracy: 0.8166 - val_loss: 0.4636 - val_accuracy: 0.7865\n",
      "Epoch 91/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4080 - accuracy: 0.8195 - val_loss: 0.4656 - val_accuracy: 0.7809\n",
      "Epoch 92/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4015 - accuracy: 0.8181 - val_loss: 0.4658 - val_accuracy: 0.7809\n",
      "Epoch 93/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4010 - accuracy: 0.8322 - val_loss: 0.4671 - val_accuracy: 0.7921\n",
      "Epoch 94/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3980 - accuracy: 0.8265 - val_loss: 0.4733 - val_accuracy: 0.7753\n",
      "Epoch 95/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4023 - accuracy: 0.8265 - val_loss: 0.4602 - val_accuracy: 0.7921\n",
      "Epoch 96/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3945 - accuracy: 0.8279 - val_loss: 0.4690 - val_accuracy: 0.7753\n",
      "Epoch 97/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4061 - accuracy: 0.8336 - val_loss: 0.4659 - val_accuracy: 0.7584\n",
      "Epoch 98/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4163 - accuracy: 0.8265 - val_loss: 0.4618 - val_accuracy: 0.7753\n",
      "Epoch 99/99\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3989 - accuracy: 0.8322 - val_loss: 0.4560 - val_accuracy: 0.7753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff982dbf198>"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs = 99, \n",
    "          batch_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xfZ-6Rh8BGgb"
   },
   "source": [
    "*In a short paragraph, answer the following:*\n",
    "\n",
    "Why does the multilayer perceptron perform better than the simple perceptron? What limits the simple perceptron? What aspects of the multilayer perceptron allow it to overcome those limitations?\n",
    "\n",
    "```\n",
    "Your Answer Here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mhki00NLpxtf"
   },
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7njBYhdeLZuk"
   },
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BwrDo98ALZu1"
   },
   "source": [
    "Define the following terms in your own words, do not simply copy and paste a definition found elsewhere but reword it to be understandable and memorable to you. *Double click the markdown to add your definitions.*\n",
    "\n",
    "**Earl Stopping:** `Early stopping catches when model is not increasing in optimatization. This saves time and money. `\n",
    "\n",
    "**Weight Decay:** `Prevents overfitting the parameters by regularizing the values.  `\n",
    "\n",
    "**Dropout:** \n",
    "```The Dropout Regularization value is a percentage of neurons that you want to be randomly deactivated during training. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. Dropout can be used on either the visible or invisible layer. ```\n",
    "\n",
    "<br/>\n",
    "The following are hyperparameters:\n",
    "\n",
    "**Activation Functions:** `Activation functions are mathematical equations that determine the output of a neural network. The type of activations functions depends on the ouputs your are looking for, such as sigmoid for binary and softmax for multiclassification functions.  `\n",
    "\n",
    "**Optimizer** `Optimizers are algorithms or methods used to change the attributes of the neural network such as weights and learning rate to reduce the losses. Optimizers are used to solve optimization problems by minimizing the function`\n",
    "\n",
    "**Number of Layers** `This effects the depth of the neural netwrok`\n",
    "\n",
    "**Number of Neurons** `This is the number of neurons in a layer. This effects the width of the ne `\n",
    "\n",
    "**Batch Size** `The number of examples used in an iteration`\n",
    "\n",
    "**Dropout Regularization** `This drops a set number/percentage of features in a single layer  of neural network`\n",
    "\n",
    "**Learning Rate** `A scalar used to train a model via gradient descent/ step size`\n",
    "\n",
    "**Number of Epochs** `Iterations`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QZ_azU3fNlu5"
   },
   "source": [
    "## Questions of Understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KFhdxE3ONlvN"
   },
   "source": [
    "1. Why is it recommended to normalize your input data?\n",
    "```\n",
    "The process of converting an actual range of values into a standard range of values, typically -1 to +1 or 0 to 1. Also know as scaling. \n",
    "```\n",
    "\n",
    "\n",
    "2. How do you go about deciding on your neural network's architecture?\n",
    "```\n",
    "The most common approach seems to be to start with a rough guess based on prior experience about networks used on similar problems. \n",
    "- Create a network with hidden layers similar size order to the input, and all the same size, on the grounds that there is no particular reason to vary the size (unless you are creating an autoencoder perhaps).\n",
    "- Start simple and build up complexity to see what improves a simple network.\n",
    "- Try varying depths of network if you expect the output to be explained well by the input data, but with a complex relationship (as opposed to just inherently noisy).\n",
    "-Try adding some dropout, it's the closest thing neural networks have to magic fairy dust that makes everything better (caveat: adding dropout may improve generalisation, but may also increase required layer sizes and training times).\n",
    "\n",
    "You can use grid / randomized search to help choose the best parameters for your model \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "3. Why is regularization important with neural networks?\n",
    "```\n",
    "Regularization allows you to not overfit your model to the training data. \n",
    "```\n",
    "\n",
    "\n",
    "4. What does `validation.data` do?\n",
    "```\n",
    "Validataion data is where you check the model you created to see if the parameters work in predicting . The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters.\n",
    "```\n",
    "\n",
    "\n",
    "5. Why is hyperparameter tuning so important with neural networks?\n",
    "```\n",
    "Hyperparameter tuning allows you to train a model to get the best possible predictions. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2q0E4XJPmnJ"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mMLs3asmPpkb"
   },
   "source": [
    "Using the same dataset as above, use Keras to build a model and find its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K5NVbY1AVNDZ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J93kXczUp6z2"
   },
   "outputs": [],
   "source": [
    "def create_model(lr):\n",
    "  adam = Adam(learning_rate=lr)\n",
    "  model = Sequential()\n",
    "  model.add(Dense(32, input_dim = 6, activation = \"relu\"))\n",
    "  model.add(Dense(1, activation =\"sigmoid\"))\n",
    "\n",
    "  #Compile model\n",
    "  model.compile(optimizer=\"adam\", loss = \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V3IZHRZrVnA2"
   },
   "outputs": [],
   "source": [
    "hyper_model = KerasClassifier(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7lhqCTsQdTF"
   },
   "source": [
    "Build upon the model you created in the cell above by adding hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oxm_p5MzV2SA"
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lr': [.001, .01, .1],\n",
    "    'batch_size' : [10, 20, 40],\n",
    "    'epochs' : [25]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UOvsvP-GV2N6"
   },
   "outputs": [],
   "source": [
    "rand = RandomizedSearchCV(estimator=hyper_model, param_distributions=param_grid, n_jobs=-1) \n",
    "#param_distributions for RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "hWokh9jZV2LY",
    "outputId": "7f7d5637-9461-4b95-8585-85e5d8919cd9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "rand_result = rand.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "ZlXgMG6OZTEE",
    "outputId": "139d9826-5651-40d9-e427-78e06ff781ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.7983018755912781 using {'lr': 0.001, 'epochs': 25, 'batch_size': 10}\n",
      "Means: 0.7983018755912781, Stdev: 0.04334258986925524 with: {'lr': 0.001, 'epochs': 25, 'batch_size': 10}\n",
      "Means: 0.7729097962379455, Stdev: 0.03898360840933016 with: {'lr': 0.01, 'epochs': 25, 'batch_size': 10}\n",
      "Means: 0.7926480889320373, Stdev: 0.03052511033474625 with: {'lr': 0.1, 'epochs': 25, 'batch_size': 10}\n",
      "Means: 0.7503645896911622, Stdev: 0.037340060711998975 with: {'lr': 0.001, 'epochs': 25, 'batch_size': 20}\n",
      "Means: 0.7559484481811524, Stdev: 0.030157838676970496 with: {'lr': 0.01, 'epochs': 25, 'batch_size': 20}\n",
      "Means: 0.7757067203521728, Stdev: 0.04458786182081137 with: {'lr': 0.1, 'epochs': 25, 'batch_size': 20}\n",
      "Means: 0.7291579246520996, Stdev: 0.023444612913861986 with: {'lr': 0.001, 'epochs': 25, 'batch_size': 40}\n",
      "Means: 0.7220956921577454, Stdev: 0.031201034065029644 with: {'lr': 0.01, 'epochs': 25, 'batch_size': 40}\n",
      "Means: 0.7164818644523621, Stdev: 0.017233620244993975 with: {'lr': 0.1, 'epochs': 25, 'batch_size': 40}\n"
     ]
    }
   ],
   "source": [
    "# Report Results\n",
    "print(f\"Best: {rand_result.best_score_} using {rand_result.best_params_}\")\n",
    "search = rand_result.best_params_\n",
    "means = rand_result.cv_results_['mean_test_score']\n",
    "stds = rand_result.cv_results_['std_test_score']\n",
    "params = rand_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0e8R0RfhZLTZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "Zfo64uRpa_p3",
    "outputId": "3f6e1aca-0560-4407-fef3-f1a641db7e6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/wrappers/scikit_learn.py:241: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "final_results = rand_result.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "whVu711pQoN3"
   },
   "source": [
    "Find the accuracy of the tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jDRUFiOHbGnt",
    "outputId": "c0958dbb-9abc-4674-c040-8e401bc606b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7359550561797753"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(final_results, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GQaJEGvQw4W"
   },
   "source": [
    "In a short paragraph, explain how the hyperparameters impacted the accuracy of your model.\n",
    "\n",
    "```\n",
    "You Answer Here\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Unit_4_Sprint_2_Neural_Networks_Study_Guide.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
