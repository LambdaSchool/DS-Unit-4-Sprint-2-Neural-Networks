{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_432_Backprop_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valogonor/DS-Unit-4-Sprint-3-Neural-Networks/blob/master/LS_DS_432_Backprop_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "NGGrt9EYlCqY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Backpropagation Practice\n",
        "\n",
        "Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
        "\n",
        "| x1 | x2 | x3 | y |\n",
        "|----|----|----|---|\n",
        "| 0  | 0  | 1  | 0 |\n",
        "| 0  | 1  | 1  | 1 |\n",
        "| 1  | 0  | 1  | 1 |\n",
        "| 0  | 1  | 0  | 1 |\n",
        "| 1  | 0  | 0  | 1 |\n",
        "| 1  | 1  | 1  | 0 |\n",
        "| 0  | 0  | 0  | 0 |\n",
        "\n",
        "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn. "
      ]
    },
    {
      "metadata": {
        "id": "nEREYT-3wI1f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D-6xXX3JCE0m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = np.array(([0, 0, 1],\n",
        "              [0, 1, 1],\n",
        "              [1, 0, 1],\n",
        "              [0, 1, 0],\n",
        "              [1, 0, 0],\n",
        "              [1, 1, 1],\n",
        "              [0, 0, 0]), dtype=float)\n",
        "y = np.array(([0],\n",
        "              [1],\n",
        "              [1],\n",
        "              [1],\n",
        "              [1],\n",
        "              [0],\n",
        "              [0]), dtype=float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SE_Nr9m2CLMx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neural_Network(object):\n",
        "    def __init__(self):\n",
        "        self.inputs = 3\n",
        "        self.hiddenNodes = 4\n",
        "        self.outputNodes = 1\n",
        "\n",
        "        # Initlize Weights\n",
        "        self.L1_weights = np.random.randn(self.inputs, self.hiddenNodes) # (3x2)\n",
        "        self.L2_weights = np.random.randn(self.hiddenNodes, self.outputNodes) # (3x1)\n",
        "\n",
        "    def feed_forward(self, X):\n",
        "        # Weighted sum between inputs and hidden layer\n",
        "        self.hidden_sum = np.dot(X, self.L1_weights)\n",
        "        # Activations of weighted sum\n",
        "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "        # Weighted sum between hidden and output\n",
        "        self.output_sum = np.dot(self.activated_hidden, self.L2_weights)\n",
        "        # final activation of output\n",
        "        self.activated_output = self.sigmoid(self.output_sum)\n",
        "        return self.activated_output\n",
        "        \n",
        "    def sigmoid(self, s):\n",
        "        return 1/(1+np.exp(-s))\n",
        "    \n",
        "    def sigmoidPrime(self, s):\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    def backward(self, X, y, o):\n",
        "        # backward propgate through the network\n",
        "        self.o_error = y - o # error in output\n",
        "        self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to error\n",
        "\n",
        "        self.z2_error = self.o_delta.dot(self.L2_weights.T) # z2 error: how much our hidden layer weights contributed to output error\n",
        "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden) # applying derivative of sigmoid to z2 error\n",
        "\n",
        "        self.L1_weights += X.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights\n",
        "        self.L2_weights += self.activated_hidden.T.dot(self.o_delta) # adjusting second set (hidden --> output) weights\n",
        "        \n",
        "    def train (self, X, y):\n",
        "        o = self.feed_forward(X)\n",
        "        self.backward(X, y, o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fVhMGt8MZLzn",
        "colab_type": "code",
        "outputId": "a274f06a-4f77-4026-b886-396ae0df74c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7920
        }
      },
      "cell_type": "code",
      "source": [
        "NN = Neural_Network()\n",
        "for i in range(10000): # trains the NN 10,000 times\n",
        "    if i+1 in [1,2,3,4,5] or (i+1) % 1000 == 0:\n",
        "        print('+---------- EPOCH', i+1, '-----------+')\n",
        "        print(\"Input: \\n\", X) \n",
        "        print(\"Actual Output: \\n\", y)  \n",
        "        print(\"Predicted Output: \\n\" + str(NN.feed_forward(X))) \n",
        "        print(\"Loss: \\n\" + str(np.mean(np.square(y - NN.feed_forward(X))))) # mean sum squared loss\n",
        "        print(\"\\n\")\n",
        "    NN.train(X, y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 1 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.13653858]\n",
            " [0.1909916 ]\n",
            " [0.22138274]\n",
            " [0.18825049]\n",
            " [0.24730936]\n",
            " [0.31590065]\n",
            " [0.13057194]]\n",
            "Loss: \n",
            "0.3745292750019488\n",
            "\n",
            "\n",
            "+---------- EPOCH 2 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.17682537]\n",
            " [0.24264905]\n",
            " [0.2783041 ]\n",
            " [0.2445514 ]\n",
            " [0.31956967]\n",
            " [0.37391984]\n",
            " [0.17166515]]\n",
            "Loss: \n",
            "0.33266651820778187\n",
            "\n",
            "\n",
            "+---------- EPOCH 3 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.22313673]\n",
            " [0.29875102]\n",
            " [0.34026883]\n",
            " [0.31151956]\n",
            " [0.40105041]\n",
            " [0.43034141]\n",
            " [0.22239997]]\n",
            "Loss: \n",
            "0.29202667988445047\n",
            "\n",
            "\n",
            "+---------- EPOCH 4 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.26950076]\n",
            " [0.35222467]\n",
            " [0.39951308]\n",
            " [0.38131745]\n",
            " [0.480275  ]\n",
            " [0.48073953]\n",
            " [0.27681951]]\n",
            "Loss: \n",
            "0.2590642584285635\n",
            "\n",
            "\n",
            "+---------- EPOCH 5 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.30931667]\n",
            " [0.39740961]\n",
            " [0.4489038 ]\n",
            " [0.44462485]\n",
            " [0.54634543]\n",
            " [0.52222155]\n",
            " [0.32634185]]\n",
            "Loss: \n",
            "0.23656533920461076\n",
            "\n",
            "\n",
            "+---------- EPOCH 1000 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.01133251]\n",
            " [0.95081844]\n",
            " [0.95369132]\n",
            " [0.97485683]\n",
            " [0.97064971]\n",
            " [0.50495056]\n",
            " [0.05132807]]\n",
            "Loss: \n",
            "0.03768499985585387\n",
            "\n",
            "\n",
            "+---------- EPOCH 2000 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.01992139]\n",
            " [0.93111369]\n",
            " [0.93282742]\n",
            " [0.95260112]\n",
            " [0.9499732 ]\n",
            " [0.09307297]\n",
            " [0.09325128]]\n",
            "Loss: \n",
            "0.004537436381011222\n",
            "\n",
            "\n",
            "+---------- EPOCH 3000 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.0083167 ]\n",
            " [0.96483533]\n",
            " [0.96566625]\n",
            " [0.96737623]\n",
            " [0.96650088]\n",
            " [0.04906111]\n",
            " [0.05633024]]\n",
            "Loss: \n",
            "0.0014644453893881985\n",
            "\n",
            "\n",
            "+---------- EPOCH 4000 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.00546128]\n",
            " [0.97305373]\n",
            " [0.97363221]\n",
            " [0.97457092]\n",
            " [0.97396917]\n",
            " [0.03762381]\n",
            " [0.04359874]]\n",
            "Loss: \n",
            "0.000870261566846286\n",
            "\n",
            "\n",
            "+---------- EPOCH 5000 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.00421404]\n",
            " [0.97737464]\n",
            " [0.97783445]\n",
            " [0.9784976 ]\n",
            " [0.97802136]\n",
            " [0.03159405]\n",
            " [0.03675278]]\n",
            "Loss: \n",
            "0.0006164773231959705\n",
            "\n",
            "\n",
            "+---------- EPOCH 6000 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.00349546]\n",
            " [0.98014122]\n",
            " [0.98052981]\n",
            " [0.98104876]\n",
            " [0.98064726]\n",
            " [0.02773335]\n",
            " [0.032332  ]]\n",
            "Loss: \n",
            "0.000476264741440966\n",
            "\n",
            "\n",
            "+---------- EPOCH 7000 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.00302121]\n",
            " [0.98210371]\n",
            " [0.98244395]\n",
            " [0.98287354]\n",
            " [0.98252256]\n",
            " [0.02499546]\n",
            " [0.02918127]]\n",
            "Loss: \n",
            "0.000387530788951769\n",
            "\n",
            "\n",
            "+---------- EPOCH 8000 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.00268151]\n",
            " [0.98358741]\n",
            " [0.98389226]\n",
            " [0.98426075]\n",
            " [0.98394665]\n",
            " [0.02292606]\n",
            " [0.02679189]]\n",
            "Loss: \n",
            "0.0003264095133796811\n",
            "\n",
            "\n",
            "+---------- EPOCH 9000 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.00242446]\n",
            " [0.98475916]\n",
            " [0.98503678]\n",
            " [0.9853607 ]\n",
            " [0.98507492]\n",
            " [0.02129211]\n",
            " [0.02490074]]\n",
            "Loss: \n",
            "0.0002817896023212688\n",
            "\n",
            "\n",
            "+---------- EPOCH 10000 -----------+\n",
            "Input: \n",
            " [[0. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "Actual Output: \n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Predicted Output: \n",
            "[[0.00222211]\n",
            " [0.98571444]\n",
            " [0.98597034]\n",
            " [0.98626021]\n",
            " [0.985997  ]\n",
            " [0.01996022]\n",
            " [0.02335635]]\n",
            "Loss: \n",
            "0.0002478059060531885\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8b-r70o8p2Dm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Try building/training a more complex MLP on a bigger dataset.\n",
        "\n",
        "Use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the cannonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
        "\n",
        "If you need inspiration, the internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n"
      ]
    },
    {
      "metadata": {
        "id": "5MOPtYdk1HgA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from mlxtend.data import mnist_data\n",
        "X, y = mnist_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jgspzy8aWVWN",
        "colab_type": "code",
        "outputId": "3867648e-5803-4dfc-c057-7649477e7ad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "r8sQnIT-WbTp",
        "colab_type": "code",
        "outputId": "8034fec0-5bf4-4708-9cd0-fda6f7cf0196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1325
        }
      },
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,  51., 159., 253., 159.,  50.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "        48., 238., 252., 252., 252., 237.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,  54., 227., 253., 252., 239., 233.,\n",
              "       252.,  57.,   6.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  10.,  60.,\n",
              "       224., 252., 253., 252., 202.,  84., 252., 253., 122.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0., 163., 252., 252., 252., 253., 252., 252.,\n",
              "        96., 189., 253., 167.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  51., 238.,\n",
              "       253., 253., 190., 114., 253., 228.,  47.,  79., 255., 168.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,  48., 238., 252., 252., 179.,  12.,  75., 121.,\n",
              "        21.,   0.,   0., 253., 243.,  50.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  38., 165., 253.,\n",
              "       233., 208.,  84.,   0.,   0.,   0.,   0.,   0.,   0., 253., 252.,\n",
              "       165.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   7., 178., 252., 240.,  71.,  19.,  28.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0., 253., 252., 195.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  57., 252., 252.,\n",
              "        63.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 253.,\n",
              "       252., 195.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0., 198., 253., 190.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0., 255., 253., 196.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  76., 246., 252.,\n",
              "       112.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "       253., 252., 148.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,  85., 252., 230.,  25.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   7., 135., 253., 186.,  12.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  85., 252.,\n",
              "       223.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   7., 131.,\n",
              "       252., 225.,  71.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,  85., 252., 145.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,  48., 165., 252., 173.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  86.,\n",
              "       253., 225.,   0.,   0.,   0.,   0.,   0.,   0., 114., 238., 253.,\n",
              "       162.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,  85., 252., 249., 146.,  48.,  29.,\n",
              "        85., 178., 225., 253., 223., 167.,  56.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "        85., 252., 252., 252., 229., 215., 252., 252., 252., 196., 130.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,  28., 199., 252., 252., 253.,\n",
              "       252., 252., 233., 145.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,  25., 128., 252., 253., 252., 141.,  37.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "FwlRJSfBlCvy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stretch Goals: \n",
        "\n",
        "- Implement Cross Validation model evaluation on your MNIST implementation \n",
        "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
        " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
        "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
      ]
    }
  ]
}