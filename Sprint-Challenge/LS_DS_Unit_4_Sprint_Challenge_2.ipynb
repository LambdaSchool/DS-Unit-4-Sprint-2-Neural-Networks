{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:** Cells in the brain connected by synapses that transmit electrical impulses. In the context of artificial neural networks, the analogous description of a neuron is to receive an input in the form of parameters (values/weights/biases) that are linearly combined and transformed into a non-linear output according to the neuron's function (activation), and passed either as a final output or to the next stage in the neural network. \n",
    "- **Input Layer:** The first layer of a neural network. The input layer is traditionally made of a neuron for each feature of the dataset that's fed to the neural network. \n",
    "- **Hidden Layer:** The intermediary neuronal computations between an input and output layer of a neural network. Hidden layers don't need to correspond to any obviously recognizable feature to the outside world. \n",
    "- **Output Layer:** The last layer of a neural network. It is responsible for computing the final result with a neuron for each output variable. Each output is in the form of a number between 0 and 1 for the output query. \n",
    "- **Activation:** The activation function of a neuron computes the output response given an input. The output shape produced is one that is either most useful for the next neuronal layer or one for the final output. Typical functions like tanh, or ReLU produce outputs with large range to maximally benefit the cost function of the neural network, while sigmoid functions typically produce final outputs between 0 and 1.\n",
    "- **Backpropagation:** An algorithm used in the training of _feedforward_ neural networks, whereby a net summation comparison of neural network's output with true output leads to the backpropogating update of the neural network parameters (weights/biases). This process trains the neural network. The gradient of the loss function determines which direction each of the parameters need to change to minimize the loss function, with larger steps on maximal gains and smaller steps on minimal gains proportional to a paramter's importance. The action continues lowering the error comparison with each iteration generating a best result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')\n",
    "candy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>gummy</th>\n",
       "      <th>ate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chocolate  gummy  ate\n",
       "0          0      1    1\n",
       "1          1      0    1\n",
       "2          0      1    1\n",
       "3          0      0    0\n",
       "4          1      1    0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chocolate=0,Gummy=0\n",
      "0    2377\n",
      "1     141\n",
      "Name: ate, dtype: int64\n",
      "\n",
      "Chocolate=0,Gummy=1\n",
      "1    2360\n",
      "0     131\n",
      "Name: ate, dtype: int64\n",
      "\n",
      "Chocolate=1,Gummy=0\n",
      "1    2359\n",
      "0     130\n",
      "Name: ate, dtype: int64\n",
      "\n",
      "Chocolate=1,Gummy=1\n",
      "0    2362\n",
      "1     140\n",
      "Name: ate, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get a feel for the data prediction\n",
    "# Data highly resembles a XOR gate\n",
    "\n",
    "print('Chocolate=0,Gummy=0')\n",
    "print(candy[(candy['chocolate']==0) & (candy['gummy']==0)]['ate'].value_counts())\n",
    "\n",
    "print('\\nChocolate=0,Gummy=1')\n",
    "print(candy[(candy['chocolate']==0) & (candy['gummy']==1)]['ate'].value_counts())\n",
    "\n",
    "print('\\nChocolate=1,Gummy=0')\n",
    "print(candy[(candy['chocolate']==1) & (candy['gummy']==0)]['ate'].value_counts())\n",
    "\n",
    "print('\\nChocolate=1,Gummy=1')\n",
    "print(candy[(candy['chocolate']==1) & (candy['gummy']==1)]['ate'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. You will not be able to achieve more than ~50% with the simple perceptron. Explain why you could not achieve a higher accuracy with the *simple perceptron* architecture, because it's possible to achieve ~95% accuracy on this dataset. Provide your answer in markdown (and *optional* data anlysis code) after your perceptron implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Start your candy perceptron here\n",
    "\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy['ate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       ...,\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code largely borrowed from lecture U4S2M1\n",
    "# Since the directions say not to do any feature engineering, let's first try to do by ignoring bias\n",
    "\n",
    "class Perceptron(object):\n",
    "    def __init__(self, rate=0.01, iters = 10):\n",
    "        self.rate = rate \n",
    "        self.iters = iters \n",
    "        \n",
    "    def train(self, X, y):\n",
    "        '''Fits the training data\n",
    "        X =  Training Vectors \n",
    "        X.shape : [#samples , #features ]\n",
    "        y = Target values \n",
    "        y.shape = [# samples]\n",
    "        '''\n",
    "                \n",
    "        # Initalize weights to random (-1,1)\n",
    "        self.weights = 2 * np.random.random((X.shape[1])) - 1\n",
    "        # Print initial weights\n",
    "        print('Initial weights: ',self.weights)\n",
    "                            \n",
    "        #loop till iteration limit is met \n",
    "        for i in range (self.iters):\n",
    "            #count=0\n",
    "            for xi,target in zip(X,y):\n",
    "                \n",
    "                #diagnostics\n",
    "                #if ((i==0) & (count<10)):\n",
    "                #    print(xi,target)\n",
    "                #    count = count + 1\n",
    "                \n",
    "                # Weighted sum of inputs / weights\n",
    "                weighted_sum = np.dot(xi, self.weights)\n",
    "\n",
    "                # Activate!\n",
    "                activated_output = self.sigmoid(weighted_sum)\n",
    "\n",
    "                # Calc error\n",
    "                error = target - activated_output\n",
    "                \n",
    "                # Get adjustments for weights, scale by self.rate\n",
    "                adjustment = self.rate * (error * self.sigmoid_derivative(activated_output))\n",
    "                \n",
    "                # Update the Weights\n",
    "                self.weights += np.dot(xi.T, adjustment)\n",
    "\n",
    "            # Print final weights\n",
    "            if i==self.iters-1:\n",
    "                print('Final weights: ',self.weights)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        return (1 / (1 + np.exp(-x)))\n",
    "    \n",
    "    def sigmoid_derivative(self,x):\n",
    "        sx = self.sigmoid(x)\n",
    "        return sx * (1 - sx)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        (Must run train function beforehand)\n",
    "        Returns class label for weights defined by class\n",
    "        \n",
    "        If activated output is over 0.5, then classify as 1 \n",
    "        otherwise, classify as 0\n",
    "        \"\"\"\n",
    "                \n",
    "        weighted_sum = np.dot(X, self.weights)\n",
    "        activated_output = self.sigmoid(weighted_sum)\n",
    "        round_result = np.where(activated_output >= 0.5, 1, 0)\n",
    "        \n",
    "        return round_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights:  [-0.25091976  0.90142861]\n",
      "Final weights:  [-0.0181422   0.02735231]\n"
     ]
    }
   ],
   "source": [
    "pn = Perceptron(0.002,10);\n",
    "pn.train(X,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuray:  0.2771\n"
     ]
    }
   ],
   "source": [
    "# Accuracy for given candy dataframe cannot reach accuracy > 0.5\n",
    "# An XOR gate cannot been written in a linear combination of two terms.\n",
    "# Another way to look at it, an XOR gate cannot be logically created with a one layer OR,AND,NOR,NAND gate\n",
    "#  which are the only similar options for how a weighted sum is computed by a symmetric activation function. \n",
    "# Also, there's no bias\n",
    "\n",
    "predictions = pn.predict(X)\n",
    "\n",
    "print('')\n",
    "print('Accuray: ',accuracy_score(y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW CHECK RESULT INCLUDING BIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code largely borrowed from lecture U4S2M1\n",
    "# Creating class, this time with a bias\n",
    "\n",
    "class Perceptron(object):\n",
    "    def __init__(self, rate=0.01, iters = 10):\n",
    "        self.rate = rate \n",
    "        self.iters = iters \n",
    "        \n",
    "    def train(self, X, y):\n",
    "        '''Fits the training data\n",
    "        X =  Training Vectors \n",
    "        X.shape : [#samples , #features ]\n",
    "        y = Target values \n",
    "        y.shape = [# samples]\n",
    "        '''\n",
    "                \n",
    "        # Initalize weights to random (-1,1)\n",
    "        self.weights = 2 * np.random.random((1+X.shape[1])) - 1\n",
    "        # Print initial weights\n",
    "        print('Initial weights: ',self.weights)\n",
    "                            \n",
    "        #loop till iteration limit is met \n",
    "        for i in range (self.iters):\n",
    "            #count=0\n",
    "            for xi,target in zip(X,y):\n",
    "                \n",
    "                #diagnostics\n",
    "                #if ((i==0) & (count<10)):\n",
    "                #    print(xi,target)\n",
    "                #    count = count + 1\n",
    "                \n",
    "                # Weighted sum of inputs / weights with bias\n",
    "                weighted_sum = np.dot(xi, self.weights[:-1]) + self.weights[-1] \n",
    "\n",
    "                # Activate!\n",
    "                activated_output = self.sigmoid(weighted_sum)\n",
    "\n",
    "                # Calc error\n",
    "                error = target - activated_output\n",
    "                \n",
    "                # Get adjustments for weights, scale by self.rate\n",
    "                adjustment = self.rate * (error * self.sigmoid_derivative(activated_output))\n",
    "                \n",
    "                # Update the Weights\n",
    "                self.weights[:-1] += adjustment * xi\n",
    "                self.weights[-1] += adjustment    #bias\n",
    "\n",
    "            # Print final weights\n",
    "            if i==self.iters-1:\n",
    "                print('Final weights: ',self.weights)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        return (1 / (1 + np.exp(-x)))\n",
    "    \n",
    "    def sigmoid_derivative(self,x):\n",
    "        sx = self.sigmoid(x)\n",
    "        return sx * (1 - sx)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        (Must run train function beforehand)\n",
    "        Returns class label for weights defined by class\n",
    "        \n",
    "        If activated output is over 0.5, then classify as 1 \n",
    "        otherwise, classify as 0\n",
    "        \"\"\"\n",
    "                \n",
    "        weighted_sum = np.dot(X, self.weights[:-1]) + self.weights[-1]\n",
    "        activated_output = self.sigmoid(weighted_sum)\n",
    "        round_result = np.where(activated_output >= 0.5, 1, 0)\n",
    "        \n",
    "        return round_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights:  [ 0.46398788  0.19731697 -0.68796272]\n",
      "Final weights:  [ 0.00966221  0.00901784 -0.00276011]\n"
     ]
    }
   ],
   "source": [
    "pn = Perceptron(0.002,50);\n",
    "pn.train(X,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuray:  0.7236\n"
     ]
    }
   ],
   "source": [
    "# Accuracy for given candy dataframe with bias may reach ~.72\n",
    "\n",
    "predictions = pn.predict(X)\n",
    "\n",
    "print('')\n",
    "print('Accuray: ',accuracy_score(y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But before, Demonstrate xor gate with nand, or, and gates\n",
    "\n",
    "XOR gate can be logically created with a two-layer set of gates utilizing OR,AND,NOR,NAND gates, which our MLP  _should_ closely converge. This should explain how our MLP should be able to generate a high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![XOR gate](https://upload.wikimedia.org/wikipedia/commons/e/ed/3_gate_XOR.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All gate weights were trained by Module1-Assignment steps. (I remove those for demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "        return (1 / (1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999998],\n",
       "       [0.99744992],\n",
       "       [0.99744992],\n",
       "       [0.00281114]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NAND gate weights\n",
    "\n",
    "weights_nand=[[-11.84042561],[-11.84042561],[ 17.80950276]] #computed externally\n",
    "\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')\n",
    "df['ones']=np.ones(4) # bias input\n",
    "df['ones']=df['ones'].astype('int')\n",
    "inputs = df[['x1','x2','ones']]\n",
    "correct_outputs = df[['y']]\n",
    "\n",
    "weighted_sum = np.dot(inputs, weights_nand)\n",
    "sigmoid(weighted_sum) # Activated output matches desired NAND gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00182739],\n",
       "       [0.9990711 ],\n",
       "       [0.9990711 ],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OR gate weights\n",
    "\n",
    "weights_or=[[13.28361249],[13.28361323],[-6.30303619]] # computed externally\n",
    "\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [0,1,1,1]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')\n",
    "df['ones']=np.ones(4) # bias input\n",
    "df['ones']=df['ones'].astype('int')\n",
    "inputs = df[['x1','x2','ones']]\n",
    "correct_outputs = df[['y']]\n",
    "\n",
    "weighted_sum = np.dot(inputs, weights_or)\n",
    "sigmoid(weighted_sum) # Activated output matches desired OR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.08952182e-08],\n",
       "       [1.82589798e-03],\n",
       "       [1.82589798e-03],\n",
       "       [9.96754484e-01]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AND gate weights\n",
    "\n",
    "weights_and = [[ 12.03108603], [ 12.03108603], [-18.33494183]] # computed externally\n",
    "\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [0,0,0,1]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')\n",
    "df['ones']=np.ones(4) # bias input\n",
    "df['ones']=df['ones'].astype('int')\n",
    "inputs = df[['x1','x2','ones']]\n",
    "correct_outputs = df[['y']]\n",
    "\n",
    "weighted_sum = np.dot(inputs, weights_and)\n",
    "sigmoid(weighted_sum) # Activated output matches desired AND gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00186641],\n",
       "       [0.99661623],\n",
       "       [0.99661623],\n",
       "       [0.00188859]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward propogate with gates of weights from above\n",
    "\n",
    "inputs = df[['x1','x2']]\n",
    "\n",
    "# Set weights\n",
    "weights1=np.hstack((weights_nand,weights_or))\n",
    "weights2=np.array((weights_and))\n",
    "\n",
    "# 1st layer propogation\n",
    "b = np.ones((4,1))\n",
    "inputs_wbias = np.hstack((inputs,b))\n",
    "hidden_sum = np.dot(inputs_wbias, weights1)\n",
    "activated_hidden = sigmoid(hidden_sum)\n",
    "\n",
    "# 2nd layer propogation\n",
    "b = np.ones((4,1))\n",
    "activated_hidden_wbias = np.hstack((activated_hidden,b))\n",
    "output_sum = np.dot(activated_hidden_wbias, weights2)\n",
    "activated_output = sigmoid(output_sum)\n",
    "\n",
    "# Result\n",
    "activated_output  # Result matches desired nand gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20573223],\n",
       "       [0.99961848],\n",
       "       [0.99961848],\n",
       "       [0.00449411]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DELETE ME\n",
    "# Forward propogate with gates of weights from above\n",
    "\n",
    "b = np.ones((4,1))\n",
    "inputs = df[['x1','x2']]\n",
    "inputs = np.hstack((inputs,b))\n",
    "\n",
    "# Set weights\n",
    "b = np.ones((3,1))\n",
    "weights1=np.hstack((weights_nand,weights_or,b))\n",
    "weights2=np.array((weights_and))\n",
    "\n",
    "# 1st layer propogation\n",
    "hidden_sum = np.dot(inputs, weights1)\n",
    "activated_hidden = sigmoid(hidden_sum)\n",
    "\n",
    "# 2nd layer propogation\n",
    "output_sum = np.dot(activated_hidden, weights2)\n",
    "activated_output = sigmoid(output_sum)\n",
    "\n",
    "# Result\n",
    "activated_output  # Result matches desired nand gate\n",
    "\n",
    "#print(inputs.shape,weights1.shape)\n",
    "#print(activated_hidden.shape,weights2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's see the results by training our own multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy[['ate']].values\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified class structure from U4S2M2 notes\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.rate = .01 \n",
    "        \n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = 2 + 1       # +1 for bias\n",
    "        self.hiddenNodes = 2  \n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        # 2x2 Matrix Array for the First Layer\n",
    "        self.weights1 = 2 * np.random.rand(self.inputs, self.hiddenNodes) - 1\n",
    "       \n",
    "        # 2x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = 2 * np.random.rand(self.hiddenNodes+1, self.outputNodes) - 1\n",
    "        \n",
    "        #print(self.weights1.shape)\n",
    "        #print(self.weights2.shape)\n",
    "        \n",
    "        #Testing forwardpropogation with above weights\n",
    "        #weights_nand=[[-11.84042561],[-11.84042561],[ 17.80950276]] \n",
    "        #weights_or=[[13.28361249],[13.28361323],[-6.30303619]]\n",
    "        #weights_and = [[ 12.03108603], [ 12.03108603], [-18.33494183]]\n",
    "        #self.weights1=np.hstack((weights_nand,weights_or))\n",
    "        #self.weights2=np.array((weights_and))\n",
    "        \n",
    "            \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        sx = self.sigmoid(s)\n",
    "        return sx * (1 - sx)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add bias column to input df\n",
    "        b = np.ones((X.shape[0],1))\n",
    "        X = np.hstack((X,b))\n",
    "\n",
    "        # Weighted sum of inputs => hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Add bias column to activated_hidden\n",
    "        b = np.ones((X.shape[0],1))\n",
    "        self.activated_hidden_wbias = np.hstack((self.activated_hidden,b))\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden_wbias, self.weights2)\n",
    "        \n",
    "        # Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "\n",
    "    def backward(self, X,y,o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"        \n",
    "        \n",
    "        # Add bias column to input df\n",
    "        b = np.ones((X.shape[0],1))\n",
    "        X = np.hstack((X,b))\n",
    "        \n",
    "        # Error in Output\n",
    "        self.o_error = y - o\n",
    "        \n",
    "        # Apply Derivative of Sigmoid to error\n",
    "        # How far off are we in relation to the Sigmoid f(x) of the output\n",
    "        # ^- aka hidden => output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        # z2 error\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        \n",
    "        # How much of that \"far off\" can explained by the input => hidden\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden_wbias)\n",
    "        \n",
    "        print(X.shape)\n",
    "        print(self.z2_delta.shape)\n",
    "        print(self.weights1.shape)\n",
    "        \n",
    "        \n",
    "        print()\n",
    "        print(\"BEFORE weights2:\")\n",
    "        print(self.weights2)\n",
    "        \n",
    "        # Apply adjustment to first set of weights (input => hidden)\n",
    "        #self.weights1 += self.rate * X.T.dot(self.z2_delta)\n",
    "        # Apply adjustment to second set of weights (hidden => output)\n",
    "        self.weights2 += self.rate * self.activated_hidden_wbias.T.dot(self.o_delta)\n",
    "        \n",
    "        print()\n",
    "        print(\"AFTER weights2:\")\n",
    "        print(self.weights2)\n",
    "        \n",
    "        print()\n",
    "        print('o_delta')\n",
    "        print(self.o_delta)\n",
    "        \n",
    "        print()\n",
    "        print('activated hidden w bias')\n",
    "        print(self.activated_hidden_wbias)\n",
    "        \n",
    "    \n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X,y,o)\n",
    "    \n",
    "    def predict(self, X, o):\n",
    "        \"\"\"\n",
    "        (Must run train function beforehand)\n",
    "        Returns class label for weights defined by class\n",
    "        \n",
    "        If activated output is over 0.5, then classify as 1 \n",
    "        otherwise, classify as 0\n",
    "        \"\"\"\n",
    "        \n",
    "        round_result = np.where(o >= 0.5, 1, 0)\n",
    "        \n",
    "        return round_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n",
      "(10000, 3)\n",
      "(3, 2)\n",
      "\n",
      "BEFORE weights2:\n",
      "[[-0.88383278]\n",
      " [ 0.73235229]\n",
      " [ 0.20223002]]\n",
      "\n",
      "AFTER weights2:\n",
      "[[-1.29940178]\n",
      " [ 0.06280733]\n",
      " [-1.08747634]]\n",
      "\n",
      "o_delta\n",
      "[[0.11173925]\n",
      " [0.09438967]\n",
      " [0.11173925]\n",
      " ...\n",
      " [0.11173925]\n",
      " [0.11173925]\n",
      " [0.09438967]]\n",
      "\n",
      "activated hidden w bias\n",
      "[[0.4442392  0.37973009 1.        ]\n",
      " [0.28112613 0.55315282 1.        ]\n",
      " [0.4442392  0.37973009 1.        ]\n",
      " ...\n",
      " [0.4442392  0.37973009 1.        ]\n",
      " [0.4442392  0.37973009 1.        ]\n",
      " [0.28112613 0.55315282 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "o = nn.feed_forward(X)\n",
    "nn.backward(X,y,o)\n",
    "\n",
    "#accuracy_score(y, nn.predict(X,o))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified class structure from U4S2M2 notes\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.rate = .01 \n",
    "        \n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = 2       # +1 for bias\n",
    "        self.hiddenNodes = 2  \n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        # 2x2 Matrix Array for the First Layer\n",
    "        self.weights1 = 2 * np.random.rand(self.inputs, self.hiddenNodes) - 1\n",
    "        self.b1 = np.ones((1, self.hiddenNodes))\n",
    "        \n",
    "        #b = np.ones((3,1))\n",
    "        #self.weights1=np.hstack((self.weights1,b))\n",
    "       \n",
    "        # 2x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = 2 * np.random.rand(self.hiddenNodes, self.outputNodes) - 1\n",
    "        self.b2 = np.ones((1, self.outputNodes))\n",
    "        \n",
    "        #print(self.weights1.shape)\n",
    "        #print(self.weights2.shape)\n",
    "        \n",
    "        #Testing forwardpropogation with above weights\n",
    "        #weights_nand=[[-11.84042561],[-11.84042561],[ 17.80950276]] \n",
    "        #weights_or=[[13.28361249],[13.28361323],[-6.30303619]]\n",
    "        #weights_and = [[ 12.03108603], [ 12.03108603], [-18.33494183]]\n",
    "        #self.weights1=np.hstack((weights_nand,weights_or))\n",
    "        #self.weights2=np.array((weights_and))\n",
    "        \n",
    "            \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        sx = self.sigmoid(s)\n",
    "        return sx * (1 - sx)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add bias column to input df\n",
    "        #b = np.ones((X.shape[0],1))\n",
    "        #X = np.hstack((X,b))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        # Weighted sum of inputs => hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1) + self.b1\n",
    "    \n",
    "        \n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Add bias column to activated_hidden\n",
    "        #b = np.ones((X.shape[0],1))\n",
    "        #self.activated_hidden_wbias = np.hstack((self.activated_hidden,b))\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2) + self.b2\n",
    "        \n",
    "        # Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "\n",
    "    def backward(self, X,y,o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"        \n",
    "        \n",
    "        # Add bias column to input df\n",
    "        #b = np.ones((X.shape[0],1))\n",
    "        #X = np.hstack((X,b))\n",
    "        \n",
    "        # Error in Output\n",
    "        self.o_error = y - o\n",
    "        \n",
    "        # Apply Derivative of Sigmoid to error\n",
    "        # How far off are we in relation to the Sigmoid f(x) of the output\n",
    "        # ^- aka hidden => output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        # z2 error\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        \n",
    "        # How much of that \"far off\" can explained by the input => hidden\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "#         print(X.shape)\n",
    "#         print(self.z2_delta.shape)\n",
    "#         print(self.weights1.shape)\n",
    "        \n",
    "        \n",
    "#         print()\n",
    "#         print(\"BEFORE weights2:\")\n",
    "#         print(self.weights2)\n",
    "        \n",
    "        # Apply adjustment to first set of weights (input => hidden)\n",
    "        self.weights1 -= self.rate * X.T.dot(self.z2_delta)\n",
    "        self.b1       -= self.rate * np.sum(self.z2_delta, axis=0)\n",
    "        # Apply adjustment to second set of weights (hidden => output)\n",
    "        self.weights2 -= self.rate * self.activated_hidden.T.dot(self.o_delta)\n",
    "        self.b2       -= self.rate * np.sum(self.o_delta, axis=0)\n",
    "        \n",
    "#         print()\n",
    "#         print(\"AFTER weights2:\")\n",
    "#         print(self.weights2)\n",
    "        \n",
    "#         print()\n",
    "#         print('o_delta')\n",
    "#         print(self.o_delta)\n",
    "        \n",
    "#         print()\n",
    "#         print('activated hidden')\n",
    "#         print(self.activated_hidden)\n",
    "        \n",
    "    \n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X,y,o)\n",
    "    \n",
    "    def predict(self, X, o):\n",
    "        \"\"\"\n",
    "        (Must run train function beforehand)\n",
    "        Returns class label for weights defined by class\n",
    "        \n",
    "        If activated output is over 0.5, then classify as 1 \n",
    "        otherwise, classify as 0\n",
    "        \"\"\"\n",
    "        \n",
    "        round_result = np.where(o >= 0.5, 1, 0)\n",
    "        \n",
    "        return round_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()\n",
    "o = nn.feed_forward(X)\n",
    "#nn.backward(X,y,o)\n",
    "\n",
    "#accuracy_score(y, nn.predict(X,o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.70016959]\n",
      " [0.71058645]\n",
      " [0.70016959]\n",
      " ...\n",
      " [0.70016959]\n",
      " [0.70016959]\n",
      " [0.71058645]]\n",
      "Loss: \n",
      " 0.2920771111398822\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.99993438]\n",
      " [0.99988071]\n",
      " [0.99993438]\n",
      " ...\n",
      " [0.99993438]\n",
      " [0.99993438]\n",
      " [0.99988071]]\n",
      "Loss: \n",
      " 0.49990383954662737\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: \n",
      " 0.49999999999999734\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 100---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: \n",
      " 0.5\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "\n",
    "# Number of Epochs / Iterations\n",
    "for i in range(100):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 100 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuray:  0.2771\n"
     ]
    }
   ],
   "source": [
    "predictions = nn.predict(X,o)\n",
    "\n",
    "print('')\n",
    "print('Accuray: ',accuracy_score(y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.7224584 ]\n",
      " [0.71890753]\n",
      " [0.7224584 ]\n",
      " ...\n",
      " [0.7224584 ]\n",
      " [0.7224584 ]\n",
      " [0.71890753]]\n",
      "Loss: \n",
      " 0.298412096304008\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.99996115]\n",
      " [0.99993822]\n",
      " [0.99996115]\n",
      " ...\n",
      " [0.99996115]\n",
      " [0.99996115]\n",
      " [0.99993822]]\n",
      "Loss: \n",
      " 0.4999466594671015\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: \n",
      " 0.4999999999999986\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 100---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: \n",
      " 0.5\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "\n",
    "# Number of Epochs / Iterations\n",
    "for i in range(100):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 100 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuray:  0.2771\n"
     ]
    }
   ],
   "source": [
    "predictions = nn.predict(X,o)\n",
    "\n",
    "print('')\n",
    "print('Accuray: ',accuracy_score(y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Largely copied, then slightly modified from U4S2M2 notes\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.rate = .01 \n",
    "        \n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 2\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        # 2x2 Matrix Array for the First Layer\n",
    "        self.weights1 = 2 * np.random.rand(self.inputs, self.hiddenNodes) - 1\n",
    "       \n",
    "        # 2x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = 2 * np.random.rand(self.hiddenNodes, self.outputNodes) - 1\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        sx = self.sigmoid(s)\n",
    "        return sx * (1 - sx)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(\"BEFORE\")\n",
    "        #print(self.weights1)\n",
    "        #print(self.weights2)\n",
    "        \n",
    "        # Weighted sum of inputs => hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "        \n",
    "    def backward(self, X,y,o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"        \n",
    "        # Error in Output\n",
    "        self.o_error = y - o\n",
    "        \n",
    "        #PRINT LOSS FUNCTION\n",
    "        #print(\"SUM: \", sum(self.o_error))\n",
    "        \n",
    "        # Apply Derivative of Sigmoid to error\n",
    "        # How far off are we in relation to the Sigmoid f(x) of the output\n",
    "        # ^- aka hidden => output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        # z2 error\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        \n",
    "        # How much of that \"far off\" can explained by the input => hidden\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        # Apply adjustment to first set of weights (input => hidden)\n",
    "        self.weights1 += self.rate * X.T.dot(self.z2_delta)\n",
    "        # Apply adjustment to second set of weights (hidden => output)\n",
    "        self.weights2 += self.rate * self.activated_hidden.T.dot(self.o_delta)\n",
    "        \n",
    "        #print(\"BACKPROPAGATED UPDATE WEIGHTS\")\n",
    "        #print(self.weights1)\n",
    "        #print(self.weights2)\n",
    "        \n",
    "\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        \n",
    "        #print(\"NEW O\")\n",
    "        #print(o)\n",
    "        \n",
    "        self.backward(X,y,o)\n",
    "        \n",
    "    def predict(self, X, o):\n",
    "        \"\"\"\n",
    "        (Must run train function beforehand)\n",
    "        Returns class label for weights defined by class\n",
    "        \n",
    "        If activated output is over 0.5, then classify as 1 \n",
    "        otherwise, classify as 0\n",
    "        \"\"\"\n",
    "        \n",
    "        round_result = np.where(o >= 0.5, 1, 0)\n",
    "        \n",
    "        return round_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork();\n",
    "nn.train(X,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71475047],\n",
       "       [0.75161591],\n",
       "       [0.71475047],\n",
       "       ...,\n",
       "       [0.71475047],\n",
       "       [0.71475047],\n",
       "       [0.75161591]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = nn.feed_forward(X);\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuray:  0.5\n"
     ]
    }
   ],
   "source": [
    "predictions = nn.predict(X,o)\n",
    "\n",
    "print('')\n",
    "print('Accuray: ',accuracy_score(y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY THIS TIME WITH A BIAS\n",
    "\n",
    "# I KNOW MY MLP NEEDS A BIAS BECAUSE THE XOR GATE DIAGRAM BELOW\n",
    "# (WHICH IS WHAT OUR MLP SHOULD CONVERGE TO)\n",
    "# INCLUDES A NAND GATE WHICH I KNOW FROM MODULE1-ASSIGNMENT NEEDS\n",
    "# A BIAS TO FUNCTION PROPERLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Largely copied, then slightly modified from U4S2M2 notes\n",
    "\n",
    "# DIFFICULTY GETTING DIMENSIONS TO LINE UP TO CONFIGURE BIAS, NOT ENOUGH TIME\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, rate=0.01, iters = 10):\n",
    "        self.rate = rate\n",
    "        self.iters = iters\n",
    "        \n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = 2 + 1         # +1 for BIAS\n",
    "        self.hiddenNodes = 2 + 1    # +1 for BIAS\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        # 2x2 Matrix Array for the First Layer\n",
    "        self.weights1 = 2 * np.random.rand(self.inputs, self.hiddenNodes) - 1\n",
    "       \n",
    "        # 2x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = 2 * np.random.rand(self.hiddenNodes, self.outputNodes) - 1\n",
    "        \n",
    "        #print(self.weights1)\n",
    "        #print(self.weights1[:-1])\n",
    "        #print(self.weights1[-1])\n",
    "        #print()\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        sx = self.sigmoid(s)\n",
    "        return sx * (1 - sx)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(\"BEFORE\")\n",
    "        #print(self.weights1)\n",
    "        #print(self.weights2)\n",
    "        \n",
    "        #for xi,target in zip(X,y):\n",
    "        #        \n",
    "        #    # Weighted sum of inputs / weights with bias\n",
    "        #    weighted_sum = np.dot(xi, self.weights1[:-1]) + self.weights1[-1] \n",
    "\n",
    "        #    # Activate!\n",
    "        #    activated_output = self.sigmoid(weighted_sum)\n",
    "\n",
    "        #    # Calc error\n",
    "        #    error = target - activated_output\n",
    "                \n",
    "        #    # Get adjustments for weights, scale by self.rate\n",
    "        #    adjustment = self.rate * (error * self.sigmoid_derivative(activated_output))\n",
    "                \n",
    "        #        # Update the Weights\n",
    "        #    self.weights[:-1] += adjustment * xi\n",
    "        #    self.weights[-1] += adjustment    #bias\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Weighted sum of inputs => hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1[:-1]) + self.weights1[-1]\n",
    "                \n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        print(self.activated_hidden.shape)\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2[:-1]) + self.weights[-1]\n",
    "        \n",
    "        # Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "        \n",
    "    def backward(self, X,y,o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"        \n",
    "        # Error in Output\n",
    "        self.o_error = y - o\n",
    "        \n",
    "        print(\"SUM: \", sum(self.o_error))\n",
    "        \n",
    "        # Apply Derivative of Sigmoid to error\n",
    "        # How far off are we in relation to the Sigmoid f(x) of the output\n",
    "        # ^- aka hidden => output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        # z2 error\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        \n",
    "        # How much of that \"far off\" can explained by the input => hidden\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        # Apply adjustment to first set of weights (input => hidden)\n",
    "        self.weights1 += self.rate * X.T.dot(self.z2_delta)\n",
    "        # Apply adjustment to second set of weights (hidden => output)\n",
    "        self.weights2 += self.rate * self.activated_hidden.T.dot(self.o_delta)\n",
    "        \n",
    "        print(\"BACKPROPAGATED UPDATE WEIGHTS\")\n",
    "        print(self.weights1)\n",
    "        print(self.weights2)\n",
    "        \n",
    "\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        \n",
    "        print(\"NEW O\")\n",
    "        print(o)\n",
    "        \n",
    "        self.backward(X,y,o)\n",
    "        \n",
    "    def predict(self, X, o):\n",
    "        \"\"\"\n",
    "        (Must run train function beforehand)\n",
    "        Returns class label for weights defined by class\n",
    "        \n",
    "        If activated output is over 0.5, then classify as 1 \n",
    "        otherwise, classify as 0\n",
    "        \"\"\"\n",
    "        \n",
    "        round_result = np.where(o >= 0.5, 1, 0)\n",
    "        \n",
    "        return round_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (10000,3) and (2,1) not aligned: 3 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-dfcc950363c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-f0cc5e3a5035>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NEW O\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-f0cc5e3a5035>\u001b[0m in \u001b[0;36mfeed_forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Weight sum between hidden and output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivated_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Final activation of output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (10000,3) and (2,1) not aligned: 3 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork();\n",
    "nn.train(X,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop neural network training\n",
    "\n",
    "# Train my 'net\n",
    "#nn = NeuralNetwork()\n",
    "\n",
    "## Number of Epochs / Iterations\n",
    "#for i in range(10000):\n",
    "#    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
    "#        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "#        print('Input: \\n', X)\n",
    "#        print('Actual Output: \\n', y)\n",
    "#        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "#        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "#    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>276</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>170</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>134</td>\n",
       "      <td>409</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "      <td>265</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "179   57    1   0       150   276    0        0      112      1      0.6   \n",
       "228   59    1   3       170   288    0        0      159      0      0.2   \n",
       "111   57    1   2       150   126    1        1      173      0      0.2   \n",
       "246   56    0   0       134   409    0        0      150      1      1.9   \n",
       "60    71    0   2       110   265    1        0      130      0      0.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "179      1   1     1       0  \n",
       "228      1   0     3       0  \n",
       "111      2   1     3       1  \n",
       "246      1   2     3       0  \n",
       "60       2   1     2       1  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 13) (303,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/martin/anaconda3/lib/python3.7/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(df.drop(columns='target'))\n",
    "y = df.target.values\n",
    "\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "242/242 [==============================] - 0s 675us/step - loss: 1.0417 - accuracy: 0.4545\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 155us/step - loss: 0.8859 - accuracy: 0.4504\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 162us/step - loss: 0.7943 - accuracy: 0.4545\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 174us/step - loss: 0.7418 - accuracy: 0.4669\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 173us/step - loss: 0.7032 - accuracy: 0.5041\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 175us/step - loss: 0.6739 - accuracy: 0.5950\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 159us/step - loss: 0.6453 - accuracy: 0.6446\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 167us/step - loss: 0.6187 - accuracy: 0.6901\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 165us/step - loss: 0.5919 - accuracy: 0.6983\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 185us/step - loss: 0.5648 - accuracy: 0.7273\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 179us/step - loss: 0.5364 - accuracy: 0.7355\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 176us/step - loss: 0.5080 - accuracy: 0.7645\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 163us/step - loss: 0.4812 - accuracy: 0.7851\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 180us/step - loss: 0.4557 - accuracy: 0.7975\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 166us/step - loss: 0.4326 - accuracy: 0.8099\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 175us/step - loss: 0.4121 - accuracy: 0.8017\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 186us/step - loss: 0.3946 - accuracy: 0.8099\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 179us/step - loss: 0.3801 - accuracy: 0.8182\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 161us/step - loss: 0.3672 - accuracy: 0.8347\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 193us/step - loss: 0.3557 - accuracy: 0.8388\n",
      "61/61 [==============================] - 0s 384us/step\n",
      "242/242 [==============================] - 0s 51us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.7516 - accuracy: 0.4091\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 157us/step - loss: 0.6973 - accuracy: 0.5331\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 187us/step - loss: 0.6598 - accuracy: 0.6033\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 177us/step - loss: 0.6305 - accuracy: 0.6446\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 183us/step - loss: 0.6031 - accuracy: 0.6777\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 201us/step - loss: 0.5768 - accuracy: 0.7025\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 217us/step - loss: 0.5497 - accuracy: 0.7273\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 200us/step - loss: 0.5230 - accuracy: 0.7521\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 214us/step - loss: 0.4979 - accuracy: 0.7686\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 205us/step - loss: 0.4752 - accuracy: 0.7934\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 210us/step - loss: 0.4529 - accuracy: 0.7934\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 213us/step - loss: 0.4339 - accuracy: 0.8058\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 205us/step - loss: 0.4169 - accuracy: 0.8099\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 222us/step - loss: 0.4022 - accuracy: 0.8223\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 208us/step - loss: 0.3898 - accuracy: 0.8347\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 215us/step - loss: 0.3793 - accuracy: 0.8264\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 213us/step - loss: 0.3704 - accuracy: 0.8430\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 216us/step - loss: 0.3617 - accuracy: 0.8430\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 212us/step - loss: 0.3545 - accuracy: 0.8471\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 251us/step - loss: 0.3483 - accuracy: 0.8388\n",
      "61/61 [==============================] - 0s 770us/step\n",
      "242/242 [==============================] - 0s 80us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 0s 630us/step - loss: 0.6373 - accuracy: 0.6736\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 248us/step - loss: 0.5974 - accuracy: 0.7190\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 285us/step - loss: 0.5610 - accuracy: 0.7438\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 243us/step - loss: 0.5226 - accuracy: 0.7934\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 242us/step - loss: 0.4843 - accuracy: 0.8140\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 241us/step - loss: 0.4502 - accuracy: 0.8306\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 269us/step - loss: 0.4171 - accuracy: 0.8347\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 202us/step - loss: 0.3924 - accuracy: 0.8471\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 221us/step - loss: 0.3719 - accuracy: 0.8512\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 239us/step - loss: 0.3563 - accuracy: 0.8471\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 251us/step - loss: 0.3437 - accuracy: 0.8512\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 237us/step - loss: 0.3321 - accuracy: 0.8595\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 212us/step - loss: 0.3257 - accuracy: 0.8595\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 236us/step - loss: 0.3180 - accuracy: 0.8595\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 210us/step - loss: 0.3126 - accuracy: 0.8595\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 234us/step - loss: 0.3083 - accuracy: 0.8636\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 204us/step - loss: 0.3040 - accuracy: 0.8636\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 241us/step - loss: 0.3000 - accuracy: 0.8636\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 195us/step - loss: 0.2963 - accuracy: 0.8678\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 209us/step - loss: 0.2920 - accuracy: 0.8802\n",
      "61/61 [==============================] - 0s 481us/step\n",
      "242/242 [==============================] - 0s 64us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 0s 533us/step - loss: 0.6861 - accuracy: 0.5144\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 149us/step - loss: 0.6176 - accuracy: 0.6173\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 154us/step - loss: 0.5711 - accuracy: 0.6831\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 170us/step - loss: 0.5325 - accuracy: 0.7243\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 223us/step - loss: 0.5040 - accuracy: 0.7366\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 282us/step - loss: 0.4769 - accuracy: 0.7778\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 241us/step - loss: 0.4550 - accuracy: 0.8189\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 237us/step - loss: 0.4355 - accuracy: 0.8436\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 309us/step - loss: 0.4179 - accuracy: 0.8395\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 353us/step - loss: 0.4026 - accuracy: 0.8477\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 319us/step - loss: 0.3894 - accuracy: 0.8477\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 247us/step - loss: 0.3778 - accuracy: 0.8477\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 284us/step - loss: 0.3672 - accuracy: 0.8436\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 305us/step - loss: 0.3571 - accuracy: 0.8477\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 278us/step - loss: 0.3488 - accuracy: 0.8477\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 239us/step - loss: 0.3397 - accuracy: 0.8519\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 0s 203us/step - loss: 0.3319 - accuracy: 0.8560\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 278us/step - loss: 0.3244 - accuracy: 0.8560\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 297us/step - loss: 0.3182 - accuracy: 0.8642\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 291us/step - loss: 0.3111 - accuracy: 0.8683\n",
      "60/60 [==============================] - 0s 573us/step\n",
      "243/243 [==============================] - 0s 116us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 0s 842us/step - loss: 0.7435 - accuracy: 0.5185\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 175us/step - loss: 0.6667 - accuracy: 0.6296\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 162us/step - loss: 0.6117 - accuracy: 0.7037\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 169us/step - loss: 0.5677 - accuracy: 0.7613\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 180us/step - loss: 0.5304 - accuracy: 0.7613\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 173us/step - loss: 0.4965 - accuracy: 0.7819\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 167us/step - loss: 0.4696 - accuracy: 0.7901\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 162us/step - loss: 0.4437 - accuracy: 0.7984\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 162us/step - loss: 0.4236 - accuracy: 0.8025\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 195us/step - loss: 0.4074 - accuracy: 0.8107\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 278us/step - loss: 0.3938 - accuracy: 0.8148\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 270us/step - loss: 0.3817 - accuracy: 0.8189\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 236us/step - loss: 0.3724 - accuracy: 0.8189\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 274us/step - loss: 0.3653 - accuracy: 0.8230\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 284us/step - loss: 0.3586 - accuracy: 0.8272\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 244us/step - loss: 0.3519 - accuracy: 0.8272\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 304us/step - loss: 0.3456 - accuracy: 0.8313\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 285us/step - loss: 0.3409 - accuracy: 0.8354\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 258us/step - loss: 0.3382 - accuracy: 0.8519\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 284us/step - loss: 0.3328 - accuracy: 0.8601\n",
      "60/60 [==============================] - 0s 504us/step\n",
      "243/243 [==============================] - 0s 108us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "303/303 [==============================] - 0s 743us/step - loss: 0.6585 - accuracy: 0.5941\n",
      "Epoch 2/20\n",
      "303/303 [==============================] - 0s 125us/step - loss: 0.6144 - accuracy: 0.6832\n",
      "Epoch 3/20\n",
      "303/303 [==============================] - 0s 132us/step - loss: 0.5773 - accuracy: 0.7591\n",
      "Epoch 4/20\n",
      "303/303 [==============================] - 0s 142us/step - loss: 0.5397 - accuracy: 0.7888\n",
      "Epoch 5/20\n",
      "303/303 [==============================] - 0s 138us/step - loss: 0.5041 - accuracy: 0.8020\n",
      "Epoch 6/20\n",
      "303/303 [==============================] - 0s 236us/step - loss: 0.4730 - accuracy: 0.8152\n",
      "Epoch 7/20\n",
      "303/303 [==============================] - 0s 258us/step - loss: 0.4437 - accuracy: 0.8284\n",
      "Epoch 8/20\n",
      "303/303 [==============================] - 0s 263us/step - loss: 0.4190 - accuracy: 0.8416\n",
      "Epoch 9/20\n",
      "303/303 [==============================] - 0s 219us/step - loss: 0.3963 - accuracy: 0.8515\n",
      "Epoch 10/20\n",
      "303/303 [==============================] - 0s 189us/step - loss: 0.3798 - accuracy: 0.8581\n",
      "Epoch 11/20\n",
      "303/303 [==============================] - 0s 169us/step - loss: 0.3634 - accuracy: 0.8647\n",
      "Epoch 12/20\n",
      "303/303 [==============================] - 0s 190us/step - loss: 0.3514 - accuracy: 0.8647\n",
      "Epoch 13/20\n",
      "303/303 [==============================] - 0s 221us/step - loss: 0.3406 - accuracy: 0.8746\n",
      "Epoch 14/20\n",
      "303/303 [==============================] - 0s 253us/step - loss: 0.3317 - accuracy: 0.8746\n",
      "Epoch 15/20\n",
      "303/303 [==============================] - 0s 263us/step - loss: 0.3223 - accuracy: 0.8779\n",
      "Epoch 16/20\n",
      "303/303 [==============================] - 0s 291us/step - loss: 0.3162 - accuracy: 0.9010\n",
      "Epoch 17/20\n",
      "303/303 [==============================] - 0s 191us/step - loss: 0.3101 - accuracy: 0.8878\n",
      "Epoch 18/20\n",
      "303/303 [==============================] - 0s 170us/step - loss: 0.3064 - accuracy: 0.8911\n",
      "Epoch 19/20\n",
      "303/303 [==============================] - 0s 198us/step - loss: 0.3005 - accuracy: 0.8845\n",
      "Epoch 20/20\n",
      "303/303 [==============================] - 0s 193us/step - loss: 0.2979 - accuracy: 0.8845\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Create function for model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(13, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, batch_size=10, \n",
    "                        epochs=20,verbose=1)\n",
    "\n",
    "# Create 5-fold cross validation \n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Grid search\n",
    "param_grid = dict()\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                    n_jobs=1, cv=kfold)\n",
    "\n",
    "# Fit\n",
    "grid_result1 = grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 82.178% using {}\n"
     ]
    }
   ],
   "source": [
    "grid_result = grid_result1\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_*100:.3f}% using {grid_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNE BATCH SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "242/242 [==============================] - 0s 547us/step - loss: 0.7195 - accuracy: 0.4793\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 150us/step - loss: 0.6805 - accuracy: 0.5289\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 150us/step - loss: 0.6510 - accuracy: 0.5868\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 167us/step - loss: 0.6247 - accuracy: 0.6446\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 162us/step - loss: 0.5980 - accuracy: 0.7107\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 172us/step - loss: 0.5710 - accuracy: 0.7397\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 189us/step - loss: 0.5433 - accuracy: 0.7562\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 160us/step - loss: 0.5161 - accuracy: 0.7851\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 174us/step - loss: 0.4905 - accuracy: 0.7893\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 184us/step - loss: 0.4651 - accuracy: 0.8058\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 177us/step - loss: 0.4424 - accuracy: 0.8058\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 171us/step - loss: 0.4204 - accuracy: 0.8182\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 180us/step - loss: 0.4029 - accuracy: 0.8264\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 192us/step - loss: 0.3867 - accuracy: 0.8264\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 178us/step - loss: 0.3711 - accuracy: 0.8388\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 169us/step - loss: 0.3569 - accuracy: 0.8388\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 171us/step - loss: 0.3471 - accuracy: 0.8430\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 176us/step - loss: 0.3359 - accuracy: 0.8512\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 184us/step - loss: 0.3258 - accuracy: 0.8595\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 185us/step - loss: 0.3179 - accuracy: 0.8636\n",
      "61/61 [==============================] - 0s 382us/step\n",
      "242/242 [==============================] - 0s 58us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 0s 522us/step - loss: 0.7741 - accuracy: 0.3471\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 160us/step - loss: 0.7082 - accuracy: 0.4628\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 167us/step - loss: 0.6702 - accuracy: 0.5868\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 173us/step - loss: 0.6395 - accuracy: 0.6901\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 147us/step - loss: 0.6116 - accuracy: 0.7149\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 138us/step - loss: 0.5832 - accuracy: 0.7686\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 150us/step - loss: 0.5498 - accuracy: 0.8058\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 177us/step - loss: 0.5176 - accuracy: 0.8058\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 156us/step - loss: 0.4874 - accuracy: 0.8223\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 171us/step - loss: 0.4589 - accuracy: 0.8223\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 173us/step - loss: 0.4346 - accuracy: 0.8182\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 180us/step - loss: 0.4132 - accuracy: 0.8182\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 168us/step - loss: 0.3964 - accuracy: 0.8388\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 167us/step - loss: 0.3822 - accuracy: 0.8306\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 169us/step - loss: 0.3708 - accuracy: 0.8388\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 178us/step - loss: 0.3604 - accuracy: 0.8430\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 161us/step - loss: 0.3518 - accuracy: 0.8512\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 163us/step - loss: 0.3440 - accuracy: 0.8595\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 157us/step - loss: 0.3371 - accuracy: 0.8636\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 174us/step - loss: 0.3307 - accuracy: 0.8678\n",
      "61/61 [==============================] - 0s 396us/step\n",
      "242/242 [==============================] - 0s 60us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 0s 539us/step - loss: 0.7089 - accuracy: 0.5620\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 146us/step - loss: 0.6518 - accuracy: 0.6570\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 158us/step - loss: 0.6095 - accuracy: 0.7645\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 147us/step - loss: 0.5712 - accuracy: 0.8182\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 153us/step - loss: 0.5337 - accuracy: 0.8347\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 150us/step - loss: 0.4966 - accuracy: 0.8306\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 158us/step - loss: 0.4580 - accuracy: 0.8430\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 137us/step - loss: 0.4274 - accuracy: 0.8430\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 170us/step - loss: 0.4005 - accuracy: 0.8471\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 185us/step - loss: 0.3800 - accuracy: 0.8554\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 165us/step - loss: 0.3640 - accuracy: 0.8595\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 182us/step - loss: 0.3522 - accuracy: 0.8595\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 166us/step - loss: 0.3432 - accuracy: 0.8636\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 159us/step - loss: 0.3361 - accuracy: 0.8678\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 161us/step - loss: 0.3298 - accuracy: 0.8760\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 153us/step - loss: 0.3244 - accuracy: 0.8760\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 152us/step - loss: 0.3193 - accuracy: 0.8802\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 180us/step - loss: 0.3146 - accuracy: 0.8802\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 183us/step - loss: 0.3112 - accuracy: 0.8802\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 152us/step - loss: 0.3067 - accuracy: 0.8760\n",
      "61/61 [==============================] - 0s 549us/step\n",
      "242/242 [==============================] - 0s 86us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 0s 542us/step - loss: 0.7700 - accuracy: 0.3992\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 164us/step - loss: 0.6988 - accuracy: 0.5556\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 168us/step - loss: 0.6478 - accuracy: 0.6502\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 143us/step - loss: 0.6045 - accuracy: 0.7037\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 166us/step - loss: 0.5636 - accuracy: 0.7531\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 171us/step - loss: 0.5261 - accuracy: 0.8025\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 176us/step - loss: 0.4920 - accuracy: 0.8148\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 161us/step - loss: 0.4623 - accuracy: 0.8272\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 181us/step - loss: 0.4347 - accuracy: 0.8272\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 170us/step - loss: 0.4126 - accuracy: 0.8272\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 178us/step - loss: 0.3945 - accuracy: 0.8313\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 174us/step - loss: 0.3805 - accuracy: 0.8354\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 223us/step - loss: 0.3676 - accuracy: 0.8354\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 197us/step - loss: 0.3583 - accuracy: 0.8354\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 223us/step - loss: 0.3505 - accuracy: 0.8313\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 203us/step - loss: 0.3431 - accuracy: 0.8313\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 0s 204us/step - loss: 0.3416 - accuracy: 0.8560\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 210us/step - loss: 0.3321 - accuracy: 0.8601\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 321us/step - loss: 0.3272 - accuracy: 0.8642\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 228us/step - loss: 0.3232 - accuracy: 0.8642\n",
      "60/60 [==============================] - 0s 567us/step\n",
      "243/243 [==============================] - 0s 168us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 0s 763us/step - loss: 0.6327 - accuracy: 0.6461\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 156us/step - loss: 0.5838 - accuracy: 0.7531\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 145us/step - loss: 0.5421 - accuracy: 0.7737\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 164us/step - loss: 0.5078 - accuracy: 0.8025\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 141us/step - loss: 0.4805 - accuracy: 0.8025\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 160us/step - loss: 0.4587 - accuracy: 0.8066\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 164us/step - loss: 0.4388 - accuracy: 0.8189\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 156us/step - loss: 0.4243 - accuracy: 0.8148\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 159us/step - loss: 0.4117 - accuracy: 0.8230\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 161us/step - loss: 0.4002 - accuracy: 0.8272\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 137us/step - loss: 0.3930 - accuracy: 0.8272\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 242us/step - loss: 0.3841 - accuracy: 0.8395\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 284us/step - loss: 0.3791 - accuracy: 0.8354\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 266us/step - loss: 0.3727 - accuracy: 0.8395\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 196us/step - loss: 0.3685 - accuracy: 0.8395\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 177us/step - loss: 0.3645 - accuracy: 0.8395\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 263us/step - loss: 0.3605 - accuracy: 0.8436\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 264us/step - loss: 0.3570 - accuracy: 0.8436\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 259us/step - loss: 0.3518 - accuracy: 0.8477\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 255us/step - loss: 0.3503 - accuracy: 0.8436\n",
      "60/60 [==============================] - 0s 763us/step\n",
      "243/243 [==============================] - 0s 123us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 0s 488us/step - loss: 0.7003 - accuracy: 0.5372\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 85us/step - loss: 0.6443 - accuracy: 0.5909\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 96us/step - loss: 0.5984 - accuracy: 0.6446\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 96us/step - loss: 0.5626 - accuracy: 0.6942\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 92us/step - loss: 0.5316 - accuracy: 0.7397\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 99us/step - loss: 0.5019 - accuracy: 0.7562\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 97us/step - loss: 0.4785 - accuracy: 0.7645\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 174us/step - loss: 0.4577 - accuracy: 0.7769\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 162us/step - loss: 0.4392 - accuracy: 0.7769\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 163us/step - loss: 0.4238 - accuracy: 0.7851\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 163us/step - loss: 0.4109 - accuracy: 0.8017\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 118us/step - loss: 0.3992 - accuracy: 0.8140\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 115us/step - loss: 0.3891 - accuracy: 0.8099\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 104us/step - loss: 0.3798 - accuracy: 0.8223\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 112us/step - loss: 0.3709 - accuracy: 0.8347\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 125us/step - loss: 0.3636 - accuracy: 0.8264\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 151us/step - loss: 0.3558 - accuracy: 0.8347\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 141us/step - loss: 0.3492 - accuracy: 0.8430\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 166us/step - loss: 0.3435 - accuracy: 0.8430\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 178us/step - loss: 0.3385 - accuracy: 0.8471\n",
      "61/61 [==============================] - 0s 455us/step\n",
      "242/242 [==============================] - 0s 67us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 0s 619us/step - loss: 0.8218 - accuracy: 0.4545\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 145us/step - loss: 0.7474 - accuracy: 0.5207\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 194us/step - loss: 0.6919 - accuracy: 0.6116\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 163us/step - loss: 0.6511 - accuracy: 0.6777\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 119us/step - loss: 0.6144 - accuracy: 0.7107\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 131us/step - loss: 0.5830 - accuracy: 0.7479\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 166us/step - loss: 0.5555 - accuracy: 0.7521\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 159us/step - loss: 0.5298 - accuracy: 0.7851\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 179us/step - loss: 0.5065 - accuracy: 0.8140\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 164us/step - loss: 0.4868 - accuracy: 0.8140\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 128us/step - loss: 0.4664 - accuracy: 0.8140\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 128us/step - loss: 0.4503 - accuracy: 0.8223\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 130us/step - loss: 0.4346 - accuracy: 0.8306\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 160us/step - loss: 0.4205 - accuracy: 0.8388\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 153us/step - loss: 0.4085 - accuracy: 0.8430\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 152us/step - loss: 0.3989 - accuracy: 0.8430\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 171us/step - loss: 0.3901 - accuracy: 0.8388\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 147us/step - loss: 0.3819 - accuracy: 0.8430\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 100us/step - loss: 0.3737 - accuracy: 0.8430\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 152us/step - loss: 0.3666 - accuracy: 0.8430\n",
      "61/61 [==============================] - 0s 705us/step\n",
      "242/242 [==============================] - 0s 78us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 0s 520us/step - loss: 0.7514 - accuracy: 0.5455\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 124us/step - loss: 0.7021 - accuracy: 0.5950\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 166us/step - loss: 0.6620 - accuracy: 0.6281\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 169us/step - loss: 0.6276 - accuracy: 0.6694\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 154us/step - loss: 0.5984 - accuracy: 0.7025\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 122us/step - loss: 0.5754 - accuracy: 0.7397\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 152us/step - loss: 0.5537 - accuracy: 0.7603\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 162us/step - loss: 0.5327 - accuracy: 0.7727\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 176us/step - loss: 0.5084 - accuracy: 0.7810\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 177us/step - loss: 0.4896 - accuracy: 0.7810\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 145us/step - loss: 0.4715 - accuracy: 0.7934\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 109us/step - loss: 0.4554 - accuracy: 0.8017\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 127us/step - loss: 0.4412 - accuracy: 0.8182\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 125us/step - loss: 0.4287 - accuracy: 0.8182\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 114us/step - loss: 0.4173 - accuracy: 0.8264\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 120us/step - loss: 0.4072 - accuracy: 0.8430\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 124us/step - loss: 0.3980 - accuracy: 0.8471\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 118us/step - loss: 0.3891 - accuracy: 0.8430\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 120us/step - loss: 0.3813 - accuracy: 0.8471\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 138us/step - loss: 0.3760 - accuracy: 0.8512\n",
      "61/61 [==============================] - 0s 445us/step\n",
      "242/242 [==============================] - 0s 43us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 0s 471us/step - loss: 0.6516 - accuracy: 0.6749\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 90us/step - loss: 0.6178 - accuracy: 0.7078\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 104us/step - loss: 0.5915 - accuracy: 0.7202\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 110us/step - loss: 0.5677 - accuracy: 0.7531\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 101us/step - loss: 0.5448 - accuracy: 0.7778\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 115us/step - loss: 0.5243 - accuracy: 0.7860\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 124us/step - loss: 0.5044 - accuracy: 0.7778\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 123us/step - loss: 0.4873 - accuracy: 0.7984\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 124us/step - loss: 0.4705 - accuracy: 0.8066\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 109us/step - loss: 0.4559 - accuracy: 0.8148\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 112us/step - loss: 0.4436 - accuracy: 0.8272\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 106us/step - loss: 0.4332 - accuracy: 0.8272\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 92us/step - loss: 0.4227 - accuracy: 0.8272\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 121us/step - loss: 0.4123 - accuracy: 0.8313\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 107us/step - loss: 0.4031 - accuracy: 0.8354\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 114us/step - loss: 0.3965 - accuracy: 0.8395\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 118us/step - loss: 0.3890 - accuracy: 0.8395\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 134us/step - loss: 0.3822 - accuracy: 0.8395\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 145us/step - loss: 0.3764 - accuracy: 0.8313\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 168us/step - loss: 0.3697 - accuracy: 0.8313\n",
      "60/60 [==============================] - 0s 706us/step\n",
      "243/243 [==============================] - 0s 69us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 0s 671us/step - loss: 0.6960 - accuracy: 0.4568\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 143us/step - loss: 0.6594 - accuracy: 0.4691\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 155us/step - loss: 0.6344 - accuracy: 0.5597\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 135us/step - loss: 0.6098 - accuracy: 0.6543\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 127us/step - loss: 0.5889 - accuracy: 0.7325\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 106us/step - loss: 0.5698 - accuracy: 0.7572\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 116us/step - loss: 0.5510 - accuracy: 0.7695\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 128us/step - loss: 0.5336 - accuracy: 0.7778\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 130us/step - loss: 0.5144 - accuracy: 0.7819\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 249us/step - loss: 0.4972 - accuracy: 0.7860\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 119us/step - loss: 0.4824 - accuracy: 0.7819\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 160us/step - loss: 0.4693 - accuracy: 0.7860\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 107us/step - loss: 0.4563 - accuracy: 0.7942\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 116us/step - loss: 0.4430 - accuracy: 0.8025\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 130us/step - loss: 0.4322 - accuracy: 0.8066\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 132us/step - loss: 0.4227 - accuracy: 0.8107\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 135us/step - loss: 0.4141 - accuracy: 0.8189\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 135us/step - loss: 0.4066 - accuracy: 0.8230\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 128us/step - loss: 0.3998 - accuracy: 0.8230\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 126us/step - loss: 0.3947 - accuracy: 0.8313\n",
      "60/60 [==============================] - 0s 505us/step\n",
      "243/243 [==============================] - 0s 42us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 0s 453us/step - loss: 0.7808 - accuracy: 0.5372\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 29us/step - loss: 0.7503 - accuracy: 0.5496\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 29us/step - loss: 0.7223 - accuracy: 0.5744\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 45us/step - loss: 0.6946 - accuracy: 0.5992\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 41us/step - loss: 0.6710 - accuracy: 0.6281\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.6497 - accuracy: 0.6570\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.6305 - accuracy: 0.6694\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.6114 - accuracy: 0.6942\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 51us/step - loss: 0.5952 - accuracy: 0.7107\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.5792 - accuracy: 0.7190\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.5651 - accuracy: 0.7066\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 39us/step - loss: 0.5508 - accuracy: 0.7149\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 51us/step - loss: 0.5374 - accuracy: 0.7231\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.5243 - accuracy: 0.7355\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 49us/step - loss: 0.5114 - accuracy: 0.7603\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.5002 - accuracy: 0.7686\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 40us/step - loss: 0.4880 - accuracy: 0.7727\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.4767 - accuracy: 0.7810\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4661 - accuracy: 0.7851\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 51us/step - loss: 0.4556 - accuracy: 0.7893\n",
      "61/61 [==============================] - 0s 513us/step\n",
      "242/242 [==============================] - 0s 26us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.8073 - accuracy: 0.4463\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 37us/step - loss: 0.7853 - accuracy: 0.4463\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.7671 - accuracy: 0.4463\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 41us/step - loss: 0.7482 - accuracy: 0.4463\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 40us/step - loss: 0.7330 - accuracy: 0.4504\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 28us/step - loss: 0.7182 - accuracy: 0.4669\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 43us/step - loss: 0.7051 - accuracy: 0.4669\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 40us/step - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 48us/step - loss: 0.6812 - accuracy: 0.5207\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 41us/step - loss: 0.6705 - accuracy: 0.5289\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.6588 - accuracy: 0.5372\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 51us/step - loss: 0.6488 - accuracy: 0.5661\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 42us/step - loss: 0.6374 - accuracy: 0.5826\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 48us/step - loss: 0.6248 - accuracy: 0.6074\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.6139 - accuracy: 0.6198\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 49us/step - loss: 0.6017 - accuracy: 0.6364\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.5899 - accuracy: 0.6570\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.5782 - accuracy: 0.6818\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.5667 - accuracy: 0.7066\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.5544 - accuracy: 0.7231\n",
      "61/61 [==============================] - 0s 373us/step\n",
      "242/242 [==============================] - 0s 15us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 0s 470us/step - loss: 0.8248 - accuracy: 0.4421\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 44us/step - loss: 0.7942 - accuracy: 0.4421\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 36us/step - loss: 0.7692 - accuracy: 0.4504\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 37us/step - loss: 0.7438 - accuracy: 0.4628\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 45us/step - loss: 0.7246 - accuracy: 0.4793\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 41us/step - loss: 0.7077 - accuracy: 0.5083\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 42us/step - loss: 0.6908 - accuracy: 0.5496\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 42us/step - loss: 0.6768 - accuracy: 0.5826\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.6643 - accuracy: 0.6198\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 48us/step - loss: 0.6518 - accuracy: 0.6281\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 47us/step - loss: 0.6408 - accuracy: 0.6446\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 48us/step - loss: 0.6297 - accuracy: 0.6612\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 42us/step - loss: 0.6192 - accuracy: 0.6901\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 45us/step - loss: 0.6093 - accuracy: 0.7025\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 48us/step - loss: 0.5995 - accuracy: 0.7355\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 46us/step - loss: 0.5892 - accuracy: 0.7645\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 41us/step - loss: 0.5796 - accuracy: 0.7769\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 46us/step - loss: 0.5698 - accuracy: 0.7851\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.5603 - accuracy: 0.7893\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 51us/step - loss: 0.5505 - accuracy: 0.7893\n",
      "61/61 [==============================] - 0s 493us/step\n",
      "242/242 [==============================] - 0s 18us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 0s 487us/step - loss: 0.6931 - accuracy: 0.5267\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 40us/step - loss: 0.6711 - accuracy: 0.5473\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 34us/step - loss: 0.6519 - accuracy: 0.6008\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 37us/step - loss: 0.6342 - accuracy: 0.6296\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 40us/step - loss: 0.6184 - accuracy: 0.6708\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 45us/step - loss: 0.6028 - accuracy: 0.7325\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 45us/step - loss: 0.5884 - accuracy: 0.7490\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 44us/step - loss: 0.5752 - accuracy: 0.7654\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 42us/step - loss: 0.5622 - accuracy: 0.7860\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 46us/step - loss: 0.5490 - accuracy: 0.8066\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.5367 - accuracy: 0.8025\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 42us/step - loss: 0.5255 - accuracy: 0.8025\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 38us/step - loss: 0.5132 - accuracy: 0.8025\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 45us/step - loss: 0.5018 - accuracy: 0.8025\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.4908 - accuracy: 0.8107\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 52us/step - loss: 0.4807 - accuracy: 0.8272\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 47us/step - loss: 0.4707 - accuracy: 0.8313\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 39us/step - loss: 0.4611 - accuracy: 0.8395\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.4513 - accuracy: 0.8395\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 56us/step - loss: 0.4430 - accuracy: 0.8436\n",
      "60/60 [==============================] - 0s 327us/step\n",
      "243/243 [==============================] - 0s 14us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 0s 446us/step - loss: 0.7058 - accuracy: 0.5185\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 24us/step - loss: 0.6941 - accuracy: 0.5267\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 39us/step - loss: 0.6828 - accuracy: 0.5679\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.6724 - accuracy: 0.5761\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 67us/step - loss: 0.6620 - accuracy: 0.6049\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.6513 - accuracy: 0.6379\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 56us/step - loss: 0.6421 - accuracy: 0.6626\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 69us/step - loss: 0.6315 - accuracy: 0.6749\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 72us/step - loss: 0.6219 - accuracy: 0.7037\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 55us/step - loss: 0.6108 - accuracy: 0.7243\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.6010 - accuracy: 0.7202\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 69us/step - loss: 0.5902 - accuracy: 0.7366\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.5795 - accuracy: 0.7490\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 79us/step - loss: 0.5679 - accuracy: 0.7613\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 70us/step - loss: 0.5574 - accuracy: 0.7778\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.5450 - accuracy: 0.7819\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 67us/step - loss: 0.5337 - accuracy: 0.7819\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.5229 - accuracy: 0.7901\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.5120 - accuracy: 0.8025\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.5023 - accuracy: 0.7984\n",
      "60/60 [==============================] - 0s 464us/step\n",
      "243/243 [==============================] - 0s 18us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 0s 488us/step - loss: 0.8150 - accuracy: 0.3471\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 24us/step - loss: 0.8007 - accuracy: 0.3471\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 29us/step - loss: 0.7871 - accuracy: 0.3554\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 36us/step - loss: 0.7743 - accuracy: 0.3719\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 26us/step - loss: 0.7616 - accuracy: 0.3884\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 41us/step - loss: 0.7493 - accuracy: 0.3967\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 36us/step - loss: 0.7371 - accuracy: 0.4215\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 34us/step - loss: 0.7251 - accuracy: 0.4587\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 43us/step - loss: 0.7132 - accuracy: 0.4876\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 49us/step - loss: 0.7017 - accuracy: 0.5248\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 40us/step - loss: 0.6905 - accuracy: 0.5413\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 36us/step - loss: 0.6797 - accuracy: 0.5579\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.6695 - accuracy: 0.5868\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 47us/step - loss: 0.6590 - accuracy: 0.6322\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 41us/step - loss: 0.6491 - accuracy: 0.6529\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.6393 - accuracy: 0.6694\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 42us/step - loss: 0.6294 - accuracy: 0.6860\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 40us/step - loss: 0.6197 - accuracy: 0.7066\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 42us/step - loss: 0.6097 - accuracy: 0.7066\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 36us/step - loss: 0.5999 - accuracy: 0.7149\n",
      "61/61 [==============================] - 0s 370us/step\n",
      "242/242 [==============================] - 0s 15us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 0s 435us/step - loss: 0.6090 - accuracy: 0.7107\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 20us/step - loss: 0.5976 - accuracy: 0.7231\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 37us/step - loss: 0.5873 - accuracy: 0.7397\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 21us/step - loss: 0.5772 - accuracy: 0.7521\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 32us/step - loss: 0.5674 - accuracy: 0.7562\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 22us/step - loss: 0.5583 - accuracy: 0.7521\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 36us/step - loss: 0.5489 - accuracy: 0.7727\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 27us/step - loss: 0.5403 - accuracy: 0.7769\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 37us/step - loss: 0.5321 - accuracy: 0.7727\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 44us/step - loss: 0.5237 - accuracy: 0.7769\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 46us/step - loss: 0.5158 - accuracy: 0.7769\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 31us/step - loss: 0.5079 - accuracy: 0.7810\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 36us/step - loss: 0.5005 - accuracy: 0.7893\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 47us/step - loss: 0.4934 - accuracy: 0.7934\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 44us/step - loss: 0.4865 - accuracy: 0.7934\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 33us/step - loss: 0.4800 - accuracy: 0.7975\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.4729 - accuracy: 0.8058\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.4668 - accuracy: 0.8099\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 48us/step - loss: 0.4611 - accuracy: 0.8140\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 41us/step - loss: 0.4550 - accuracy: 0.8140\n",
      "61/61 [==============================] - 0s 434us/step\n",
      "242/242 [==============================] - 0s 11us/step\n",
      "Epoch 1/20\n",
      "242/242 [==============================] - 0s 471us/step - loss: 0.7627 - accuracy: 0.4793\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 21us/step - loss: 0.7464 - accuracy: 0.4959\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 25us/step - loss: 0.7320 - accuracy: 0.5000\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 25us/step - loss: 0.7179 - accuracy: 0.5124\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 28us/step - loss: 0.7041 - accuracy: 0.5248\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 28us/step - loss: 0.6912 - accuracy: 0.5248\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 27us/step - loss: 0.6789 - accuracy: 0.5413\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 43us/step - loss: 0.6660 - accuracy: 0.5702\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 31us/step - loss: 0.6540 - accuracy: 0.5950\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 35us/step - loss: 0.6428 - accuracy: 0.6240\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.6317 - accuracy: 0.6364\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.6205 - accuracy: 0.6488\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 27us/step - loss: 0.6098 - accuracy: 0.6570\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 39us/step - loss: 0.6004 - accuracy: 0.6736\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 37us/step - loss: 0.5902 - accuracy: 0.6860\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 30us/step - loss: 0.5814 - accuracy: 0.6983\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 47us/step - loss: 0.5728 - accuracy: 0.7066\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 43us/step - loss: 0.5640 - accuracy: 0.7231\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 38us/step - loss: 0.5559 - accuracy: 0.7273\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 29us/step - loss: 0.5484 - accuracy: 0.7355\n",
      "61/61 [==============================] - 0s 326us/step\n",
      "242/242 [==============================] - 0s 9us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 0s 432us/step - loss: 0.7990 - accuracy: 0.4362\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 18us/step - loss: 0.7780 - accuracy: 0.4444\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 26us/step - loss: 0.7581 - accuracy: 0.4691\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 31us/step - loss: 0.7382 - accuracy: 0.5267\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 33us/step - loss: 0.7202 - accuracy: 0.5432\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 34us/step - loss: 0.7031 - accuracy: 0.5597\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 24us/step - loss: 0.6855 - accuracy: 0.5802\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 48us/step - loss: 0.6703 - accuracy: 0.6008\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 29us/step - loss: 0.6539 - accuracy: 0.6091\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 30us/step - loss: 0.6399 - accuracy: 0.6420\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 41us/step - loss: 0.6246 - accuracy: 0.6626\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 32us/step - loss: 0.6119 - accuracy: 0.6667\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 31us/step - loss: 0.5985 - accuracy: 0.6872\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 25us/step - loss: 0.5865 - accuracy: 0.7119\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 23us/step - loss: 0.5735 - accuracy: 0.7284\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 23us/step - loss: 0.5620 - accuracy: 0.7366\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 28us/step - loss: 0.5504 - accuracy: 0.7490\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 23us/step - loss: 0.5400 - accuracy: 0.7490\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 38us/step - loss: 0.5296 - accuracy: 0.7613\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 0s 46us/step - loss: 0.5200 - accuracy: 0.7654\n",
      "60/60 [==============================] - 0s 379us/step\n",
      "243/243 [==============================] - 0s 17us/step\n",
      "Epoch 1/20\n",
      "243/243 [==============================] - 0s 447us/step - loss: 0.9046 - accuracy: 0.3210\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 0s 20us/step - loss: 0.8851 - accuracy: 0.3251\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 0s 25us/step - loss: 0.8673 - accuracy: 0.3292\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 0s 34us/step - loss: 0.8506 - accuracy: 0.3539\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 0s 24us/step - loss: 0.8343 - accuracy: 0.3704\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 0s 30us/step - loss: 0.8193 - accuracy: 0.3827\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 0s 32us/step - loss: 0.8049 - accuracy: 0.3868\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 0s 33us/step - loss: 0.7908 - accuracy: 0.3909\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 0s 24us/step - loss: 0.7779 - accuracy: 0.4280\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 0s 25us/step - loss: 0.7653 - accuracy: 0.4239\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 0s 49us/step - loss: 0.7537 - accuracy: 0.4280\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 0s 31us/step - loss: 0.7425 - accuracy: 0.4362\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 0s 43us/step - loss: 0.7315 - accuracy: 0.4527\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 0s 34us/step - loss: 0.7207 - accuracy: 0.4733\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 0s 24us/step - loss: 0.7110 - accuracy: 0.4938\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 0s 36us/step - loss: 0.7015 - accuracy: 0.4979\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 0s 30us/step - loss: 0.6923 - accuracy: 0.5144\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 0s 39us/step - loss: 0.6832 - accuracy: 0.5309\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 0s 43us/step - loss: 0.6745 - accuracy: 0.5391\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 0s 31us/step - loss: 0.6664 - accuracy: 0.5597\n",
      "60/60 [==============================] - 0s 559us/step\n",
      "243/243 [==============================] - 0s 16us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "303/303 [==============================] - 0s 407us/step - loss: 0.6196 - accuracy: 0.6601\n",
      "Epoch 2/20\n",
      "303/303 [==============================] - 0s 59us/step - loss: 0.5795 - accuracy: 0.7525\n",
      "Epoch 3/20\n",
      "303/303 [==============================] - 0s 77us/step - loss: 0.5455 - accuracy: 0.7624\n",
      "Epoch 4/20\n",
      "303/303 [==============================] - 0s 84us/step - loss: 0.5168 - accuracy: 0.7855\n",
      "Epoch 5/20\n",
      "303/303 [==============================] - 0s 91us/step - loss: 0.4909 - accuracy: 0.8020\n",
      "Epoch 6/20\n",
      "303/303 [==============================] - 0s 84us/step - loss: 0.4678 - accuracy: 0.8053\n",
      "Epoch 7/20\n",
      "303/303 [==============================] - 0s 89us/step - loss: 0.4479 - accuracy: 0.8218\n",
      "Epoch 8/20\n",
      "303/303 [==============================] - 0s 87us/step - loss: 0.4286 - accuracy: 0.8284\n",
      "Epoch 9/20\n",
      "303/303 [==============================] - 0s 82us/step - loss: 0.4142 - accuracy: 0.8284\n",
      "Epoch 10/20\n",
      "303/303 [==============================] - 0s 89us/step - loss: 0.4024 - accuracy: 0.8284\n",
      "Epoch 11/20\n",
      "303/303 [==============================] - 0s 93us/step - loss: 0.3921 - accuracy: 0.8317\n",
      "Epoch 12/20\n",
      "303/303 [==============================] - 0s 82us/step - loss: 0.3830 - accuracy: 0.8284\n",
      "Epoch 13/20\n",
      "303/303 [==============================] - 0s 90us/step - loss: 0.3756 - accuracy: 0.8284\n",
      "Epoch 14/20\n",
      "303/303 [==============================] - 0s 90us/step - loss: 0.3700 - accuracy: 0.8251\n",
      "Epoch 15/20\n",
      "303/303 [==============================] - 0s 95us/step - loss: 0.3630 - accuracy: 0.8317\n",
      "Epoch 16/20\n",
      "303/303 [==============================] - 0s 86us/step - loss: 0.3584 - accuracy: 0.8416\n",
      "Epoch 17/20\n",
      "303/303 [==============================] - 0s 100us/step - loss: 0.3532 - accuracy: 0.8449\n",
      "Epoch 18/20\n",
      "303/303 [==============================] - 0s 100us/step - loss: 0.3494 - accuracy: 0.8548\n",
      "Epoch 19/20\n",
      "303/303 [==============================] - 0s 81us/step - loss: 0.3449 - accuracy: 0.8614\n",
      "Epoch 20/20\n",
      "303/303 [==============================] - 0s 107us/step - loss: 0.3411 - accuracy: 0.8581\n"
     ]
    }
   ],
   "source": [
    "batch_size = [10, 20, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                    n_jobs=1, cv=kfold)\n",
    "\n",
    "grid_result2 = grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.19%  Params: {'batch_size': 10}\n",
      "Accuracy: 83.83%  Params: {'batch_size': 20}\n",
      "Accuracy: 78.88%  Params: {'batch_size': 50}\n",
      "Accuracy: 76.24%  Params: {'batch_size': 100}\n",
      "\n",
      "Best: 83.83% using {'batch_size': 20}\n"
     ]
    }
   ],
   "source": [
    "grid_result = grid_result2\n",
    "\n",
    "means = [x*100 for x in grid_result.cv_results_['mean_test_score']]\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, param in zip(means,params):\n",
    "    print(f'Accuracy: {mean:.2f}%  Params: {param}')\n",
    "print()\n",
    "print(f\"Best: {grid_result.best_score_*100:.2f}% using {grid_result.best_params_}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
