{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS_Unit_4_Sprint_3_Neural_Nets_Spring_Challenge.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y6SKlgYrpcym"
      },
      "source": [
        "# Neural Networks Sprint Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BrEbRrjVphPM"
      },
      "source": [
        "## 1) Define the following terms:\n",
        "\n",
        "- Neuron\n",
        "- Input Layer\n",
        "- Hidden Layer\n",
        "- Output Layer\n",
        "- Activation\n",
        "- Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9C2DjVEzU2I",
        "colab_type": "text"
      },
      "source": [
        "Neuron = An individual node in a neural output.\n",
        "\n",
        "Input Layer = This is the layer that takes in the columns of data used to make the predictions. Each column is a node in the input layer\n",
        "\n",
        "Hidden Layer = Connects the input layer to the output layer, there are a variable amount of hidden layers in each neural network, while there is only one input and output layer.\n",
        "\n",
        "Output Layer =  The output layer receives information from the hidden layer(s) to create an output\n",
        "\n",
        "Activation = Activation controls how much signal gets passed from one layer of a neural network to another layer, the activation I will use is sigmoid.\n",
        "\n",
        "Backpropagation = This is an algorithm used to update parameters of neural networks. The most common form of backpropagation is gradient descent and it is also what I use in section 3. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ri_gRA2Jp728"
      },
      "source": [
        "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
        "\n",
        "| x1 | x2 | x3 | y |\n",
        "|----|----|----|---|\n",
        "| 1  | 1  | 1  | 1 |\n",
        "| 1  | 0  | 1  | 0 |\n",
        "| 0  | 1  | 1  | 0 |\n",
        "| 0  | 0  | 1  | 0 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep3gq267x0P-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPa63whRx0QA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "063684c4-2858-48c0-c748-3afdbf1d60a8"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ig6ZTH8tpQ19",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APbIahWax0QD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Perceptron(object):\n",
        "    def __init__(self, rate = 0.01, niter = 10):\n",
        "        self.rate = rate\n",
        "        self.niter = niter\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        self.weight = np.random.uniform(-1, 1, X.shape[1] + 1)\n",
        "        \n",
        "        self.errors = []\n",
        "        \n",
        "        for i in range(self.niter):\n",
        "            err = 0\n",
        "            for xi, target in zip(X, y):\n",
        "                delta_w = self.rate * (target - self.predict(xi))\n",
        "                self.weight[1:] += delta_w * xi\n",
        "                self.weight[0] += delta_w\n",
        "                err += int(delta_w != 0.0)\n",
        "            self.errors.append(err)\n",
        "        return self\n",
        "    \n",
        "    def net_input(self, X):\n",
        "        return np.dot(X, self.weight[1:]) + self.weight[0]\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return np.where(self.net_input(X) >= 0.5, 1, 0)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk5kR2p1x0QE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.array([[1,1,1],\n",
        "             [1,0,1],\n",
        "             [0,1,1],\n",
        "             [0,0,1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAXCe8nSx0QG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = np.array([1, 0, 0, 0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg0B2sJtx0QI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Perceptron(rate = 1, niter=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk8KNLjBx0QM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "be4685aa-0f6a-4e2a-a776-00a4d83f61a8"
      },
      "source": [
        "model.fit(X, y)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Perceptron at 0x7f679f8e7c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yugAthIx0QO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fd43e43-393d-4bca-b91a-476681c2f5a6"
      },
      "source": [
        "model.predict(X)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "86HyRi8Osr3U"
      },
      "source": [
        "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
        "- Your network must have one hidden layer. \n",
        "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
        "- Train your model on the Heart Disease dataset from UCI:\n",
        "\n",
        "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
        "\n",
        "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7QkSK5Ix0QR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CNfiajv3v4Ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "1ea4c512-fd8e-428c-d2a9-ef87418d1cde"
      },
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
        "data.head(20)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>140</td>\n",
              "      <td>192</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>148</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>140</td>\n",
              "      <td>294</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>153</td>\n",
              "      <td>0</td>\n",
              "      <td>1.3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>44</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>263</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>173</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>172</td>\n",
              "      <td>199</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>162</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>150</td>\n",
              "      <td>168</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>174</td>\n",
              "      <td>0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>140</td>\n",
              "      <td>239</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>160</td>\n",
              "      <td>0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>275</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>139</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>49</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>266</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>171</td>\n",
              "      <td>0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>110</td>\n",
              "      <td>211</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>144</td>\n",
              "      <td>1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>150</td>\n",
              "      <td>283</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>162</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>120</td>\n",
              "      <td>219</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>158</td>\n",
              "      <td>0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>120</td>\n",
              "      <td>340</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>66</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>150</td>\n",
              "      <td>226</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>114</td>\n",
              "      <td>0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>247</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>171</td>\n",
              "      <td>0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>140</td>\n",
              "      <td>239</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>151</td>\n",
              "      <td>0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
              "0    63    1   3       145   233    1        0      150      0      2.3   \n",
              "1    37    1   2       130   250    0        1      187      0      3.5   \n",
              "2    41    0   1       130   204    0        0      172      0      1.4   \n",
              "3    56    1   1       120   236    0        1      178      0      0.8   \n",
              "4    57    0   0       120   354    0        1      163      1      0.6   \n",
              "5    57    1   0       140   192    0        1      148      0      0.4   \n",
              "6    56    0   1       140   294    0        0      153      0      1.3   \n",
              "7    44    1   1       120   263    0        1      173      0      0.0   \n",
              "8    52    1   2       172   199    1        1      162      0      0.5   \n",
              "9    57    1   2       150   168    0        1      174      0      1.6   \n",
              "10   54    1   0       140   239    0        1      160      0      1.2   \n",
              "11   48    0   2       130   275    0        1      139      0      0.2   \n",
              "12   49    1   1       130   266    0        1      171      0      0.6   \n",
              "13   64    1   3       110   211    0        0      144      1      1.8   \n",
              "14   58    0   3       150   283    1        0      162      0      1.0   \n",
              "15   50    0   2       120   219    0        1      158      0      1.6   \n",
              "16   58    0   2       120   340    0        1      172      0      0.0   \n",
              "17   66    0   3       150   226    0        1      114      0      2.6   \n",
              "18   43    1   0       150   247    0        1      171      0      1.5   \n",
              "19   69    0   3       140   239    0        1      151      0      1.8   \n",
              "\n",
              "    slope  ca  thal  target  \n",
              "0       0   0     1       1  \n",
              "1       0   0     2       1  \n",
              "2       2   0     2       1  \n",
              "3       2   0     2       1  \n",
              "4       2   0     2       1  \n",
              "5       1   0     1       1  \n",
              "6       1   0     2       1  \n",
              "7       2   0     3       1  \n",
              "8       2   0     3       1  \n",
              "9       2   0     2       1  \n",
              "10      2   0     2       1  \n",
              "11      2   0     2       1  \n",
              "12      2   0     2       1  \n",
              "13      1   0     2       1  \n",
              "14      2   0     2       1  \n",
              "15      1   0     2       1  \n",
              "16      2   0     2       1  \n",
              "17      0   0     2       1  \n",
              "18      2   0     2       1  \n",
              "19      2   2     2       1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHTpzMrfx0QV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data2 = data.drop('target', axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4P2odjyx0QY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlyEJ-t4x0Qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = StandardScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_0T8Jojx0Qe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "4a6fc693-5a77-4d9a-acd8-c052499c395e"
      },
      "source": [
        "scaler.fit(data2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StandardScaler(copy=True, with_mean=True, with_std=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if9lxTOux0Qg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "bedbaaa0-5629-4bc5-d4d6-4e29ce129bbb"
      },
      "source": [
        "scaled_df = scaler.fit_transform(data.drop('target', axis=1))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
            "  return self.fit(X, **fit_params).transform(X)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "553Xyonkx0Qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaled_df = pd.DataFrame(scaled_df, columns=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
        "       'exang', 'oldpeak', 'slope', 'ca', 'thal'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY9jKtzzx0Ql",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "outputId": "19dc3927-b66d-4877-82b7-e4ffacecc52b"
      },
      "source": [
        "scaled_df.head(15)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.952197</td>\n",
              "      <td>0.681005</td>\n",
              "      <td>1.973123</td>\n",
              "      <td>0.763956</td>\n",
              "      <td>-0.256334</td>\n",
              "      <td>2.394438</td>\n",
              "      <td>-1.005832</td>\n",
              "      <td>0.015443</td>\n",
              "      <td>-0.696631</td>\n",
              "      <td>1.087338</td>\n",
              "      <td>-2.274579</td>\n",
              "      <td>-0.714429</td>\n",
              "      <td>-2.148873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.915313</td>\n",
              "      <td>0.681005</td>\n",
              "      <td>1.002577</td>\n",
              "      <td>-0.092738</td>\n",
              "      <td>0.072199</td>\n",
              "      <td>-0.417635</td>\n",
              "      <td>0.898962</td>\n",
              "      <td>1.633471</td>\n",
              "      <td>-0.696631</td>\n",
              "      <td>2.122573</td>\n",
              "      <td>-2.274579</td>\n",
              "      <td>-0.714429</td>\n",
              "      <td>-0.512922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.474158</td>\n",
              "      <td>-1.468418</td>\n",
              "      <td>0.032031</td>\n",
              "      <td>-0.092738</td>\n",
              "      <td>-0.816773</td>\n",
              "      <td>-0.417635</td>\n",
              "      <td>-1.005832</td>\n",
              "      <td>0.977514</td>\n",
              "      <td>-0.696631</td>\n",
              "      <td>0.310912</td>\n",
              "      <td>0.976352</td>\n",
              "      <td>-0.714429</td>\n",
              "      <td>-0.512922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.180175</td>\n",
              "      <td>0.681005</td>\n",
              "      <td>0.032031</td>\n",
              "      <td>-0.663867</td>\n",
              "      <td>-0.198357</td>\n",
              "      <td>-0.417635</td>\n",
              "      <td>0.898962</td>\n",
              "      <td>1.239897</td>\n",
              "      <td>-0.696631</td>\n",
              "      <td>-0.206705</td>\n",
              "      <td>0.976352</td>\n",
              "      <td>-0.714429</td>\n",
              "      <td>-0.512922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.290464</td>\n",
              "      <td>-1.468418</td>\n",
              "      <td>-0.938515</td>\n",
              "      <td>-0.663867</td>\n",
              "      <td>2.082050</td>\n",
              "      <td>-0.417635</td>\n",
              "      <td>0.898962</td>\n",
              "      <td>0.583939</td>\n",
              "      <td>1.435481</td>\n",
              "      <td>-0.379244</td>\n",
              "      <td>0.976352</td>\n",
              "      <td>-0.714429</td>\n",
              "      <td>-0.512922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.290464</td>\n",
              "      <td>0.681005</td>\n",
              "      <td>-0.938515</td>\n",
              "      <td>0.478391</td>\n",
              "      <td>-1.048678</td>\n",
              "      <td>-0.417635</td>\n",
              "      <td>0.898962</td>\n",
              "      <td>-0.072018</td>\n",
              "      <td>-0.696631</td>\n",
              "      <td>-0.551783</td>\n",
              "      <td>-0.649113</td>\n",
              "      <td>-0.714429</td>\n",
              "      <td>-2.148873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.180175</td>\n",
              "      <td>-1.468418</td>\n",
              "      <td>0.032031</td>\n",
              "      <td>0.478391</td>\n",
              "      <td>0.922521</td>\n",
              "      <td>-0.417635</td>\n",
              "      <td>-1.005832</td>\n",
              "      <td>0.146634</td>\n",
              "      <td>-0.696631</td>\n",
              "      <td>0.224643</td>\n",
              "      <td>-0.649113</td>\n",
              "      <td>-0.714429</td>\n",
              "      <td>-0.512922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-1.143291</td>\n",
              "      <td>0.681005</td>\n",
              "      <td>0.032031</td>\n",
              "      <td>-0.663867</td>\n",
              "      <td>0.323431</td>\n",
              "      <td>-0.417635</td>\n",
              "      <td>0.898962</td>\n",
              "      <td>1.021244</td>\n",
              "      <td>-0.696631</td>\n",
              "      <td>-0.896862</td>\n",
              "      <td>0.976352</td>\n",
              "      <td>-0.714429</td>\n",
              "      <td>1.123029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.260980</td>\n",
              "      <td>0.681005</td>\n",
              "      <td>1.002577</td>\n",
              "      <td>2.306004</td>\n",
              "      <td>-0.913400</td>\n",
              "      <td>2.394438</td>\n",
              "      <td>0.898962</td>\n",
              "      <td>0.540209</td>\n",
              "      <td>-0.696631</td>\n",
              "      <td>-0.465514</td>\n",
              "      <td>0.976352</td>\n",
              "      <td>-0.714429</td>\n",
              "      <td>1.123029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.290464</td>\n",
              "      <td>0.681005</td>\n",
              "      <td>1.002577</td>\n",
              "      <td>1.049520</td>\n",
              "      <td>-1.512490</td>\n",
              "      <td>-0.417635</td>\n",
              "      <td>0.898962</td>\n",
              "      <td>1.064975</td>\n",
              "      <td>-0.696631</td>\n",
              "      <td>0.483451</td>\n",
              "      <td>0.976352</td>\n",
              "      <td>-0.714429</td>\n",
              "      <td>-0.512922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-0.040403</td>\n",
              "      <td>0.681005</td>\n",
              "      <td>-0.938515</td>\n",
              "      <td>0.478391</td>\n",
              "      <td>-0.140381</td>\n",
              "      <td>-0.417635</td>\n",
              "      <td>0.898962</td>\n",
              "      <td>0.452748</td>\n",
              "      <td>-0.696631</td>\n",
              "      <td>0.138373</td>\n",
              "      <td>0.976352</td>\n",
              "      <td>-0.714429</td>\n",
              "      <td>-0.512922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>-0.702136</td>\n",
              "      <td>-1.468418</td>\n",
              "      <td>1.002577</td>\n",
              "      <td>-0.092738</td>\n",
              "      <td>0.555337</td>\n",
              "      <td>-0.417635</td>\n",
              "      <td>0.898962</td>\n",
              "      <td>-0.465593</td>\n",
              "      <td>-0.696631</td>\n",
              "      <td>-0.724323</td>\n",
              "      <td>0.976352</td>\n",
              "      <td>-0.714429</td>\n",
              "      <td>-0.512922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>-0.591847</td>\n",
              "      <td>0.681005</td>\n",
              "      <td>0.032031</td>\n",
              "      <td>-0.092738</td>\n",
              "      <td>0.381407</td>\n",
              "      <td>-0.417635</td>\n",
              "      <td>0.898962</td>\n",
              "      <td>0.933783</td>\n",
              "      <td>-0.696631</td>\n",
              "      <td>-0.379244</td>\n",
              "      <td>0.976352</td>\n",
              "      <td>-0.714429</td>\n",
              "      <td>-0.512922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1.062485</td>\n",
              "      <td>0.681005</td>\n",
              "      <td>1.973123</td>\n",
              "      <td>-1.234996</td>\n",
              "      <td>-0.681494</td>\n",
              "      <td>-0.417635</td>\n",
              "      <td>-1.005832</td>\n",
              "      <td>-0.246940</td>\n",
              "      <td>1.435481</td>\n",
              "      <td>0.655990</td>\n",
              "      <td>-0.649113</td>\n",
              "      <td>-0.714429</td>\n",
              "      <td>-0.512922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.400752</td>\n",
              "      <td>-1.468418</td>\n",
              "      <td>1.973123</td>\n",
              "      <td>1.049520</td>\n",
              "      <td>0.709940</td>\n",
              "      <td>2.394438</td>\n",
              "      <td>-1.005832</td>\n",
              "      <td>0.540209</td>\n",
              "      <td>-0.696631</td>\n",
              "      <td>-0.034166</td>\n",
              "      <td>0.976352</td>\n",
              "      <td>-0.714429</td>\n",
              "      <td>-0.512922</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         age       sex        cp  trestbps      chol       fbs   restecg  \\\n",
              "0   0.952197  0.681005  1.973123  0.763956 -0.256334  2.394438 -1.005832   \n",
              "1  -1.915313  0.681005  1.002577 -0.092738  0.072199 -0.417635  0.898962   \n",
              "2  -1.474158 -1.468418  0.032031 -0.092738 -0.816773 -0.417635 -1.005832   \n",
              "3   0.180175  0.681005  0.032031 -0.663867 -0.198357 -0.417635  0.898962   \n",
              "4   0.290464 -1.468418 -0.938515 -0.663867  2.082050 -0.417635  0.898962   \n",
              "5   0.290464  0.681005 -0.938515  0.478391 -1.048678 -0.417635  0.898962   \n",
              "6   0.180175 -1.468418  0.032031  0.478391  0.922521 -0.417635 -1.005832   \n",
              "7  -1.143291  0.681005  0.032031 -0.663867  0.323431 -0.417635  0.898962   \n",
              "8  -0.260980  0.681005  1.002577  2.306004 -0.913400  2.394438  0.898962   \n",
              "9   0.290464  0.681005  1.002577  1.049520 -1.512490 -0.417635  0.898962   \n",
              "10 -0.040403  0.681005 -0.938515  0.478391 -0.140381 -0.417635  0.898962   \n",
              "11 -0.702136 -1.468418  1.002577 -0.092738  0.555337 -0.417635  0.898962   \n",
              "12 -0.591847  0.681005  0.032031 -0.092738  0.381407 -0.417635  0.898962   \n",
              "13  1.062485  0.681005  1.973123 -1.234996 -0.681494 -0.417635 -1.005832   \n",
              "14  0.400752 -1.468418  1.973123  1.049520  0.709940  2.394438 -1.005832   \n",
              "\n",
              "     thalach     exang   oldpeak     slope        ca      thal  \n",
              "0   0.015443 -0.696631  1.087338 -2.274579 -0.714429 -2.148873  \n",
              "1   1.633471 -0.696631  2.122573 -2.274579 -0.714429 -0.512922  \n",
              "2   0.977514 -0.696631  0.310912  0.976352 -0.714429 -0.512922  \n",
              "3   1.239897 -0.696631 -0.206705  0.976352 -0.714429 -0.512922  \n",
              "4   0.583939  1.435481 -0.379244  0.976352 -0.714429 -0.512922  \n",
              "5  -0.072018 -0.696631 -0.551783 -0.649113 -0.714429 -2.148873  \n",
              "6   0.146634 -0.696631  0.224643 -0.649113 -0.714429 -0.512922  \n",
              "7   1.021244 -0.696631 -0.896862  0.976352 -0.714429  1.123029  \n",
              "8   0.540209 -0.696631 -0.465514  0.976352 -0.714429  1.123029  \n",
              "9   1.064975 -0.696631  0.483451  0.976352 -0.714429 -0.512922  \n",
              "10  0.452748 -0.696631  0.138373  0.976352 -0.714429 -0.512922  \n",
              "11 -0.465593 -0.696631 -0.724323  0.976352 -0.714429 -0.512922  \n",
              "12  0.933783 -0.696631 -0.379244  0.976352 -0.714429 -0.512922  \n",
              "13 -0.246940  1.435481  0.655990 -0.649113 -0.714429 -0.512922  \n",
              "14  0.540209 -0.696631 -0.034166  0.976352 -0.714429 -0.512922  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ5NFfiTx0Qn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.array(scaled_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9Ypmhnux0Qo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "aeaeb7ae-f4d0-4580-a081-69cc61868b64"
      },
      "source": [
        "X"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.9521966 ,  0.68100522,  1.97312292, ..., -2.27457861,\n",
              "        -0.71442887, -2.14887271],\n",
              "       [-1.91531289,  0.68100522,  1.00257707, ..., -2.27457861,\n",
              "        -0.71442887, -0.51292188],\n",
              "       [-1.47415758, -1.46841752,  0.03203122, ...,  0.97635214,\n",
              "        -0.71442887, -0.51292188],\n",
              "       ...,\n",
              "       [ 1.50364073,  0.68100522, -0.93851463, ..., -0.64911323,\n",
              "         1.24459328,  1.12302895],\n",
              "       [ 0.29046364,  0.68100522, -0.93851463, ..., -0.64911323,\n",
              "         0.26508221,  1.12302895],\n",
              "       [ 0.29046364, -1.46841752,  0.03203122, ..., -0.64911323,\n",
              "         0.26508221, -0.51292188]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iShvqRqRx0Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMzZq5m7x0Qr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = np.array([data['target']]).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHRe6yFOx0Qt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93f1b1b0-4411-47fb-cc2a-b2be52a9a12b"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(303, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeHkS2lmx0Qu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c370a2a-c3f6-4f6e-bd84-80b0500958bb"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((303, 13), (303, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWIvqDZTx0Qx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b59132e-e669-41e6-8850-42078c7708c3"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(303, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-Fu2TtLx0Qz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b=np.random.randn(303, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64ICfBQCx0Q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import optimize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvsKFC0ix0Q2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Neural_Network(object):\n",
        "    def __init__(self):\n",
        "        self.inputs = 13\n",
        "        self.hiddenNodes = 303\n",
        "        self.outputNodes = 1\n",
        "        \n",
        "        self.L1_weights = np.random.randn(self.inputs, self.hiddenNodes)\n",
        "        self.L2_weights = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
        "\n",
        "    def feed_forward(self, X):\n",
        "        self.hidden_sum = np.dot(X, self.L1_weights)\n",
        "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "        self.output_sum = np.dot(self.activated_hidden, self.L2_weights)\n",
        "        self.activated_output = self.sigmoid(self.output_sum)\n",
        "        return self.activated_output\n",
        "        \n",
        "    def sigmoid(self, s):\n",
        "        return 1/(1+np.exp(-s))\n",
        "    \n",
        "    def sigmoidPrime(self, s):\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    def backward(self, X, y, o):\n",
        "        self.o_error = y - o \n",
        "        self.o_delta = self.o_error*self.sigmoidPrime(o) \n",
        "\n",
        "        self.z2_error = self.o_delta.dot(self.L2_weights.T)\n",
        "        self.z2_delta = self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden) \n",
        "\n",
        "        self.L1_weights += X.T.dot(self.z2_delta) \n",
        "        self.L2_weights += self.activated_hidden.T.dot(self.o_delta) \n",
        "        \n",
        "    def train (self, X, y):\n",
        "        o = self.feed_forward(X)\n",
        "        self.backward(X, y, o)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipiMVDxHx0Q3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn = Neural_Network()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDgWCmpQx0Q5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn.train(X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA4xQ6mYx0Q6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7057
        },
        "outputId": "3c7c9331-6828-4793-980c-1b22410da147"
      },
      "source": [
        "for i in range (100):\n",
        "    print('Epoch', i+1, ': ')\n",
        "    print('loss: ', str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
        "    print('\\n')\n",
        "    nn.train(X=X, y=y)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 2 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 3 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 4 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 5 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 6 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 7 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 8 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 9 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 10 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 11 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 12 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 13 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 14 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 15 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 16 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 17 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 18 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 19 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 20 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 21 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 22 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 23 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 24 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 25 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 26 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 27 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 28 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 29 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 30 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 31 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 32 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 33 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 34 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 35 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 36 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 37 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 38 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 39 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 40 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 41 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 42 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 43 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 44 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 45 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 46 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 47 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 48 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 49 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 50 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 51 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 52 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 53 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 54 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 55 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 56 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 57 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 58 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 59 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 60 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 61 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 62 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 63 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 64 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 65 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 66 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 67 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 68 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 69 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 70 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 71 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 72 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 73 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 74 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 75 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 76 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 77 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 78 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 79 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 80 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 81 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 82 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 83 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 84 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 85 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 86 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 87 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 88 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 89 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 90 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 91 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 92 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 93 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 94 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 95 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 96 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 97 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 98 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 99 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n",
            "Epoch 100 : \n",
            "loss:  0.45544554455445546\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAkTq9kEx0Q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GGT1oRzXw3H9"
      },
      "source": [
        "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
        "\n",
        "- Use the Heart Disease Dataset (binary classification)\n",
        "- Use an appropriate loss function for a binary classification task\n",
        "- Use an appropriate activation function on the final layer of your network. \n",
        "- Train your model using verbose output for ease of grading.\n",
        "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
        "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
        "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
        "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwsL1KdPyu_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uyj4PQN1iyeJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5458
        },
        "outputId": "050b9606-d9ad-4479-97f9-9aabbc46c99b"
      },
      "source": [
        "def keras_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(6, input_dim=13, activation='sigmoid'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "  \n",
        "model = KerasClassifier(build_fn=keras_model)\n",
        "kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "batch_size = [10,30,50]\n",
        "epochs = [50,100,150]\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=3, cv=kfold)\n",
        "\n",
        "results = grid_search.fit(X, y)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "303/303 [==============================] - 0s 608us/step - loss: 1.1353 - acc: 0.5446\n",
            "Epoch 2/150\n",
            "303/303 [==============================] - 0s 58us/step - loss: 1.1031 - acc: 0.5446\n",
            "Epoch 3/150\n",
            "303/303 [==============================] - 0s 54us/step - loss: 1.0723 - acc: 0.5446\n",
            "Epoch 4/150\n",
            "303/303 [==============================] - 0s 53us/step - loss: 1.0431 - acc: 0.5446\n",
            "Epoch 5/150\n",
            "303/303 [==============================] - 0s 48us/step - loss: 1.0152 - acc: 0.5446\n",
            "Epoch 6/150\n",
            "303/303 [==============================] - 0s 51us/step - loss: 0.9890 - acc: 0.5446\n",
            "Epoch 7/150\n",
            "303/303 [==============================] - 0s 58us/step - loss: 0.9641 - acc: 0.5446\n",
            "Epoch 8/150\n",
            "303/303 [==============================] - 0s 58us/step - loss: 0.9401 - acc: 0.5446\n",
            "Epoch 9/150\n",
            "303/303 [==============================] - 0s 65us/step - loss: 0.9177 - acc: 0.5446\n",
            "Epoch 10/150\n",
            "303/303 [==============================] - 0s 53us/step - loss: 0.8969 - acc: 0.5446\n",
            "Epoch 11/150\n",
            "303/303 [==============================] - 0s 53us/step - loss: 0.8778 - acc: 0.5446\n",
            "Epoch 12/150\n",
            "303/303 [==============================] - 0s 55us/step - loss: 0.8589 - acc: 0.5446\n",
            "Epoch 13/150\n",
            "303/303 [==============================] - 0s 68us/step - loss: 0.8417 - acc: 0.5446\n",
            "Epoch 14/150\n",
            "303/303 [==============================] - 0s 53us/step - loss: 0.8244 - acc: 0.5446\n",
            "Epoch 15/150\n",
            "303/303 [==============================] - 0s 63us/step - loss: 0.8086 - acc: 0.5446\n",
            "Epoch 16/150\n",
            "303/303 [==============================] - 0s 51us/step - loss: 0.7928 - acc: 0.5446\n",
            "Epoch 17/150\n",
            "303/303 [==============================] - 0s 66us/step - loss: 0.7771 - acc: 0.5446\n",
            "Epoch 18/150\n",
            "303/303 [==============================] - 0s 68us/step - loss: 0.7620 - acc: 0.5446\n",
            "Epoch 19/150\n",
            "303/303 [==============================] - 0s 57us/step - loss: 0.7491 - acc: 0.5446\n",
            "Epoch 20/150\n",
            "303/303 [==============================] - 0s 60us/step - loss: 0.7365 - acc: 0.5446\n",
            "Epoch 21/150\n",
            "303/303 [==============================] - 0s 51us/step - loss: 0.7254 - acc: 0.5446\n",
            "Epoch 22/150\n",
            "303/303 [==============================] - 0s 49us/step - loss: 0.7145 - acc: 0.5446\n",
            "Epoch 23/150\n",
            "303/303 [==============================] - 0s 60us/step - loss: 0.7038 - acc: 0.5446\n",
            "Epoch 24/150\n",
            "303/303 [==============================] - 0s 61us/step - loss: 0.6936 - acc: 0.5446\n",
            "Epoch 25/150\n",
            "303/303 [==============================] - 0s 60us/step - loss: 0.6839 - acc: 0.5446\n",
            "Epoch 26/150\n",
            "303/303 [==============================] - 0s 58us/step - loss: 0.6747 - acc: 0.5446\n",
            "Epoch 27/150\n",
            "303/303 [==============================] - 0s 57us/step - loss: 0.6654 - acc: 0.5446\n",
            "Epoch 28/150\n",
            "303/303 [==============================] - 0s 52us/step - loss: 0.6568 - acc: 0.5446\n",
            "Epoch 29/150\n",
            "303/303 [==============================] - 0s 51us/step - loss: 0.6492 - acc: 0.5446\n",
            "Epoch 30/150\n",
            "303/303 [==============================] - 0s 58us/step - loss: 0.6415 - acc: 0.5479\n",
            "Epoch 31/150\n",
            "303/303 [==============================] - 0s 50us/step - loss: 0.6348 - acc: 0.5545\n",
            "Epoch 32/150\n",
            "303/303 [==============================] - 0s 50us/step - loss: 0.6285 - acc: 0.5611\n",
            "Epoch 33/150\n",
            "303/303 [==============================] - 0s 48us/step - loss: 0.6228 - acc: 0.5677\n",
            "Epoch 34/150\n",
            "303/303 [==============================] - 0s 55us/step - loss: 0.6176 - acc: 0.5710\n",
            "Epoch 35/150\n",
            "303/303 [==============================] - 0s 67us/step - loss: 0.6125 - acc: 0.5842\n",
            "Epoch 36/150\n",
            "303/303 [==============================] - 0s 63us/step - loss: 0.6075 - acc: 0.5875\n",
            "Epoch 37/150\n",
            "303/303 [==============================] - 0s 67us/step - loss: 0.6028 - acc: 0.5974\n",
            "Epoch 38/150\n",
            "303/303 [==============================] - 0s 50us/step - loss: 0.5980 - acc: 0.6040\n",
            "Epoch 39/150\n",
            "303/303 [==============================] - 0s 54us/step - loss: 0.5933 - acc: 0.6073\n",
            "Epoch 40/150\n",
            "303/303 [==============================] - 0s 53us/step - loss: 0.5888 - acc: 0.6172\n",
            "Epoch 41/150\n",
            "303/303 [==============================] - 0s 52us/step - loss: 0.5848 - acc: 0.6337\n",
            "Epoch 42/150\n",
            "303/303 [==============================] - 0s 52us/step - loss: 0.5810 - acc: 0.6502\n",
            "Epoch 43/150\n",
            "303/303 [==============================] - 0s 51us/step - loss: 0.5777 - acc: 0.6568\n",
            "Epoch 44/150\n",
            "303/303 [==============================] - 0s 55us/step - loss: 0.5743 - acc: 0.6601\n",
            "Epoch 45/150\n",
            "303/303 [==============================] - 0s 59us/step - loss: 0.5711 - acc: 0.6667\n",
            "Epoch 46/150\n",
            "303/303 [==============================] - 0s 54us/step - loss: 0.5680 - acc: 0.6733\n",
            "Epoch 47/150\n",
            "303/303 [==============================] - 0s 48us/step - loss: 0.5650 - acc: 0.6766\n",
            "Epoch 48/150\n",
            "303/303 [==============================] - 0s 58us/step - loss: 0.5624 - acc: 0.6799\n",
            "Epoch 49/150\n",
            "303/303 [==============================] - 0s 57us/step - loss: 0.5598 - acc: 0.6832\n",
            "Epoch 50/150\n",
            "303/303 [==============================] - 0s 51us/step - loss: 0.5571 - acc: 0.6964\n",
            "Epoch 51/150\n",
            "303/303 [==============================] - 0s 57us/step - loss: 0.5542 - acc: 0.6997\n",
            "Epoch 52/150\n",
            "303/303 [==============================] - 0s 54us/step - loss: 0.5518 - acc: 0.7063\n",
            "Epoch 53/150\n",
            "303/303 [==============================] - 0s 61us/step - loss: 0.5492 - acc: 0.7162\n",
            "Epoch 54/150\n",
            "303/303 [==============================] - 0s 65us/step - loss: 0.5471 - acc: 0.7261\n",
            "Epoch 55/150\n",
            "303/303 [==============================] - 0s 51us/step - loss: 0.5451 - acc: 0.7261\n",
            "Epoch 56/150\n",
            "303/303 [==============================] - 0s 54us/step - loss: 0.5428 - acc: 0.7294\n",
            "Epoch 57/150\n",
            "303/303 [==============================] - 0s 49us/step - loss: 0.5407 - acc: 0.7294\n",
            "Epoch 58/150\n",
            "303/303 [==============================] - 0s 48us/step - loss: 0.5387 - acc: 0.7327\n",
            "Epoch 59/150\n",
            "303/303 [==============================] - 0s 59us/step - loss: 0.5368 - acc: 0.7360\n",
            "Epoch 60/150\n",
            "303/303 [==============================] - 0s 51us/step - loss: 0.5350 - acc: 0.7360\n",
            "Epoch 61/150\n",
            "303/303 [==============================] - 0s 58us/step - loss: 0.5333 - acc: 0.7360\n",
            "Epoch 62/150\n",
            "303/303 [==============================] - 0s 65us/step - loss: 0.5314 - acc: 0.7492\n",
            "Epoch 63/150\n",
            "303/303 [==============================] - 0s 54us/step - loss: 0.5295 - acc: 0.7459\n",
            "Epoch 64/150\n",
            "303/303 [==============================] - 0s 50us/step - loss: 0.5278 - acc: 0.7459\n",
            "Epoch 65/150\n",
            "303/303 [==============================] - 0s 50us/step - loss: 0.5260 - acc: 0.7558\n",
            "Epoch 66/150\n",
            "303/303 [==============================] - 0s 51us/step - loss: 0.5242 - acc: 0.7657\n",
            "Epoch 67/150\n",
            "303/303 [==============================] - 0s 58us/step - loss: 0.5224 - acc: 0.7723\n",
            "Epoch 68/150\n",
            "303/303 [==============================] - 0s 50us/step - loss: 0.5208 - acc: 0.7756\n",
            "Epoch 69/150\n",
            "303/303 [==============================] - 0s 56us/step - loss: 0.5193 - acc: 0.7756\n",
            "Epoch 70/150\n",
            "303/303 [==============================] - 0s 69us/step - loss: 0.5177 - acc: 0.7756\n",
            "Epoch 71/150\n",
            "303/303 [==============================] - 0s 50us/step - loss: 0.5163 - acc: 0.7789\n",
            "Epoch 72/150\n",
            "303/303 [==============================] - 0s 57us/step - loss: 0.5148 - acc: 0.7822\n",
            "Epoch 73/150\n",
            "303/303 [==============================] - 0s 48us/step - loss: 0.5134 - acc: 0.7822\n",
            "Epoch 74/150\n",
            "303/303 [==============================] - 0s 50us/step - loss: 0.5121 - acc: 0.7888\n",
            "Epoch 75/150\n",
            "303/303 [==============================] - 0s 51us/step - loss: 0.5106 - acc: 0.7921\n",
            "Epoch 76/150\n",
            "303/303 [==============================] - 0s 50us/step - loss: 0.5092 - acc: 0.7921\n",
            "Epoch 77/150\n",
            "303/303 [==============================] - 0s 50us/step - loss: 0.5077 - acc: 0.7921\n",
            "Epoch 78/150\n",
            "303/303 [==============================] - 0s 55us/step - loss: 0.5064 - acc: 0.7954\n",
            "Epoch 79/150\n",
            "303/303 [==============================] - 0s 59us/step - loss: 0.5052 - acc: 0.7954\n",
            "Epoch 80/150\n",
            "303/303 [==============================] - 0s 50us/step - loss: 0.5040 - acc: 0.7954\n",
            "Epoch 81/150\n",
            "303/303 [==============================] - 0s 52us/step - loss: 0.5026 - acc: 0.7954\n",
            "Epoch 82/150\n",
            "303/303 [==============================] - 0s 50us/step - loss: 0.5013 - acc: 0.7954\n",
            "Epoch 83/150\n",
            "303/303 [==============================] - 0s 52us/step - loss: 0.4999 - acc: 0.7987\n",
            "Epoch 84/150\n",
            "303/303 [==============================] - 0s 52us/step - loss: 0.4985 - acc: 0.8086\n",
            "Epoch 85/150\n",
            "303/303 [==============================] - 0s 49us/step - loss: 0.4972 - acc: 0.8086\n",
            "Epoch 86/150\n",
            "303/303 [==============================] - 0s 48us/step - loss: 0.4958 - acc: 0.8086\n",
            "Epoch 87/150\n",
            "303/303 [==============================] - 0s 49us/step - loss: 0.4946 - acc: 0.8086\n",
            "Epoch 88/150\n",
            "303/303 [==============================] - 0s 58us/step - loss: 0.4934 - acc: 0.8119\n",
            "Epoch 89/150\n",
            "303/303 [==============================] - 0s 50us/step - loss: 0.4921 - acc: 0.8053\n",
            "Epoch 90/150\n",
            "303/303 [==============================] - 0s 55us/step - loss: 0.4908 - acc: 0.8053\n",
            "Epoch 91/150\n",
            "303/303 [==============================] - 0s 50us/step - loss: 0.4896 - acc: 0.8053\n",
            "Epoch 92/150\n",
            "303/303 [==============================] - 0s 52us/step - loss: 0.4882 - acc: 0.8053\n",
            "Epoch 93/150\n",
            "303/303 [==============================] - 0s 49us/step - loss: 0.4870 - acc: 0.8053\n",
            "Epoch 94/150\n",
            "303/303 [==============================] - 0s 48us/step - loss: 0.4858 - acc: 0.8053\n",
            "Epoch 95/150\n",
            "303/303 [==============================] - 0s 49us/step - loss: 0.4846 - acc: 0.8053\n",
            "Epoch 96/150\n",
            "303/303 [==============================] - 0s 53us/step - loss: 0.4834 - acc: 0.8053\n",
            "Epoch 97/150\n",
            "303/303 [==============================] - 0s 56us/step - loss: 0.4822 - acc: 0.8086\n",
            "Epoch 98/150\n",
            "303/303 [==============================] - 0s 61us/step - loss: 0.4810 - acc: 0.8086\n",
            "Epoch 99/150\n",
            "303/303 [==============================] - 0s 52us/step - loss: 0.4799 - acc: 0.8119\n",
            "Epoch 100/150\n",
            "303/303 [==============================] - 0s 47us/step - loss: 0.4787 - acc: 0.8119\n",
            "Epoch 101/150\n",
            "303/303 [==============================] - 0s 49us/step - loss: 0.4775 - acc: 0.8152\n",
            "Epoch 102/150\n",
            "303/303 [==============================] - 0s 59us/step - loss: 0.4764 - acc: 0.8152\n",
            "Epoch 103/150\n",
            "303/303 [==============================] - 0s 51us/step - loss: 0.4752 - acc: 0.8152\n",
            "Epoch 104/150\n",
            "303/303 [==============================] - 0s 52us/step - loss: 0.4741 - acc: 0.8152\n",
            "Epoch 105/150\n",
            "303/303 [==============================] - 0s 52us/step - loss: 0.4729 - acc: 0.8185\n",
            "Epoch 106/150\n",
            "303/303 [==============================] - 0s 50us/step - loss: 0.4718 - acc: 0.8185\n",
            "Epoch 107/150\n",
            "303/303 [==============================] - 0s 51us/step - loss: 0.4707 - acc: 0.8185\n",
            "Epoch 108/150\n",
            "303/303 [==============================] - 0s 49us/step - loss: 0.4695 - acc: 0.8218\n",
            "Epoch 109/150\n",
            "303/303 [==============================] - 0s 51us/step - loss: 0.4685 - acc: 0.8251\n",
            "Epoch 110/150\n",
            "303/303 [==============================] - 0s 52us/step - loss: 0.4675 - acc: 0.8251\n",
            "Epoch 111/150\n",
            "303/303 [==============================] - 0s 53us/step - loss: 0.4665 - acc: 0.8251\n",
            "Epoch 112/150\n",
            "303/303 [==============================] - 0s 56us/step - loss: 0.4655 - acc: 0.8251\n",
            "Epoch 113/150\n",
            "303/303 [==============================] - 0s 53us/step - loss: 0.4644 - acc: 0.8251\n",
            "Epoch 114/150\n",
            "303/303 [==============================] - 0s 52us/step - loss: 0.4635 - acc: 0.8251\n",
            "Epoch 115/150\n",
            "303/303 [==============================] - 0s 57us/step - loss: 0.4625 - acc: 0.8251\n",
            "Epoch 116/150\n",
            "303/303 [==============================] - 0s 51us/step - loss: 0.4614 - acc: 0.8284\n",
            "Epoch 117/150\n",
            "303/303 [==============================] - 0s 52us/step - loss: 0.4605 - acc: 0.8284\n",
            "Epoch 118/150\n",
            "303/303 [==============================] - 0s 53us/step - loss: 0.4595 - acc: 0.8284\n",
            "Epoch 119/150\n",
            "303/303 [==============================] - 0s 57us/step - loss: 0.4585 - acc: 0.8284\n",
            "Epoch 120/150\n",
            "303/303 [==============================] - 0s 56us/step - loss: 0.4575 - acc: 0.8284\n",
            "Epoch 121/150\n",
            "303/303 [==============================] - 0s 73us/step - loss: 0.4565 - acc: 0.8284\n",
            "Epoch 122/150\n",
            "303/303 [==============================] - 0s 49us/step - loss: 0.4556 - acc: 0.8284\n",
            "Epoch 123/150\n",
            "303/303 [==============================] - 0s 57us/step - loss: 0.4545 - acc: 0.8284\n",
            "Epoch 124/150\n",
            "303/303 [==============================] - 0s 54us/step - loss: 0.4536 - acc: 0.8284\n",
            "Epoch 125/150\n",
            "303/303 [==============================] - 0s 48us/step - loss: 0.4527 - acc: 0.8251\n",
            "Epoch 126/150\n",
            "303/303 [==============================] - 0s 49us/step - loss: 0.4518 - acc: 0.8251\n",
            "Epoch 127/150\n",
            "303/303 [==============================] - 0s 56us/step - loss: 0.4508 - acc: 0.8251\n",
            "Epoch 128/150\n",
            "303/303 [==============================] - 0s 51us/step - loss: 0.4499 - acc: 0.8284\n",
            "Epoch 129/150\n",
            "303/303 [==============================] - 0s 51us/step - loss: 0.4490 - acc: 0.8317\n",
            "Epoch 130/150\n",
            "303/303 [==============================] - 0s 50us/step - loss: 0.4481 - acc: 0.8317\n",
            "Epoch 131/150\n",
            "303/303 [==============================] - 0s 62us/step - loss: 0.4471 - acc: 0.8317\n",
            "Epoch 132/150\n",
            "303/303 [==============================] - 0s 56us/step - loss: 0.4462 - acc: 0.8317\n",
            "Epoch 133/150\n",
            "303/303 [==============================] - 0s 52us/step - loss: 0.4452 - acc: 0.8317\n",
            "Epoch 134/150\n",
            "303/303 [==============================] - 0s 58us/step - loss: 0.4443 - acc: 0.8317\n",
            "Epoch 135/150\n",
            "303/303 [==============================] - 0s 69us/step - loss: 0.4433 - acc: 0.8317\n",
            "Epoch 136/150\n",
            "303/303 [==============================] - 0s 51us/step - loss: 0.4423 - acc: 0.8317\n",
            "Epoch 137/150\n",
            "303/303 [==============================] - 0s 50us/step - loss: 0.4415 - acc: 0.8350\n",
            "Epoch 138/150\n",
            "303/303 [==============================] - 0s 55us/step - loss: 0.4407 - acc: 0.8350\n",
            "Epoch 139/150\n",
            "303/303 [==============================] - 0s 52us/step - loss: 0.4398 - acc: 0.8350\n",
            "Epoch 140/150\n",
            "303/303 [==============================] - 0s 55us/step - loss: 0.4389 - acc: 0.8350\n",
            "Epoch 141/150\n",
            "303/303 [==============================] - 0s 51us/step - loss: 0.4380 - acc: 0.8350\n",
            "Epoch 142/150\n",
            "303/303 [==============================] - 0s 56us/step - loss: 0.4373 - acc: 0.8383\n",
            "Epoch 143/150\n",
            "303/303 [==============================] - 0s 49us/step - loss: 0.4365 - acc: 0.8383\n",
            "Epoch 144/150\n",
            "303/303 [==============================] - 0s 60us/step - loss: 0.4357 - acc: 0.8383\n",
            "Epoch 145/150\n",
            "303/303 [==============================] - 0s 58us/step - loss: 0.4348 - acc: 0.8383\n",
            "Epoch 146/150\n",
            "303/303 [==============================] - 0s 49us/step - loss: 0.4340 - acc: 0.8383\n",
            "Epoch 147/150\n",
            "303/303 [==============================] - 0s 50us/step - loss: 0.4332 - acc: 0.8416\n",
            "Epoch 148/150\n",
            "303/303 [==============================] - 0s 54us/step - loss: 0.4324 - acc: 0.8416\n",
            "Epoch 149/150\n",
            "303/303 [==============================] - 0s 50us/step - loss: 0.4316 - acc: 0.8449\n",
            "Epoch 150/150\n",
            "303/303 [==============================] - 0s 54us/step - loss: 0.4309 - acc: 0.8416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L21ioD6VzHrb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd2878dc-dcaa-4923-b9dc-1ee8965f58e1"
      },
      "source": [
        "print(f\"Best Score: {results.best_score_:2f} using {results.best_params_}\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Score: 0.858086 using {'batch_size': 30, 'epochs': 150}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCV_bG9u0elM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}