{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(812)\n",
    "\n",
    "# First col of inputs matches output\n",
    "inputs = np.array([\n",
    "    [0, 0, 1],\n",
    "    [1, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 1]\n",
    "])\n",
    "\n",
    "correct_outputs = [[0], [1], [1], [0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0099616 ],\n",
       "       [ 0.21185521],\n",
       "       [-0.08502562]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep weights bounded [-1, 1]\n",
    "weights = 2 * np.random.random((3, 1)) - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training\n",
      "[[15.03804491]\n",
      " [-0.40666422]\n",
      " [-7.23278107]]\n",
      "\n",
      "Output after training\n",
      "[[7.22059439e-04]\n",
      " [9.99388204e-01]\n",
      " [9.99592541e-01]\n",
      " [4.80912067e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Steps\n",
    "# 1. Randomly Initialized Weights\n",
    "# 2. Got input data & correct_outputs\n",
    "\n",
    "# Update 10,000 times - iterations or \"epochs\"\n",
    "for iteration in range(10000):\n",
    "    \n",
    "    # Weighted sum of inputs\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "    # Activate with Sigmoid\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    \n",
    "    # Cac error\n",
    "    error = correct_outputs - activated_output\n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "    \n",
    "    # Update weights\n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "    \n",
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "\n",
    "print ('')\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    def __init__(self, rate = 0.01, niter = 10):\n",
    "    self.rate = rate\n",
    "    self.niter = niter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "    \"\"\"Fit training data\n",
    "    X : Training vectors, X.shape : [#samples, #features]\n",
    "    y : Target values, y.shape : [#samples]\n",
    "    \"\"\"\n",
    "\n",
    "    # weights\n",
    "    self.weight = np.zeros(1 + X.shape[1])\n",
    "\n",
    "    # Number of misclassifications\n",
    "    self.errors = []  # Number of misclassifications\n",
    "\n",
    "    for i in range(self.niter):\n",
    "      err = 0\n",
    "      for xi, target in zip(X, y):\n",
    "        delta_w = self.rate * (target - self.predict(xi))\n",
    "        self.weight[1:] += delta_w * xi\n",
    "        self.weight[0] += delta_w\n",
    "        err += int(delta_w != 0.0)\n",
    "      self.errors.append(err)\n",
    "    return self\n",
    "\n",
    "  def net_input(self, X):\n",
    "    \"\"\"Calculate net input\"\"\"\n",
    "    return np.dot(X, self.weight[1:]) + self.weight[0]\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"Return class label after unit step\"\"\"\n",
    "    return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want activations that correspond to negative weights to be lower\n",
    "# and activations that correspond to positive weights to be higher\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3\n",
    "        self.output = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        # 2x3 Matrix Array for the First Layer\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
    "       \n",
    "        # 3x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.output)\n",
    "    \n",
    "    # Define activation function\n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    # Slope of activation function\n",
    "    def sigmoidPrime(self, s):\n",
    "        sx = self.sigmoid(s)\n",
    "        return sx * (1 - sx)\n",
    "    \n",
    "    # Feed forward / prediction method\n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka 'predict'\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weighted sum of inputs\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activations of Weighted Sums\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        # Next layer\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final node - activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        return self.activated_output\n",
    "    \n",
    "    def backward(self, X, y, y_pred):\n",
    "        \"\"\"\n",
    "        Back propagate through the network\n",
    "        \"\"\"\n",
    "        \n",
    "        # Error in output\n",
    "        self.output_error = y - y_pred\n",
    "        \n",
    "        \n",
    "        # hidden --> Output\n",
    "        # Apply Derivative of Sigmoid to error\n",
    "        self.output_delta = self.output_error * self.sigmoidPrime(y_pred)\n",
    "        \n",
    "        # z2 Error\n",
    "        self.z2_error = self.output_delta.dot(self.weights2.T)\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        # Update weights\n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        # Adjust second set of weights\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.output_delta)\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        output = self.feed_forward(X)\n",
    "        self.backward(X, y, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow & Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(812)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8c7d567f8abd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m               metrics=[\"accuracy\"])\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add a dense layer\n",
    "model.add(Dense(1, input_dim=8, activation=\"sigmoid\"))\n",
    "\n",
    "# Define loss function and compile\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X, y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, y)\n",
    "print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-fold Cross-Val Results - Mean: 22.33 StDev: 13.10 MSE\n"
     ]
    }
   ],
   "source": [
    "# Keras imports\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "# sklearn imports\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Random Seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Load Dataset\n",
    "boston_dataset = load_boston()\n",
    "df = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "df['MEDV'] = boston_dataset.target\n",
    "\n",
    "# Split into X and y and turn into numpy arays\n",
    "y = df.MEDV.values\n",
    "X = df.drop(\"MEDV\", axis='columns').values\n",
    "\n",
    "# Important Hyperparameters\n",
    "inputs = X.shape[1]\n",
    "epochs = 50\n",
    "batch_size = 10\n",
    "\n",
    "# define base model\n",
    "def baseline_model():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(64, input_dim=inputs, activation='relu')) \n",
    "  model.add(Dense(64, activation='relu')) \n",
    "  model.add(Dense(1))\n",
    "  # Compile model\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=epochs, batch_size=batch_size,\n",
    "    verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=5, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "print(f\"K-fold Cross-Val Results - Mean: {-results.mean():.2f} StDev: {results.std():.2f} MSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid CV Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2052: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "512/512 [==============================] - 0s 337us/sample - loss: 7.2902 - acc: 0.5957\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 0s 238us/sample - loss: 4.4856 - acc: 0.5410\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 0s 261us/sample - loss: 2.5586 - acc: 0.5449\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 0s 254us/sample - loss: 1.8141 - acc: 0.5645s - loss: 1.8774 - acc: 0.554\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 0s 204us/sample - loss: 1.6869 - acc: 0.5820\n",
      "Epoch 6/20\n",
      "512/512 [==============================] - 0s 155us/sample - loss: 1.6259 - acc: 0.5840\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 0s 210us/sample - loss: 1.5118 - acc: 0.5918\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 0s 193us/sample - loss: 1.4505 - acc: 0.5898\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 0s 184us/sample - loss: 1.3531 - acc: 0.5879\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 0s 184us/sample - loss: 1.2423 - acc: 0.5977\n",
      "Epoch 11/20\n",
      "512/512 [==============================] - 0s 191us/sample - loss: 1.2176 - acc: 0.5938\n",
      "Epoch 12/20\n",
      "512/512 [==============================] - 0s 200us/sample - loss: 1.0663 - acc: 0.6230\n",
      "Epoch 13/20\n",
      "512/512 [==============================] - 0s 165us/sample - loss: 1.0774 - acc: 0.6035\n",
      "Epoch 14/20\n",
      "512/512 [==============================] - 0s 318us/sample - loss: 0.9777 - acc: 0.6113\n",
      "Epoch 15/20\n",
      "512/512 [==============================] - 0s 155us/sample - loss: 0.9176 - acc: 0.6289\n",
      "Epoch 16/20\n",
      "512/512 [==============================] - 0s 234us/sample - loss: 0.8839 - acc: 0.6250\n",
      "Epoch 17/20\n",
      "512/512 [==============================] - 0s 198us/sample - loss: 0.8749 - acc: 0.6445\n",
      "Epoch 18/20\n",
      "512/512 [==============================] - 0s 190us/sample - loss: 0.8295 - acc: 0.6367\n",
      "Epoch 19/20\n",
      "512/512 [==============================] - 0s 177us/sample - loss: 0.8655 - acc: 0.6445\n",
      "Epoch 20/20\n",
      "512/512 [==============================] - 0s 195us/sample - loss: 0.7777 - acc: 0.6211\n",
      "256/256 [==============================] - 0s 266us/sample - loss: 0.9448 - acc: 0.6367\n",
      "512/512 [==============================] - 0s 157us/sample - loss: 0.7783 - acc: 0.6758\n",
      "Epoch 1/20\n",
      "512/512 [==============================] - 0s 431us/sample - loss: 24.1273 - acc: 0.3535\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 0s 202us/sample - loss: 7.5914 - acc: 0.4824\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 0s 217us/sample - loss: 3.4996 - acc: 0.5879\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 0s 168us/sample - loss: 2.5234 - acc: 0.6309\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 0s 331us/sample - loss: 2.1457 - acc: 0.6289\n",
      "Epoch 6/20\n",
      "512/512 [==============================] - 0s 210us/sample - loss: 1.9933 - acc: 0.6289\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 0s 158us/sample - loss: 1.7915 - acc: 0.6445\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 0s 265us/sample - loss: 1.6503 - acc: 0.6445\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 0s 221us/sample - loss: 1.5936 - acc: 0.6309\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 0s 166us/sample - loss: 1.4110 - acc: 0.6387\n",
      "Epoch 11/20\n",
      "512/512 [==============================] - 0s 236us/sample - loss: 1.3048 - acc: 0.6562\n",
      "Epoch 12/20\n",
      "512/512 [==============================] - 0s 201us/sample - loss: 1.1978 - acc: 0.6562\n",
      "Epoch 13/20\n",
      "512/512 [==============================] - 0s 197us/sample - loss: 1.1408 - acc: 0.6562\n",
      "Epoch 14/20\n",
      "512/512 [==============================] - 0s 169us/sample - loss: 1.0494 - acc: 0.6621\n",
      "Epoch 15/20\n",
      "512/512 [==============================] - 0s 252us/sample - loss: 0.9857 - acc: 0.6680\n",
      "Epoch 16/20\n",
      "512/512 [==============================] - 0s 222us/sample - loss: 0.9232 - acc: 0.6855\n",
      "Epoch 17/20\n",
      "512/512 [==============================] - 0s 216us/sample - loss: 0.9223 - acc: 0.6641\n",
      "Epoch 18/20\n",
      "512/512 [==============================] - 0s 180us/sample - loss: 0.8827 - acc: 0.6797\n",
      "Epoch 19/20\n",
      "512/512 [==============================] - 0s 227us/sample - loss: 0.8835 - acc: 0.6582\n",
      "Epoch 20/20\n",
      "512/512 [==============================] - 0s 216us/sample - loss: 0.8449 - acc: 0.6738\n",
      "256/256 [==============================] - 0s 270us/sample - loss: 0.8653 - acc: 0.6172\n",
      "512/512 [==============================] - 0s 135us/sample - loss: 0.8147 - acc: 0.6582\n",
      "Epoch 1/20\n",
      "512/512 [==============================] - 0s 396us/sample - loss: 21.7989 - acc: 0.6504\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 0s 190us/sample - loss: 12.3275 - acc: 0.6035\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 0s 183us/sample - loss: 8.2673 - acc: 0.5352\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 0s 190us/sample - loss: 5.7276 - acc: 0.5391\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 0s 185us/sample - loss: 4.3390 - acc: 0.5039\n",
      "Epoch 6/20\n",
      "512/512 [==============================] - 0s 256us/sample - loss: 3.2706 - acc: 0.5391\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 0s 245us/sample - loss: 2.5099 - acc: 0.5547\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 0s 196us/sample - loss: 1.9622 - acc: 0.5762\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 0s 201us/sample - loss: 1.6412 - acc: 0.6016\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 0s 144us/sample - loss: 1.5329 - acc: 0.5898\n",
      "Epoch 11/20\n",
      "512/512 [==============================] - 0s 218us/sample - loss: 1.3502 - acc: 0.6172\n",
      "Epoch 12/20\n",
      "512/512 [==============================] - 0s 137us/sample - loss: 1.2702 - acc: 0.6445\n",
      "Epoch 13/20\n",
      "512/512 [==============================] - 0s 163us/sample - loss: 1.1748 - acc: 0.6504\n",
      "Epoch 14/20\n",
      "512/512 [==============================] - 0s 222us/sample - loss: 1.1730 - acc: 0.6621\n",
      "Epoch 15/20\n",
      "512/512 [==============================] - 0s 233us/sample - loss: 1.1057 - acc: 0.6602\n",
      "Epoch 16/20\n",
      "512/512 [==============================] - 0s 203us/sample - loss: 1.0499 - acc: 0.6504\n",
      "Epoch 17/20\n",
      "512/512 [==============================] - 0s 139us/sample - loss: 1.0224 - acc: 0.6602\n",
      "Epoch 18/20\n",
      "512/512 [==============================] - 0s 179us/sample - loss: 1.0144 - acc: 0.6582\n",
      "Epoch 19/20\n",
      "512/512 [==============================] - 0s 136us/sample - loss: 0.9827 - acc: 0.6426\n",
      "Epoch 20/20\n",
      "512/512 [==============================] - 0s 177us/sample - loss: 0.9971 - acc: 0.6973\n",
      "256/256 [==============================] - 0s 307us/sample - loss: 1.1614 - acc: 0.6016\n",
      "512/512 [==============================] - 0s 96us/sample - loss: 0.9017 - acc: 0.6719\n",
      "Epoch 1/20\n",
      "512/512 [==============================] - 0s 706us/sample - loss: 47.3465 - acc: 0.3320\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 0s 121us/sample - loss: 29.7868 - acc: 0.3301\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 0s 132us/sample - loss: 13.4498 - acc: 0.3965\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 0s 163us/sample - loss: 6.0891 - acc: 0.5508\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 0s 215us/sample - loss: 5.3176 - acc: 0.5977\n",
      "Epoch 6/20\n",
      "512/512 [==============================] - 0s 128us/sample - loss: 4.8782 - acc: 0.5918\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 0s 133us/sample - loss: 4.3974 - acc: 0.5938\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 0s 190us/sample - loss: 3.7115 - acc: 0.6230\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 0s 57us/sample - loss: 3.3372 - acc: 0.6367\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 0s 117us/sample - loss: 3.1460 - acc: 0.6406\n",
      "Epoch 11/20\n",
      "512/512 [==============================] - 0s 68us/sample - loss: 2.8934 - acc: 0.6445\n",
      "Epoch 12/20\n",
      "512/512 [==============================] - 0s 123us/sample - loss: 2.7191 - acc: 0.6387\n",
      "Epoch 13/20\n",
      "512/512 [==============================] - 0s 111us/sample - loss: 2.5647 - acc: 0.6445\n",
      "Epoch 14/20\n",
      "512/512 [==============================] - 0s 274us/sample - loss: 2.4249 - acc: 0.6621\n",
      "Epoch 15/20\n",
      "512/512 [==============================] - 0s 200us/sample - loss: 2.2914 - acc: 0.6543\n",
      "Epoch 16/20\n",
      "512/512 [==============================] - 0s 186us/sample - loss: 2.1895 - acc: 0.6621\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 0s 156us/sample - loss: 2.0809 - acc: 0.6641\n",
      "Epoch 18/20\n",
      "512/512 [==============================] - 0s 147us/sample - loss: 1.9960 - acc: 0.6641\n",
      "Epoch 19/20\n",
      "512/512 [==============================] - 0s 90us/sample - loss: 1.9072 - acc: 0.6797\n",
      "Epoch 20/20\n",
      "512/512 [==============================] - 0s 116us/sample - loss: 1.7926 - acc: 0.6816\n",
      "256/256 [==============================] - 0s 310us/sample - loss: 2.0206 - acc: 0.6641\n",
      "512/512 [==============================] - 0s 58us/sample - loss: 1.7979 - acc: 0.6914\n",
      "Epoch 1/20\n",
      "512/512 [==============================] - 0s 494us/sample - loss: 26.7298 - acc: 0.6426\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 0s 113us/sample - loss: 16.3755 - acc: 0.6230\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 0s 81us/sample - loss: 9.5057 - acc: 0.5000\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 0s 61us/sample - loss: 8.0229 - acc: 0.4785\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 0s 83us/sample - loss: 7.2650 - acc: 0.4922\n",
      "Epoch 6/20\n",
      "512/512 [==============================] - 0s 97us/sample - loss: 6.6295 - acc: 0.4844\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 0s 152us/sample - loss: 6.0356 - acc: 0.4902\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 0s 160us/sample - loss: 5.4997 - acc: 0.5000\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 0s 138us/sample - loss: 5.0244 - acc: 0.5176\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 0s 130us/sample - loss: 4.6431 - acc: 0.5156\n",
      "Epoch 11/20\n",
      "512/512 [==============================] - 0s 122us/sample - loss: 4.2653 - acc: 0.5312\n",
      "Epoch 12/20\n",
      "512/512 [==============================] - 0s 125us/sample - loss: 3.9759 - acc: 0.5215\n",
      "Epoch 13/20\n",
      "512/512 [==============================] - 0s 138us/sample - loss: 3.6672 - acc: 0.5117\n",
      "Epoch 14/20\n",
      "512/512 [==============================] - 0s 139us/sample - loss: 3.3143 - acc: 0.5527\n",
      "Epoch 15/20\n",
      "512/512 [==============================] - 0s 115us/sample - loss: 2.8497 - acc: 0.5371\n",
      "Epoch 16/20\n",
      "512/512 [==============================] - 0s 74us/sample - loss: 2.5694 - acc: 0.5566\n",
      "Epoch 17/20\n",
      "512/512 [==============================] - 0s 61us/sample - loss: 2.2046 - acc: 0.5684\n",
      "Epoch 18/20\n",
      "512/512 [==============================] - 0s 106us/sample - loss: 1.8952 - acc: 0.5898\n",
      "Epoch 19/20\n",
      "512/512 [==============================] - 0s 68us/sample - loss: 1.6268 - acc: 0.6074\n",
      "Epoch 20/20\n",
      "512/512 [==============================] - 0s 91us/sample - loss: 1.4283 - acc: 0.6094\n",
      "256/256 [==============================] - 0s 259us/sample - loss: 1.4539 - acc: 0.5742\n",
      "512/512 [==============================] - 0s 34us/sample - loss: 1.3272 - acc: 0.6113\n",
      "Epoch 1/20\n",
      "512/512 [==============================] - 0s 321us/sample - loss: 32.6384 - acc: 0.6387\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 0s 114us/sample - loss: 20.0305 - acc: 0.6211\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 0s 79us/sample - loss: 11.9965 - acc: 0.5137\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 0s 64us/sample - loss: 9.7701 - acc: 0.4570\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 0s 93us/sample - loss: 8.6740 - acc: 0.4648\n",
      "Epoch 6/20\n",
      "512/512 [==============================] - 0s 83us/sample - loss: 7.7955 - acc: 0.4746\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 0s 72us/sample - loss: 6.9072 - acc: 0.4727\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 0s 106us/sample - loss: 5.9554 - acc: 0.4766\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 0s 61us/sample - loss: 5.1111 - acc: 0.4961\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 0s 112us/sample - loss: 4.4272 - acc: 0.4941\n",
      "Epoch 11/20\n",
      "512/512 [==============================] - 0s 79us/sample - loss: 3.5287 - acc: 0.5312\n",
      "Epoch 12/20\n",
      "512/512 [==============================] - 0s 98us/sample - loss: 2.9003 - acc: 0.5547\n",
      "Epoch 13/20\n",
      "512/512 [==============================] - 0s 114us/sample - loss: 2.4228 - acc: 0.5781\n",
      "Epoch 14/20\n",
      "512/512 [==============================] - 0s 171us/sample - loss: 2.0787 - acc: 0.5938\n",
      "Epoch 15/20\n",
      "512/512 [==============================] - 0s 256us/sample - loss: 1.8545 - acc: 0.6035\n",
      "Epoch 16/20\n",
      "512/512 [==============================] - 0s 215us/sample - loss: 1.6491 - acc: 0.6270s - loss: 1.5657 - acc: 0.656\n",
      "Epoch 17/20\n",
      "512/512 [==============================] - 0s 171us/sample - loss: 1.5249 - acc: 0.6309\n",
      "Epoch 18/20\n",
      "512/512 [==============================] - 0s 186us/sample - loss: 1.3646 - acc: 0.6387\n",
      "Epoch 19/20\n",
      "512/512 [==============================] - 0s 177us/sample - loss: 1.3071 - acc: 0.6523\n",
      "Epoch 20/20\n",
      "512/512 [==============================] - 0s 200us/sample - loss: 1.2585 - acc: 0.6348\n",
      "256/256 [==============================] - 0s 279us/sample - loss: 1.5454 - acc: 0.5703\n",
      "512/512 [==============================] - 0s 66us/sample - loss: 1.2166 - acc: 0.6211\n",
      "Epoch 1/20\n",
      "512/512 [==============================] - 0s 430us/sample - loss: 6.9220 - acc: 0.5664\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 0s 156us/sample - loss: 5.2834 - acc: 0.5957\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 0s 169us/sample - loss: 4.2894 - acc: 0.6133\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 0s 111us/sample - loss: 3.8227 - acc: 0.6074\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 0s 258us/sample - loss: 3.4884 - acc: 0.6133\n",
      "Epoch 6/20\n",
      "512/512 [==============================] - 0s 120us/sample - loss: 3.2397 - acc: 0.6172\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 0s 67us/sample - loss: 2.9789 - acc: 0.6191\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 0s 83us/sample - loss: 2.7815 - acc: 0.6309\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 0s 138us/sample - loss: 2.5980 - acc: 0.6289\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 0s 94us/sample - loss: 2.4056 - acc: 0.6270\n",
      "Epoch 11/20\n",
      "512/512 [==============================] - 0s 131us/sample - loss: 2.2726 - acc: 0.6270\n",
      "Epoch 12/20\n",
      "512/512 [==============================] - 0s 61us/sample - loss: 2.1707 - acc: 0.5996\n",
      "Epoch 13/20\n",
      "512/512 [==============================] - 0s 172us/sample - loss: 2.0619 - acc: 0.6074\n",
      "Epoch 14/20\n",
      "512/512 [==============================] - 0s 174us/sample - loss: 1.9690 - acc: 0.6230\n",
      "Epoch 15/20\n",
      "512/512 [==============================] - 0s 212us/sample - loss: 1.8942 - acc: 0.6094\n",
      "Epoch 16/20\n",
      "512/512 [==============================] - 0s 76us/sample - loss: 1.8318 - acc: 0.5977\n",
      "Epoch 17/20\n",
      "512/512 [==============================] - 0s 166us/sample - loss: 1.7847 - acc: 0.6113\n",
      "Epoch 18/20\n",
      "512/512 [==============================] - 0s 84us/sample - loss: 1.6997 - acc: 0.6250\n",
      "Epoch 19/20\n",
      "512/512 [==============================] - 0s 111us/sample - loss: 1.6365 - acc: 0.6133\n",
      "Epoch 20/20\n",
      "512/512 [==============================] - 0s 159us/sample - loss: 1.5874 - acc: 0.6074\n",
      "256/256 [==============================] - 0s 399us/sample - loss: 1.6822 - acc: 0.5820\n",
      "512/512 [==============================] - 0s 26us/sample - loss: 1.5417 - acc: 0.5820\n",
      "Epoch 1/20\n",
      "512/512 [==============================] - 1s 1ms/sample - loss: 36.9735 - acc: 0.3535\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 0s 281us/sample - loss: 28.8148 - acc: 0.3574\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 0s 103us/sample - loss: 22.0793 - acc: 0.3672\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 0s 59us/sample - loss: 17.6984 - acc: 0.3906\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 0s 86us/sample - loss: 14.8340 - acc: 0.3965\n",
      "Epoch 6/20\n",
      "512/512 [==============================] - 0s 146us/sample - loss: 12.5388 - acc: 0.4297\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 0s 46us/sample - loss: 10.2848 - acc: 0.4434\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 0s 369us/sample - loss: 7.9801 - acc: 0.4707\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 0s 72us/sample - loss: 5.2594 - acc: 0.5059\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 0s 76us/sample - loss: 3.3455 - acc: 0.5566\n",
      "Epoch 11/20\n",
      "512/512 [==============================] - 0s 165us/sample - loss: 2.4097 - acc: 0.5293\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 0s 116us/sample - loss: 2.2801 - acc: 0.5352\n",
      "Epoch 13/20\n",
      "512/512 [==============================] - 0s 82us/sample - loss: 2.1372 - acc: 0.5488\n",
      "Epoch 14/20\n",
      "512/512 [==============================] - 0s 40us/sample - loss: 1.9980 - acc: 0.5566\n",
      "Epoch 15/20\n",
      "512/512 [==============================] - 0s 137us/sample - loss: 1.9082 - acc: 0.5625\n",
      "Epoch 16/20\n",
      "512/512 [==============================] - 0s 237us/sample - loss: 1.8324 - acc: 0.5488s - loss: 1.8142 - acc: 0.556\n",
      "Epoch 17/20\n",
      "512/512 [==============================] - 0s 194us/sample - loss: 1.7501 - acc: 0.5625s - loss: 1.7198 - acc: 0.575\n",
      "Epoch 18/20\n",
      "512/512 [==============================] - 0s 149us/sample - loss: 1.6902 - acc: 0.5684\n",
      "Epoch 19/20\n",
      "512/512 [==============================] - 0s 212us/sample - loss: 1.6299 - acc: 0.5762\n",
      "Epoch 20/20\n",
      "512/512 [==============================] - 0s 181us/sample - loss: 1.5785 - acc: 0.5684\n",
      "256/256 [==============================] - 0s 345us/sample - loss: 1.6392 - acc: 0.5781\n",
      "512/512 [==============================] - 0s 27us/sample - loss: 1.5273 - acc: 0.5742\n",
      "Epoch 1/20\n",
      "512/512 [==============================] - 0s 364us/sample - loss: 55.7735 - acc: 0.6387\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 0s 82us/sample - loss: 48.0839 - acc: 0.6387\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 0s 81us/sample - loss: 41.0598 - acc: 0.6387\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 0s 135us/sample - loss: 34.4330 - acc: 0.6289\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 0s 125us/sample - loss: 28.9408 - acc: 0.5879\n",
      "Epoch 6/20\n",
      "512/512 [==============================] - 0s 95us/sample - loss: 26.1088 - acc: 0.5254\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 0s 125us/sample - loss: 23.8900 - acc: 0.5234\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 0s 157us/sample - loss: 21.8547 - acc: 0.5293\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 0s 113us/sample - loss: 19.8877 - acc: 0.5273\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 0s 42us/sample - loss: 17.9694 - acc: 0.5332\n",
      "Epoch 11/20\n",
      "512/512 [==============================] - 0s 120us/sample - loss: 16.0569 - acc: 0.5352\n",
      "Epoch 12/20\n",
      "512/512 [==============================] - 0s 79us/sample - loss: 14.3163 - acc: 0.5332\n",
      "Epoch 13/20\n",
      "512/512 [==============================] - 0s 125us/sample - loss: 12.4896 - acc: 0.5254\n",
      "Epoch 14/20\n",
      "512/512 [==============================] - 0s 201us/sample - loss: 10.7811 - acc: 0.5234\n",
      "Epoch 15/20\n",
      "512/512 [==============================] - 0s 162us/sample - loss: 9.1769 - acc: 0.5176\n",
      "Epoch 16/20\n",
      "512/512 [==============================] - 0s 93us/sample - loss: 7.6180 - acc: 0.5137\n",
      "Epoch 17/20\n",
      "512/512 [==============================] - 0s 113us/sample - loss: 6.1819 - acc: 0.5000\n",
      "Epoch 18/20\n",
      "512/512 [==============================] - 0s 54us/sample - loss: 4.9642 - acc: 0.4707\n",
      "Epoch 19/20\n",
      "512/512 [==============================] - 0s 97us/sample - loss: 3.8881 - acc: 0.4727\n",
      "Epoch 20/20\n",
      "512/512 [==============================] - 0s 101us/sample - loss: 3.1826 - acc: 0.4648\n",
      "256/256 [==============================] - 0s 402us/sample - loss: 2.7931 - acc: 0.4961\n",
      "512/512 [==============================] - 0s 188us/sample - loss: 2.8633 - acc: 0.4707\n",
      "Epoch 1/20\n",
      "512/512 [==============================] - 0s 284us/sample - loss: 5.7727 - acc: 0.5977\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 0s 37us/sample - loss: 4.5781 - acc: 0.5293\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 0s 135us/sample - loss: 3.8858 - acc: 0.4844\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 0s 100us/sample - loss: 3.1869 - acc: 0.5039\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 0s 62us/sample - loss: 2.7722 - acc: 0.4961\n",
      "Epoch 6/20\n",
      "512/512 [==============================] - 0s 61us/sample - loss: 2.5142 - acc: 0.5039\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 0s 104us/sample - loss: 2.3377 - acc: 0.5117\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 0s 61us/sample - loss: 2.1710 - acc: 0.5332\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 0s 69us/sample - loss: 2.0679 - acc: 0.5312\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 0s 66us/sample - loss: 1.9601 - acc: 0.5527\n",
      "Epoch 11/20\n",
      "512/512 [==============================] - 0s 203us/sample - loss: 1.8585 - acc: 0.5664\n",
      "Epoch 12/20\n",
      "512/512 [==============================] - 0s 183us/sample - loss: 1.7854 - acc: 0.5527\n",
      "Epoch 13/20\n",
      "512/512 [==============================] - 0s 97us/sample - loss: 1.7150 - acc: 0.5586\n",
      "Epoch 14/20\n",
      "512/512 [==============================] - 0s 54us/sample - loss: 1.6461 - acc: 0.5684\n",
      "Epoch 15/20\n",
      "512/512 [==============================] - 0s 57us/sample - loss: 1.5899 - acc: 0.5684\n",
      "Epoch 16/20\n",
      "512/512 [==============================] - 0s 62us/sample - loss: 1.5404 - acc: 0.5547\n",
      "Epoch 17/20\n",
      "512/512 [==============================] - 0s 98us/sample - loss: 1.4680 - acc: 0.5801\n",
      "Epoch 18/20\n",
      "512/512 [==============================] - 0s 25us/sample - loss: 1.4246 - acc: 0.5703\n",
      "Epoch 19/20\n",
      "512/512 [==============================] - 0s 81us/sample - loss: 1.3617 - acc: 0.5762\n",
      "Epoch 20/20\n",
      "512/512 [==============================] - 0s 64us/sample - loss: 1.3109 - acc: 0.5820\n",
      "256/256 [==============================] - 0s 431us/sample - loss: 1.2665 - acc: 0.6055\n",
      "512/512 [==============================] - 0s 24us/sample - loss: 1.2851 - acc: 0.5684\n",
      "Epoch 1/20\n",
      "512/512 [==============================] - 0s 314us/sample - loss: 25.0511 - acc: 0.3555\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 0s 70us/sample - loss: 20.4390 - acc: 0.3555\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 0s 75us/sample - loss: 16.1791 - acc: 0.3574\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 0s 71us/sample - loss: 11.9413 - acc: 0.3613\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 0s 131us/sample - loss: 8.0832 - acc: 0.3613\n",
      "Epoch 6/20\n",
      "512/512 [==============================] - 0s 63us/sample - loss: 5.5957 - acc: 0.4082\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 0s 94us/sample - loss: 4.6728 - acc: 0.4688\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 0s 78us/sample - loss: 4.0378 - acc: 0.4980\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 0s 74us/sample - loss: 3.5316 - acc: 0.4941\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 0s 128us/sample - loss: 3.1123 - acc: 0.5039\n",
      "Epoch 11/20\n",
      "512/512 [==============================] - 0s 76us/sample - loss: 2.8150 - acc: 0.5234\n",
      "Epoch 12/20\n",
      "512/512 [==============================] - 0s 546us/sample - loss: 2.5619 - acc: 0.5332s - loss: 2.7413 - acc: 0.\n",
      "Epoch 13/20\n",
      "512/512 [==============================] - 0s 91us/sample - loss: 2.3844 - acc: 0.5527\n",
      "Epoch 14/20\n",
      "512/512 [==============================] - 0s 168us/sample - loss: 2.2677 - acc: 0.5566\n",
      "Epoch 15/20\n",
      "512/512 [==============================] - 0s 160us/sample - loss: 2.1566 - acc: 0.5625\n",
      "Epoch 16/20\n",
      "512/512 [==============================] - 0s 92us/sample - loss: 2.0634 - acc: 0.5488\n",
      "Epoch 17/20\n",
      "512/512 [==============================] - 0s 120us/sample - loss: 1.9811 - acc: 0.5352\n",
      "Epoch 18/20\n",
      "512/512 [==============================] - 0s 81us/sample - loss: 1.9045 - acc: 0.5371\n",
      "Epoch 19/20\n",
      "512/512 [==============================] - 0s 69us/sample - loss: 1.8485 - acc: 0.5410\n",
      "Epoch 20/20\n",
      "512/512 [==============================] - 0s 81us/sample - loss: 1.7801 - acc: 0.5352\n",
      "256/256 [==============================] - 0s 1ms/sample - loss: 1.5617 - acc: 0.5820\n",
      "512/512 [==============================] - 0s 21us/sample - loss: 1.7427 - acc: 0.5254\n",
      "Epoch 1/20\n",
      "512/512 [==============================] - 0s 560us/sample - loss: 18.3206 - acc: 0.6309\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 0s 69us/sample - loss: 12.2501 - acc: 0.6309\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 0s 197us/sample - loss: 8.1890 - acc: 0.6445\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 0s 178us/sample - loss: 6.3670 - acc: 0.6328\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 0s 136us/sample - loss: 4.8723 - acc: 0.6250\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 0s 142us/sample - loss: 3.3375 - acc: 0.6289\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 0s 48us/sample - loss: 2.1849 - acc: 0.6270\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 0s 80us/sample - loss: 1.7779 - acc: 0.6465\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 0s 126us/sample - loss: 1.6845 - acc: 0.6270\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 0s 143us/sample - loss: 1.5600 - acc: 0.6367\n",
      "Epoch 11/20\n",
      "512/512 [==============================] - 0s 70us/sample - loss: 1.4634 - acc: 0.6328\n",
      "Epoch 12/20\n",
      "512/512 [==============================] - 0s 134us/sample - loss: 1.4229 - acc: 0.6582\n",
      "Epoch 13/20\n",
      "512/512 [==============================] - 0s 79us/sample - loss: 1.3517 - acc: 0.6348\n",
      "Epoch 14/20\n",
      "512/512 [==============================] - 0s 176us/sample - loss: 1.3018 - acc: 0.6426\n",
      "Epoch 15/20\n",
      "512/512 [==============================] - 0s 124us/sample - loss: 1.2489 - acc: 0.6484\n",
      "Epoch 16/20\n",
      "512/512 [==============================] - 0s 141us/sample - loss: 1.2369 - acc: 0.6152\n",
      "Epoch 17/20\n",
      "512/512 [==============================] - 0s 71us/sample - loss: 1.1879 - acc: 0.6562\n",
      "Epoch 18/20\n",
      "512/512 [==============================] - 0s 85us/sample - loss: 1.1496 - acc: 0.6270\n",
      "Epoch 19/20\n",
      "512/512 [==============================] - 0s 68us/sample - loss: 1.1110 - acc: 0.6328\n",
      "Epoch 20/20\n",
      "512/512 [==============================] - 0s 108us/sample - loss: 1.0822 - acc: 0.6367\n",
      "256/256 [==============================] - 0s 402us/sample - loss: 1.1877 - acc: 0.5156\n",
      "512/512 [==============================] - 0s 25us/sample - loss: 1.0569 - acc: 0.6543\n",
      "Epoch 1/20\n",
      "512/512 [==============================] - 0s 330us/sample - loss: 26.4195 - acc: 0.3418\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 0s 52us/sample - loss: 21.9771 - acc: 0.3477\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 0s 59us/sample - loss: 17.6172 - acc: 0.3652\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 0s 69us/sample - loss: 13.7400 - acc: 0.3750\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 0s 40us/sample - loss: 10.3499 - acc: 0.3496\n",
      "Epoch 6/20\n",
      "512/512 [==============================] - 0s 31us/sample - loss: 7.4536 - acc: 0.3574\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 0s 23us/sample - loss: 5.6517 - acc: 0.4297\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 0s 51us/sample - loss: 5.0077 - acc: 0.4531\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 0s 76us/sample - loss: 4.9235 - acc: 0.5000\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 0s 72us/sample - loss: 4.8098 - acc: 0.5059\n",
      "Epoch 11/20\n",
      "512/512 [==============================] - 0s 54us/sample - loss: 4.6129 - acc: 0.4902\n",
      "Epoch 12/20\n",
      "512/512 [==============================] - 0s 55us/sample - loss: 4.4150 - acc: 0.4727\n",
      "Epoch 13/20\n",
      "512/512 [==============================] - 0s 39us/sample - loss: 4.2418 - acc: 0.4609\n",
      "Epoch 14/20\n",
      "512/512 [==============================] - 0s 35us/sample - loss: 4.1041 - acc: 0.4570\n",
      "Epoch 15/20\n",
      "512/512 [==============================] - 0s 28us/sample - loss: 3.9594 - acc: 0.4609\n",
      "Epoch 16/20\n",
      "512/512 [==============================] - 0s 21us/sample - loss: 3.8255 - acc: 0.4629\n",
      "Epoch 17/20\n",
      "512/512 [==============================] - 0s 74us/sample - loss: 3.6992 - acc: 0.4629\n",
      "Epoch 18/20\n",
      "512/512 [==============================] - 0s 36us/sample - loss: 3.5894 - acc: 0.4453\n",
      "Epoch 19/20\n",
      "512/512 [==============================] - 0s 24us/sample - loss: 3.4625 - acc: 0.4434\n",
      "Epoch 20/20\n",
      "512/512 [==============================] - 0s 60us/sample - loss: 3.3480 - acc: 0.4434\n",
      "256/256 [==============================] - 0s 380us/sample - loss: 2.9527 - acc: 0.4609\n",
      "512/512 [==============================] - 0s 26us/sample - loss: 3.2844 - acc: 0.4414\n",
      "Epoch 1/20\n",
      "512/512 [==============================] - 0s 323us/sample - loss: 37.4778 - acc: 0.6465\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 0s 33us/sample - loss: 34.0786 - acc: 0.6465\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 0s 76us/sample - loss: 30.7676 - acc: 0.6465\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 0s 43us/sample - loss: 27.4982 - acc: 0.6465\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 0s 89us/sample - loss: 24.5557 - acc: 0.6465\n",
      "Epoch 6/20\n",
      "512/512 [==============================] - 0s 64us/sample - loss: 21.4100 - acc: 0.6465\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 0s 206us/sample - loss: 18.4669 - acc: 0.6465\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 0s 69us/sample - loss: 15.4243 - acc: 0.6465\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 0s 114us/sample - loss: 11.9162 - acc: 0.6465\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 0s 61us/sample - loss: 8.4678 - acc: 0.6309\n",
      "Epoch 11/20\n",
      "512/512 [==============================] - 0s 53us/sample - loss: 6.2276 - acc: 0.4980\n",
      "Epoch 12/20\n",
      "512/512 [==============================] - 0s 68us/sample - loss: 6.1568 - acc: 0.4453\n",
      "Epoch 13/20\n",
      "512/512 [==============================] - 0s 81us/sample - loss: 5.8996 - acc: 0.4434\n",
      "Epoch 14/20\n",
      "512/512 [==============================] - 0s 28us/sample - loss: 5.3451 - acc: 0.4492\n",
      "Epoch 15/20\n",
      "512/512 [==============================] - 0s 59us/sample - loss: 4.8990 - acc: 0.4785\n",
      "Epoch 16/20\n",
      "512/512 [==============================] - 0s 55us/sample - loss: 4.6293 - acc: 0.4805\n",
      "Epoch 17/20\n",
      "512/512 [==============================] - 0s 50us/sample - loss: 4.3082 - acc: 0.4863\n",
      "Epoch 18/20\n",
      "512/512 [==============================] - 0s 101us/sample - loss: 4.0102 - acc: 0.4746\n",
      "Epoch 19/20\n",
      "512/512 [==============================] - 0s 89us/sample - loss: 3.7458 - acc: 0.4688\n",
      "Epoch 20/20\n",
      "512/512 [==============================] - 0s 49us/sample - loss: 3.4929 - acc: 0.4785\n",
      "256/256 [==============================] - 0s 552us/sample - loss: 3.5518 - acc: 0.4844\n",
      "512/512 [==============================] - 0s 31us/sample - loss: 3.3396 - acc: 0.4883\n",
      "Epoch 1/20\n",
      "512/512 [==============================] - 0s 717us/sample - loss: 19.5688 - acc: 0.6387\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 0s 56us/sample - loss: 16.3002 - acc: 0.6406\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 0s 56us/sample - loss: 13.1021 - acc: 0.6211\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 0s 76us/sample - loss: 10.2085 - acc: 0.5996\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 0s 67us/sample - loss: 7.8705 - acc: 0.5508\n",
      "Epoch 6/20\n",
      "512/512 [==============================] - 0s 83us/sample - loss: 6.3514 - acc: 0.4941\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 0s 40us/sample - loss: 5.5307 - acc: 0.4336\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 0s 92us/sample - loss: 4.9850 - acc: 0.4160\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 0s 62us/sample - loss: 4.3846 - acc: 0.4297\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 0s 42us/sample - loss: 3.8140 - acc: 0.4395\n",
      "Epoch 11/20\n",
      "512/512 [==============================] - 0s 198us/sample - loss: 3.3647 - acc: 0.4668\n",
      "Epoch 12/20\n",
      "512/512 [==============================] - 0s 61us/sample - loss: 2.9803 - acc: 0.4941\n",
      "Epoch 13/20\n",
      "512/512 [==============================] - 0s 83us/sample - loss: 2.6771 - acc: 0.5078\n",
      "Epoch 14/20\n",
      "512/512 [==============================] - 0s 389us/sample - loss: 2.4497 - acc: 0.5215\n",
      "Epoch 15/20\n",
      "512/512 [==============================] - 0s 111us/sample - loss: 2.2977 - acc: 0.5547\n",
      "Epoch 16/20\n",
      "512/512 [==============================] - 0s 127us/sample - loss: 2.1647 - acc: 0.5723\n",
      "Epoch 17/20\n",
      "512/512 [==============================] - 0s 106us/sample - loss: 2.0446 - acc: 0.5723\n",
      "Epoch 18/20\n",
      "512/512 [==============================] - 0s 55us/sample - loss: 1.9470 - acc: 0.5801\n",
      "Epoch 19/20\n",
      "512/512 [==============================] - 0s 130us/sample - loss: 1.8792 - acc: 0.6016\n",
      "Epoch 20/20\n",
      "512/512 [==============================] - 0s 63us/sample - loss: 1.8148 - acc: 0.6113\n",
      "256/256 [==============================] - 0s 693us/sample - loss: 1.9458 - acc: 0.5781\n",
      "512/512 [==============================] - 0s 19us/sample - loss: 1.7767 - acc: 0.6172\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 0s 360us/sample - loss: 10.1789 - acc: 0.5137\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 0s 24us/sample - loss: 9.2615 - acc: 0.5234\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 0s 51us/sample - loss: 8.4033 - acc: 0.5215\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 0s 42us/sample - loss: 7.6051 - acc: 0.5195\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 0s 40us/sample - loss: 7.0339 - acc: 0.5098\n",
      "Epoch 6/20\n",
      "512/512 [==============================] - 0s 34us/sample - loss: 6.3665 - acc: 0.5098\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 0s 40us/sample - loss: 5.8305 - acc: 0.5352\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 0s 19us/sample - loss: 5.4404 - acc: 0.5371\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 0s 62us/sample - loss: 5.0507 - acc: 0.5410\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 0s 55us/sample - loss: 4.7559 - acc: 0.5430\n",
      "Epoch 11/20\n",
      "512/512 [==============================] - 0s 58us/sample - loss: 4.5047 - acc: 0.5410\n",
      "Epoch 12/20\n",
      "512/512 [==============================] - 0s 67us/sample - loss: 4.2085 - acc: 0.5215\n",
      "Epoch 13/20\n",
      "512/512 [==============================] - 0s 32us/sample - loss: 4.0018 - acc: 0.5195\n",
      "Epoch 14/20\n",
      "512/512 [==============================] - 0s 28us/sample - loss: 3.7934 - acc: 0.5332\n",
      "Epoch 15/20\n",
      "512/512 [==============================] - 0s 83us/sample - loss: 3.6456 - acc: 0.5488\n",
      "Epoch 16/20\n",
      "512/512 [==============================] - 0s 104us/sample - loss: 3.4533 - acc: 0.5527\n",
      "Epoch 17/20\n",
      "512/512 [==============================] - 0s 82us/sample - loss: 3.3634 - acc: 0.5293\n",
      "Epoch 18/20\n",
      "512/512 [==============================] - 0s 96us/sample - loss: 3.2097 - acc: 0.5293\n",
      "Epoch 19/20\n",
      "512/512 [==============================] - 0s 95us/sample - loss: 3.0626 - acc: 0.5547\n",
      "Epoch 20/20\n",
      "512/512 [==============================] - 0s 57us/sample - loss: 2.9310 - acc: 0.5762\n",
      "256/256 [==============================] - 0s 634us/sample - loss: 3.6507 - acc: 0.5625\n",
      "512/512 [==============================] - 0s 75us/sample - loss: 2.8612 - acc: 0.5820\n",
      "Epoch 1/20\n",
      "512/512 [==============================] - 0s 395us/sample - loss: 11.3541 - acc: 0.4199\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 0s 22us/sample - loss: 8.3462 - acc: 0.4434\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 0s 77us/sample - loss: 6.3830 - acc: 0.5078\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 0s 61us/sample - loss: 5.4247 - acc: 0.5352\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 0s 60us/sample - loss: 5.1030 - acc: 0.5801\n",
      "Epoch 6/20\n",
      "512/512 [==============================] - 0s 59us/sample - loss: 4.8320 - acc: 0.6016\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 0s 61us/sample - loss: 4.4386 - acc: 0.5820\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 0s 103us/sample - loss: 4.0217 - acc: 0.5781\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 0s 54us/sample - loss: 3.7696 - acc: 0.5684\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 0s 64us/sample - loss: 3.5689 - acc: 0.5605\n",
      "Epoch 11/20\n",
      "512/512 [==============================] - 0s 80us/sample - loss: 3.3564 - acc: 0.5605\n",
      "Epoch 12/20\n",
      "512/512 [==============================] - 0s 78us/sample - loss: 3.1418 - acc: 0.5605\n",
      "Epoch 13/20\n",
      "512/512 [==============================] - 0s 56us/sample - loss: 2.9162 - acc: 0.5547\n",
      "Epoch 14/20\n",
      "512/512 [==============================] - 0s 27us/sample - loss: 2.6865 - acc: 0.5586\n",
      "Epoch 15/20\n",
      "512/512 [==============================] - 0s 43us/sample - loss: 2.4848 - acc: 0.5742\n",
      "Epoch 16/20\n",
      "512/512 [==============================] - 0s 71us/sample - loss: 2.3017 - acc: 0.5645\n",
      "Epoch 17/20\n",
      "512/512 [==============================] - 0s 80us/sample - loss: 2.1635 - acc: 0.5566\n",
      "Epoch 18/20\n",
      "512/512 [==============================] - 0s 207us/sample - loss: 2.0087 - acc: 0.5645\n",
      "Epoch 19/20\n",
      "512/512 [==============================] - 0s 158us/sample - loss: 1.8956 - acc: 0.5977\n",
      "Epoch 20/20\n",
      "512/512 [==============================] - 0s 99us/sample - loss: 1.8040 - acc: 0.5898\n",
      "256/256 [==============================] - 0s 589us/sample - loss: 1.9302 - acc: 0.6016\n",
      "512/512 [==============================] - 0s 38us/sample - loss: 1.7372 - acc: 0.5898\n",
      "Epoch 1/20\n",
      "512/512 [==============================] - 0s 393us/sample - loss: 11.0423 - acc: 0.6387\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 0s 28us/sample - loss: 8.8223 - acc: 0.6328\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 0s 39us/sample - loss: 6.9140 - acc: 0.5645\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 0s 65us/sample - loss: 5.7283 - acc: 0.5059\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 0s 82us/sample - loss: 5.2071 - acc: 0.4688\n",
      "Epoch 6/20\n",
      "512/512 [==============================] - 0s 88us/sample - loss: 4.9187 - acc: 0.4180\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 0s 93us/sample - loss: 4.5890 - acc: 0.4258\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 0s 97us/sample - loss: 4.2074 - acc: 0.4512\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 0s 91us/sample - loss: 3.8631 - acc: 0.4727\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 0s 80us/sample - loss: 3.5362 - acc: 0.4941\n",
      "Epoch 11/20\n",
      "512/512 [==============================] - 0s 70us/sample - loss: 3.2320 - acc: 0.4922\n",
      "Epoch 12/20\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 2.9218 - acc: 0.5000\n",
      "Epoch 13/20\n",
      "512/512 [==============================] - 0s 126us/sample - loss: 2.6573 - acc: 0.5254\n",
      "Epoch 14/20\n",
      "512/512 [==============================] - 0s 61us/sample - loss: 2.4157 - acc: 0.5449\n",
      "Epoch 15/20\n",
      "512/512 [==============================] - 0s 42us/sample - loss: 2.1785 - acc: 0.5488\n",
      "Epoch 16/20\n",
      "512/512 [==============================] - 0s 110us/sample - loss: 1.9929 - acc: 0.5527\n",
      "Epoch 17/20\n",
      "512/512 [==============================] - 0s 25us/sample - loss: 1.8378 - acc: 0.5664\n",
      "Epoch 18/20\n",
      "512/512 [==============================] - 0s 43us/sample - loss: 1.7340 - acc: 0.5879\n",
      "Epoch 19/20\n",
      "512/512 [==============================] - 0s 47us/sample - loss: 1.6440 - acc: 0.5977\n",
      "Epoch 20/20\n",
      "512/512 [==============================] - 0s 68us/sample - loss: 1.5579 - acc: 0.6074\n",
      "256/256 [==============================] - 0s 453us/sample - loss: 1.6745 - acc: 0.5664\n",
      "512/512 [==============================] - 0s 16us/sample - loss: 1.5162 - acc: 0.6133\n",
      "Epoch 1/20\n",
      "768/768 [==============================] - 1s 851us/sample - loss: 6.8439 - acc: 0.4883\n",
      "Epoch 2/20\n",
      "768/768 [==============================] - 0s 440us/sample - loss: 3.3041 - acc: 0.5312\n",
      "Epoch 3/20\n",
      "768/768 [==============================] - 0s 278us/sample - loss: 2.7301 - acc: 0.5495\n",
      "Epoch 4/20\n",
      "768/768 [==============================] - 0s 357us/sample - loss: 2.4867 - acc: 0.5599\n",
      "Epoch 5/20\n",
      "768/768 [==============================] - 0s 197us/sample - loss: 2.0644 - acc: 0.5807\n",
      "Epoch 6/20\n",
      "768/768 [==============================] - 0s 203us/sample - loss: 1.6374 - acc: 0.5938\n",
      "Epoch 7/20\n",
      "768/768 [==============================] - 0s 267us/sample - loss: 1.4005 - acc: 0.6159\n",
      "Epoch 8/20\n",
      "768/768 [==============================] - 0s 288us/sample - loss: 1.3747 - acc: 0.6393\n",
      "Epoch 9/20\n",
      "768/768 [==============================] - 0s 297us/sample - loss: 1.2593 - acc: 0.6198\n",
      "Epoch 10/20\n",
      "768/768 [==============================] - 0s 256us/sample - loss: 1.2094 - acc: 0.6237\n",
      "Epoch 11/20\n",
      "768/768 [==============================] - 0s 268us/sample - loss: 1.1324 - acc: 0.6367\n",
      "Epoch 12/20\n",
      "768/768 [==============================] - 0s 247us/sample - loss: 1.1397 - acc: 0.6328\n",
      "Epoch 13/20\n",
      "768/768 [==============================] - 0s 297us/sample - loss: 1.0910 - acc: 0.6211\n",
      "Epoch 14/20\n",
      "768/768 [==============================] - 0s 244us/sample - loss: 1.0295 - acc: 0.6237\n",
      "Epoch 15/20\n",
      "768/768 [==============================] - 0s 362us/sample - loss: 1.0629 - acc: 0.6094\n",
      "Epoch 16/20\n",
      "768/768 [==============================] - 0s 244us/sample - loss: 1.0255 - acc: 0.6094\n",
      "Epoch 17/20\n",
      "768/768 [==============================] - 0s 256us/sample - loss: 1.0014 - acc: 0.6315\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 184us/sample - loss: 0.9736 - acc: 0.6302\n",
      "Epoch 19/20\n",
      "768/768 [==============================] - 0s 165us/sample - loss: 0.9192 - acc: 0.6276\n",
      "Epoch 20/20\n",
      "768/768 [==============================] - 0s 161us/sample - loss: 0.9641 - acc: 0.6276\n",
      "Best: 0.6184895833333334 using {'batch_size': 10, 'epochs': 20}\n",
      "Means: 0.6184895833333334, Stdev: 0.014381980491129247 with: {'batch_size': 10, 'epochs': 20}\n",
      "Means: 0.6028645833333334, Stdev: 0.0433028363237393 with: {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.5520833333333334, Stdev: 0.03962271889245557 with: {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.5677083333333334, Stdev: 0.038051143740110566 with: {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.5078125, Stdev: 0.05063078670631141 with: {'batch_size': 80, 'epochs': 20}\n",
      "Means: 0.5768229166666666, Stdev: 0.017566064535458385 with: {'batch_size': 100, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "url =\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "\n",
    "dataset = pd.read_csv(url, header=None).values\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "              'epochs': [20]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
