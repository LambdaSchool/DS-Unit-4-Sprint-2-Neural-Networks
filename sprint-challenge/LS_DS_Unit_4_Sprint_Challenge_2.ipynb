{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:**\n",
    "A Neuron is a node in a neural network. It takes the sum of the weighted outputs of the previous layer and outputs a transformed value.\n",
    "- **Input Layer:**\n",
    "The input layer provides the points of entry to the network. There is one node for each input, or feature, of the data set.\n",
    "- **Hidden Layer:**\n",
    "A hidden layer is any layer between the input layer and the output layer in a neural network. It can have an arbitrary number of nodes.\n",
    "- **Output Layer:**\n",
    "The output layer provides the results of the network. For classification models, the output will have one node for each class. A regression model will have one output node for the numerical prediction.\n",
    "- **Activation:**\n",
    "After a node receives the sum of the weighted outputs it passes it through an activation, or transform, function before sending the output to the next layer.\n",
    "- **Backpropagation:**\n",
    "Backpropagation is used to update the weights by calculating a gradient. The gradient gives a direction in which to adjust the weight. The gradient and the error determine how much to move the weight. This is were the learning takes place and it happens after every batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>gummy</th>\n",
       "      <th>ate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chocolate  gummy  ate\n",
       "0          0      1    1\n",
       "1          1      0    1\n",
       "2          0      1    1\n",
       "3          0      0    0\n",
       "4          1      1    0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(candy.shape)\n",
    "candy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why you could not achieve a higher accuracy with a *simple perceptron*. It's possible to achieve ~95% accuracy on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 2), (10000, 1))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start your candy perceptron here\n",
    "\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy[['ate']].values\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    " \n",
    "\n",
    "    def __init__(self, niter=10):\n",
    "        self.niter = niter\n",
    "        np.random.seed(42)\n",
    "        self.weights = 2 * np.random.random((2, 1)) - 1\n",
    "  \n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        sx = self.sigmoid(x)\n",
    "        return sx * (1-sx)\n",
    "\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        for i in range(self.niter):\n",
    "            # Weighted sum of inputs / weights\n",
    "            weighted_sum = np.dot(X, self.weights)\n",
    "            # Activate\n",
    "            activated_output = self.sigmoid(weighted_sum)\n",
    "            # Calculate error\n",
    "            error = y - activated_output\n",
    "            adjustments = error * self.sigmoid_derivative(activated_output)\n",
    "            # Update the Weights\n",
    "            self.weights += np.dot(X.T, adjustments)\n",
    "            \n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Weighted sum of inputs / weights\n",
    "        weighted_sum = np.dot(X, self.weights)\n",
    "        predictions = np.round(self.sigmoid(weighted_sum))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for a single layer perceptron: 0.7229\n"
     ]
    }
   ],
   "source": [
    "yuck = Perceptron()\n",
    "yuck.train(X, y)\n",
    "yuck_score = accuracy_score(y, yuck.predict(X))\n",
    "print(f'Accuracy for a single layer perceptron: {yuck_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Limitations With a Simple Perceptron\n",
    "Imagine a unit square. At (0,0) and (1,1) the output, or height, should be 0 (95 percent of the time). Conversely, at the other two diagonal points - (1, 0) and (0,1) - the height should be 1. It is impossible to construct a single plane that is both above the positive points and below the zero points. The two conditions are not linearly separable, which is a requirement for this type of model to give useful results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 8\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        np.random.seed(42)\n",
    "        # 2x3 Matrix Array for the First Layer\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
    "       \n",
    "        # 3x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weighted sum of inputs => hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "        \n",
    "    def backward(self, X, y, o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        \n",
    "        # Error in Output\n",
    "        self.o_error = y - o\n",
    "        \n",
    "        # Apply Derivative of Sigmoid to error\n",
    "        # How far off are we in relation to the Sigmoid f(x) of the output\n",
    "        # ^- aka hidden => output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        # z2 error\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        # How much of that \"far off\" can explained by the input => hidden\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        # Adjustment to first set of weights (input => hidden)\n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        # Adjustment to second set of weights (hidden => output)\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "        \n",
    "\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = np.round(self.feed_forward(X) * 2)\n",
    "        return np.round(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "# train a neurll net\n",
    "yum = NeuralNetwork()\n",
    "for i in range(10000):\n",
    "    yum.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for a single layer perceptron: 0.7229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "predictions = yum.predict(X)\n",
    "yum_score = accuracy_score(y, yuck.predict(X))\n",
    "print(f'Accuracy for a single layer perceptron: {yum_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP performance\n",
    "Theoretically, this model should be able to handle non-linear relationships and give a better accuracy score than the previous model. Theoretically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "from category_encoders import OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# fix random seed for reproducibility (lol)\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>276</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>170</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>134</td>\n",
       "      <td>409</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "      <td>265</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "179   57    1   0       150   276    0        0      112      1      0.6   \n",
       "228   59    1   3       170   288    0        0      159      0      0.2   \n",
       "111   57    1   2       150   126    1        1      173      0      0.2   \n",
       "246   56    0   0       134   409    0        0      150      1      1.9   \n",
       "60    71    0   2       110   265    1        0      130      0      0.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "179      1   1     1       0  \n",
       "228      1   0     3       0  \n",
       "111      2   1     3       1  \n",
       "246      1   2     3       0  \n",
       "60       2   1     2       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/r-m-mauney/heart-disease/runs/p5l9wt88\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
       "            in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "W&B Run: https://app.wandb.ai/r-m-mauney/heart-disease/runs/p5l9wt88"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"heart-disease\", entity='r-m-mauney')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((303, 13), (303, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(columns = 'target').to_numpy()\n",
    "y = df['target'].to_numpy().reshape(-1, 1)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5445544554455446"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Majority Classifier\n",
    "sum(y)[0] / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "num_inputs = X.shape[1]\n",
    "\n",
    "def create_model(optimizer='adam', hidden_1_nodes=13):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_1_nodes, input_dim=num_inputs, activation='relu'))\n",
    "    model.add(Dense(7, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203 samples, validate on 100 samples\n",
      "Epoch 1/20\n",
      "203/203 [==============================] - 1s 6ms/sample - loss: 13.1979 - accuracy: 0.4581 - val_loss: 11.1583 - val_accuracy: 0.4500\n",
      "Epoch 2/20\n",
      "203/203 [==============================] - 0s 411us/sample - loss: 11.0659 - accuracy: 0.4581 - val_loss: 9.1337 - val_accuracy: 0.4500\n",
      "Epoch 3/20\n",
      "203/203 [==============================] - 0s 424us/sample - loss: 9.0714 - accuracy: 0.4631 - val_loss: 7.2824 - val_accuracy: 0.4700\n",
      "Epoch 4/20\n",
      "203/203 [==============================] - 0s 431us/sample - loss: 7.2592 - accuracy: 0.4631 - val_loss: 5.7418 - val_accuracy: 0.4900\n",
      "Epoch 5/20\n",
      "203/203 [==============================] - 0s 456us/sample - loss: 5.6939 - accuracy: 0.4975 - val_loss: 4.4576 - val_accuracy: 0.5400\n",
      "Epoch 6/20\n",
      "203/203 [==============================] - 0s 422us/sample - loss: 4.2942 - accuracy: 0.5172 - val_loss: 3.5345 - val_accuracy: 0.5200\n",
      "Epoch 7/20\n",
      "203/203 [==============================] - 0s 675us/sample - loss: 3.3037 - accuracy: 0.5616 - val_loss: 3.0236 - val_accuracy: 0.5700\n",
      "Epoch 8/20\n",
      "203/203 [==============================] - 0s 405us/sample - loss: 2.7543 - accuracy: 0.5961 - val_loss: 2.6621 - val_accuracy: 0.6400\n",
      "Epoch 9/20\n",
      "203/203 [==============================] - 0s 452us/sample - loss: 2.3856 - accuracy: 0.6207 - val_loss: 2.3927 - val_accuracy: 0.6200\n",
      "Epoch 10/20\n",
      "203/203 [==============================] - 0s 419us/sample - loss: 2.1178 - accuracy: 0.6404 - val_loss: 2.2530 - val_accuracy: 0.6300\n",
      "Epoch 11/20\n",
      "203/203 [==============================] - 0s 404us/sample - loss: 1.9987 - accuracy: 0.6404 - val_loss: 2.1752 - val_accuracy: 0.6200\n",
      "Epoch 12/20\n",
      "203/203 [==============================] - 0s 397us/sample - loss: 1.9261 - accuracy: 0.6404 - val_loss: 2.1237 - val_accuracy: 0.6200\n",
      "Epoch 13/20\n",
      "203/203 [==============================] - 0s 392us/sample - loss: 1.8855 - accuracy: 0.6502 - val_loss: 2.0924 - val_accuracy: 0.6200\n",
      "Epoch 14/20\n",
      "203/203 [==============================] - 0s 395us/sample - loss: 1.8623 - accuracy: 0.6552 - val_loss: 2.0456 - val_accuracy: 0.5800\n",
      "Epoch 15/20\n",
      "203/203 [==============================] - 0s 537us/sample - loss: 1.8185 - accuracy: 0.6552 - val_loss: 2.0018 - val_accuracy: 0.5800\n",
      "Epoch 16/20\n",
      "203/203 [==============================] - 0s 500us/sample - loss: 1.7935 - accuracy: 0.6601 - val_loss: 1.9824 - val_accuracy: 0.5800\n",
      "Epoch 17/20\n",
      "203/203 [==============================] - 0s 466us/sample - loss: 1.7787 - accuracy: 0.6700 - val_loss: 1.9651 - val_accuracy: 0.5800\n",
      "Epoch 18/20\n",
      "203/203 [==============================] - 0s 438us/sample - loss: 1.7668 - accuracy: 0.6650 - val_loss: 1.9502 - val_accuracy: 0.5600\n",
      "Epoch 19/20\n",
      "203/203 [==============================] - 0s 379us/sample - loss: 1.7699 - accuracy: 0.6552 - val_loss: 1.9422 - val_accuracy: 0.5800\n",
      "Epoch 20/20\n",
      "203/203 [==============================] - 0s 378us/sample - loss: 1.7570 - accuracy: 0.6650 - val_loss: 1.9316 - val_accuracy: 0.5800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f72ac5a0518>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline Keras model\n",
    "\n",
    "wandb.config.epochs = 20\n",
    "wandb.config.batch_size = 101\n",
    "\n",
    "model.fit(X, y, \n",
    "          validation_split=0.33, \n",
    "          epochs=wandb.config.epochs, \n",
    "          batch_size=wandb.config.batch_size,\n",
    "          verbose=True,\n",
    "          callbacks=[WandbCallback()]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.712871273358663 using {'batch_size': 3}\n",
      "Mean: 0.712871273358663, Stdev: 0.08440074111595575 with: {'batch_size': 3}\n",
      "Mean: 0.561056117216746, Stdev: 0.03987802763223187 with: {'batch_size': 101}\n",
      "Mean: 0.5577557881673177, Stdev: 0.12688042313253176 with: {'batch_size': 303}\n",
      "CPU times: user 31.2 s, sys: 3.58 s, total: 34.8 s\n",
      "Wall time: 28.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define the grid search parameters\n",
    "# start with batch size\n",
    "wandb.config.epochs = 20\n",
    "\n",
    "param_grid = {'batch_size': [3, 101, 303]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=1)\n",
    "grid_result = grid.fit(X, y,\n",
    "                       epochs=wandb.config.epochs,\n",
    "                       verbose=False,\n",
    "                       callbacks=[WandbCallback()]\n",
    "                      )\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Mean: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.813650). Check your callbacks.\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.406840). Check your callbacks.\n",
      "Best: 0.6237623691558838 using {'batch_size': 3, 'epochs': 100}\n",
      "Mean: 0.6204620401064554, Stdev: 0.03645332582644608 with: {'batch_size': 3, 'epochs': 20}\n",
      "Mean: 0.5973597566286722, Stdev: 0.09779229135042866 with: {'batch_size': 3, 'epochs': 50}\n",
      "Mean: 0.6237623691558838, Stdev: 0.03523787151819811 with: {'batch_size': 3, 'epochs': 100}\n",
      "CPU times: user 12.9 s, sys: 1.1 s, total: 14 s\n",
      "Wall time: 22.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define the grid search parameters\n",
    "# second, try epoch size\n",
    "# wandb.config.batch_size = 3\n",
    "\n",
    "param_grid = {'batch_size': [3],\n",
    "              'epochs': [20, 50, 100]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=1)\n",
    "grid_result = grid.fit(X, y,\n",
    "                       verbose=False,\n",
    "                       callbacks=[WandbCallback()]\n",
    "                      )\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Mean: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.829567). Check your callbacks.\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.414798). Check your callbacks.\n",
      "Best: 0.7656765580177307 using {'batch_size': 3, 'epochs': 100, 'optimizer': 'Adamax'}\n",
      "Mean: 0.5445544719696045, Stdev: 0.03233648861753595 with: {'batch_size': 3, 'epochs': 100, 'optimizer': 'SGD'}\n",
      "Mean: 0.7557755708694458, Stdev: 0.07335350674429997 with: {'batch_size': 3, 'epochs': 100, 'optimizer': 'RMSprop'}\n",
      "Mean: 0.5643564263979594, Stdev: 0.1050936160841007 with: {'batch_size': 3, 'epochs': 100, 'optimizer': 'Adagrad'}\n",
      "Mean: 0.5412541329860687, Stdev: 0.03645333841793224 with: {'batch_size': 3, 'epochs': 100, 'optimizer': 'Adadelta'}\n",
      "Mean: 0.6798679828643799, Stdev: 0.07335350138082229 with: {'batch_size': 3, 'epochs': 100, 'optimizer': 'Adam'}\n",
      "Mean: 0.7656765580177307, Stdev: 0.05197366159998095 with: {'batch_size': 3, 'epochs': 100, 'optimizer': 'Adamax'}\n",
      "Mean: 0.7293729384740194, Stdev: 0.07335350495647412 with: {'batch_size': 3, 'epochs': 100, 'optimizer': 'Nadam'}\n",
      "CPU times: user 11min 4s, sys: 1min 19s, total: 12min 24s\n",
      "Wall time: 8min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define the grid search parameters\n",
    "# third, optimizers\n",
    "# wandb.config.epochs = 100\n",
    "# wandb.config.batch_size = 101\n",
    "\n",
    "param_grid = {'batch_size': [3],\n",
    "              'epochs': [100],\n",
    "              'optimizer': ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=1)\n",
    "grid_result = grid.fit(X, y,\n",
    "                       verbose=False,\n",
    "                       callbacks=[WandbCallback()]\n",
    "                      )\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Mean: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.863559). Check your callbacks.\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.431796). Check your callbacks.\n",
      "Best: 0.9458000021457672 using {'batch_size': 3, 'epochs': 100, 'hidden_1_nodes': 7, 'optimizer': 'Adamax'}\n",
      "Mean: 0.9458000021457672, Stdev: 0.005441949973255031 with: {'batch_size': 3, 'epochs': 100, 'hidden_1_nodes': 7, 'optimizer': 'Adamax'}\n",
      "Mean: 0.9458000021457672, Stdev: 0.005441949973255031 with: {'batch_size': 3, 'epochs': 100, 'hidden_1_nodes': 13, 'optimizer': 'Adamax'}\n",
      "Mean: 0.9458000021457672, Stdev: 0.005441949973255031 with: {'batch_size': 3, 'epochs': 100, 'hidden_1_nodes': 26, 'optimizer': 'Adamax'}\n",
      "CPU times: user 2h 27min 44s, sys: 17min 44s, total: 2h 45min 28s\n",
      "Wall time: 1h 42min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define the grid search parameters\n",
    "# fourth, number of nodes in first hidden layer\n",
    "\n",
    "param_grid = {'batch_size': [3],\n",
    "              'epochs': [100],\n",
    "              'optimizer': ['Adamax'],\n",
    "              'hidden_1_nodes' : [7, 13, 26]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=1)\n",
    "grid_result = grid.fit(X, y,\n",
    "                       verbose=False,\n",
    "                       callbacks=[WandbCallback()]\n",
    "                      )\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Mean: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best hyperparameters\n",
    "\n",
    "batch size: 3\n",
    "\n",
    "epochs: 100\n",
    "\n",
    "optimizer: Adamax\n",
    "\n",
    "nodes for first hidden layer: 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
