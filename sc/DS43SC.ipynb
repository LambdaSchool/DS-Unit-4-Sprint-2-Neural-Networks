{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6SKlgYrpcym"
   },
   "source": [
    "# Neural Networks Sprint Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrEbRrjVphPM"
   },
   "source": [
    "## 1) Define the following terms:\n",
    "\n",
    "- Neuron\n",
    "- Input Layer\n",
    "- Hidden Layer\n",
    "- Output Layer\n",
    "- Activation\n",
    "- Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5EksLqnp4oB"
   },
   "source": [
    "#### Neuron:\n",
    "or a perceptron, is a basic unit of a neural network, it has several inputs, for each input there is a weight (weight of that spesific connection). When the artifitial neuron activates, it computes its state, by adding all the incoming inputs multiplied by its corresponding connection weight. After computing its state the neuron passes it through its activation function, which normalizes the result. (between 0:1, -1:1, or only +). The neuron / perceptron consists of 4 parts:\n",
    "Input values or One input layer\n",
    "Weights and Bias\n",
    "Net sum\n",
    "Activation Function\n",
    "\n",
    "#### Input Layer:\n",
    "is also called a visible layer, since we see and interact with it. This is where we feed a dataset into Neural Networks. The input layer passes the data directly to the first hidden layer where the data is multiplied by the first hidden layer's weights. Also, the input layer might have its own weights that multiply the incoming data.\n",
    "\n",
    "\n",
    "#### Hidden Layer:\n",
    "is a layer which transforms inputs from the previous layer into something that the output layer can use. A feed forward neural network applies a series of functions to the data. The exact function will depend on the neural network (for ex., it can be a linear transformation of the previous layer, followed by a squashing nonlinearity, or computing logical functions). This layer is responsible extracting the required features from the input data.\n",
    "\n",
    "#### Output Layer:\n",
    "The output layer of the neural network collects and transmits the information accordingly in way it has been designed to give. The pattern presented by the output layer can be directly traced back to the input layer.\n",
    "\n",
    "#### Activation:\n",
    "The activation is the result of applying activation function to a weighted sum of inputs in a neuron. The activation function is the non linear transformation that we do over weighted sum of inputs. This activation is then sent to the next layer of neurons as input. There are several activation functions, such as sigmoid, tahn, relu, LeakyRelU, softmax, and others.\n",
    "\n",
    "#### Backpropagation:\n",
    "is short for 'Backwards propagation of errors' and refers to the process/algorithm for how weights in NN are updated in reverse at the end of each training epoch. The weights are updated by comparing the desired and actual output of NN.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ri_gRA2Jp728"
   },
   "source": [
    "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 1  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.02192721],\n",
       "       [ 3.03264893],\n",
       "       [-4.84302163]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "#AND Gate data:\n",
    "X = np.array([[1,0,1],\n",
    "            [1,1,1],\n",
    "            [0,0,1],\n",
    "            [0,1,1]])\n",
    "y = np.array([1,1,0,0])\n",
    "\n",
    "def sigmoid(X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "def sigmoid_derivative(X):\n",
    "    return sigmoid(X) * (1 - sigmoid(X))    \n",
    "\n",
    "\n",
    "class Perceptron():\n",
    "    \"\"\"\n",
    "    Perceptron class for binary classification\n",
    "    \"\"\"\n",
    "    def __init__(self, n_iter=100):\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "        \n",
    "    def sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "    def sigmoid_derivative(self, X):\n",
    "        return sigmoid(X) * (1 - sigmoid(X))    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # initialize weights and cost list\n",
    "        self.weights = 2 * np.random.random((3,1)) - 1\n",
    "        # iterate \n",
    "        for i in range(self.n_iter):\n",
    "            # Weighted sum of inputs and weights\n",
    "            weighted_sum = np.dot(inputs, self.weights)\n",
    "\n",
    "            # Activate with sigmoid function\n",
    "            activated_output = sigmoid(weighted_sum)\n",
    "\n",
    "            # Calculate Error\n",
    "            error = correct_outputs - activated_output\n",
    "\n",
    "            # Calculate weight adjustments with sigmoid_derivative\n",
    "            adjustments = error * sigmoid_derivative(activated_output)\n",
    "\n",
    "            # Update weights\n",
    "            self.weights += np.dot(inputs.T, adjustments)\n",
    "            \n",
    "            \n",
    "        return self\n",
    "    \n",
    "#Fitting AND gate:\n",
    "ppn = Perceptron(n_iter=100)\n",
    "ppn.fit(X, y)\n",
    "ppn.weights     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86HyRi8Osr3U"
   },
   "source": [
    "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
    "- Your network must have one hidden layer. \n",
    "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "- Train your model on the Heart Disease dataset from UCI:\n",
    "\n",
    "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
    "\n",
    "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNfiajv3v4Ed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      "age         303 non-null int64\n",
      "sex         303 non-null int64\n",
      "cp          303 non-null int64\n",
      "trestbps    303 non-null int64\n",
      "chol        303 non-null int64\n",
      "fbs         303 non-null int64\n",
      "restecg     303 non-null int64\n",
      "thalach     303 non-null int64\n",
      "exang       303 non-null int64\n",
      "oldpeak     303 non-null float64\n",
      "slope       303 non-null int64\n",
      "ca          303 non-null int64\n",
      "thal        303 non-null int64\n",
      "target      303 non-null int64\n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 33.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 13)\n",
      "(303,)\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:, 0:13].values\n",
    "y = df.target.values\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9521966 ,  0.68100522,  1.97312292, ..., -2.27457861,\n",
       "        -0.71442887, -2.14887271],\n",
       "       [-1.91531289,  0.68100522,  1.00257707, ..., -2.27457861,\n",
       "        -0.71442887, -0.51292188],\n",
       "       [-1.47415758, -1.46841752,  0.03203122, ...,  0.97635214,\n",
       "        -0.71442887, -0.51292188],\n",
       "       ...,\n",
       "       [ 1.50364073,  0.68100522, -0.93851463, ..., -0.64911323,\n",
       "         1.24459328,  1.12302895],\n",
       "       [ 0.29046364,  0.68100522, -0.93851463, ..., -0.64911323,\n",
       "         0.26508221,  1.12302895],\n",
       "       [ 0.29046364, -1.46841752,  0.03203122, ..., -0.64911323,\n",
       "         0.26508221, -0.51292188]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multilayer perceptron algorithm with backpropagation\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from random import random\n",
    "from csv import reader\n",
    "from math import exp\n",
    " \n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    " \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    " \n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    " \n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    stats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "    return stats\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    "\n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))\n",
    " \n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    " \n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    " \n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    " \n",
    "\n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    "\n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "            \n",
    " \n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    " \n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))\n",
    " \n",
    "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
    "    n_inputs = len(train[0]) - 1\n",
    "    n_outputs = len(set([row[-1] for row in train]))\n",
    "    network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "    train_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict(network, row)\n",
    "        predictions.append(prediction)\n",
    "    return(predictions)\n",
    "\n",
    "# Test Backprop on Seeds dataset\n",
    "seed(1)\n",
    "\n",
    "from mlxtend.data import mnist_data\n",
    "X, y = mnist_data()\n",
    "\n",
    "# load and prepare data\n",
    "filename = 'heart.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# normalize input variables\n",
    "minmax = dataset_minmax(df)\n",
    "normalize_dataset(df, minmax)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.3\n",
    "n_epoch = 500\n",
    "n_hidden = 5\n",
    "scores = evaluate_algorithm(df, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGT1oRzXw3H9"
   },
   "source": [
    "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
    "\n",
    "- Use the Heart Disease Dataset (binary classification)\n",
    "- Use an appropriate loss function for a binary classification task\n",
    "- Use an appropriate activation function on the final layer of your network. \n",
    "- Train your model using verbose output for ease of grading.\n",
    "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
    "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam, Nadam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, StratifiedKFold, KFold, GridSearchCV \n",
    "              \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWw4IYxLxKwH"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD, Adam, Nadam\n",
    "\n",
    "def create_model(lr=0.05,\n",
    "                 activation='relu',                 \n",
    "                 input_shape=(X.shape[1],),\n",
    "                 optimizer=Adam,\n",
    "                 relu_alpha = 0.003,\n",
    "                 dropout_rate = 0.2,\n",
    "                weight_initializer='random_normal'):\n",
    "    \n",
    "    # initialize a model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add input layer\n",
    "    model.add(Dense(10, input_shape=input_shape, kernel_initializer=weight_initializer,))\n",
    "    model.add(LeakyReLU(alpha=relu_alpha)) \n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    \n",
    "    # add hidden layers\n",
    "    model.add(Dense(10, kernel_initializer=weight_initializer,))\n",
    "    model.add(LeakyReLU(alpha=relu_alpha)) \n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "        \n",
    "    model.add(Dense(10, kernel_initializer=weight_initializer,))\n",
    "    model.add(LeakyReLU(alpha=relu_alpha)) \n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    \n",
    "    model.add(Dense(8, kernel_initializer=weight_initializer,))\n",
    "    model.add(LeakyReLU(alpha=relu_alpha)) \n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    \n",
    "    # add final output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer=optimizer(lr=lr)\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
    "              \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.6897689856515072 using {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.5511551115772512, Stdev: 0.17901057722136335 with: {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.6897689856515072, Stdev: 0.09576645732787446 with: {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.6303630315824704, Stdev: 0.09878963523601354 with: {'batch_size': 100, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model, \n",
    "                               epochs=epochs,\n",
    "                               batch_size=100,\n",
    "                               verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [20, 60, 100],\n",
    "              'epochs': [20]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")\n",
    "#Best: 0.6501650212228102 using {'batch_size': 20, 'epochs': 20}    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.6963696316523914 using {'batch_size': 60, 'epochs': 30}\n",
      "Means: 0.6666666772892766, Stdev: 0.13143437280589265 with: {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.6963696316523914, Stdev: 0.04452389308394785 with: {'batch_size': 60, 'epochs': 30}\n",
      "Means: 0.66996699669967, Stdev: 0.10550737415285161 with: {'batch_size': 60, 'epochs': 40}\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "model = KerasClassifier(build_fn=create_model, \n",
    "                               epochs=epochs,\n",
    "                               batch_size=60,\n",
    "                               verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [60],\n",
    "              'epochs': [20, 30, 40]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.7194719477848645 using {'dropout_rate': 0.2}\n",
      "Means: 0.6798679891592598, Stdev: 0.11035151162175309 with: {'dropout_rate': 0.0}\n",
      "Means: 0.7194719477848645, Stdev: 0.025986831487018202 with: {'dropout_rate': 0.2}\n",
      "Means: 0.6600660036499351, Stdev: 0.14241221521980998 with: {'dropout_rate': 0.3}\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, \n",
    "                               epochs=30,\n",
    "                               batch_size=60,\n",
    "                               verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'dropout_rate' : [0.0, 0.2, 0.3]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and learning rate¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.7392739283763142 using {'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
      "Means: 0.6600660120103226, Stdev: 0.10765351243438732 with: {'lr': 0.05, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
      "Means: 0.12211220768025212, Stdev: 0.17269274023273257 with: {'lr': 0.05, 'optimizer': <class 'keras.optimizers.SGD'>}\n",
      "Means: 0.6765676542083816, Stdev: 0.07245708078828142 with: {'lr': 0.03, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
      "Means: 0.21122112162042372, Stdev: 0.29871177485526024 with: {'lr': 0.03, 'optimizer': <class 'keras.optimizers.SGD'>}\n",
      "Means: 0.7392739283763142, Stdev: 0.052598599214012005 with: {'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
      "Means: 0.21122112162042372, Stdev: 0.29871177485526024 with: {'lr': 0.01, 'optimizer': <class 'keras.optimizers.SGD'>}\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, \n",
    "                               epochs=30,\n",
    "                               batch_size=60,\n",
    "                               verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'optimizer': [Adam, SGD],\n",
    "              'lr': [.05, .03, 0.01]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeakyRelu Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.6963696471928763 using {'relu_alpha': 0.001}\n",
      "Means: 0.6963696471928763, Stdev: 0.13750934506691428 with: {'relu_alpha': 0.001}\n",
      "Means: 0.6765676622736966, Stdev: 0.09014191082830694 with: {'relu_alpha': 0.003}\n",
      "Means: 0.6831683160448232, Stdev: 0.12601901723521391 with: {'relu_alpha': 0.005}\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model, \n",
    "                               epochs=30,\n",
    "                               batch_size=60,\n",
    "                               verbose=0)\n",
    "# define the grid search parameters\n",
    "param_grid = {'relu_alpha': [.001, .003, 0.005]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best results were achieved with \n",
    "optimizer Adam, \n",
    "learning rate = 0.01, \n",
    "droupout rate = 0.2, \n",
    "epoch size = 30, \n",
    "batch size=60\n",
    "LeakyRelU alpha rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DS43SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
