{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6SKlgYrpcym"
   },
   "source": [
    "# Neural Networks Sprint Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrEbRrjVphPM"
   },
   "source": [
    "## 1) Define the following terms:\n",
    "\n",
    "### Neuron  \n",
    "     A neuron in a neural network is the set of inputs being passed, their associated synaptic weights, and the activation function that is applied to the associative weights.\n",
    "### Input Layer\n",
    "    Any layer is the highest level organization you think of for a grouping of units in a neural network. So when we speak of input layers, we're talking about the grouping of inputs that is connected by either the training data, or another linked model's output. The input layer will pass it's data to the hidden layer, if applicable. Some inputs that would not be passed are things like bias nodes. Often times data will be normalized before the input layer is created, but this is not to be confused with batch normalization, which occurs in the hidden layer(s).\n",
    "    \n",
    "### Hidden Layer\n",
    "    Hidden layers recieve distributed data from the input layers or from proceeding layers in multi-layer networks. The hidden layer applies some transformative measure to the data being recieved from the input layer, most commonly an aggregation of the weighted inputs and an applied activation function, though this process can certainly deviate given more complex/specialized networks. The layer creates data/signal/logical processing to be delivered to the output layer.\n",
    "    \n",
    "### Output Layer\n",
    "    The output layer is essentially just the finalized grouping of neurons that represents the transformative measures applied by the hidden layers to the data we initialized the network with from the input layer. The type of output we would expect to see will be determined by the applied activation function. For sigmoidial transforms, we expect probability clamps between 0-1. For tanh, we expect the data to be transformed between -1 and 1. You can also expect to see more complex data returned with things like Leaky Relu, which uses a small alpha value to transform values <= 0, and returns all other values as their raw inputs.\n",
    "### Activation\n",
    "    We're sort of beating a dead horse at this point as I've already explained what an activation function is and does, but we can talk about why they're neccessary. For example, why not just take the dot product of the inputs, their weights, and applied them to a transposed bias matrix to calculate their output? Well, that's sort of the beauty of neural networks. If we simply did the above suggestion, we'd fail to capture any non-linear relationships with the data. The whole point of activation functions is to find those complex relationships and make sense of them. There is no linear combination of inputs that can produce those non-linear outputs, so matrix activation is out of the question, though a great tool nevertheless. It's really important to understand your data, and the type of relationship you need. You have to think about your problem, for example, i need specific activation functions for classification. And even different activation functions for singular vs. multi-classification. \n",
    "    \n",
    "More detailed process with components of the aggregation that proceeds the applied activation function.\n",
    "![aggregation](https://cdn-images-1.medium.com/max/900/1*4MVN69gdM72BtTtY75tntg.png)\n",
    "       \n",
    "       b = bias\n",
    "       x = input to neuron\n",
    "       w = weights\n",
    "       n = number of inputs from incoming layer\n",
    "       i = counter from 0 to n\n",
    "\n",
    "       1. We multiply every incoming input by it's corresponding weight.\n",
    "       2. We sum total all the values.\n",
    "       3. We add the bias to the neuron in question.\n",
    "\n",
    "       We take all the weights and begin to throw them in matrices for processing.\n",
    "           1. Create a  weight matrices from n-to-M\n",
    "           2. A M-by-1 matrix is made from the biases\n",
    "           3. We can view inputs as a n-by-1 matrix.\n",
    "           4. We tranpose the weight matrix to M-by-n\n",
    "           5. Find the dot product of the transposed weights and inputs. \n",
    "              Meaning we multiply M-by-n and n-by-1, which gives us M-by-1\n",
    "           6. We add the output of this to the bias matrix.\n",
    "           7. Now we can run an activation function on each value in the vector.\n",
    "### Backpropagation\n",
    "    We know that forward propagation is the applied matrix transformation being fed to our hidden layers/activations, and to our output, so we can approach this logically.\n",
    "    We can think of back propagation in simple terms. An iterative process that seeks to optimize our forward propagation.\n",
    "    More technically, back propagation is a gradient descent that seeks to determine the optimized weights and their associative bias for our inputs \n",
    "    We calculate that gradient by using the generalized application of the delta rule. \n",
    "    \n",
    "\\begin{eqnarray} \n",
    "  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j).\n",
    "\\tag{BP1}\\end{eqnarray}\n",
    "    \n",
    "    It's a bit more complex than I can explain well at this time, but essentially\n",
    "    1. ∂C/∂aLj is a measurement of how quickly our cost changes given the function of some layer, let's call it j's,  output.\n",
    "   \n",
    "      If C is not depedent on the neuron it was derived from, then we can expect the below to be a smaller value  \n",
    "\\begin{eqnarray}\n",
    "    \\delta^L_j  \n",
    " \\end{eqnarray}\n",
    "    \n",
    "   \n",
    "       For the right hand side of the equation, \n",
    "  \\begin{eqnarray}\\sigma'(z^L_j)\\end{eqnarray}\n",
    "  \n",
    "       We are measuring how fast the activation function is changing.\n",
    "       I need to study some more on how exactly everything is computed, as pieces of it go above my head, \n",
    "       but that's a high level overview of the equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from csv import reader\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from math import exp\n",
    "from random import seed\n",
    "from random import random\n",
    "from sklearn import model_selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ri_gRA2Jp728"
   },
   "source": [
    "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 1  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ig6ZTH8tpQ19"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AndGatePerceptron(object):\n",
    "    \"\"\"A class that models a logical AND gate with a single layer neural network\"\"\"\n",
    "    \n",
    "    def __init__(self, input_length, epochs=100, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        params: input_lenth: determines amount of weights needed when performing aggregation and simple activation overwrite for logical \n",
    "                epochs: iteration count for model\n",
    "                learning_rate: the rate at which the weights are modified per iteration\n",
    "        \n",
    "        \"\"\"\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        # notice here we add the bias weight to our input weights.\n",
    "        self.weights = np.zeros(input_length + 1)\n",
    "           \n",
    "    def predict(self, inputs):\n",
    "        \"\"\" Function to aggregate the weights and the 'bias', the fake constant value starts as an init: defined by self.weights\"\"\"\n",
    "        aggregate = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        if aggregate > 0:\n",
    "            activation = 1\n",
    "        else:\n",
    "            activation = 0            \n",
    "        return activation\n",
    "\n",
    "    def train(self, input_data, label_data):\n",
    "        \"\"\" Function to train on our inputs, and overwrite our weights and 'bias' for each iteration. This is some ugly code :D \"\"\"\n",
    "        for iteration in range(self.epochs):\n",
    "            for inputs, label in zip(input, label_data):\n",
    "                prediction = self.predict(inputs)\n",
    "                self.weights[1:] += self.learning_rate * (label - prediction) * inputs\n",
    "                self.weights[0] += self.learning_rate * (label - prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86HyRi8Osr3U"
   },
   "source": [
    "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
    "- Your network must have one hidden layer. \n",
    "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "- Train your model on the Heart Disease dataset from UCI:\n",
    "\n",
    "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
    "\n",
    "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNfiajv3v4Ed"
   },
   "outputs": [],
   "source": [
    "def create_mlp(input_layers, hidden_layers, output_layers):\n",
    "    neuralnet = list()\n",
    "    \n",
    "    hidden_layer = [{'weights':[random() for i in range(input_layers + 1)]} for i in range(hidden_layers)]\n",
    "    neuralnet.append(hidden_layer)\n",
    "    \n",
    "    output_layer = [{'weights':[random() for i in range(hidden_layers + 1)]} for i in range(output_layers)]\n",
    "    neuralnet.append(output_layer)\n",
    "    \n",
    "    return neuralnet\n",
    "\n",
    "\n",
    "def simple_activation(weights, inputs):\n",
    "    \"\"\"Function that calculates neuron activation for an input by aggregating the weights and inputs \"\"\"\n",
    "    activation = weights[-1]\n",
    "    \n",
    "    \n",
    "    for i in range(len(weights) - 1):\n",
    "        activation += int(weights[i]) * int(inputs[i])\n",
    "    return activation\n",
    "\n",
    "def neural_transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))\n",
    "\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        \n",
    "        current_inputs = []\n",
    "        \n",
    "        for neuron in layer:\n",
    "            \n",
    "            activation = simple_activation(neuron['weights'], inputs)\n",
    "            neuron['output'] = neural_transfer(activation)\n",
    "            current_inputs.append(neuron['output'])\n",
    "            \n",
    "        inputs = current_inputs\n",
    "        \n",
    "    return inputs\n",
    "\n",
    "def calc_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    " \n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * calc_derivative(neuron['output'])\n",
    "            \n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    "\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "        \n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'heart.csv'\n",
    "file = load_csv(filepath)\n",
    "\n",
    "\n",
    "input_layers = len(file[0]) - 1\n",
    "output_layers = len(set([row[-1] for row in file]))\n",
    "network = create_mlp(input_layers, 2, output_layers)\n",
    "train_network(network, file, 0.5, 20, output_layers)\n",
    "for layer in network:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGT1oRzXw3H9"
   },
   "source": [
    "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
    "\n",
    "- Use the Heart Disease Dataset (binary classification)\n",
    "- Use an appropriate loss function for a binary classification task\n",
    "- Use an appropriate activation function on the final layer of your network. \n",
    "- Train your model using verbose output for ease of grading.\n",
    "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
    "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWw4IYxLxKwH"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.drop(['target'], 1))\n",
    "y = np.array(df['target'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "Y_train = to_categorical(y_train, num_classes=None)\n",
    "Y_test = to_categorical(y_test, num_classes=None)\n",
    "\n",
    "Y_train_binary = y_train.copy()\n",
    "Y_test_binary = y_test.copy()\n",
    "\n",
    "Y_train_binary[Y_train_binary > 0] = 1\n",
    "Y_test_binary[Y_test_binary > 0] = 1\n",
    "\n",
    "print (Y_train_binary[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 6)                 84        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 117\n",
      "Trainable params: 117\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(4, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # compile model\n",
    "    adam = Adam(lr=0.001)\n",
    "    model.compile(loss='mse', optimizer=adam, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2430 - acc: 0.5372\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 214us/step - loss: 0.2392 - acc: 0.5868\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 224us/step - loss: 0.2353 - acc: 0.5868\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 210us/step - loss: 0.2315 - acc: 0.5868\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 221us/step - loss: 0.2303 - acc: 0.6405\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 214us/step - loss: 0.2252 - acc: 0.6570\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 219us/step - loss: 0.2265 - acc: 0.6777\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 212us/step - loss: 0.2216 - acc: 0.6529\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 206us/step - loss: 0.2213 - acc: 0.6694\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 218us/step - loss: 0.2165 - acc: 0.6736\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 214us/step - loss: 0.2116 - acc: 0.6860\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 235us/step - loss: 0.2119 - acc: 0.7025\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 214us/step - loss: 0.2107 - acc: 0.6860\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 214us/step - loss: 0.2044 - acc: 0.7273\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 210us/step - loss: 0.2026 - acc: 0.7438\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 214us/step - loss: 0.2023 - acc: 0.7314\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 194us/step - loss: 0.1995 - acc: 0.7562\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 206us/step - loss: 0.1987 - acc: 0.7355\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 206us/step - loss: 0.1986 - acc: 0.7314\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 210us/step - loss: 0.1955 - acc: 0.7438\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 210us/step - loss: 0.1949 - acc: 0.7479\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 227us/step - loss: 0.1915 - acc: 0.7521\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 235us/step - loss: 0.1742 - acc: 0.7438\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 223us/step - loss: 0.1665 - acc: 0.7562\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 194us/step - loss: 0.1683 - acc: 0.7355\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 206us/step - loss: 0.1713 - acc: 0.7562\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 194us/step - loss: 0.1531 - acc: 0.7603\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 203us/step - loss: 0.1548 - acc: 0.7727\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 206us/step - loss: 0.1482 - acc: 0.7893\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 210us/step - loss: 0.1476 - acc: 0.7934\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 202us/step - loss: 0.1508 - acc: 0.7727\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 206us/step - loss: 0.1477 - acc: 0.7769\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 206us/step - loss: 0.1558 - acc: 0.7645\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 202us/step - loss: 0.1738 - acc: 0.7479\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 202us/step - loss: 0.1441 - acc: 0.7686\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 198us/step - loss: 0.1398 - acc: 0.7934\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 198us/step - loss: 0.1458 - acc: 0.7934\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 202us/step - loss: 0.1425 - acc: 0.7727\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 214us/step - loss: 0.1394 - acc: 0.7934\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 218us/step - loss: 0.1417 - acc: 0.8058\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 218us/step - loss: 0.1336 - acc: 0.8264\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 209us/step - loss: 0.1321 - acc: 0.8058\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 210us/step - loss: 0.1339 - acc: 0.7975\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 212us/step - loss: 0.1377 - acc: 0.8140\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 201us/step - loss: 0.1366 - acc: 0.7975\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 218us/step - loss: 0.1334 - acc: 0.8058\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 218us/step - loss: 0.1502 - acc: 0.7934\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 210us/step - loss: 0.1364 - acc: 0.7934\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 214us/step - loss: 0.1368 - acc: 0.8058\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 210us/step - loss: 0.1249 - acc: 0.8430\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 210us/step - loss: 0.1292 - acc: 0.8347\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 198us/step - loss: 0.1290 - acc: 0.8140\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 209us/step - loss: 0.1356 - acc: 0.8017\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 223us/step - loss: 0.1258 - acc: 0.8264\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 210us/step - loss: 0.1255 - acc: 0.8140\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 206us/step - loss: 0.1294 - acc: 0.8058\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 206us/step - loss: 0.1314 - acc: 0.8182\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 206us/step - loss: 0.1269 - acc: 0.8182\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 214us/step - loss: 0.1310 - acc: 0.7851\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 198us/step - loss: 0.1217 - acc: 0.8430\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 210us/step - loss: 0.1219 - acc: 0.8306\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 198us/step - loss: 0.1331 - acc: 0.7810\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 194us/step - loss: 0.1209 - acc: 0.8512\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 207us/step - loss: 0.1183 - acc: 0.8512\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 198us/step - loss: 0.1201 - acc: 0.8512\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 214us/step - loss: 0.1195 - acc: 0.8471\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 214us/step - loss: 0.1208 - acc: 0.8264\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 218us/step - loss: 0.1176 - acc: 0.8430\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 206us/step - loss: 0.1191 - acc: 0.8430\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 202us/step - loss: 0.1173 - acc: 0.8512\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 215us/step - loss: 0.1205 - acc: 0.8554\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 218us/step - loss: 0.1245 - acc: 0.8306\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 231us/step - loss: 0.1190 - acc: 0.8554\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 210us/step - loss: 0.1179 - acc: 0.8430\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 202us/step - loss: 0.1156 - acc: 0.8554\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 210us/step - loss: 0.1319 - acc: 0.8347\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 202us/step - loss: 0.1140 - acc: 0.8554\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 223us/step - loss: 0.1237 - acc: 0.8471\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 206us/step - loss: 0.1191 - acc: 0.8430\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 206us/step - loss: 0.1156 - acc: 0.8471\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 210us/step - loss: 0.1134 - acc: 0.8512\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 206us/step - loss: 0.1172 - acc: 0.8306\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 206us/step - loss: 0.1146 - acc: 0.8512\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 213us/step - loss: 0.1172 - acc: 0.8430\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 204us/step - loss: 0.1174 - acc: 0.8388\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 202us/step - loss: 0.1234 - acc: 0.8388\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 198us/step - loss: 0.1143 - acc: 0.8388\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 198us/step - loss: 0.1205 - acc: 0.8388\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 194us/step - loss: 0.1144 - acc: 0.8595\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 190us/step - loss: 0.1117 - acc: 0.8595\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 202us/step - loss: 0.1123 - acc: 0.8595\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 206us/step - loss: 0.1149 - acc: 0.8595\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 198us/step - loss: 0.1182 - acc: 0.8306\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 202us/step - loss: 0.1155 - acc: 0.8471\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 194us/step - loss: 0.1178 - acc: 0.8388\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 214us/step - loss: 0.1188 - acc: 0.8595\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 194us/step - loss: 0.1211 - acc: 0.8347\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 202us/step - loss: 0.1176 - acc: 0.8471\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 198us/step - loss: 0.1151 - acc: 0.8512\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 210us/step - loss: 0.1149 - acc: 0.8595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fef8371898>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train_binary, epochs=100, batch_size=10, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model: 0.819672131147541\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.78      0.79        27\n",
      "           1       0.83      0.85      0.84        34\n",
      "\n",
      "   micro avg       0.82      0.82      0.82        61\n",
      "   macro avg       0.82      0.82      0.82        61\n",
      "weighted avg       0.82      0.82      0.82        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate classification report using predictions for binary model \n",
    "predictions = np.round(model.predict(X_test)).astype(int)\n",
    "\n",
    "print('Accuracy of model:', accuracy_score(Y_test_binary, predictions))\n",
    "print(classification_report(Y_test_binary, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DS43SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "604px",
    "left": "1278px",
    "top": "48.4444px",
    "width": "341.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
