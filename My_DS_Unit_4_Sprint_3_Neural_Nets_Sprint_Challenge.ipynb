{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "My DS_Unit_4_Sprint_3_Neural_Nets_Sprint_Challenge.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wel51x/DS-Unit-4-Sprint-3-Neural-Networks/blob/master/My_DS_Unit_4_Sprint_3_Neural_Nets_Sprint_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6SKlgYrpcym",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks Sprint Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrEbRrjVphPM",
        "colab_type": "text"
      },
      "source": [
        "## 1) Define the following terms:\n",
        "\n",
        "- Neuron\n",
        "- Input Layer\n",
        "- Hidden Layer\n",
        "- Output Layer\n",
        "- Activation\n",
        "- Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5EksLqnp4oB",
        "colab_type": "text"
      },
      "source": [
        "### Neuron:\n",
        "![Wikipedia Neuron Diagram](https://cdn4.explainthatstuff.com/neuron-labeled-diagram.png)\n",
        "\n",
        "In biology, a neuron is a basic building block of the brain, a cell that receives signals at the dendrite of the cell and sends them to the cell body. If the received signals surpass a certain threshold with a given timing then the neuron fires, sending a large spike of energy down the axon and through the axon terminals to other neurons down the line (reference above diagram). In AI a 'neuron' is one node in a neural network. It receives information from previous nodes in a network, computes an output based on weights given to those pieces of information, and passes on a output to other nodes via an activation \n",
        "### Input Layer:\n",
        "The Input Layer is what receives input from the features in a dataset.\n",
        "### Hidden Layer:\n",
        "Layers after the input layer are called Hidden Layers. This is because they cannot be accessed except through the input layer. They're composed of a user-specified number of neurons, and allow neural networks to fit non-linear patterns in data.\n",
        "### Output Layer:\n",
        "The final layer is called the Output Layer. The purpose of the output layer is to output values that are in a format suitable for interpretation in light of the type of problem that we're trying to address. \n",
        "function.\n",
        "### Activation:\n",
        "The activation function decides whether a cell \"fires\" or not. Sometimes it is said that the cell is \"activated\" or not.\n",
        "### Backpropagation:\n",
        "In backpropagation the error (the \"cost\" that computed by comparing the calculated output and the known correct target output) is used to update the model parameters. The total error at the output nodes is calculated and propagated back through the network in each layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K0ytzvPSD4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD, Adam, Nadam, RMSprop\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "pd.set_option('display.width', 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri_gRA2Jp728",
        "colab_type": "text"
      },
      "source": [
        "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
        "\n",
        "| x1 | x2 | x3 | y |\n",
        "|----|----|----|---|\n",
        "| 1  | 1  | 1  | 1 |\n",
        "| 1  | 0  | 1  | 0 |\n",
        "| 0  | 1  | 1  | 0 |\n",
        "| 0  | 0  | 1  | 0 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig6ZTH8tpQ19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "inputs = np.array([\n",
        "    [0, 0, 1],\n",
        "    [0, 1, 1],\n",
        "    [1, 0, 1],\n",
        "    [1, 1, 1]    \n",
        "])\n",
        "\n",
        "correct_outputs = [[0], [0], [0], [1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7fWxdDxewH6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1a645e5a-0ea4-4574-c91a-3f8e10968a57"
      },
      "source": [
        "weights = 2 * np.random.random((3, 1)) - 1\n",
        "weights"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.39533485],\n",
              "       [-0.70648822],\n",
              "       [-0.81532281]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gHZxp2Tg1Yh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perceptrn(inputs, weights, iter):\n",
        "  def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "  def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "  \n",
        "  for iteration in range(iter):\n",
        "    weighted_sum = np.dot(inputs, weights)\n",
        "    activated_output = sigmoid(weighted_sum)\n",
        "    error = correct_outputs - activated_output\n",
        "    sd = sigmoid_derivative(activated_output)\n",
        "    adjustments = error * sd\n",
        "    weights += np.dot(inputs.T, adjustments)\n",
        "\n",
        "  return (weights, activated_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nb7QeeUhIkA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "82981245-e47c-4423-d2fd-237346fb2f5c"
      },
      "source": [
        "w, ao = perceptrn(inputs, weights, 1000)\n",
        "\n",
        "print(\"Input:\")\n",
        "print(inputs[:,:2])\n",
        "\n",
        "print(\"\\nWeights after training:\")\n",
        "print(w)\n",
        "\n",
        "print(\"\\nOutput after training:\")\n",
        "print(ao)\n",
        "print(\"\\nOutput after training rounded as int:\")\n",
        "for i in ao:\n",
        "  print(int(i+.5))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:\n",
            "[[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "\n",
            "Weights after training:\n",
            "[[  7.20591048]\n",
            " [  7.20591023]\n",
            " [-11.09703577]]\n",
            "\n",
            "Output after training:\n",
            "[[1.52026008e-05]\n",
            " [2.00332884e-02]\n",
            " [2.00332934e-02]\n",
            " [9.64898617e-01]]\n",
            "\n",
            "Output after training rounded as int:\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86HyRi8Osr3U",
        "colab_type": "text"
      },
      "source": [
        "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
        "- Your network must have one hidden layer. \n",
        "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
        "- Train your model on the Heart Disease dataset from UCI:\n",
        "\n",
        "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
        "\n",
        "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNfiajv3v4Ed",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "bc2db2c3-22f2-436f-fd3c-1d9f3cd11267"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv\"\n",
        "\n",
        "df = pd.read_csv(url, header=0)\n",
        "print(\"df.shape=\", df.shape)\n",
        "print(\"\\n\", df.head(5))\n",
        "print(\"\\nTarget Value Counts\")\n",
        "print(df.target.value_counts())\n",
        "\n",
        "y = df.target\n",
        "X = df.drop(columns=[\"target\"],axis=1)\n",
        "\n",
        "X = MinMaxScaler().fit_transform(X)\n",
        "print(\"X.shape=\", X.shape)\n",
        "print(\"y.shape=\", y.shape)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df.shape= (303, 14)\n",
            "\n",
            "    age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  ca  thal  target\n",
            "0   63    1   3       145   233    1        0      150      0      2.3      0   0     1       1\n",
            "1   37    1   2       130   250    0        1      187      0      3.5      0   0     2       1\n",
            "2   41    0   1       130   204    0        0      172      0      1.4      2   0     2       1\n",
            "3   56    1   1       120   236    0        1      178      0      0.8      2   0     2       1\n",
            "4   57    0   0       120   354    0        1      163      1      0.6      2   0     2       1\n",
            "\n",
            "Target Value Counts\n",
            "1    165\n",
            "0    138\n",
            "Name: target, dtype: int64\n",
            "X.shape= (303, 13)\n",
            "y.shape= (303,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
            "  return self.partial_fit(X, y)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFIHJl3vSVzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNet(object):\n",
        "       \n",
        "    def __init__(self, n_hidden=30, l2=0.01, epochs=100, eta=0.001,\n",
        "                shuffle=True, minibatch_size=1, seed=None):\n",
        "      self.random = np.random.RandomState(seed)\n",
        "      self.n_hidden = n_hidden\n",
        "      self.l2 = l2\n",
        "      self.epochs = epochs\n",
        "      self.eta = eta\n",
        "      self.shuffle = shuffle\n",
        "      self.minibatch_size = minibatch_size\n",
        "        \n",
        "    def onehot(self, y, n_classes):        \n",
        "      onehot = np.zeros((n_classes, y.shape[0]))\n",
        "      for idx, val in enumerate(y.astype(int)):\n",
        "        onehot[val, idx] = 1.\n",
        "      return onehot.T\n",
        "    \n",
        "    def sigmoid(self, z):\n",
        "      return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n",
        "    \n",
        "    def forward(self, X):\n",
        "      # step 1: net input of hidden layer\n",
        "      z_h = np.dot(X, self.w_h) + self.b_h\n",
        "        \n",
        "      # step 2: activation of hidden layer\n",
        "      a_h = self.sigmoid(z_h)\n",
        "        \n",
        "      # step 3: net input of output layer\n",
        "      z_out = np.dot(a_h, self.w_out) + self.b_out\n",
        "        \n",
        "      # step 4: activation layer output\n",
        "      a_out = self.sigmoid(z_out)\n",
        "        \n",
        "      return z_h, a_h, z_out, a_out\n",
        "    \n",
        "    def compute_cost(self, y_enc, output):\n",
        "      e = 0.000000001\n",
        "      L2_term = (self.l2 * (np.sum(self.w_h ** 2.) + np.sum(self.w_out ** 2.)))\n",
        "      t1 = -y_enc * (np.log(output + e))\n",
        "      t2 = (1. - y_enc) * np.log(1. - output + e)\n",
        "      cost = np.sum(t1 - t2) + L2_term\n",
        "      return cost\n",
        "    \n",
        "    def predict(self, X):\n",
        "      z_h, a_h, z_out, a_out = self.forward(X)\n",
        "      y_pred = np.argmax(z_out, axis=1)\n",
        "      return y_pred\n",
        "    \n",
        "    def fit(self, X_train, y_train, X_val, y_val):\n",
        "      n_output = np.unique(y_train).shape[0]\n",
        "      n_features = X_train.shape[1]\n",
        "      \n",
        "      # weights for input -> hidden\n",
        "      self.b_h = np.zeros(self.n_hidden)\n",
        "      self.w_h = self.random.normal(loc=0.0, scale=0.1,\n",
        "                                    size=(n_features,\n",
        "                                          self.n_hidden))\n",
        "        \n",
        "      # weights for hidden -> output\n",
        "      self.b_out = np.zeros(n_output)\n",
        "      self.w_out = self.random.normal(loc=0.0, scale=0.1,\n",
        "                                       size=(self.n_hidden,\n",
        "                                       n_output))\n",
        "        \n",
        "      epoch_strlen = len(str(self.epochs)) # for progr. format\n",
        "      self.eval_ = {'cost' : [], 'train_acc' : [], 'val_acc' : []}\n",
        "        \n",
        "      y_train_enc = self.onehot(y_train, n_output)\n",
        "        \n",
        "      # iterate over training epochs\n",
        "      for i in range(self.epochs):\n",
        "            \n",
        "          # iterate over mini batches\n",
        "          indices = np.arange(X_train.shape[0])\n",
        "          \n",
        "          if self.shuffle:\n",
        "            self.random.shuffle(indices)\n",
        "          \n",
        "          for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size):\n",
        "            batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
        "            \n",
        "            # Fwd propagation\n",
        "            z_h, a_h, z_out, a_out = self.forward(X_train[batch_idx])\n",
        "            \n",
        "            # Back propagation\n",
        "            \n",
        "            # [n_samples n_classlabels]\n",
        "            sigma_out = a_out - y_train_enc[batch_idx]\n",
        "            \n",
        "            # [n_samples n_hidden]\n",
        "            sigmoid_derivative_h = a_h * (1. - a_h)\n",
        "            \n",
        "            # [n_samples, n_classlabels] dot [n_classlabels, n_hidden]\n",
        "            # -> [n_samples, n_hidden]\n",
        "            sigma_h = (np.dot(sigma_out, self.w_out.T) * sigmoid_derivative_h)\n",
        "            \n",
        "            # [n_features, n_samples] dot [n_samples, n_hidden]\n",
        "            # -> [n_features, n_hidden]\n",
        "            grad_w_h = np.dot(X_train[batch_idx].T, sigma_h)\n",
        "            grad_b_h = np.sum(sigma_h, axis=0)\n",
        "            \n",
        "            # [n_hidden, n_samples] dot [n_samples, n_classlabels]\n",
        "            # -> [n_hidden, n_classlabels]\n",
        "            grad_w_out = np.dot(a_h.T, sigma_out)\n",
        "            grad_b_out = np.sum(sigma_out, axis=0)\n",
        "            \n",
        "            # Regularization and weight updates\n",
        "            delta_w_h = (grad_w_h + self.l2 * self.w_h)\n",
        "            delta_b_h = grad_b_h # bias not regularized\n",
        "            self.w_h -= self.eta * delta_w_h\n",
        "            self.b_h -= self.eta * delta_b_h\n",
        "            \n",
        "            delta_w_out = grad_w_out + self.l2 * self.w_out\n",
        "            delta_b_out = grad_b_out # bias not regularized\n",
        "            self.w_out -= self.eta * delta_w_out\n",
        "            self.b_out -= self.eta * delta_b_out\n",
        "\n",
        "          # evaluation after each epoch during training\n",
        "          z_h, a_h, z_out, a_out = self.forward(X_train)\n",
        "          \n",
        "          cost = self.compute_cost(y_enc=y_train_enc, output=a_out)\n",
        "          \n",
        "          y_train_pred = self.predict(X_train)\n",
        "          y_val_pred = self.predict(X_val)\n",
        "          \n",
        "          temp = (np.sum(y_train == y_train_pred)).astype(np.float)\n",
        "          train_acc = temp / X_train.shape[0]\n",
        "          temp = (np.sum(y_val == y_val_pred)).astype(np.float)\n",
        "          val_acc = temp / X_val.shape[0]\n",
        "          \n",
        "          self.eval_['cost'].append(cost)\n",
        "          self.eval_['train_acc'].append(train_acc)\n",
        "          self.eval_['val_acc'].append(val_acc)\n",
        "          \n",
        "          if self.epochs > 100 and i % 20 != 0:\n",
        "            continue\n",
        "          else:\n",
        "            print('Iteration: %0*d/%d; Cost: %.2f; Train/Test Accuracy: %.2f%%/%.2f%%'\n",
        "                        % (epoch_strlen, i+1, self.epochs, cost,\n",
        "                           train_acc*100, val_acc*100))\n",
        "            \n",
        "      print('Iteration: %0*d/%d; Cost: %.2f; Train/Test Accuracy: %.2f%%/%.2f%%'\n",
        "                      % (epoch_strlen, i+1, self.epochs, cost,\n",
        "                         train_acc*100, val_acc*100))\n",
        "\n",
        "      return self"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eajeRC8fSs4T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "740c98cb-dc3e-4ec4-f328-f54d29fa033d"
      },
      "source": [
        "# Test Train Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
        "\n",
        "print (\"X_train, y_test shapes=\", X_train.shape, y_train.shape)\n",
        "print (\"X_test, y_test shapes=\", X_test.shape, y_test.shape)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train, y_test shapes= (257, 13) (257,)\n",
            "X_test, y_test shapes= (46, 13) (46,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPbLRf0_S0Qi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "474eecda-8b6d-4676-fc88-58778ba5f57e"
      },
      "source": [
        "# Initialize a new NN\n",
        "nn = NeuralNet(epochs= 500, minibatch_size= 25, seed = 69)\n",
        "\n",
        "# Run the NN\n",
        "nn.fit(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 001/500; Cost: 358.34; Train/Test Accuracy: 53.31%/60.87%\n",
            "Iteration: 021/500; Cost: 352.35; Train/Test Accuracy: 53.31%/60.87%\n",
            "Iteration: 041/500; Cost: 348.48; Train/Test Accuracy: 53.31%/60.87%\n",
            "Iteration: 061/500; Cost: 342.49; Train/Test Accuracy: 63.81%/71.74%\n",
            "Iteration: 081/500; Cost: 332.98; Train/Test Accuracy: 72.37%/76.09%\n",
            "Iteration: 101/500; Cost: 319.03; Train/Test Accuracy: 77.04%/76.09%\n",
            "Iteration: 121/500; Cost: 300.70; Train/Test Accuracy: 78.60%/78.26%\n",
            "Iteration: 141/500; Cost: 280.42; Train/Test Accuracy: 79.77%/78.26%\n",
            "Iteration: 161/500; Cost: 260.99; Train/Test Accuracy: 80.54%/78.26%\n",
            "Iteration: 181/500; Cost: 244.85; Train/Test Accuracy: 80.54%/80.43%\n",
            "Iteration: 201/500; Cost: 232.31; Train/Test Accuracy: 80.54%/78.26%\n",
            "Iteration: 221/500; Cost: 222.93; Train/Test Accuracy: 80.54%/78.26%\n",
            "Iteration: 241/500; Cost: 215.81; Train/Test Accuracy: 80.93%/78.26%\n",
            "Iteration: 261/500; Cost: 210.24; Train/Test Accuracy: 80.93%/78.26%\n",
            "Iteration: 281/500; Cost: 205.85; Train/Test Accuracy: 81.32%/78.26%\n",
            "Iteration: 301/500; Cost: 202.33; Train/Test Accuracy: 82.10%/78.26%\n",
            "Iteration: 321/500; Cost: 199.40; Train/Test Accuracy: 81.71%/78.26%\n",
            "Iteration: 341/500; Cost: 196.98; Train/Test Accuracy: 82.49%/78.26%\n",
            "Iteration: 361/500; Cost: 194.94; Train/Test Accuracy: 82.88%/78.26%\n",
            "Iteration: 381/500; Cost: 193.27; Train/Test Accuracy: 82.49%/82.61%\n",
            "Iteration: 401/500; Cost: 191.80; Train/Test Accuracy: 81.71%/82.61%\n",
            "Iteration: 421/500; Cost: 190.55; Train/Test Accuracy: 82.49%/82.61%\n",
            "Iteration: 441/500; Cost: 189.46; Train/Test Accuracy: 82.10%/82.61%\n",
            "Iteration: 461/500; Cost: 188.58; Train/Test Accuracy: 83.66%/82.61%\n",
            "Iteration: 481/500; Cost: 187.78; Train/Test Accuracy: 84.44%/80.43%\n",
            "Iteration: 500/500; Cost: 187.12; Train/Test Accuracy: 85.21%/80.43%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.NeuralNet at 0x7fc7a4892e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSFcFXJOS-bG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "6a947233-7a92-456d-f644-4a5ca9376b6f"
      },
      "source": [
        "# Graph results\n",
        "plt.plot(range(nn.epochs), nn.eval_['train_acc'], label = \"Training\")\n",
        "plt.plot(range(nn.epochs), nn.eval_['val_acc'], label = \"Test\")\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMMYYk5AFC2OMMQn9/0aQ5vbCJ3r4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGT1oRzXw3H9",
        "colab_type": "text"
      },
      "source": [
        "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
        "\n",
        "- Use the Heart Disease Dataset (binary classification)\n",
        "- Use an appropriate loss function for a binary classification task\n",
        "- Use an appropriate activation function on the final layer of your network. \n",
        "- Train your model using verbose output for ease of grading.\n",
        "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
        "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
        "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
        "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoULSLYoYimL",
        "colab_type": "text"
      },
      "source": [
        "#### Tune Batch and Epoch Hyperparameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWw4IYxLxKwH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "outputId": "415cd150-fe55-4ad2-bb68-0a6be9cd65e9"
      },
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(12, input_dim=13, activation='relu'))\n",
        "\tmodel.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy',\n",
        "                optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
        "\n",
        "# define the grid search parameters\n",
        "# batch_size = [10, 20, 40, 60, 80, 100]\n",
        "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "# define the grid search parameters\n",
        "param_grid = {'batch_size': [10, 20, 50, 80, 100],\n",
        "              'epochs': [20, 40, 60, 80, 100]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X, y, verbose=0)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.6930693061438331 using {'batch_size': 10, 'epochs': 60}\n",
            "Means: 0.5016501660000766, Stdev: 0.23904051073659488 with: {'batch_size': 10, 'epochs': 20}\n",
            "Means: 0.6600660082727375, Stdev: 0.09334743118104609 with: {'batch_size': 10, 'epochs': 40}\n",
            "Means: 0.6930693061438331, Stdev: 0.09933937796101726 with: {'batch_size': 10, 'epochs': 60}\n",
            "Means: 0.6666666691256041, Stdev: 0.07851733860078751 with: {'batch_size': 10, 'epochs': 80}\n",
            "Means: 0.6600660050761188, Stdev: 0.104261178156353 with: {'batch_size': 10, 'epochs': 100}\n",
            "Means: 0.4851485215397951, Stdev: 0.19113442245680692 with: {'batch_size': 20, 'epochs': 20}\n",
            "Means: 0.66006601092839, Stdev: 0.10673902414339122 with: {'batch_size': 20, 'epochs': 40}\n",
            "Means: 0.66996699945368, Stdev: 0.09779228528919548 with: {'batch_size': 20, 'epochs': 60}\n",
            "Means: 0.6897689765042597, Stdev: 0.1042611942388907 with: {'batch_size': 20, 'epochs': 80}\n",
            "Means: 0.6402640267960703, Stdev: 0.11005499554696682 with: {'batch_size': 20, 'epochs': 100}\n",
            "Means: 0.2343234382446843, Stdev: 0.3313833843475269 with: {'batch_size': 50, 'epochs': 20}\n",
            "Means: 0.4191419171913229, Stdev: 0.2758491630416328 with: {'batch_size': 50, 'epochs': 40}\n",
            "Means: 0.5808580858085809, Stdev: 0.15472881760078352 with: {'batch_size': 50, 'epochs': 60}\n",
            "Means: 0.5280528071493206, Stdev: 0.2578485135331014 with: {'batch_size': 50, 'epochs': 80}\n",
            "Means: 0.66996699355223, Stdev: 0.15260234558627003 with: {'batch_size': 50, 'epochs': 100}\n",
            "Means: 0.26402640775485403, Stdev: 0.37338972667156345 with: {'batch_size': 80, 'epochs': 20}\n",
            "Means: 0.2673267341609245, Stdev: 0.36414472440625034 with: {'batch_size': 80, 'epochs': 40}\n",
            "Means: 0.6237623956140512, Stdev: 0.13503150677513076 with: {'batch_size': 80, 'epochs': 60}\n",
            "Means: 0.5973597461044198, Stdev: 0.12943018441672907 with: {'batch_size': 80, 'epochs': 80}\n",
            "Means: 0.6369637061070295, Stdev: 0.12269062722691221 with: {'batch_size': 80, 'epochs': 100}\n",
            "Means: 0.2541254093938141, Stdev: 0.3593876005083471 with: {'batch_size': 100, 'epochs': 20}\n",
            "Means: 0.2739274002144439, Stdev: 0.32641582947434894 with: {'batch_size': 100, 'epochs': 40}\n",
            "Means: 0.29702969640493393, Stdev: 0.33847293211394336 with: {'batch_size': 100, 'epochs': 60}\n",
            "Means: 0.3333333234975834, Stdev: 0.32960366843461847 with: {'batch_size': 100, 'epochs': 80}\n",
            "Means: 0.4488448758329889, Stdev: 0.32390347193168095 with: {'batch_size': 100, 'epochs': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etLbqAG_Y9kg",
        "colab_type": "text"
      },
      "source": [
        "#### Tune Optimizer and Learning Rate Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyj4PQN1iyeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to create model, add optimizer, learning rate params\n",
        "def create_opt_model(optimizer=SGD,\n",
        "                     lr=0.0001):\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(12, input_dim=13, activation='relu'))\n",
        "\tmodel.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\toptimizer=optimizer(lr=lr)\n",
        "  # Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy',\n",
        "                optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBXqNIe5ZkZG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "outputId": "b79401f7-4856-4db2-ff1b-624c938ac089"
      },
      "source": [
        "# use 'batch_size': 10, 'epochs': 60 - gave best results in previous step\n",
        "model = KerasClassifier(build_fn=create_opt_model, batch_size=10,\n",
        "                        epochs=60, verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "param_grid = {'optimizer': [SGD, Adam, RMSprop, Nadam],\n",
        "              'lr': [.01, .001, .0001, .00001]} # learning rate\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X, y)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.6897689774878347 using {'lr': 0.001, 'optimizer': <class 'keras.optimizers.RMSprop'>}\n",
            "Means: 0.66006600896124, Stdev: 0.10172299415884367 with: {'lr': 0.01, 'optimizer': <class 'keras.optimizers.SGD'>}\n",
            "Means: 0.6468646871571494, Stdev: 0.11327393722677023 with: {'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.66996699551938, Stdev: 0.10550736791334848 with: {'lr': 0.01, 'optimizer': <class 'keras.optimizers.RMSprop'>}\n",
            "Means: 0.6798679899461199, Stdev: 0.09299671409861901 with: {'lr': 0.01, 'optimizer': <class 'keras.optimizers.Nadam'>}\n",
            "Means: 0.6897689755206847, Stdev: 0.11356204915824429 with: {'lr': 0.001, 'optimizer': <class 'keras.optimizers.SGD'>}\n",
            "Means: 0.6699670017159024, Stdev: 0.12790641637584946 with: {'lr': 0.001, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.6897689774878347, Stdev: 0.1132739373585937 with: {'lr': 0.001, 'optimizer': <class 'keras.optimizers.RMSprop'>}\n",
            "Means: 0.66006601289554, Stdev: 0.09644646488950359 with: {'lr': 0.001, 'optimizer': <class 'keras.optimizers.Nadam'>}\n",
            "Means: 0.6765676566673191, Stdev: 0.1314343705337075 with: {'lr': 0.0001, 'optimizer': <class 'keras.optimizers.SGD'>}\n",
            "Means: 0.6765676569623915, Stdev: 0.08137840111580065 with: {'lr': 0.0001, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.6732673281096783, Stdev: 0.1012937883929235 with: {'lr': 0.0001, 'optimizer': <class 'keras.optimizers.RMSprop'>}\n",
            "Means: 0.6567656833543243, Stdev: 0.08414223781821827 with: {'lr': 0.0001, 'optimizer': <class 'keras.optimizers.Nadam'>}\n",
            "Means: 0.6666666708960391, Stdev: 0.08296900700498827 with: {'lr': 1e-05, 'optimizer': <class 'keras.optimizers.SGD'>}\n",
            "Means: 0.6666666733549766, Stdev: 0.1023634435489167 with: {'lr': 1e-05, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.6666666715845416, Stdev: 0.0894139711243035 with: {'lr': 1e-05, 'optimizer': <class 'keras.optimizers.RMSprop'>}\n",
            "Means: 0.6369636972056757, Stdev: 0.10299990709153115 with: {'lr': 1e-05, 'optimizer': <class 'keras.optimizers.Nadam'>}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}