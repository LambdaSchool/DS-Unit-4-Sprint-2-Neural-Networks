{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nteract": {
      "version": "0.14.3"
    },
    "colab": {
      "name": "LS_DS_Unit_4_Sprint_Challenge_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQy4lfor54y-",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2*\n",
        "\n",
        "# Sprint Challenge - Neural Network Foundations\n",
        "\n",
        "Table of Problems\n",
        "\n",
        "1. [Defining Neural Networks](#Q1)\n",
        "2. [Perceptron on XOR Gates](#Q2)\n",
        "3. [Multilayer Perceptron](#Q3)\n",
        "4. [Keras MMP](#Q4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ke9kK2s54zK",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"Q1\"></a>\n",
        "## 1. Define the following terms:\n",
        "\n",
        "- **Neuron:** Each neuron is a mathematical operation that takes it’s input, multiplies it by it’s weights and then passes the sum through the activation function to the other neurons\n",
        "- **Input Layer:** The Input nodes provide information from the outside world to the network and are together referred to as the “Input Layer”. No computation is performed in any of the Input nodes – they just pass on the information to the hidden nodes.\n",
        "- **Hidden Layer:** The Hidden nodes have no direct connection with the outside world (hence the name “hidden”). They perform computations and transfer information from the input nodes to the output nodes. A collection of hidden nodes forms a “Hidden Layer”.¶\n",
        "- **Output Layer:** The Output nodes are collectively referred to as the “Output Layer” and are responsible for computations and transferring information from the network to the outside world.¶\n",
        "- **Activation:** The node applies the activation function to the weighted sum of its inputs. The purpose of the activation function is to introduce non-linearity into the output of a neuron. This is important because most real world data is non linear and we want neurons to learn these non linear representations.\n",
        "- **Backpropagation:** gives us detailed insights into how changing the weights and biases changes the overall behaviour of the network. Uses the partial derivative ∂C/∂w of the cost function C with respect to any weight w (or bias b) in the network. The expression tells us how quickly the cost changes when we change the weights and biases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMdsIxk854zS",
        "colab_type": "text"
      },
      "source": [
        "## 2. Perceptron on AND Gates <a id=\"Q3=2\"></a>\n",
        "\n",
        "Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
        "\n",
        "|x1\t|x2|x3|\ty|\n",
        "|---|---|---|---|\n",
        "1|\t1|\t1|\t1|\n",
        "1|\t0|\t1|\t0|\n",
        "0|\t1|\t1|\t0|\n",
        "0|\t0|\t1|\t0|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "Hp5AeNeq54za",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Establish inputs\n",
        "inputs = np.array([\n",
        "    [1,1,1],\n",
        "    [1,0,1],\n",
        "    [0,1,1],\n",
        "    [0,0,1]\n",
        "])\n",
        "\n",
        "# Establish Target\n",
        "target = [[1],\n",
        "          [0],\n",
        "          [0],\n",
        "          [0]]\n",
        "    \n",
        "class Perceptron:\n",
        "    def __init__(self,\n",
        "                inputLayerSize=3,\n",
        "                outputLayerSize=1,\n",
        "                hiddenLayerSize=4):        \n",
        "        #Define Hyperparameters\n",
        "        self.inputLayerSize = inputLayerSize\n",
        "        self.outputLayerSize = outputLayerSize\n",
        "        self.hiddenLayerSize = hiddenLayerSize\n",
        "        \n",
        "        #Input Node\n",
        "        self.weights1 = np.random.randn(self.inputLayerSize,\n",
        "                                        self.hiddenLayerSize)\n",
        "        #Output Node\n",
        "        self.weights2 = np.random.randn(self.hiddenLayerSize, \n",
        "                                        self.outputLayerSize)\n",
        "        \n",
        "    def sigmoid(self, s):\n",
        "        return 1 / (1+np.exp(-s))\n",
        "    \n",
        "    def sigmoidPrime(self, s):\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    def feed_forward(self,X):\n",
        "        \"\"\"\n",
        "        Calculate the NN inference using feed forward.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Weighted sum of inputs & hidden\n",
        "        self.hidden_sum = np.dot(X, self.weights1)\n",
        "        \n",
        "        # Activations of weighted sum\n",
        "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "        \n",
        "        # Weighted sum between hidden and output\n",
        "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
        "        \n",
        "        # Final Activation of output\n",
        "        self.activated_output = self.sigmoid(self.output_sum)\n",
        "        \n",
        "        return self.activated_output\n",
        "    \n",
        "    def backward(self, X, y, o):\n",
        "        \"\"\"\n",
        "        Backward propagate through the network\n",
        "        \"\"\"\n",
        "        self.o_error = y - o #error in output\n",
        "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
        "        \n",
        "        self.z2_error = self.o_delta.dot(self.weights2.T) \n",
        "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)\n",
        "        \n",
        "        self.weights1 += X.T.dot(self.z2_delta) \n",
        "        self.weights2 += self.activated_hidden.T.dot(self.o_delta) \n",
        "    \n",
        "    def train(self, X, y):\n",
        "      o = self.feed_forward(X)\n",
        "      self.backward(X, y, o)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gulggfxdK8-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d4045297-3d05-47b9-a7a8-e989e091f3c2"
      },
      "source": [
        "p1 = Perceptron()\n",
        "\n",
        "for i in range(300):\n",
        "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 50 ==0):\n",
        "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
        "        print('Input: \\n', inputs)\n",
        "        print('Actual Output: \\n', target)\n",
        "        print('Predicted Output: \\n', str(np.around(p1.feed_forward(inputs))))\n",
        "        print(\"Loss: \\n\", str(np.mean(np.square(target - p1.feed_forward(inputs)))))\n",
        "    p1.train(inputs,target)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------EPOCH 1---------+\n",
            "Input: \n",
            " [[1 1 1]\n",
            " [1 0 1]\n",
            " [0 1 1]\n",
            " [0 0 1]]\n",
            "Actual Output: \n",
            " [[1], [0], [0], [0]]\n",
            "Predicted Output: \n",
            " [[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n",
            "Loss: \n",
            " 0.40981696807820067\n",
            "+---------EPOCH 2---------+\n",
            "Input: \n",
            " [[1 1 1]\n",
            " [1 0 1]\n",
            " [0 1 1]\n",
            " [0 0 1]]\n",
            "Actual Output: \n",
            " [[1], [0], [0], [0]]\n",
            "Predicted Output: \n",
            " [[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n",
            "Loss: \n",
            " 0.32116477216469014\n",
            "+---------EPOCH 3---------+\n",
            "Input: \n",
            " [[1 1 1]\n",
            " [1 0 1]\n",
            " [0 1 1]\n",
            " [0 0 1]]\n",
            "Actual Output: \n",
            " [[1], [0], [0], [0]]\n",
            "Predicted Output: \n",
            " [[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n",
            "Loss: \n",
            " 0.25257136047733036\n",
            "+---------EPOCH 4---------+\n",
            "Input: \n",
            " [[1 1 1]\n",
            " [1 0 1]\n",
            " [0 1 1]\n",
            " [0 0 1]]\n",
            "Actual Output: \n",
            " [[1], [0], [0], [0]]\n",
            "Predicted Output: \n",
            " [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Loss: \n",
            " 0.2136158506669704\n",
            "+---------EPOCH 5---------+\n",
            "Input: \n",
            " [[1 1 1]\n",
            " [1 0 1]\n",
            " [0 1 1]\n",
            " [0 0 1]]\n",
            "Actual Output: \n",
            " [[1], [0], [0], [0]]\n",
            "Predicted Output: \n",
            " [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Loss: \n",
            " 0.19417785778402386\n",
            "+---------EPOCH 50---------+\n",
            "Input: \n",
            " [[1 1 1]\n",
            " [1 0 1]\n",
            " [0 1 1]\n",
            " [0 0 1]]\n",
            "Actual Output: \n",
            " [[1], [0], [0], [0]]\n",
            "Predicted Output: \n",
            " [[1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Loss: \n",
            " 0.08594660184075785\n",
            "+---------EPOCH 100---------+\n",
            "Input: \n",
            " [[1 1 1]\n",
            " [1 0 1]\n",
            " [0 1 1]\n",
            " [0 0 1]]\n",
            "Actual Output: \n",
            " [[1], [0], [0], [0]]\n",
            "Predicted Output: \n",
            " [[1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Loss: \n",
            " 0.031062521685128172\n",
            "+---------EPOCH 150---------+\n",
            "Input: \n",
            " [[1 1 1]\n",
            " [1 0 1]\n",
            " [0 1 1]\n",
            " [0 0 1]]\n",
            "Actual Output: \n",
            " [[1], [0], [0], [0]]\n",
            "Predicted Output: \n",
            " [[1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Loss: \n",
            " 0.014468438464256682\n",
            "+---------EPOCH 200---------+\n",
            "Input: \n",
            " [[1 1 1]\n",
            " [1 0 1]\n",
            " [0 1 1]\n",
            " [0 0 1]]\n",
            "Actual Output: \n",
            " [[1], [0], [0], [0]]\n",
            "Predicted Output: \n",
            " [[1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Loss: \n",
            " 0.008515946725137622\n",
            "+---------EPOCH 250---------+\n",
            "Input: \n",
            " [[1 1 1]\n",
            " [1 0 1]\n",
            " [0 1 1]\n",
            " [0 0 1]]\n",
            "Actual Output: \n",
            " [[1], [0], [0], [0]]\n",
            "Predicted Output: \n",
            " [[1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Loss: \n",
            " 0.005768600753273532\n",
            "+---------EPOCH 300---------+\n",
            "Input: \n",
            " [[1 1 1]\n",
            " [1 0 1]\n",
            " [0 1 1]\n",
            " [0 0 1]]\n",
            "Actual Output: \n",
            " [[1], [0], [0], [0]]\n",
            "Predicted Output: \n",
            " [[1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Loss: \n",
            " 0.004261195503136504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdM8PIsm54z2",
        "colab_type": "text"
      },
      "source": [
        "## 3. Multilayer Perceptron <a id=\"Q3\"></a>\n",
        "\n",
        "Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights.\n",
        "Your network must have one hidden layer.\n",
        "You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
        "Train your model on the Heart Disease dataset from UCI:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7omNjFMAVO91",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "2551751d-fdda-46a1-cd67-efcbecd3d58d"
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
        "\n",
        "X = df.drop(columns='target').values\n",
        "y = df[['target']].values\n",
        "print(df.shape)\n",
        "df.head(2)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(303, 14)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
              "0   63    1   3       145   233    1  ...      0      2.3      0   0     1       1\n",
              "1   37    1   2       130   250    0  ...      0      3.5      0   0     2       1\n",
              "\n",
              "[2 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSxlzVxPT3FX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "Z98_axuD54z8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Neural_Network(object):\n",
        "    def __init__(self, inputLayerSize=4, outputLayerSize=1, hiddenLayerSize=4):\n",
        "        \n",
        "        #Define Hyperparameters\n",
        "        self.inputLayerSize = inputLayerSize\n",
        "        self.outputLayerSize = outputLayerSize\n",
        "        self.hiddenLayerSize = hiddenLayerSize\n",
        "        \n",
        "        #Weights (parameters)\n",
        "        #Input Layer\n",
        "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
        "        # Hidden Layers\n",
        "        self.Wh = np.random.randn(self.hiddenLayerSize,self.hiddenLayerSize)\n",
        "        # Output Layer\n",
        "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Propagate inputs though network\n",
        "        \"\"\"\n",
        "        # Input/1st Hidden Layer\n",
        "        self.z2 = np.dot(X, self.W1)\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "        # 2nd Hidden Layer\n",
        "        self.zh = np.dot(self.a2, self.Wh)\n",
        "        self.a3 = self.sigmoid(self.zh)\n",
        "        # Output Layer\n",
        "        self.z3 = np.dot(self.a3, self.W2)\n",
        "        yHat = self.sigmoid(self.z3) \n",
        "        return yHat\n",
        "        \n",
        "    def sigmoid(self, z):\n",
        "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
        "        return 1/(1+np.exp(-z))\n",
        "    \n",
        "    def sigmoidPrime(self,z):\n",
        "        #Gradient of sigmoid\n",
        "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
        "    \n",
        "    def costFunction(self, X, y):\n",
        "        #Compute cost for given X,y, use weights already stored in class.\n",
        "        self.yHat = self.forward(X)\n",
        "        J = 0.5*sum((y-self.yHat)**2)\n",
        "        return J\n",
        "        \n",
        "    def costFunctionPrime(self, X, y):\n",
        "        #Compute derivative with respect to W and W2 for a given X and y:\n",
        "        self.yHat = self.forward(X)\n",
        "        \n",
        "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
        "        dJdW2 = np.dot(self.a2.T, delta3)\n",
        "        \n",
        "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
        "        dJdW1 = np.dot(X.T, delta2)  \n",
        "        \n",
        "        return dJdW1, dJdW2\n",
        "    \n",
        "    #Helper Functions for interacting with other classes:\n",
        "    def getParams(self):\n",
        "        #Get W1 and W2 unrolled into vector:\n",
        "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
        "        return params\n",
        "    \n",
        "    def setParams(self, params):\n",
        "        #Set W1 and W2 using single paramater vector.\n",
        "        W1_start = 0\n",
        "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
        "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
        "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
        "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
        "        \n",
        "    def computeGradients(self, X, y):\n",
        "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
        "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))\n",
        "    \n",
        "\n",
        "class trainer(object):\n",
        "    def __init__(self, N):\n",
        "        #Make Local reference to network:\n",
        "        self.N = N\n",
        "        \n",
        "    def callbackF(self, params):\n",
        "        self.N.setParams(params)\n",
        "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
        "        \n",
        "    def costFunctionWrapper(self, params, X, y):\n",
        "        self.N.setParams(params)\n",
        "        cost = self.N.costFunction(X, y)\n",
        "        grad = self.N.computeGradients(X,y)\n",
        "        \n",
        "        return cost, grad\n",
        "        \n",
        "    def train(self, X, y):\n",
        "        #Make an internal variable for the callback function:\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        #Make empty list to store costs:\n",
        "        self.J = []\n",
        "        \n",
        "        params0 = self.N.getParams()\n",
        "\n",
        "        options = {'maxiter': 200, 'disp' : True}\n",
        "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
        "                                 args=(X, y), options=options, callback=self.callbackF)\n",
        "\n",
        "        self.N.setParams(_res.x)\n",
        "        self.optimizationResults = _res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP65v4hpS6-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN = Neural_Network(inputLayerSize=x.shape[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ngwhzTOTFm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "T = trainer(NN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBZRqjFXTFjm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "ee216ddd-e153-4db0-c764-48fc862d2f8c"
      },
      "source": [
        "T.train(X, y)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Desired error not necessarily achieved due to precision loss.\n",
            "         Current function value: 37.559564\n",
            "         Iterations: 1\n",
            "         Function evaluations: 110\n",
            "         Gradient evaluations: 98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmkRM00UTPff",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "664be227-c648-4b07-8e7e-41b26dcf840c"
      },
      "source": [
        "X.shape, y.shape, type(x)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((303, 13), (303, 1), numpy.ndarray)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BsijFxy540M",
        "colab_type": "text"
      },
      "source": [
        "## 4. Keras MMP <a id=\"Q4\"></a>\n",
        "\n",
        "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
        "Use the Heart Disease Dataset (binary classification)\n",
        "Use an appropriate loss function for a binary classification task\n",
        "Use an appropriate activation function on the final layer of your network.\n",
        "Train your model using verbose output for ease of grading.\n",
        "Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
        "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
        "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
        "You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "djpl77Yn540R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a958279-af07-4afc-be36-627d7b4d32b6"
      },
      "source": [
        "import pandas as pd\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import keras\n",
        "import tensorflow"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9OHtzNTG3f-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = keras.utils.to_categorical(y, len(set(y)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-pjXI-0CvSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(4, input_dim=13, activation='relu'))\n",
        "model.add(Dense(5, activation='tanh'))\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye2byruDE7UT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9f46df30-65a6-452d-afe6-9fcc5d159fdb"
      },
      "source": [
        "model.fit(x, y, epochs=50, validation_split=.1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 272 samples, validate on 31 samples\n",
            "Epoch 1/50\n",
            "272/272 [==============================] - 0s 1ms/sample - loss: 0.6293 - acc: 0.6324 - val_loss: 0.7947 - val_acc: 0.3548\n",
            "Epoch 2/50\n",
            "272/272 [==============================] - 0s 74us/sample - loss: 0.6151 - acc: 0.6379 - val_loss: 0.8020 - val_acc: 0.3226\n",
            "Epoch 3/50\n",
            "272/272 [==============================] - 0s 87us/sample - loss: 0.6023 - acc: 0.6526 - val_loss: 0.8109 - val_acc: 0.3871\n",
            "Epoch 4/50\n",
            "272/272 [==============================] - 0s 71us/sample - loss: 0.5911 - acc: 0.6930 - val_loss: 0.8169 - val_acc: 0.3871\n",
            "Epoch 5/50\n",
            "272/272 [==============================] - 0s 78us/sample - loss: 0.5811 - acc: 0.6967 - val_loss: 0.8225 - val_acc: 0.4032\n",
            "Epoch 6/50\n",
            "272/272 [==============================] - 0s 75us/sample - loss: 0.5717 - acc: 0.7132 - val_loss: 0.8259 - val_acc: 0.4032\n",
            "Epoch 7/50\n",
            "272/272 [==============================] - 0s 78us/sample - loss: 0.5624 - acc: 0.7243 - val_loss: 0.8277 - val_acc: 0.4032\n",
            "Epoch 8/50\n",
            "272/272 [==============================] - 0s 71us/sample - loss: 0.5541 - acc: 0.7261 - val_loss: 0.8300 - val_acc: 0.4194\n",
            "Epoch 9/50\n",
            "272/272 [==============================] - 0s 77us/sample - loss: 0.5466 - acc: 0.7224 - val_loss: 0.8297 - val_acc: 0.4516\n",
            "Epoch 10/50\n",
            "272/272 [==============================] - 0s 80us/sample - loss: 0.5393 - acc: 0.7298 - val_loss: 0.8288 - val_acc: 0.4516\n",
            "Epoch 11/50\n",
            "272/272 [==============================] - 0s 83us/sample - loss: 0.5323 - acc: 0.7426 - val_loss: 0.8257 - val_acc: 0.4516\n",
            "Epoch 12/50\n",
            "272/272 [==============================] - 0s 79us/sample - loss: 0.5259 - acc: 0.7463 - val_loss: 0.8249 - val_acc: 0.4677\n",
            "Epoch 13/50\n",
            "272/272 [==============================] - 0s 90us/sample - loss: 0.5198 - acc: 0.7555 - val_loss: 0.8257 - val_acc: 0.4677\n",
            "Epoch 14/50\n",
            "272/272 [==============================] - 0s 89us/sample - loss: 0.5131 - acc: 0.7592 - val_loss: 0.8244 - val_acc: 0.4839\n",
            "Epoch 15/50\n",
            "272/272 [==============================] - 0s 81us/sample - loss: 0.5071 - acc: 0.7702 - val_loss: 0.8204 - val_acc: 0.5000\n",
            "Epoch 16/50\n",
            "272/272 [==============================] - 0s 73us/sample - loss: 0.5011 - acc: 0.8015 - val_loss: 0.8164 - val_acc: 0.5645\n",
            "Epoch 17/50\n",
            "272/272 [==============================] - 0s 78us/sample - loss: 0.4951 - acc: 0.8125 - val_loss: 0.8125 - val_acc: 0.5806\n",
            "Epoch 18/50\n",
            "272/272 [==============================] - 0s 89us/sample - loss: 0.4898 - acc: 0.8180 - val_loss: 0.8094 - val_acc: 0.5968\n",
            "Epoch 19/50\n",
            "272/272 [==============================] - 0s 75us/sample - loss: 0.4844 - acc: 0.8235 - val_loss: 0.8059 - val_acc: 0.5968\n",
            "Epoch 20/50\n",
            "272/272 [==============================] - 0s 79us/sample - loss: 0.4793 - acc: 0.8254 - val_loss: 0.8022 - val_acc: 0.5968\n",
            "Epoch 21/50\n",
            "272/272 [==============================] - 0s 78us/sample - loss: 0.4738 - acc: 0.8272 - val_loss: 0.8006 - val_acc: 0.5968\n",
            "Epoch 22/50\n",
            "272/272 [==============================] - 0s 78us/sample - loss: 0.4691 - acc: 0.8272 - val_loss: 0.7947 - val_acc: 0.5968\n",
            "Epoch 23/50\n",
            "272/272 [==============================] - 0s 82us/sample - loss: 0.4636 - acc: 0.8327 - val_loss: 0.7913 - val_acc: 0.5968\n",
            "Epoch 24/50\n",
            "272/272 [==============================] - 0s 76us/sample - loss: 0.4590 - acc: 0.8309 - val_loss: 0.7849 - val_acc: 0.5968\n",
            "Epoch 25/50\n",
            "272/272 [==============================] - 0s 83us/sample - loss: 0.4535 - acc: 0.8346 - val_loss: 0.7831 - val_acc: 0.5968\n",
            "Epoch 26/50\n",
            "272/272 [==============================] - 0s 87us/sample - loss: 0.4483 - acc: 0.8364 - val_loss: 0.7837 - val_acc: 0.5968\n",
            "Epoch 27/50\n",
            "272/272 [==============================] - 0s 91us/sample - loss: 0.4438 - acc: 0.8382 - val_loss: 0.7762 - val_acc: 0.5968\n",
            "Epoch 28/50\n",
            "272/272 [==============================] - 0s 88us/sample - loss: 0.4387 - acc: 0.8419 - val_loss: 0.7730 - val_acc: 0.5968\n",
            "Epoch 29/50\n",
            "272/272 [==============================] - 0s 92us/sample - loss: 0.4341 - acc: 0.8419 - val_loss: 0.7698 - val_acc: 0.5968\n",
            "Epoch 30/50\n",
            "272/272 [==============================] - 0s 93us/sample - loss: 0.4298 - acc: 0.8419 - val_loss: 0.7648 - val_acc: 0.5968\n",
            "Epoch 31/50\n",
            "272/272 [==============================] - 0s 79us/sample - loss: 0.4251 - acc: 0.8438 - val_loss: 0.7630 - val_acc: 0.5968\n",
            "Epoch 32/50\n",
            "272/272 [==============================] - 0s 79us/sample - loss: 0.4207 - acc: 0.8438 - val_loss: 0.7620 - val_acc: 0.5968\n",
            "Epoch 33/50\n",
            "272/272 [==============================] - 0s 73us/sample - loss: 0.4161 - acc: 0.8419 - val_loss: 0.7566 - val_acc: 0.5968\n",
            "Epoch 34/50\n",
            "272/272 [==============================] - 0s 83us/sample - loss: 0.4118 - acc: 0.8419 - val_loss: 0.7533 - val_acc: 0.5968\n",
            "Epoch 35/50\n",
            "272/272 [==============================] - 0s 82us/sample - loss: 0.4075 - acc: 0.8419 - val_loss: 0.7553 - val_acc: 0.5968\n",
            "Epoch 36/50\n",
            "272/272 [==============================] - 0s 78us/sample - loss: 0.4029 - acc: 0.8438 - val_loss: 0.7512 - val_acc: 0.5968\n",
            "Epoch 37/50\n",
            "272/272 [==============================] - 0s 75us/sample - loss: 0.3992 - acc: 0.8438 - val_loss: 0.7485 - val_acc: 0.6290\n",
            "Epoch 38/50\n",
            "272/272 [==============================] - 0s 83us/sample - loss: 0.3955 - acc: 0.8474 - val_loss: 0.7499 - val_acc: 0.6452\n",
            "Epoch 39/50\n",
            "272/272 [==============================] - 0s 74us/sample - loss: 0.3924 - acc: 0.8493 - val_loss: 0.7448 - val_acc: 0.6452\n",
            "Epoch 40/50\n",
            "272/272 [==============================] - 0s 73us/sample - loss: 0.3887 - acc: 0.8511 - val_loss: 0.7458 - val_acc: 0.6613\n",
            "Epoch 41/50\n",
            "272/272 [==============================] - 0s 75us/sample - loss: 0.3854 - acc: 0.8511 - val_loss: 0.7429 - val_acc: 0.6774\n",
            "Epoch 42/50\n",
            "272/272 [==============================] - 0s 92us/sample - loss: 0.3823 - acc: 0.8511 - val_loss: 0.7454 - val_acc: 0.6774\n",
            "Epoch 43/50\n",
            "272/272 [==============================] - 0s 82us/sample - loss: 0.3788 - acc: 0.8529 - val_loss: 0.7435 - val_acc: 0.6774\n",
            "Epoch 44/50\n",
            "272/272 [==============================] - 0s 93us/sample - loss: 0.3756 - acc: 0.8548 - val_loss: 0.7423 - val_acc: 0.6774\n",
            "Epoch 45/50\n",
            "272/272 [==============================] - 0s 96us/sample - loss: 0.3726 - acc: 0.8566 - val_loss: 0.7448 - val_acc: 0.6774\n",
            "Epoch 46/50\n",
            "272/272 [==============================] - 0s 73us/sample - loss: 0.3694 - acc: 0.8585 - val_loss: 0.7441 - val_acc: 0.6613\n",
            "Epoch 47/50\n",
            "272/272 [==============================] - 0s 94us/sample - loss: 0.3664 - acc: 0.8585 - val_loss: 0.7441 - val_acc: 0.6613\n",
            "Epoch 48/50\n",
            "272/272 [==============================] - 0s 94us/sample - loss: 0.3634 - acc: 0.8603 - val_loss: 0.7437 - val_acc: 0.6613\n",
            "Epoch 49/50\n",
            "272/272 [==============================] - 0s 73us/sample - loss: 0.3604 - acc: 0.8603 - val_loss: 0.7468 - val_acc: 0.6452\n",
            "Epoch 50/50\n",
            "272/272 [==============================] - 0s 84us/sample - loss: 0.3573 - acc: 0.8640 - val_loss: 0.7453 - val_acc: 0.6452\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1b48252e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM8wiThtJ-HB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(4, input_dim=13, activation='relu'))\n",
        "  model.add(Dense(5, activation='tanh'))\n",
        "  model.add(Dense(2, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
        "\n",
        "param_grid = {'batch_size': [10, 20],\n",
        "              'epochs': [20, 40]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqehvt2aK6SZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73bc9727-40da-4165-bb70-e8ed3d08cc1d"
      },
      "source": [
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(x, y)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "303/303 [==============================] - 0s 591us/sample - loss: 0.7183 - acc: 0.4736\n",
            "Epoch 2/40\n",
            "303/303 [==============================] - 0s 155us/sample - loss: 0.6828 - acc: 0.5413\n",
            "Epoch 3/40\n",
            "303/303 [==============================] - 0s 138us/sample - loss: 0.6504 - acc: 0.6271\n",
            "Epoch 4/40\n",
            "303/303 [==============================] - 0s 143us/sample - loss: 0.6222 - acc: 0.6848\n",
            "Epoch 5/40\n",
            "303/303 [==============================] - 0s 139us/sample - loss: 0.5964 - acc: 0.7096\n",
            "Epoch 6/40\n",
            "303/303 [==============================] - 0s 143us/sample - loss: 0.5723 - acc: 0.7327\n",
            "Epoch 7/40\n",
            "303/303 [==============================] - 0s 146us/sample - loss: 0.5506 - acc: 0.7492\n",
            "Epoch 8/40\n",
            "303/303 [==============================] - 0s 150us/sample - loss: 0.5301 - acc: 0.7624\n",
            "Epoch 9/40\n",
            "303/303 [==============================] - 0s 144us/sample - loss: 0.5115 - acc: 0.7772\n",
            "Epoch 10/40\n",
            "303/303 [==============================] - 0s 147us/sample - loss: 0.4950 - acc: 0.7838\n",
            "Epoch 11/40\n",
            "303/303 [==============================] - 0s 199us/sample - loss: 0.4797 - acc: 0.7937\n",
            "Epoch 12/40\n",
            "303/303 [==============================] - 0s 205us/sample - loss: 0.4660 - acc: 0.7954\n",
            "Epoch 13/40\n",
            "303/303 [==============================] - 0s 157us/sample - loss: 0.4540 - acc: 0.8003\n",
            "Epoch 14/40\n",
            "303/303 [==============================] - 0s 142us/sample - loss: 0.4433 - acc: 0.8069\n",
            "Epoch 15/40\n",
            "303/303 [==============================] - 0s 147us/sample - loss: 0.4343 - acc: 0.8102\n",
            "Epoch 16/40\n",
            "303/303 [==============================] - 0s 135us/sample - loss: 0.4258 - acc: 0.8135\n",
            "Epoch 17/40\n",
            "303/303 [==============================] - 0s 158us/sample - loss: 0.4180 - acc: 0.8201\n",
            "Epoch 18/40\n",
            "303/303 [==============================] - 0s 157us/sample - loss: 0.4110 - acc: 0.8201\n",
            "Epoch 19/40\n",
            "303/303 [==============================] - 0s 158us/sample - loss: 0.4055 - acc: 0.8333\n",
            "Epoch 20/40\n",
            "303/303 [==============================] - 0s 146us/sample - loss: 0.3990 - acc: 0.8366\n",
            "Epoch 21/40\n",
            "303/303 [==============================] - 0s 159us/sample - loss: 0.3939 - acc: 0.8383\n",
            "Epoch 22/40\n",
            "303/303 [==============================] - 0s 144us/sample - loss: 0.3898 - acc: 0.8383\n",
            "Epoch 23/40\n",
            "303/303 [==============================] - 0s 152us/sample - loss: 0.3859 - acc: 0.8383\n",
            "Epoch 24/40\n",
            "303/303 [==============================] - 0s 154us/sample - loss: 0.3816 - acc: 0.8366\n",
            "Epoch 25/40\n",
            "303/303 [==============================] - 0s 152us/sample - loss: 0.3780 - acc: 0.8399\n",
            "Epoch 26/40\n",
            "303/303 [==============================] - 0s 150us/sample - loss: 0.3745 - acc: 0.8416\n",
            "Epoch 27/40\n",
            "303/303 [==============================] - 0s 198us/sample - loss: 0.3711 - acc: 0.8416\n",
            "Epoch 28/40\n",
            "303/303 [==============================] - 0s 138us/sample - loss: 0.3686 - acc: 0.8383\n",
            "Epoch 29/40\n",
            "303/303 [==============================] - 0s 143us/sample - loss: 0.3653 - acc: 0.8416\n",
            "Epoch 30/40\n",
            "303/303 [==============================] - 0s 140us/sample - loss: 0.3631 - acc: 0.8416\n",
            "Epoch 31/40\n",
            "303/303 [==============================] - 0s 158us/sample - loss: 0.3597 - acc: 0.8449\n",
            "Epoch 32/40\n",
            "303/303 [==============================] - 0s 164us/sample - loss: 0.3579 - acc: 0.8449\n",
            "Epoch 33/40\n",
            "303/303 [==============================] - 0s 152us/sample - loss: 0.3555 - acc: 0.8432\n",
            "Epoch 34/40\n",
            "303/303 [==============================] - 0s 145us/sample - loss: 0.3535 - acc: 0.8482\n",
            "Epoch 35/40\n",
            "303/303 [==============================] - 0s 149us/sample - loss: 0.3508 - acc: 0.8498\n",
            "Epoch 36/40\n",
            "303/303 [==============================] - 0s 157us/sample - loss: 0.3492 - acc: 0.8498\n",
            "Epoch 37/40\n",
            "303/303 [==============================] - 0s 142us/sample - loss: 0.3469 - acc: 0.8515\n",
            "Epoch 38/40\n",
            "303/303 [==============================] - 0s 135us/sample - loss: 0.3456 - acc: 0.8515\n",
            "Epoch 39/40\n",
            "303/303 [==============================] - 0s 153us/sample - loss: 0.3434 - acc: 0.8548\n",
            "Epoch 40/40\n",
            "303/303 [==============================] - 0s 158us/sample - loss: 0.3424 - acc: 0.8564\n",
            "Best: 0.6666666865348816 using {'batch_size': 10, 'epochs': 40}\n",
            "Means: 0.4785478512446086, Stdev: 0.22203153125059896 with: {'batch_size': 10, 'epochs': 20}\n",
            "Means: 0.6666666865348816, Stdev: 0.12421260077050067 with: {'batch_size': 10, 'epochs': 40}\n",
            "Means: 0.39273926615715027, Stdev: 0.26709233886057665 with: {'batch_size': 20, 'epochs': 20}\n",
            "Means: 0.4686468690633774, Stdev: 0.2588603522693043 with: {'batch_size': 20, 'epochs': 40}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}