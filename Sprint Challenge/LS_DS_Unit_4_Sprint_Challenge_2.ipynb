{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:** This is what takes the inputs, multiplies them by the weights, and does the sum or whatever calculation is being performed on the data. This is basically what does the work for the hidden layers. \n",
    "\n",
    "- **Input Layer:**  This is what takes in the data that is being fed into the model. \n",
    "\n",
    "- **Hidden Layer:** This is where the neural network takes the input and processes it into the output given weights and biases.\n",
    "\n",
    "- **Output Layer:** This is what comes out of the model, either a classification or a prediction value or set of values. \n",
    "\n",
    "- **Activation:** This defines the output of the neuron given a particular input. This is what is used to do the calculation. \n",
    "\n",
    "- **Backpropagation:** This computs the gradient with respect to the loss function and the weights of the network for a single input-output. This algorithm works recursively and the neurons are updated in reverse order. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(10000, 3)\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chocolate</th>\n      <th>gummy</th>\n      <th>ate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   chocolate  gummy  ate\n0          0      1    1\n1          1      0    1\n2          0      1    1\n3          0      0    0\n4          1      1    0"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(candy.shape)\n",
    "candy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. You will not be able to achieve more than ~50% with the simple perceptron. Explain why you could not achieve a higher accuracy with the *simple perceptron* architecture, because it's possible to achieve ~95% accuracy on this dataset. Provide your answer in markdown (and *optional* data anlysis code) after your perceptron implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "\n",
    "# Creating sigmoid functions\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return 1/(1 - sigmoid(x))\n",
    "\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy['ate'].values\n",
    "\n",
    "correct_output = np.array(y)\n",
    "correct_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "weights:\n [[0.37454012]\n [0.95071431]]\n\n(10000, 2) (2, 1) \n\nweighted_sum:\n [[0.95071431]\n [0.37454012]\n [0.95071431]\n ...\n [0.95071431]\n [0.95071431]\n [0.37454012]]\n\nactivated_outputs:\n [[0.72125881]\n [0.59255557]\n [0.72125881]\n ...\n [0.72125881]\n [0.72125881]\n [0.59255557]]\n\nerror:\n [[0.27874119 0.27874119 0.27874119 ... 0.27874119 0.27874119 0.27874119]\n [0.40744443 0.40744443 0.40744443 ... 0.40744443 0.40744443 0.40744443]\n [0.27874119 0.27874119 0.27874119 ... 0.27874119 0.27874119 0.27874119]\n ...\n [0.27874119 0.27874119 0.27874119 ... 0.27874119 0.27874119 0.27874119]\n [0.27874119 0.27874119 0.27874119 ... 0.27874119 0.27874119 0.27874119]\n [0.40744443 0.40744443 0.40744443 ... 0.40744443 0.40744443 0.40744443]]\n\nadjustments:\n [[0.85211767 0.85211767 0.85211767 ... 0.85211767 0.85211767 0.85211767]\n [1.14435026 1.14435026 1.14435026 ... 1.14435026 1.14435026 1.14435026]\n [0.85211767 0.85211767 0.85211767 ... 0.85211767 0.85211767 0.85211767]\n ...\n [0.85211767 0.85211767 0.85211767 ... 0.85211767 0.85211767 0.85211767]\n [0.85211767 0.85211767 0.85211767 ... 0.85211767 0.85211767 0.85211767]\n [1.14435026 1.14435026 1.14435026 ... 1.14435026 1.14435026 1.14435026]]\n\nweights:\n [[4531.41663252 4531.41663252 4531.41663252 ... 4531.41663252\n  4531.41663252 4531.41663252]\n [3806.33011798 3806.33011798 3806.33011798 ... 3806.33011798\n  3806.33011798 3806.33011798]]\n\n"
    }
   ],
   "source": [
    "# Perceptron function\n",
    "\n",
    "np.random.seed(42)\n",
    "weights = np.random.random((2,1))\n",
    "print(f'weights:\\n {weights}\\n')\n",
    "print(X.shape, weights.shape, '\\n')\n",
    "\n",
    "inputs = X\n",
    "weighted_sum = np.dot(inputs, weights)\n",
    "print(f'weighted_sum:\\n {weighted_sum}\\n')\n",
    "\n",
    "activated_outputs = sigmoid(weighted_sum)\n",
    "print(f'activated_outputs:\\n {activated_outputs}\\n')\n",
    "\n",
    "error = correct_output - activated_outputs\n",
    "print(f'error:\\n {error}\\n')\n",
    "\n",
    "adjustments = error * sigmoid_derivative(activated_outputs)\n",
    "print(f'adjustments:\\n {adjustments}\\n')\n",
    "\n",
    "weights = weights + np.dot(inputs.T, adjustments)\n",
    "print(f'weights:\\n {weights}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\Rob\\.virtualenvs\\DS-Unit-4-Sprint-2-Neural-Networks-AITeJwQM\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: overflow encountered in exp\n  \nWeights after training\n[[4531.41663252 4531.41663252 4531.41663252 ... 4531.41663252\n  4531.41663252 4531.41663252]\n [3806.33011798 3806.33011798 3806.33011798 ... 3806.33011798\n  3806.33011798 3806.33011798]]\nOutput after training\n[[1. 1. 1. ... 1. 1. 1.]\n [1. 1. 1. ... 1. 1. 1.]\n [1. 1. 1. ... 1. 1. 1.]\n ...\n [1. 1. 1. ... 1. 1. 1.]\n [1. 1. 1. ... 1. 1. 1.]\n [1. 1. 1. ... 1. 1. 1.]]\n"
    }
   ],
   "source": [
    "for iteration in range(100):\n",
    "    \n",
    "    # Weighted sum of inputs / weights\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "    # Activate!\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    \n",
    "    # Cac error\n",
    "    error = correct_output - activated_output\n",
    "    \n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "    \n",
    "    # Update the Weights\n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "    \n",
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You can see that it turned out much better above with more iterations! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "((10000, 2), (10000,))"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / np.amax(X, axis=0)\n",
    "y = y / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = X.shape[1]\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, x):\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weighted sum of inputs => hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "        \n",
    "    def backward(self, X,y,o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        \n",
    "        # Error in Output\n",
    "        self.o_error = y - o\n",
    "        \n",
    "        # Apply Derivative of Sigmoid to error\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        # z2 error\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        # Adjustment to first set of weights (input => hidden)\n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "\n",
    "        # Adjustment to second set of weights (hidden => output)\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---------EPOCH 1---------+\nInput: \n [[0. 1.]\n [1. 0.]\n [0. 1.]\n ...\n [0. 1.]\n [0. 1.]\n [1. 0.]]\nActual Output: \n [0.01 0.01 0.01 ... 0.01 0.01 0.01]\nPredicted Output: \n [[0.66585232]\n [0.69899744]\n [0.66585232]\n ...\n [0.66585232]\n [0.66585232]\n [0.69899744]]\nLoss: \n 0.4583749010751494\n"
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (10000,10000) and (1,3) not aligned: 10000 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-204eb52c88cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Predicted Output: \\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss: \\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-77-a4c7fb266462>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-77-a4c7fb266462>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, X, y, o)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# z2 error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz2_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mo_delta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz2_delta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz2_error\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoidPrime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivated_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (10000,10000) and (1,3) not aligned: 10000 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "\n",
    "# Number of Epochs / Iterations\n",
    "for i in range(10):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = NeuralNetwork()\n",
    "# nn.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(303, 14)\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>cp</th>\n      <th>trestbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalach</th>\n      <th>exang</th>\n      <th>oldpeak</th>\n      <th>slope</th>\n      <th>ca</th>\n      <th>thal</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>71</th>\n      <td>51</td>\n      <td>1</td>\n      <td>2</td>\n      <td>94</td>\n      <td>227</td>\n      <td>0</td>\n      <td>1</td>\n      <td>154</td>\n      <td>1</td>\n      <td>0.00000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>39</td>\n      <td>1</td>\n      <td>2</td>\n      <td>140</td>\n      <td>321</td>\n      <td>0</td>\n      <td>0</td>\n      <td>182</td>\n      <td>0</td>\n      <td>0.00000</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>46</td>\n      <td>1</td>\n      <td>1</td>\n      <td>101</td>\n      <td>197</td>\n      <td>1</td>\n      <td>1</td>\n      <td>156</td>\n      <td>0</td>\n      <td>0.00000</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>64</td>\n      <td>1</td>\n      <td>3</td>\n      <td>110</td>\n      <td>211</td>\n      <td>0</td>\n      <td>0</td>\n      <td>144</td>\n      <td>1</td>\n      <td>1.80000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>48</td>\n      <td>1</td>\n      <td>1</td>\n      <td>130</td>\n      <td>245</td>\n      <td>0</td>\n      <td>0</td>\n      <td>180</td>\n      <td>0</td>\n      <td>0.20000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "    age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n71   51    1   2        94   227    0        1      154      1  0.00000   \n44   39    1   2       140   321    0        0      182      0  0.00000   \n87   46    1   1       101   197    1        1      156      0  0.00000   \n13   64    1   3       110   211    0        0      144      1  1.80000   \n41   48    1   1       130   245    0        0      180      0  0.20000   \n\n    slope  ca  thal  target  \n71      2   1     3       1  \n44      2   0     2       1  \n87      2   0     3       1  \n13      1   0     2       1  \n41      1   0     2       1  "
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(242, 13) (242,) (61, 13) (61,)\n"
    },
    {
     "data": {
      "text/plain": "array([-0.3896464 ,  0.67015058, -0.9200267 ,  0.44004645,  1.0187346 ,\n       -0.43831293,  0.88510589, -1.17558097,  1.46385011,  2.66494207,\n       -0.62579811,  2.07087229,  1.10598945])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Setting target and features\n",
    "target = 'target'\n",
    "\n",
    "# Dropping 'result' and 'age' as they seem to be confounding and not helpful\n",
    "features = df.columns.drop([target])\n",
    "\n",
    "# Setting X and y\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Checking data\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "ean_squared_error: 0.4917\nEpoch 107/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.4492 - accuracy: 0.5130 - mean_squared_error: 0.4492 - val_loss: 0.4892 - val_accuracy: 0.5510 - val_mean_squared_error: 0.4892\nEpoch 108/200\n193/193 [==============================] - 0s 104us/sample - loss: 0.4480 - accuracy: 0.5130 - mean_squared_error: 0.4480 - val_loss: 0.4868 - val_accuracy: 0.5510 - val_mean_squared_error: 0.4868\nEpoch 109/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.4468 - accuracy: 0.5130 - mean_squared_error: 0.4468 - val_loss: 0.4829 - val_accuracy: 0.5510 - val_mean_squared_error: 0.4829\nEpoch 110/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.4448 - accuracy: 0.5181 - mean_squared_error: 0.4448 - val_loss: 0.4801 - val_accuracy: 0.5510 - val_mean_squared_error: 0.4801\nEpoch 111/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.4432 - accuracy: 0.5181 - mean_squared_error: 0.4432 - val_loss: 0.4770 - val_accuracy: 0.5510 - val_mean_squared_error: 0.4770\nEpoch 112/200\n193/193 [==============================] - 0s 104us/sample - loss: 0.4415 - accuracy: 0.5181 - mean_squared_error: 0.4415 - val_loss: 0.4744 - val_accuracy: 0.5714 - val_mean_squared_error: 0.4744\nEpoch 113/200\n193/193 [==============================] - 0s 98us/sample - loss: 0.4400 - accuracy: 0.5285 - mean_squared_error: 0.4400 - val_loss: 0.4722 - val_accuracy: 0.5714 - val_mean_squared_error: 0.4722\nEpoch 114/200\n193/193 [==============================] - 0s 88us/sample - loss: 0.4385 - accuracy: 0.5285 - mean_squared_error: 0.4385 - val_loss: 0.4693 - val_accuracy: 0.5918 - val_mean_squared_error: 0.4693\nEpoch 115/200\n193/193 [==============================] - 0s 88us/sample - loss: 0.4367 - accuracy: 0.5285 - mean_squared_error: 0.4367 - val_loss: 0.4661 - val_accuracy: 0.5918 - val_mean_squared_error: 0.4661\nEpoch 116/200\n193/193 [==============================] - 0s 93us/sample - loss: 0.4352 - accuracy: 0.5285 - mean_squared_error: 0.4352 - val_loss: 0.4629 - val_accuracy: 0.5918 - val_mean_squared_error: 0.4629\nEpoch 117/200\n193/193 [==============================] - 0s 88us/sample - loss: 0.4326 - accuracy: 0.5285 - mean_squared_error: 0.4326 - val_loss: 0.4581 - val_accuracy: 0.5714 - val_mean_squared_error: 0.4581\nEpoch 118/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.4308 - accuracy: 0.5285 - mean_squared_error: 0.4308 - val_loss: 0.4549 - val_accuracy: 0.5714 - val_mean_squared_error: 0.4549\nEpoch 119/200\n193/193 [==============================] - 0s 88us/sample - loss: 0.4293 - accuracy: 0.5285 - mean_squared_error: 0.4293 - val_loss: 0.4526 - val_accuracy: 0.5714 - val_mean_squared_error: 0.4526\nEpoch 120/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.4277 - accuracy: 0.5285 - mean_squared_error: 0.4277 - val_loss: 0.4506 - val_accuracy: 0.5714 - val_mean_squared_error: 0.4506\nEpoch 121/200\n193/193 [==============================] - 0s 98us/sample - loss: 0.4265 - accuracy: 0.5285 - mean_squared_error: 0.4265 - val_loss: 0.4487 - val_accuracy: 0.5714 - val_mean_squared_error: 0.4487\nEpoch 122/200\n193/193 [==============================] - 0s 93us/sample - loss: 0.4253 - accuracy: 0.5337 - mean_squared_error: 0.4253 - val_loss: 0.4468 - val_accuracy: 0.5714 - val_mean_squared_error: 0.4468\nEpoch 123/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.4242 - accuracy: 0.5337 - mean_squared_error: 0.4242 - val_loss: 0.4449 - val_accuracy: 0.5714 - val_mean_squared_error: 0.4449\nEpoch 124/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.4231 - accuracy: 0.5337 - mean_squared_error: 0.4231 - val_loss: 0.4428 - val_accuracy: 0.5714 - val_mean_squared_error: 0.4428\nEpoch 125/200\n193/193 [==============================] - 0s 98us/sample - loss: 0.4217 - accuracy: 0.5389 - mean_squared_error: 0.4217 - val_loss: 0.4409 - val_accuracy: 0.5714 - val_mean_squared_error: 0.4409\nEpoch 126/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.4204 - accuracy: 0.5440 - mean_squared_error: 0.4204 - val_loss: 0.4389 - val_accuracy: 0.5714 - val_mean_squared_error: 0.4389\nEpoch 127/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.4191 - accuracy: 0.5440 - mean_squared_error: 0.4191 - val_loss: 0.4368 - val_accuracy: 0.5714 - val_mean_squared_error: 0.4368\nEpoch 128/200\n193/193 [==============================] - 0s 88us/sample - loss: 0.4172 - accuracy: 0.5440 - mean_squared_error: 0.4172 - val_loss: 0.4331 - val_accuracy: 0.5714 - val_mean_squared_error: 0.4331\nEpoch 129/200\n193/193 [==============================] - 0s 98us/sample - loss: 0.4145 - accuracy: 0.5544 - mean_squared_error: 0.4145 - val_loss: 0.4292 - val_accuracy: 0.5918 - val_mean_squared_error: 0.4292\nEpoch 130/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.4117 - accuracy: 0.5596 - mean_squared_error: 0.4117 - val_loss: 0.4265 - val_accuracy: 0.5918 - val_mean_squared_error: 0.4265\nEpoch 131/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.4093 - accuracy: 0.5648 - mean_squared_error: 0.4093 - val_loss: 0.4242 - val_accuracy: 0.5918 - val_mean_squared_error: 0.4242\nEpoch 132/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.4077 - accuracy: 0.5699 - mean_squared_error: 0.4077 - val_loss: 0.4222 - val_accuracy: 0.5918 - val_mean_squared_error: 0.4222\nEpoch 133/200\n193/193 [==============================] - 0s 88us/sample - loss: 0.4057 - accuracy: 0.5699 - mean_squared_error: 0.4057 - val_loss: 0.4204 - val_accuracy: 0.5918 - val_mean_squared_error: 0.4204\nEpoch 134/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.4039 - accuracy: 0.5699 - mean_squared_error: 0.4039 - val_loss: 0.4187 - val_accuracy: 0.5918 - val_mean_squared_error: 0.4187\nEpoch 135/200\n193/193 [==============================] - 0s 73us/sample - loss: 0.4023 - accuracy: 0.5751 - mean_squared_error: 0.4023 - val_loss: 0.4170 - val_accuracy: 0.5918 - val_mean_squared_error: 0.4170\nEpoch 136/200\n193/193 [==============================] - 0s 88us/sample - loss: 0.4007 - accuracy: 0.5751 - mean_squared_error: 0.4007 - val_loss: 0.4154 - val_accuracy: 0.5918 - val_mean_squared_error: 0.4154\nEpoch 137/200\n193/193 [==============================] - 0s 88us/sample - loss: 0.3991 - accuracy: 0.5803 - mean_squared_error: 0.3991 - val_loss: 0.4135 - val_accuracy: 0.6122 - val_mean_squared_error: 0.4135\nEpoch 138/200\n193/193 [==============================] - 0s 88us/sample - loss: 0.3967 - accuracy: 0.5803 - mean_squared_error: 0.3967 - val_loss: 0.4109 - val_accuracy: 0.6122 - val_mean_squared_error: 0.4109\nEpoch 139/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.3946 - accuracy: 0.5803 - mean_squared_error: 0.3946 - val_loss: 0.4088 - val_accuracy: 0.6122 - val_mean_squared_error: 0.4088\nEpoch 140/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.3927 - accuracy: 0.5803 - mean_squared_error: 0.3927 - val_loss: 0.4072 - val_accuracy: 0.6122 - val_mean_squared_error: 0.4072\nEpoch 141/200\n193/193 [==============================] - 0s 73us/sample - loss: 0.3908 - accuracy: 0.5803 - mean_squared_error: 0.3908 - val_loss: 0.4056 - val_accuracy: 0.6122 - val_mean_squared_error: 0.4056\nEpoch 142/200\n193/193 [==============================] - 0s 93us/sample - loss: 0.3891 - accuracy: 0.5855 - mean_squared_error: 0.3891 - val_loss: 0.4038 - val_accuracy: 0.6122 - val_mean_squared_error: 0.4038\nEpoch 143/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.3877 - accuracy: 0.5855 - mean_squared_error: 0.3877 - val_loss: 0.4021 - val_accuracy: 0.6122 - val_mean_squared_error: 0.4021\nEpoch 144/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.3859 - accuracy: 0.5855 - mean_squared_error: 0.3859 - val_loss: 0.4004 - val_accuracy: 0.6122 - val_mean_squared_error: 0.4004\nEpoch 145/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.3842 - accuracy: 0.5855 - mean_squared_error: 0.3842 - val_loss: 0.3981 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3981\nEpoch 146/200\n193/193 [==============================] - 0s 73us/sample - loss: 0.3826 - accuracy: 0.5855 - mean_squared_error: 0.3826 - val_loss: 0.3957 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3957\nEpoch 147/200\n193/193 [==============================] - 0s 93us/sample - loss: 0.3813 - accuracy: 0.5855 - mean_squared_error: 0.3813 - val_loss: 0.3938 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3938\nEpoch 148/200\n193/193 [==============================] - 0s 88us/sample - loss: 0.3801 - accuracy: 0.5855 - mean_squared_error: 0.3801 - val_loss: 0.3923 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3923\nEpoch 149/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.3788 - accuracy: 0.5907 - mean_squared_error: 0.3788 - val_loss: 0.3906 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3906\nEpoch 150/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.3769 - accuracy: 0.5959 - mean_squared_error: 0.3769 - val_loss: 0.3886 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3886\nEpoch 151/200\n193/193 [==============================] - 0s 98us/sample - loss: 0.3750 - accuracy: 0.6010 - mean_squared_error: 0.3750 - val_loss: 0.3868 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3868\nEpoch 152/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.3728 - accuracy: 0.6010 - mean_squared_error: 0.3728 - val_loss: 0.3844 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3844\nEpoch 153/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.3705 - accuracy: 0.6010 - mean_squared_error: 0.3705 - val_loss: 0.3827 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3827\nEpoch 154/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.3687 - accuracy: 0.5959 - mean_squared_error: 0.3687 - val_loss: 0.3811 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3811\nEpoch 155/200\n193/193 [==============================] - 0s 88us/sample - loss: 0.3661 - accuracy: 0.6010 - mean_squared_error: 0.3661 - val_loss: 0.3782 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3782\nEpoch 156/200\n193/193 [==============================] - 0s 93us/sample - loss: 0.3632 - accuracy: 0.6010 - mean_squared_error: 0.3632 - val_loss: 0.3759 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3759\nEpoch 157/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.3609 - accuracy: 0.6010 - mean_squared_error: 0.3609 - val_loss: 0.3741 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3741\nEpoch 158/200\n 32/193 [===>..........................] - ETA: 0s - loss: 0.3305 - accuracy: 0.5938 - mean_squared_error: 0.330193/193 [==============================] - 0s 83us/sample - loss: 0.3590 - accuracy: 0.6062 - mean_squared_error: 0.3590 - val_loss: 0.3725 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3725\nEpoch 159/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.3575 - accuracy: 0.6062 - mean_squared_error: 0.3575 - val_loss: 0.3710 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3710\nEpoch 160/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.3557 - accuracy: 0.6062 - mean_squared_error: 0.3557 - val_loss: 0.3695 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3695\nEpoch 161/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.3539 - accuracy: 0.6114 - mean_squared_error: 0.3539 - val_loss: 0.3679 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3679\nEpoch 162/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.3522 - accuracy: 0.6166 - mean_squared_error: 0.3522 - val_loss: 0.3665 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3665\nEpoch 163/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.3505 - accuracy: 0.6166 - mean_squared_error: 0.3505 - val_loss: 0.3652 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3652\nEpoch 164/200\n193/193 [==============================] - 0s 93us/sample - loss: 0.3487 - accuracy: 0.6218 - mean_squared_error: 0.3487 - val_loss: 0.3639 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3639\nEpoch 165/200\n193/193 [==============================] - 0s 98us/sample - loss: 0.3473 - accuracy: 0.6269 - mean_squared_error: 0.3473 - val_loss: 0.3623 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3623\nEpoch 166/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.3454 - accuracy: 0.6269 - mean_squared_error: 0.3454 - val_loss: 0.3607 - val_accuracy: 0.6122 - val_mean_squared_error: 0.3607\nEpoch 167/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.3432 - accuracy: 0.6269 - mean_squared_error: 0.3432 - val_loss: 0.3591 - val_accuracy: 0.6327 - val_mean_squared_error: 0.3591\nEpoch 168/200\n193/193 [==============================] - 0s 93us/sample - loss: 0.3416 - accuracy: 0.6321 - mean_squared_error: 0.3416 - val_loss: 0.3573 - val_accuracy: 0.6327 - val_mean_squared_error: 0.3573\nEpoch 169/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.3400 - accuracy: 0.6321 - mean_squared_error: 0.3400 - val_loss: 0.3556 - val_accuracy: 0.6327 - val_mean_squared_error: 0.3556\nEpoch 170/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.3382 - accuracy: 0.6321 - mean_squared_error: 0.3382 - val_loss: 0.3540 - val_accuracy: 0.6327 - val_mean_squared_error: 0.3540\nEpoch 171/200\n193/193 [==============================] - 0s 67us/sample - loss: 0.3364 - accuracy: 0.6321 - mean_squared_error: 0.3364 - val_loss: 0.3523 - val_accuracy: 0.6327 - val_mean_squared_error: 0.3523\nEpoch 172/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.3346 - accuracy: 0.6321 - mean_squared_error: 0.3346 - val_loss: 0.3508 - val_accuracy: 0.6531 - val_mean_squared_error: 0.3508\nEpoch 173/200\n193/193 [==============================] - 0s 93us/sample - loss: 0.3330 - accuracy: 0.6373 - mean_squared_error: 0.3330 - val_loss: 0.3493 - val_accuracy: 0.6531 - val_mean_squared_error: 0.3493\nEpoch 174/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.3316 - accuracy: 0.6373 - mean_squared_error: 0.3316 - val_loss: 0.3478 - val_accuracy: 0.6531 - val_mean_squared_error: 0.3478\nEpoch 175/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.3302 - accuracy: 0.6373 - mean_squared_error: 0.3302 - val_loss: 0.3462 - val_accuracy: 0.6531 - val_mean_squared_error: 0.3462\nEpoch 176/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.3288 - accuracy: 0.6373 - mean_squared_error: 0.3288 - val_loss: 0.3436 - val_accuracy: 0.6531 - val_mean_squared_error: 0.3436\nEpoch 177/200\n193/193 [==============================] - 0s 98us/sample - loss: 0.3274 - accuracy: 0.6373 - mean_squared_error: 0.3274 - val_loss: 0.3416 - val_accuracy: 0.6531 - val_mean_squared_error: 0.3416\nEpoch 178/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.3261 - accuracy: 0.6373 - mean_squared_error: 0.3261 - val_loss: 0.3395 - val_accuracy: 0.6531 - val_mean_squared_error: 0.3395\nEpoch 179/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.3245 - accuracy: 0.6425 - mean_squared_error: 0.3245 - val_loss: 0.3368 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3368\nEpoch 180/200\n193/193 [==============================] - 0s 73us/sample - loss: 0.3227 - accuracy: 0.6425 - mean_squared_error: 0.3227 - val_loss: 0.3348 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3348\nEpoch 181/200\n193/193 [==============================] - 0s 93us/sample - loss: 0.3215 - accuracy: 0.6425 - mean_squared_error: 0.3215 - val_loss: 0.3333 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3333\nEpoch 182/200\n193/193 [==============================] - 0s 98us/sample - loss: 0.3208 - accuracy: 0.6425 - mean_squared_error: 0.3208 - val_loss: 0.3314 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3314\nEpoch 183/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.3200 - accuracy: 0.6425 - mean_squared_error: 0.3200 - val_loss: 0.3302 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3302\nEpoch 184/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.3194 - accuracy: 0.6425 - mean_squared_error: 0.3194 - val_loss: 0.3287 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3287\nEpoch 185/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.3185 - accuracy: 0.6425 - mean_squared_error: 0.3185 - val_loss: 0.3272 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3272\nEpoch 186/200\n193/193 [==============================] - 0s 93us/sample - loss: 0.3175 - accuracy: 0.6477 - mean_squared_error: 0.3175 - val_loss: 0.3256 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3256\nEpoch 187/200\n193/193 [==============================] - 0s 88us/sample - loss: 0.3163 - accuracy: 0.6477 - mean_squared_error: 0.3163 - val_loss: 0.3240 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3240\nEpoch 188/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.3151 - accuracy: 0.6425 - mean_squared_error: 0.3151 - val_loss: 0.3226 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3226\nEpoch 189/200\n193/193 [==============================] - 0s 88us/sample - loss: 0.3136 - accuracy: 0.6477 - mean_squared_error: 0.3136 - val_loss: 0.3214 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3214\nEpoch 190/200\n193/193 [==============================] - 0s 88us/sample - loss: 0.3122 - accuracy: 0.6477 - mean_squared_error: 0.3122 - val_loss: 0.3200 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3200\nEpoch 191/200\n193/193 [==============================] - 0s 88us/sample - loss: 0.3110 - accuracy: 0.6477 - mean_squared_error: 0.3110 - val_loss: 0.3178 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3178\nEpoch 192/200\n193/193 [==============================] - 0s 83us/sample - loss: 0.3099 - accuracy: 0.6477 - mean_squared_error: 0.3099 - val_loss: 0.3160 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3160\nEpoch 193/200\n193/193 [==============================] - 0s 93us/sample - loss: 0.3086 - accuracy: 0.6477 - mean_squared_error: 0.3086 - val_loss: 0.3139 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3139\nEpoch 194/200\n193/193 [==============================] - 0s 104us/sample - loss: 0.3068 - accuracy: 0.6528 - mean_squared_error: 0.3068 - val_loss: 0.3109 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3109\nEpoch 195/200\n193/193 [==============================] - 0s 88us/sample - loss: 0.3045 - accuracy: 0.6632 - mean_squared_error: 0.3045 - val_loss: 0.3080 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3080\nEpoch 196/200\n193/193 [==============================] - 0s 88us/sample - loss: 0.3023 - accuracy: 0.6632 - mean_squared_error: 0.3023 - val_loss: 0.3070 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3070\nEpoch 197/200\n193/193 [==============================] - 0s 78us/sample - loss: 0.3011 - accuracy: 0.6684 - mean_squared_error: 0.3011 - val_loss: 0.3059 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3059\nEpoch 198/200\n193/193 [==============================] - 0s 98us/sample - loss: 0.2995 - accuracy: 0.6684 - mean_squared_error: 0.2995 - val_loss: 0.3049 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3049\nEpoch 199/200\n193/193 [==============================] - 0s 88us/sample - loss: 0.2984 - accuracy: 0.6684 - mean_squared_error: 0.2984 - val_loss: 0.3038 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3038\nEpoch 200/200\n193/193 [==============================] - 0s 93us/sample - loss: 0.2974 - accuracy: 0.6684 - mean_squared_error: 0.2974 - val_loss: 0.3024 - val_accuracy: 0.6735 - val_mean_squared_error: 0.3024\n61/61 [==============================] - 0s 82us/sample - loss: 0.3336 - accuracy: 0.6230 - mean_squared_error: 0.3336\n"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=[X_train.shape[1]], activation='relu'))\n",
    "model.compile(\n",
    "    loss='mse', \n",
    "    optimizer='adam', \n",
    "    metrics=[['accuracy', 'mean_squared_error']]\n",
    "    )\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=200, verbose=1)\n",
    "scores = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The baseline model above gives man an accuracy of roughly 67% at its best point around epoch 200. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter tuning below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Best: 0.8176870703697204 using {'batch_size': 15, 'epochs': 60}\nMeans: 0.7973639488220214, Stdev: 0.038805327845369435 with: {'batch_size': 15, 'epochs': 20}\nMeans: 0.8053571462631226, Stdev: 0.07810289489649826 with: {'batch_size': 15, 'epochs': 40}\nMeans: 0.8176870703697204, Stdev: 0.0670990046567613 with: {'batch_size': 15, 'epochs': 60}\nMeans: 0.8094387769699096, Stdev: 0.05874022064060323 with: {'batch_size': 15, 'epochs': 100}\nMeans: 0.7765306115150452, Stdev: 0.049372923178846624 with: {'batch_size': 15, 'epochs': 300}\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=13, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'mse'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [15],\n",
    "              'epochs': [20, 40, 60, 100, 300]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I was able to achieve a high of almost 82% using gridsearch. "
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python (NN)",
   "language": "python",
   "name": "python-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}