{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:** A neuron is the fundamental building block of a neural network. It consists of a set of weights (used to generate a weighted sum of inputs) and an activation, or transfer, function (run on the aforementioned weighted sum to produce an output).\n",
    "\n",
    "- **Input Layer:** Neurons in the first, or input, layer of a neural network contain a number weights equal to the number of external inputs, bias term included.\n",
    "\n",
    "- **Hidden Layer:** Every neural network does not contain a hidden layer, although most do, and this is necessary for complete classification of problems that are not linearly separable. Nodes in a hidden layer contain a number of weights equal to the number of nodes in the previous layer, plus an optional bias term. ReLU activation functions are very commonly used in hidden layers.\n",
    "\n",
    "- **Output Layer:** Neurons in the output layer of a neural network contain a number of weights equal to the number of neurons in the preceding layer, plus an optional bias term. Many networks have only a single output neuron, although some may have more. \n",
    "\n",
    "- **Activation:** The activation function of a neuron transforms the weighted sum it calculates into an output (generally with some given set of properties). Common activation functions include the sigmoid and tanh functions, as well as the ReLU family of functions. The identity function can also be considered an activation function!\n",
    "\n",
    "- **Backpropagation:** Just as a neural network propagates information about its inputs *forward* through its layers, so it propagates information about the error as compared with a target output *backward* through those same layers. This allows the weights in those layers to be updated little by little, gradually reducing the error produced by the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import optimize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility.\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Load data.\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data shape.\n",
    "candy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>gummy</th>\n",
       "      <th>ate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chocolate  gummy  ate\n",
       "0          0      1    1\n",
       "1          1      0    1\n",
       "2          0      1    1\n",
       "3          0      0    0\n",
       "4          1      1    0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview data.\n",
    "candy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.5\n",
       "0    0.5\n",
       "Name: ate, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check majority class frequency.\n",
    "candy['ate'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2377\n",
       "1     141\n",
       "Name: ate, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Candies that are neither chocolate nor gummy.\n",
    "# Mostly not eaten (94.4%).\n",
    "candy[candy['chocolate'] + candy['gummy'] == 0]['ate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2360\n",
       "0     131\n",
       "Name: ate, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Candies that are gummy but not chocolate.\n",
    "# Mostly eaten (94.8%).\n",
    "candy[(candy['chocolate'] == 0) & (candy['gummy'] == 1)]['ate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2359\n",
       "0     130\n",
       "Name: ate, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Candies that are chocolate but not gummy.\n",
    "# Mostly eaten (94.8%).\n",
    "candy[(candy['chocolate'] == 1) & (candy['gummy'] == 0)]['ate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2362\n",
       "1     140\n",
       "Name: ate, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Candies that are both chocolate and gummy.\n",
    "# Mostly not eaten (94.4%).\n",
    "candy[candy['chocolate'] + candy['gummy'] == 2]['ate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     9458\n",
       "False     542\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not do any feature engineering (xor function).\n",
    "xor_candy = (candy['chocolate'] + candy['gummy']) % 2\n",
    "(xor_candy == candy['ate']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     0.9458\n",
       "False    0.0542\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not normalize any feature engineering (it's just moving\n",
    "# the decimal point anyway, since we have an even 10k examples.\n",
    "#\n",
    "# (Exclusive or accuracy 94.6%.)\n",
    "(xor_candy == candy['ate']).value_counts(normalize='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. You will not be able to achieve more than ~50% with the simple perceptron. Explain why you could not achieve a higher accuracy with the *simple perceptron* architecture, because it's possible to achieve ~95% accuracy on this dataset. Provide your answer in markdown (and *optional* data analysis code) after your perceptron implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Define input matrix and target output.\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy['ate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "  \"\"\"\n",
    "  The perceptron class from our module one assignment.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               activation='sigmoid',\n",
    "               learning_rate=0.01,\n",
    "               max_epochs=1000):\n",
    "    self.activation = activation\n",
    "    self.learning_rate = learning_rate\n",
    "    self.max_epochs = max_epochs\n",
    "    \n",
    "  def sigmoid(self, x):\n",
    "    \"\"\"\n",
    "    The sigmoid function.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "  def d_sigmoid(self, x):\n",
    "    \"\"\"\n",
    "    The derivative of the sigmoid function. For gradient descent.\n",
    "    \"\"\"\n",
    "    return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "\n",
    "  def predict(self, inputs):\n",
    "    \"\"\"\n",
    "    Add bias term; generate predictions.\n",
    "    \"\"\"\n",
    "    inputs = np.c_[np.ones(len(inputs)), inputs]\n",
    "    return self.sigmoid(np.dot(inputs, self.weights))\n",
    "\n",
    "  def predict_bias(self, inputs_with_bias):\n",
    "    \"\"\"\n",
    "    Generate predictions - used when the inputs already include a bias.\n",
    "    \"\"\"\n",
    "    return self.sigmoid(np.dot(inputs_with_bias, self.weights))\n",
    "\n",
    "  def train(self, inputs, target):\n",
    "    \"\"\"\n",
    "    Fit the perceptron to a given input dataset and target output.\n",
    "    \"\"\"\n",
    "    # Add bias term.\n",
    "    input = np.c_[np.ones(len(inputs)), inputs]\n",
    "\n",
    "    # Adjust dimensionality for single-column target - we don't want to type a\n",
    "    # bunch of nested brackets.\n",
    "    target = np.expand_dims(target, axis=1)\n",
    "\n",
    "    # Initialize random weights in the range (-1, 1).\n",
    "    self.weights = 2 * np.random.random((len(input[0]), 1)) - 1\n",
    "\n",
    "    for epoch in range(self.max_epochs):\n",
    "        prediction = self.predict_bias(input)\n",
    "        error = prediction - target\n",
    "        gradient = self.d_sigmoid(np.dot(input, self.weights))\n",
    "        adjustments = self.learning_rate * error * gradient\n",
    "        self.weights = self.weights - np.dot(input.T, adjustments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate perceptron.\n",
    "p = Perceptron(max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train perceptron model.\n",
    "p.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output predictions from the trained model.\n",
    "perceptron_predictions = np.rint(p.predict(X)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2518, 7482], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarize predictions (2518 not eaten, 7482 eaten).\n",
    "np.bincount(np.squeeze(perceptron_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Perceptron Accuracy\n",
    "\n",
    "Once you've trained your model, report your accuracy. You will not be able to achieve more than ~50% with the simple perceptron. Explain why you could not achieve a higher accuracy with the simple perceptron architecture, because it's possible to achieve ~95% accuracy on this dataset. Provide your answer in markdown (and optional data analysis code) after your perceptron implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7236"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Report simple perceptron accuracy.\n",
    "accuracy_score(y, perceptron_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Perceptron Accuracy (Explanation)\n",
    "\n",
    "A simple perceptron can only completely classify a linearly separable function. The exclusive or (xor) function, which this data approximates (94.6% accuracy), is not linearly separable.\n",
    "\n",
    "100% accuracy will not be achievable, however, even with a multilayer perceptron, because the data is not perfectly clean (it only _approximates_ the xor function - identical inputs do not always produce identical output).\n",
    "\n",
    "I have no idea what I've done wrong (or non-simple?) to get about 72% accuracy here, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    A simple multilayer perceptron, or feed-forward neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs=2, hiddenNodes=[3], outputNodes=1):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = inputs\n",
    "        self.hiddenNodes = hiddenNodes\n",
    "        self.outputNodes = outputNodes\n",
    "\n",
    "        # Initial Weights\n",
    "        if len(hiddenNodes) == 0:\n",
    "            self.weights = [np.random.rand(self.inputs, self.outputNodes)]\n",
    "        else:\n",
    "            self.weights = [np.random.rand(self.inputs, self.hiddenNodes[0])]\n",
    "            for i in range(1, len(hiddenNodes)):\n",
    "                self.weights.append(np.random.rand((hiddenNodes[i-1]), hiddenNodes[i]))\n",
    "            self.weights.append(np.random.rand(hiddenNodes[-1], outputNodes))\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        \"\"\"\n",
    "        The sigmoid function.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        \"\"\"\n",
    "        The derivative of the sigmoid function. For gradient descent.\n",
    "        \"\"\"\n",
    "        return self.sigmoid(s) * (1 - self.sigmoid(s))\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"      \n",
    "        # Weighted sum of inputs => hidden layer\n",
    "        weighted_sum = np.dot(X, self.weights[0])\n",
    "        self.activated_outputs = [self.sigmoid(weighted_sum)]\n",
    "        for i in range(1, len(self.weights)):\n",
    "            weighted_sum = np.dot(self.activated_outputs[i-1], self.weights[i])\n",
    "            self.activated_outputs.append(self.sigmoid(weighted_sum))\n",
    "                \n",
    "        return self.activated_outputs[-1]\n",
    "        \n",
    "    def backward(self, X, y, o):\n",
    "        \"\"\"\n",
    "        Backward propagate error & updates through the network.\n",
    "        \"\"\"\n",
    "        # Error in Output\n",
    "        self.o_error = y - o\n",
    "        \n",
    "        # Apply Derivative of Sigmoid to error\n",
    "        # How far off are we in relation to the Sigmoid f(x) of the output\n",
    "        # ^- aka hidden => output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        z2_error = self.o_delta.dot(self.weights[-1].T)\n",
    "\n",
    "        # Adjustment to final set of weights (hidden => output)\n",
    "        if len(self.weights) == 1:\n",
    "            self.weights[0] += X.T.dot(self.o_delta)\n",
    "            return\n",
    "        \n",
    "        self.weights[-1] += self.activated_outputs[-2].T.dot(self.o_delta)\n",
    "        \n",
    "        # Hidden layer error\n",
    "        for i in reversed(range(2, len(self.weights))):\n",
    "            z2_delta = z2_error * self.sigmoidPrime(self.activated_outputs[i-1])\n",
    "            z2_error = z2_delta.dot(self.weights[i-1].T)\n",
    "        \n",
    "            # Adjustment to interior set of weights (hidden => hidden)\n",
    "            self.weights[i-1] += self.activated_outputs[i-2].T.dot(z2_delta)\n",
    "        \n",
    "        # Input layer error (if not same layer as output)\n",
    "        z2_delta = z2_error * self.sigmoidPrime(self.activated_outputs[0])\n",
    "        self.weights[0] += X.T.dot(z2_delta)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Run a single training epoch, one pass in each direction.\n",
    "        \"\"\"\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)\n",
    "\n",
    "    def fit(self, X, y,  max_epochs=100, print_epochs=[]):\n",
    "        \"\"\"\n",
    "        Fit the MLP to the given training input and target output.\n",
    "        \"\"\"\n",
    "        for i in range(max_epochs):\n",
    "            if (i+ 1 in print_epochs): \n",
    "                print('+' + '---' * 3 + f'EPOCH {i + 1}' + '---' * 3 + '+')\n",
    "                print('Input: \\n', X)\n",
    "                print('Actual Output: \\n', y)\n",
    "                print('Predicted Output: \\n', str(self.feed_forward(X)))\n",
    "                print('Loss: \\n', str(np.mean(np.square(y - self.feed_forward(X)))))\n",
    "            self.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(2, [2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 100---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Loss: \n",
      " 0.20115\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(X, np.expand_dims(y, axis=1), 100, [100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Accuracy\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Report multilayer perceptron accuracy.\n",
    "accuracy_score(y, np.rint(mlp.feed_forward(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Accuracy (Explanation)\n",
    "\n",
    "The performance we're getting here is abysmal, worse than that of the simple perceptron. Perhaps we're getting stuck at a local minimum? Or perhaps the lack of a tunable learning rate is tripping us up? We're not using the ideal loss function for a binary classification problem either, but I doubt that alone is the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Redux\n",
    "\n",
    "A true Stochastic GD-based implementation from Welch Labs (code taken from module 2 lecture notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, inputs=2, outputs=1, hidden_neurons=3):        \n",
    "        # Define Hyperparameters\n",
    "        self.inputLayerSize = inputs\n",
    "        self.outputLayerSize = outputs\n",
    "        self.hiddenLayerSize = hidden_neurons\n",
    "        \n",
    "        # Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Propagate inputs though network.\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        # Apply sigmoid activation function to scalar, vector, or matrix.\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        # Gradient of sigmoid\n",
    "        return np.exp(-z)/((1 + np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        # Compute cost for given X, y; use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y - self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        # Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y - self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T) * self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    # Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        # Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        # Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], \n",
    "                             (self.inputLayerSize, self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], \n",
    "                             (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        # Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        # Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        # Make empty list to store costs:\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, \n",
    "                                 params0, \n",
    "                                 jac=True, \n",
    "                                 method='BFGS',\n",
    "                                 args=(X, y), \n",
    "                                 options=options,\n",
    "                                 callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network and trainer.\n",
    "nn = NeuralNetwork()\n",
    "t = trainer(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 256.295819\n",
      "         Iterations: 105\n",
      "         Function evaluations: 149\n",
      "         Gradient evaluations: 149\n"
     ]
    }
   ],
   "source": [
    "# Train network.\n",
    "t.train(X, np.expand_dims(y, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Performance Redux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9458"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Report Welch Labs multilayer perceptron accuracy.\n",
    "accuracy_score(y, np.rint(nn.forward(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the accuracy we were looking for, but it's not my code that's getting it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "- Use the Heart Disease Dataset (binary classification).\n",
    "- Use an appropriate loss function for a binary classification task.\n",
    "- Use an appropriate activation function on the final layer of your network.\n",
    "- Train your model using verbose output for ease of grading.\n",
    "- Use GridSearchCV or RandomSearchCV to hyperparameter tune your model (for at least two hyperparameters).\n",
    "- When hyperparameter tuning, show your work by adding code cells for each new experiment.\n",
    "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "- You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>231</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>134</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>248</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "54    63    0   2       135   252    0        0      172      0      0.0   \n",
       "52    62    1   2       130   231    0        1      146      0      1.8   \n",
       "275   52    1   0       125   212    0        1      168      0      1.0   \n",
       "271   61    1   3       134   234    0        1      145      0      2.6   \n",
       "199   65    1   0       110   248    0        0      158      0      0.6   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "54       2   0     2       1  \n",
       "52       1   3     3       1  \n",
       "275      2   2     3       0  \n",
       "271      1   2     2       0  \n",
       "199      2   2     1       0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data.\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/'\n",
    "                 'datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate scaler.\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs, target output.\n",
    "X = scaler.fit_transform(df.drop(columns='target').values)\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.544554\n",
       "0    0.455446\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check majority class frequency.\n",
    "df['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate baseline model.\n",
    "baseline_model = keras.Sequential([\n",
    "    keras.layers.Dense(4, activation='relu', input_shape=((X.shape[1],))),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile baseline model.\n",
    "baseline_model.compile(optimizer='adam', \n",
    "                       loss='binary_crossentropy', \n",
    "                       metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model.\n",
    "\n",
    "Use verbose output for ease of grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 272 samples, validate on 31 samples\n",
      "Epoch 1/100\n",
      "272/272 [==============================] - 1s 2ms/sample - loss: 0.8489 - acc: 0.4485 - val_loss: 0.8310 - val_acc: 0.4516\n",
      "Epoch 2/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.8257 - acc: 0.4596 - val_loss: 0.8134 - val_acc: 0.4516\n",
      "Epoch 3/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.8048 - acc: 0.4669 - val_loss: 0.7964 - val_acc: 0.4516\n",
      "Epoch 4/100\n",
      "272/272 [==============================] - 0s 66us/sample - loss: 0.7845 - acc: 0.4853 - val_loss: 0.7806 - val_acc: 0.4839\n",
      "Epoch 5/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.7675 - acc: 0.5110 - val_loss: 0.7658 - val_acc: 0.5161\n",
      "Epoch 6/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.7500 - acc: 0.5221 - val_loss: 0.7521 - val_acc: 0.5484\n",
      "Epoch 7/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.7351 - acc: 0.5257 - val_loss: 0.7392 - val_acc: 0.5806\n",
      "Epoch 8/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.7214 - acc: 0.5404 - val_loss: 0.7273 - val_acc: 0.5806\n",
      "Epoch 9/100\n",
      "272/272 [==============================] - 0s 66us/sample - loss: 0.7078 - acc: 0.5551 - val_loss: 0.7166 - val_acc: 0.5806\n",
      "Epoch 10/100\n",
      "272/272 [==============================] - 0s 66us/sample - loss: 0.6960 - acc: 0.5625 - val_loss: 0.7062 - val_acc: 0.5806\n",
      "Epoch 11/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.6846 - acc: 0.5699 - val_loss: 0.6960 - val_acc: 0.5806\n",
      "Epoch 12/100\n",
      "272/272 [==============================] - 0s 66us/sample - loss: 0.6731 - acc: 0.5882 - val_loss: 0.6869 - val_acc: 0.5806\n",
      "Epoch 13/100\n",
      "272/272 [==============================] - 0s 63us/sample - loss: 0.6631 - acc: 0.6066 - val_loss: 0.6778 - val_acc: 0.5806\n",
      "Epoch 14/100\n",
      "272/272 [==============================] - 0s 77us/sample - loss: 0.6534 - acc: 0.6176 - val_loss: 0.6694 - val_acc: 0.6452\n",
      "Epoch 15/100\n",
      "272/272 [==============================] - 0s 62us/sample - loss: 0.6440 - acc: 0.6397 - val_loss: 0.6612 - val_acc: 0.6452\n",
      "Epoch 16/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.6346 - acc: 0.6471 - val_loss: 0.6533 - val_acc: 0.6452\n",
      "Epoch 17/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.6255 - acc: 0.6581 - val_loss: 0.6465 - val_acc: 0.6452\n",
      "Epoch 18/100\n",
      "272/272 [==============================] - 0s 62us/sample - loss: 0.6175 - acc: 0.6728 - val_loss: 0.6399 - val_acc: 0.6452\n",
      "Epoch 19/100\n",
      "272/272 [==============================] - 0s 62us/sample - loss: 0.6093 - acc: 0.6875 - val_loss: 0.6334 - val_acc: 0.6129\n",
      "Epoch 20/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.6015 - acc: 0.6949 - val_loss: 0.6271 - val_acc: 0.6452\n",
      "Epoch 21/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.5943 - acc: 0.6985 - val_loss: 0.6208 - val_acc: 0.6452\n",
      "Epoch 22/100\n",
      "272/272 [==============================] - 0s 66us/sample - loss: 0.5870 - acc: 0.7022 - val_loss: 0.6145 - val_acc: 0.6452\n",
      "Epoch 23/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.5802 - acc: 0.7169 - val_loss: 0.6089 - val_acc: 0.6452\n",
      "Epoch 24/100\n",
      "272/272 [==============================] - 0s 74us/sample - loss: 0.5733 - acc: 0.7279 - val_loss: 0.6029 - val_acc: 0.6452\n",
      "Epoch 25/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.5666 - acc: 0.7279 - val_loss: 0.5969 - val_acc: 0.7097\n",
      "Epoch 26/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.5602 - acc: 0.7279 - val_loss: 0.5912 - val_acc: 0.7097\n",
      "Epoch 27/100\n",
      "272/272 [==============================] - 0s 63us/sample - loss: 0.5540 - acc: 0.7353 - val_loss: 0.5860 - val_acc: 0.7097\n",
      "Epoch 28/100\n",
      "272/272 [==============================] - 0s 63us/sample - loss: 0.5476 - acc: 0.7463 - val_loss: 0.5803 - val_acc: 0.7097\n",
      "Epoch 29/100\n",
      "272/272 [==============================] - 0s 66us/sample - loss: 0.5413 - acc: 0.7574 - val_loss: 0.5749 - val_acc: 0.7097\n",
      "Epoch 30/100\n",
      "272/272 [==============================] - 0s 66us/sample - loss: 0.5354 - acc: 0.7721 - val_loss: 0.5702 - val_acc: 0.7097\n",
      "Epoch 31/100\n",
      "272/272 [==============================] - 0s 62us/sample - loss: 0.5295 - acc: 0.7757 - val_loss: 0.5652 - val_acc: 0.7097\n",
      "Epoch 32/100\n",
      "272/272 [==============================] - 0s 66us/sample - loss: 0.5237 - acc: 0.7794 - val_loss: 0.5600 - val_acc: 0.7419\n",
      "Epoch 33/100\n",
      "272/272 [==============================] - 0s 74us/sample - loss: 0.5178 - acc: 0.7794 - val_loss: 0.5550 - val_acc: 0.7419\n",
      "Epoch 34/100\n",
      "272/272 [==============================] - 0s 74us/sample - loss: 0.5124 - acc: 0.7794 - val_loss: 0.5504 - val_acc: 0.7419\n",
      "Epoch 35/100\n",
      "272/272 [==============================] - 0s 66us/sample - loss: 0.5069 - acc: 0.7831 - val_loss: 0.5453 - val_acc: 0.7419\n",
      "Epoch 36/100\n",
      "272/272 [==============================] - 0s 70us/sample - loss: 0.5012 - acc: 0.7794 - val_loss: 0.5405 - val_acc: 0.7419\n",
      "Epoch 37/100\n",
      "272/272 [==============================] - 0s 62us/sample - loss: 0.4959 - acc: 0.7794 - val_loss: 0.5356 - val_acc: 0.7419\n",
      "Epoch 38/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.4909 - acc: 0.7794 - val_loss: 0.5306 - val_acc: 0.7419\n",
      "Epoch 39/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.4854 - acc: 0.7794 - val_loss: 0.5261 - val_acc: 0.7419\n",
      "Epoch 40/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.4803 - acc: 0.7868 - val_loss: 0.5217 - val_acc: 0.7419\n",
      "Epoch 41/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.4754 - acc: 0.7941 - val_loss: 0.5172 - val_acc: 0.7419\n",
      "Epoch 42/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.4705 - acc: 0.7941 - val_loss: 0.5131 - val_acc: 0.7419\n",
      "Epoch 43/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.4658 - acc: 0.7978 - val_loss: 0.5094 - val_acc: 0.7742\n",
      "Epoch 44/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.4610 - acc: 0.7978 - val_loss: 0.5054 - val_acc: 0.7742\n",
      "Epoch 45/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.4565 - acc: 0.7978 - val_loss: 0.5016 - val_acc: 0.7742\n",
      "Epoch 46/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.4520 - acc: 0.7978 - val_loss: 0.4976 - val_acc: 0.7742\n",
      "Epoch 47/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.4475 - acc: 0.7978 - val_loss: 0.4932 - val_acc: 0.7742\n",
      "Epoch 48/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.4435 - acc: 0.7978 - val_loss: 0.4882 - val_acc: 0.8065\n",
      "Epoch 49/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.4391 - acc: 0.8051 - val_loss: 0.4843 - val_acc: 0.8065\n",
      "Epoch 50/100\n",
      "272/272 [==============================] - 0s 51us/sample - loss: 0.4352 - acc: 0.8088 - val_loss: 0.4803 - val_acc: 0.8065\n",
      "Epoch 51/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.4315 - acc: 0.8088 - val_loss: 0.4761 - val_acc: 0.8065\n",
      "Epoch 52/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.4277 - acc: 0.8051 - val_loss: 0.4720 - val_acc: 0.8065\n",
      "Epoch 53/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.4239 - acc: 0.8015 - val_loss: 0.4680 - val_acc: 0.8065\n",
      "Epoch 54/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.4203 - acc: 0.8088 - val_loss: 0.4645 - val_acc: 0.8065\n",
      "Epoch 55/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.4172 - acc: 0.8162 - val_loss: 0.4611 - val_acc: 0.8065\n",
      "Epoch 56/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.4138 - acc: 0.8162 - val_loss: 0.4577 - val_acc: 0.8065\n",
      "Epoch 57/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.4106 - acc: 0.8199 - val_loss: 0.4542 - val_acc: 0.8065\n",
      "Epoch 58/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.4077 - acc: 0.8199 - val_loss: 0.4506 - val_acc: 0.8065\n",
      "Epoch 59/100\n",
      "272/272 [==============================] - 0s 51us/sample - loss: 0.4048 - acc: 0.8199 - val_loss: 0.4473 - val_acc: 0.8065\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 0s 59us/sample - loss: 0.4021 - acc: 0.8272 - val_loss: 0.4439 - val_acc: 0.8065\n",
      "Epoch 61/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.3995 - acc: 0.8272 - val_loss: 0.4410 - val_acc: 0.8065\n",
      "Epoch 62/100\n",
      "272/272 [==============================] - 0s 63us/sample - loss: 0.3968 - acc: 0.8272 - val_loss: 0.4382 - val_acc: 0.8065\n",
      "Epoch 63/100\n",
      "272/272 [==============================] - ETA: 0s - loss: 0.3460 - acc: 0.875 - 0s 55us/sample - loss: 0.3944 - acc: 0.8309 - val_loss: 0.4354 - val_acc: 0.8065\n",
      "Epoch 64/100\n",
      "272/272 [==============================] - 0s 63us/sample - loss: 0.3921 - acc: 0.8309 - val_loss: 0.4323 - val_acc: 0.8065\n",
      "Epoch 65/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.3898 - acc: 0.8346 - val_loss: 0.4291 - val_acc: 0.8065\n",
      "Epoch 66/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.3877 - acc: 0.8382 - val_loss: 0.4263 - val_acc: 0.8065\n",
      "Epoch 67/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.3855 - acc: 0.8382 - val_loss: 0.4237 - val_acc: 0.8065\n",
      "Epoch 68/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.3836 - acc: 0.8419 - val_loss: 0.4214 - val_acc: 0.8065\n",
      "Epoch 69/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.3817 - acc: 0.8419 - val_loss: 0.4186 - val_acc: 0.8065\n",
      "Epoch 70/100\n",
      "272/272 [==============================] - 0s 62us/sample - loss: 0.3799 - acc: 0.8419 - val_loss: 0.4158 - val_acc: 0.8065\n",
      "Epoch 71/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.3780 - acc: 0.8419 - val_loss: 0.4131 - val_acc: 0.8065\n",
      "Epoch 72/100\n",
      "272/272 [==============================] - 0s 66us/sample - loss: 0.3764 - acc: 0.8419 - val_loss: 0.4111 - val_acc: 0.8065\n",
      "Epoch 73/100\n",
      "272/272 [==============================] - 0s 66us/sample - loss: 0.3748 - acc: 0.8419 - val_loss: 0.4089 - val_acc: 0.8065\n",
      "Epoch 74/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.3733 - acc: 0.8456 - val_loss: 0.4069 - val_acc: 0.8065\n",
      "Epoch 75/100\n",
      "272/272 [==============================] - 0s 66us/sample - loss: 0.3718 - acc: 0.8456 - val_loss: 0.4052 - val_acc: 0.8065\n",
      "Epoch 76/100\n",
      "272/272 [==============================] - 0s 66us/sample - loss: 0.3703 - acc: 0.8456 - val_loss: 0.4032 - val_acc: 0.8065\n",
      "Epoch 77/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.3690 - acc: 0.8456 - val_loss: 0.4015 - val_acc: 0.8065\n",
      "Epoch 78/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.3676 - acc: 0.8456 - val_loss: 0.3996 - val_acc: 0.8065\n",
      "Epoch 79/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.3662 - acc: 0.8456 - val_loss: 0.3980 - val_acc: 0.8065\n",
      "Epoch 80/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.3651 - acc: 0.8456 - val_loss: 0.3967 - val_acc: 0.8065\n",
      "Epoch 81/100\n",
      "272/272 [==============================] - 0s 62us/sample - loss: 0.3638 - acc: 0.8456 - val_loss: 0.3952 - val_acc: 0.8065\n",
      "Epoch 82/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.3627 - acc: 0.8456 - val_loss: 0.3937 - val_acc: 0.8065\n",
      "Epoch 83/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.3614 - acc: 0.8456 - val_loss: 0.3922 - val_acc: 0.8065\n",
      "Epoch 84/100\n",
      "272/272 [==============================] - 0s 62us/sample - loss: 0.3603 - acc: 0.8456 - val_loss: 0.3906 - val_acc: 0.8065\n",
      "Epoch 85/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.3590 - acc: 0.8456 - val_loss: 0.3895 - val_acc: 0.8065\n",
      "Epoch 86/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.3579 - acc: 0.8456 - val_loss: 0.3886 - val_acc: 0.8065\n",
      "Epoch 87/100\n",
      "272/272 [==============================] - ETA: 0s - loss: 0.2585 - acc: 0.906 - 0s 62us/sample - loss: 0.3568 - acc: 0.8456 - val_loss: 0.3874 - val_acc: 0.8065\n",
      "Epoch 88/100\n",
      "272/272 [==============================] - 0s 62us/sample - loss: 0.3558 - acc: 0.8456 - val_loss: 0.3860 - val_acc: 0.8065\n",
      "Epoch 89/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.3547 - acc: 0.8456 - val_loss: 0.3850 - val_acc: 0.8065\n",
      "Epoch 90/100\n",
      "272/272 [==============================] - 0s 63us/sample - loss: 0.3535 - acc: 0.8456 - val_loss: 0.3843 - val_acc: 0.8065\n",
      "Epoch 91/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.3525 - acc: 0.8529 - val_loss: 0.3829 - val_acc: 0.8065\n",
      "Epoch 92/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.3516 - acc: 0.8529 - val_loss: 0.3816 - val_acc: 0.8065\n",
      "Epoch 93/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.3507 - acc: 0.8529 - val_loss: 0.3807 - val_acc: 0.8065\n",
      "Epoch 94/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.3498 - acc: 0.8529 - val_loss: 0.3796 - val_acc: 0.8065\n",
      "Epoch 95/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.3489 - acc: 0.8529 - val_loss: 0.3791 - val_acc: 0.8065\n",
      "Epoch 96/100\n",
      "272/272 [==============================] - 0s 62us/sample - loss: 0.3482 - acc: 0.8566 - val_loss: 0.3782 - val_acc: 0.8065\n",
      "Epoch 97/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.3471 - acc: 0.8566 - val_loss: 0.3773 - val_acc: 0.8065\n",
      "Epoch 98/100\n",
      "272/272 [==============================] - 0s 62us/sample - loss: 0.3464 - acc: 0.8566 - val_loss: 0.3765 - val_acc: 0.8065\n",
      "Epoch 99/100\n",
      "272/272 [==============================] - 0s 59us/sample - loss: 0.3456 - acc: 0.8566 - val_loss: 0.3760 - val_acc: 0.8065\n",
      "Epoch 100/100\n",
      "272/272 [==============================] - 0s 55us/sample - loss: 0.3448 - acc: 0.8566 - val_loss: 0.3756 - val_acc: 0.8065\n"
     ]
    }
   ],
   "source": [
    "baseline_history = baseline_model.fit(X, \n",
    "                                      y, \n",
    "                                      validation_split=0.1, \n",
    "                                      epochs=100,              \n",
    "                                      verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(baseline_history.history['loss'])\n",
    "plt.plot(baseline_history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(baseline_history.history['acc'])\n",
    "plt.plot(baseline_history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_predictions = np.rint(baseline_model.predict(X)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8514851485148515"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y, baseline_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters\n",
    "\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model (for at least two hyperparameters). When hyperparameter tuning, show your work by adding code cells for each new experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(nodes=[4],\n",
    "                 init_mode='glorot_uniform',\n",
    "                 optimizer='adam'):\n",
    "    \"\"\"\n",
    "    Model creation function for use with KerasClassifier.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Add input layer.\n",
    "    model.add(keras.layers.Dense(nodes[0],\n",
    "                                 activation='relu', \n",
    "                                 input_shape=(X.shape[1],)))\n",
    "    # Add hidden layers.\n",
    "    for n in nodes[1:]:\n",
    "        model.add(keras.layers.Dense(n, \n",
    "                                     activation='relu'))\n",
    "        \n",
    "    # Add output layer.\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model.\n",
    "    model.compile(init_mode=init_mode,\n",
    "                  optimizer=optimizer, \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model for tuning.\n",
    "model = KerasClassifier(build_fn=create_model, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for search.\n",
    "param_grid = {'nodes': [[4], [2, 2], [4, 4]],\n",
    "              'init_mode': ['glorot_uniform', 'ones', 'zeros'],\n",
    "              'optimizer': ['Adam', 'Adadelta', 'SGD'],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up grid search.\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run search.\n",
    "grid_result = grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Report Accuracy for Each Combination of Hyperparameters.\n",
    "\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_nodes</th>\n",
       "      <th>param_init_mode</th>\n",
       "      <th>param_optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>ones</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.146707</td>\n",
       "      <td>0.375763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>glorot_uniform</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.590759</td>\n",
       "      <td>0.122691</td>\n",
       "      <td>0.424303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>zeros</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.587459</td>\n",
       "      <td>0.032672</td>\n",
       "      <td>0.395434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[4]</td>\n",
       "      <td>ones</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.564356</td>\n",
       "      <td>0.042777</td>\n",
       "      <td>0.338899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>ones</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.557756</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.398682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>zeros</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.554455</td>\n",
       "      <td>0.097009</td>\n",
       "      <td>0.354671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>ones</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.551155</td>\n",
       "      <td>0.033657</td>\n",
       "      <td>0.314622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[4]</td>\n",
       "      <td>zeros</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.544554</td>\n",
       "      <td>0.024252</td>\n",
       "      <td>0.356171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>zeros</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.531353</td>\n",
       "      <td>0.158691</td>\n",
       "      <td>0.365329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>glorot_uniform</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.524752</td>\n",
       "      <td>0.077960</td>\n",
       "      <td>0.396759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>glorot_uniform</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.521452</td>\n",
       "      <td>0.028391</td>\n",
       "      <td>0.383196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>glorot_uniform</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.521452</td>\n",
       "      <td>0.045968</td>\n",
       "      <td>0.366936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>zeros</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.521452</td>\n",
       "      <td>0.068755</td>\n",
       "      <td>0.337662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[4]</td>\n",
       "      <td>zeros</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.511551</td>\n",
       "      <td>0.079756</td>\n",
       "      <td>0.387371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>ones</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.511551</td>\n",
       "      <td>0.121621</td>\n",
       "      <td>0.420677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>zeros</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.508251</td>\n",
       "      <td>0.030606</td>\n",
       "      <td>0.447289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>ones</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.491749</td>\n",
       "      <td>0.072457</td>\n",
       "      <td>0.412120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>glorot_uniform</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.485149</td>\n",
       "      <td>0.042777</td>\n",
       "      <td>0.389277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>glorot_uniform</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.475248</td>\n",
       "      <td>0.091818</td>\n",
       "      <td>0.433341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[4]</td>\n",
       "      <td>glorot_uniform</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.465347</td>\n",
       "      <td>0.061034</td>\n",
       "      <td>0.293011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[4]</td>\n",
       "      <td>ones</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.458746</td>\n",
       "      <td>0.078517</td>\n",
       "      <td>0.358335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[4]</td>\n",
       "      <td>glorot_uniform</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.448845</td>\n",
       "      <td>0.033657</td>\n",
       "      <td>0.337405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[4]</td>\n",
       "      <td>glorot_uniform</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.442244</td>\n",
       "      <td>0.110352</td>\n",
       "      <td>0.393538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>zeros</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.419142</td>\n",
       "      <td>0.078517</td>\n",
       "      <td>0.342518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[4]</td>\n",
       "      <td>ones</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.415842</td>\n",
       "      <td>0.058295</td>\n",
       "      <td>0.377240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>ones</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.399340</td>\n",
       "      <td>0.095766</td>\n",
       "      <td>0.400387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[4]</td>\n",
       "      <td>zeros</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.353135</td>\n",
       "      <td>0.109161</td>\n",
       "      <td>0.305330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                param_nodes param_init_mode param_optimizer  mean_test_score  \\\n",
       "rank_test_score                                                                \n",
       "1                    [4, 4]            ones             SGD         0.666667   \n",
       "2                    [4, 4]  glorot_uniform            Adam         0.590759   \n",
       "3                    [2, 2]           zeros             SGD         0.587459   \n",
       "4                       [4]            ones             SGD         0.564356   \n",
       "5                    [2, 2]            ones        Adadelta         0.557756   \n",
       "6                    [4, 4]           zeros        Adadelta         0.554455   \n",
       "7                    [2, 2]            ones             SGD         0.551155   \n",
       "8                       [4]           zeros             SGD         0.544554   \n",
       "9                    [2, 2]           zeros            Adam         0.531353   \n",
       "10                   [2, 2]  glorot_uniform             SGD         0.524752   \n",
       "11                   [4, 4]  glorot_uniform        Adadelta         0.521452   \n",
       "11                   [4, 4]  glorot_uniform             SGD         0.521452   \n",
       "13                   [4, 4]           zeros             SGD         0.521452   \n",
       "14                      [4]           zeros            Adam         0.511551   \n",
       "15                   [4, 4]            ones            Adam         0.511551   \n",
       "16                   [4, 4]           zeros            Adam         0.508251   \n",
       "17                   [2, 2]            ones            Adam         0.491749   \n",
       "18                   [2, 2]  glorot_uniform        Adadelta         0.485149   \n",
       "19                   [2, 2]  glorot_uniform            Adam         0.475248   \n",
       "20                      [4]  glorot_uniform             SGD         0.465347   \n",
       "21                      [4]            ones        Adadelta         0.458746   \n",
       "22                      [4]  glorot_uniform        Adadelta         0.448845   \n",
       "23                      [4]  glorot_uniform            Adam         0.442244   \n",
       "24                   [2, 2]           zeros        Adadelta         0.419142   \n",
       "25                      [4]            ones            Adam         0.415842   \n",
       "26                   [4, 4]            ones        Adadelta         0.399340   \n",
       "27                      [4]           zeros        Adadelta         0.353135   \n",
       "\n",
       "                 std_test_score  mean_fit_time  \n",
       "rank_test_score                                 \n",
       "1                      0.146707       0.375763  \n",
       "2                      0.122691       0.424303  \n",
       "3                      0.032672       0.395434  \n",
       "4                      0.042777       0.338899  \n",
       "5                      0.094737       0.398682  \n",
       "6                      0.097009       0.354671  \n",
       "7                      0.033657       0.314622  \n",
       "8                      0.024252       0.356171  \n",
       "9                      0.158691       0.365329  \n",
       "10                     0.077960       0.396759  \n",
       "11                     0.028391       0.383196  \n",
       "11                     0.045968       0.366936  \n",
       "13                     0.068755       0.337662  \n",
       "14                     0.079756       0.387371  \n",
       "15                     0.121621       0.420677  \n",
       "16                     0.030606       0.447289  \n",
       "17                     0.072457       0.412120  \n",
       "18                     0.042777       0.389277  \n",
       "19                     0.091818       0.433341  \n",
       "20                     0.061034       0.293011  \n",
       "21                     0.078517       0.358335  \n",
       "22                     0.033657       0.337405  \n",
       "23                     0.110352       0.393538  \n",
       "24                     0.078517       0.342518  \n",
       "25                     0.058295       0.377240  \n",
       "26                     0.095766       0.400387  \n",
       "27                     0.109161       0.305330  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarize search results.\n",
    "results_df = pd.DataFrame(grid_result.cv_results_)[['rank_test_score',\n",
    "                                                    'param_nodes', \n",
    "                                                    'param_init_mode',\n",
    "                                                    'param_optimizer',\n",
    "                                                    'mean_test_score',\n",
    "                                                    'std_test_score',\n",
    "                                                    'mean_fit_time',\n",
    "                                                   ]]\n",
    "results_df = results_df.set_index('rank_test_score').sort_index()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The above results appear highly unstable and sensitive to small random fluctuations."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
