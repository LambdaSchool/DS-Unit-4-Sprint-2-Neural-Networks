{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS43SC.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donw385/DS-Unit-4-Sprint-3-Neural-Networks/blob/master/DS_Unit_4_Sprint_3_Neural_Nets_Spring_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6SKlgYrpcym",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks Sprint Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrEbRrjVphPM",
        "colab_type": "text"
      },
      "source": [
        "## 1) Define the following terms:\n",
        "\n",
        "- Neuron: neurons receive inputs and pass on their signal to the next layer of nodes if a certain threshold is reached\n",
        "- Input Layer: The Input Layer is what receives input from our dataset. Sometimes it is called the visible layer because it's the only part that is exposed to our data and that our data interacts with directly.\n",
        "- Hidden Layer: Layers after the input layer are called Hidden Layers. This is because they cannot be accessed except through the input layer. They're inside of the network and they perform their functions, but we don't directly interact with them. \n",
        "- Output Layer: The final layer is called the Output Layer. The purpose of the output layer is to output a vector of values that is in a format that is suitable for the type of problem that we're trying to address\n",
        "- Activation: The activation function decides whether a cell \"fires\" or not. Sometimes it is said that the cell is \"activated\" or not. In Artificial Neural Networks activation functions decide how much signal to pass onto the next layer. \n",
        "- Backpropagation: Back-propagation is the essence of neural net training. It is the practice of fine-tuning the weights of a neural net based on the error rate (i.e. loss) obtained in the previous epoch (i.e. iteration). Proper tuning of the weights ensures lower error rates, making the model reliable by increasing its generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5EksLqnp4oB",
        "colab_type": "text"
      },
      "source": [
        " YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri_gRA2Jp728",
        "colab_type": "text"
      },
      "source": [
        "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
        "\n",
        "| x1 | x2 | x3 | y |\n",
        "|----|----|----|---|\n",
        "| 1  | 1  | 1  | 1 |\n",
        "| 1  | 0  | 1  | 0 |\n",
        "| 0  | 1  | 1  | 0 |\n",
        "| 0  | 0  | 1  | 0 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig6ZTH8tpQ19",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "355e1947-10dd-423b-b541-a20390dccd1f"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD, Adam, Nadam\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kqt5FlIR_I7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PerceptronClassifier():\n",
        "    \"\"\"\n",
        "    Basic perceptron class for binary classification\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.1, n_iter=100, tolerance=0.000001):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iter = n_iter\n",
        "        self.tolerance = tolerance\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit perceptron to a set of training data using gradient descent\n",
        "        \"\"\"\n",
        "        # initialize weights and cost list\n",
        "        self.weights_ = np.random.uniform(-0.01, 0.01, X.shape[1] + 1)\n",
        "        self.costs_ = []\n",
        "        # iterate until fit is adequate\n",
        "        for i in range(self.n_iter):\n",
        "            preds = self.predict_proba(X)\n",
        "            errors = preds - y\n",
        "            cost = np.sum(errors ** 2)\n",
        "            self.costs_.append(cost)\n",
        "            gradient = np.dot(X.T, errors)\n",
        "            self.weights_[1:] -= self.learning_rate * gradient\n",
        "            self.weights_[0] -= np.mean(errors)\n",
        "            \n",
        "            # break the loop if we are close enough\n",
        "            if cost < self.tolerance:\n",
        "                break\n",
        "            \n",
        "        return self\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Computes sigmoid output value given X\n",
        "        \"\"\"\n",
        "        return 1. / (1. + np.exp(-(np.dot(X, self.weights_[1:]) + self.weights_[0])))\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts the binary class of X values\n",
        "        \"\"\"\n",
        "        return np.where(self.predict_proba(X)>=0.5, 1, 0)\n",
        "    \n",
        "    def show_loss(self):\n",
        "        \"\"\"\n",
        "        Shows loss along epochs\n",
        "        \"\"\"\n",
        "        try:\n",
        "            iters = range(len(self.costs_))\n",
        "            fig, ax = plt.subplots()\n",
        "            ax.plot(iters, self.costs_)\n",
        "            ax.set_xlabel('Number of Iterations')\n",
        "            ax.set_ylabel('Training Loss (SSE)')\n",
        "            ax.set_title('Training Loss')\n",
        "            plt.show()\n",
        "        except:\n",
        "            print ('Please train me first :)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8imh4Sv6_U0k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "323f942b-23fa-4fa1-a862-2e05a8b554ee"
      },
      "source": [
        "X = np.array([[1,1,1],\n",
        "            [1,0,1],\n",
        "            [0,1,1],\n",
        "            [0,0,1]])\n",
        "y = np.array([1,0,0,0])\n",
        "\n",
        "ppn = PerceptronClassifier(learning_rate = 1.0, n_iter=100)\n",
        "ppn.fit(X, y)\n",
        "ppn.predict(X)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86HyRi8Osr3U",
        "colab_type": "text"
      },
      "source": [
        "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
        "- Your network must have one hidden layer. \n",
        "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
        "- Train your model on the Heart Disease dataset from UCI:\n",
        "\n",
        "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
        "\n",
        "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNfiajv3v4Ed",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "e209b2b8-db03-4d2a-9d06-30ddfd489d68"
      },
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
              "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
              "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
              "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
              "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
              "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
              "\n",
              "   ca  thal  target  \n",
              "0   0     1       1  \n",
              "1   0     2       1  \n",
              "2   0     2       1  \n",
              "3   0     2       1  \n",
              "4   0     2       1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2prADOkAiYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNetMLP(object):\n",
        "    \"\"\"\n",
        "    Feed forward nueral network / multi-layer perceptron classifier\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    n_hidden : int (default: 30)\n",
        "        Number of hidden units\n",
        "    l2 : float (default 0.)\n",
        "        Lambda value for l2 normalization\n",
        "    epochs : int (default: 100)\n",
        "        Number of training epochs\n",
        "    eta : float (default = 0.001)\n",
        "        Learning rate\n",
        "    shuffle : bool (default: True)\n",
        "        Shuffles the training data every epoch if True\n",
        "    minibatch_size : int (default : 1)\n",
        "        Number of training samples per minibatch\n",
        "    seed : int (default: None)\n",
        "        Random seed for initalizing weights and shuffling\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    eval_ : dict\n",
        "        Dictionary collecting the cost, training accuracy,\n",
        "        and validation accuracy for each epoch during training\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_hidden=30, l2=0.,\n",
        "                epochs=100, eta=0.0001,\n",
        "                shuffle=True, minibatch_size=1, seed=None):\n",
        "        self.random = np.random.RandomState(seed)\n",
        "        self.n_hidden = n_hidden\n",
        "        self.l2 = l2\n",
        "        self.epochs = epochs\n",
        "        self.eta = eta\n",
        "        self.shuffle = shuffle\n",
        "        self.minibatch_size = minibatch_size\n",
        "        \n",
        "    def _onehot(self, y, n_classes):\n",
        "        \"\"\"\n",
        "        Encode labels into one hot representation\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        y : array, shape = [n_samples]\n",
        "            Target values\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        onehot : array, shape = (n_samples, n_labels)\n",
        "        \"\"\"\n",
        "        \n",
        "        onehot = np.zeros((n_classes, y.shape[0]))\n",
        "        for idx, val in enumerate(y.astype(int)):\n",
        "            onehot[val, idx] = 1.\n",
        "        return onehot.T\n",
        "    \n",
        "    def _sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Compute logistic function (sigmoid)\n",
        "        \"\"\"\n",
        "        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n",
        "    \n",
        "    def _forward(self, X):\n",
        "        \"\"\"\n",
        "        Compute forward propogation step\n",
        "        \"\"\"\n",
        "        # step 1: net input of hidden layer\n",
        "        z_h = np.dot(X, self.w_h) + self.b_h\n",
        "        \n",
        "        # step 2: activation of hidden layer\n",
        "        a_h = self._sigmoid(z_h)\n",
        "        \n",
        "        # step 3: net input of output layer\n",
        "        z_out = np.dot(a_h, self.w_out) + self.b_out\n",
        "        \n",
        "        # step 4: activation layer output\n",
        "        a_out = self._sigmoid(z_out)\n",
        "        \n",
        "        return z_h, a_h, z_out, a_out\n",
        "    \n",
        "    def _compute_cost(self, y_enc, output):\n",
        "        \"\"\"\n",
        "        Compute cost function\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        y_enc : array, shape = (n_samples, n_labels)\n",
        "            one-hot encoded class labels\n",
        "        output : array, shape = [n_samples, n_output_units]\n",
        "            Activation of the output layer (forward propoagation)\n",
        "        Returns\n",
        "        -------\n",
        "        cost : float\n",
        "            Regularized cost\n",
        "        \"\"\"\n",
        "        e = 0.000000001\n",
        "        L2_term = (self.l2 *\n",
        "                  (np.sum(self.w_h ** 2.) + \n",
        "                  np.sum(self.w_out ** 2.)))\n",
        "        term1 = -y_enc * (np.log(output + e))\n",
        "        term2 = (1. - y_enc) * np.log(1. - output + e)\n",
        "        cost = np.sum(term1 - term2) + L2_term\n",
        "        return cost\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels\n",
        "        \n",
        "        Paramters\n",
        "        ---------\n",
        "        X : array, shape = [n_samples, n_features]\n",
        "            Input layer with original features\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : array, shape = [n_samples]\n",
        "            Predicted class labels\n",
        "        \"\"\"\n",
        "        z_h, a_h, z_out, a_out = self._forward(X)\n",
        "        y_pred = np.argmax(z_out, axis=1)\n",
        "        return y_pred\n",
        "    \n",
        "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
        "        \"\"\"\n",
        "        Learn weights from training data\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X_train : array, shape = [n_samples, n_features]\n",
        "            Input layer with original features\n",
        "        y_train : array, shape = [n_samples]\n",
        "            Target class labels\n",
        "        X_test : array, shape = [n_samples, n_features]\n",
        "            Sample features for validation during training\n",
        "        y_test : array, shape = [n_samples]\n",
        "            Samples test class labels for validation during training\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        self\n",
        "        \"\"\"\n",
        "        n_output = np.unique(y_train).shape[0]\n",
        "        n_features = X_train.shape[1]\n",
        "        \n",
        "        #######################\n",
        "        # Weight Initialization\n",
        "        #######################\n",
        "        \n",
        "        # weights for input -> hidden\n",
        "        self.b_h = np.zeros(self.n_hidden)\n",
        "        self.w_h = self.random.normal(loc=0.0,\n",
        "                                     scale=0.1,\n",
        "                                     size=(n_features,\n",
        "                                          self.n_hidden))\n",
        "        \n",
        "        # weights for hidden -> output\n",
        "        self.b_out = np.zeros(n_output)\n",
        "        self.w_out = self.random.normal(loc=0.0,\n",
        "                                       scale=0.1,\n",
        "                                       size=(self.n_hidden,\n",
        "                                       n_output))\n",
        "        \n",
        "        epoch_strlen = len(str(self.epochs)) # for progr. format\n",
        "        self.eval_ = {'train_cost' : [],\n",
        "                      'val_cost' : [],\n",
        "                     'train_acc' : [],\n",
        "                     'valid_acc' : []}\n",
        "        \n",
        "        y_train_enc = self._onehot(y_train, n_output)\n",
        "        y_valid_enc = self._onehot(y_valid, n_output)\n",
        "        \n",
        "        # iterate over training epochs\n",
        "        for i in range(self.epochs):\n",
        "            \n",
        "            # iterate over mini batches\n",
        "            indices = np.arange(X_train.shape[0])\n",
        "            \n",
        "            if self.shuffle:\n",
        "                self.random.shuffle(indices)\n",
        "            \n",
        "            for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size):\n",
        "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
        "                \n",
        "                # forward propagation\n",
        "                z_h, a_h, z_out, a_out = self._forward(X_train[batch_idx])\n",
        "                \n",
        "                #################\n",
        "                # Backpropagation\n",
        "                #################\n",
        "                \n",
        "                # [n_samples, n_classlabels]\n",
        "                sigma_out = a_out - y_train_enc[batch_idx]\n",
        "                \n",
        "                # [n_samples, n_hidden]\n",
        "                sigmoid_derivative_h = a_h * (1. - a_h)\n",
        "                \n",
        "                # [n_samples, n_classlabels] dot [n_classlabels, n_hidden]\n",
        "                # -> [n_samples, n_hidden]\n",
        "                sigma_h = (np.dot(sigma_out, self.w_out.T) * sigmoid_derivative_h)\n",
        "                \n",
        "                # [n_features, n_samples] dot [n_samples, n_hidden]\n",
        "                # -> [n_features, n_hidden]\n",
        "                grad_w_h = np.dot(X_train[batch_idx].T, sigma_h)\n",
        "                grad_b_h = np.sum(sigma_h, axis=0)\n",
        "                \n",
        "                # [n_hidden, n_samples] dot [n_samples, n_classlabels]\n",
        "                # -> [n_hidden, n_classlabels]\n",
        "                grad_w_out = np.dot(a_h.T, sigma_out)\n",
        "                grad_b_out = np.sum(sigma_out, axis=0)\n",
        "                \n",
        "                # Regularization and weight updates\n",
        "                delta_w_h = (grad_w_h + self.l2 * self.w_h)\n",
        "                delta_b_h = grad_b_h # bias is not regularized\n",
        "                self.w_h -= self.eta * delta_w_h\n",
        "                self.b_h -= self.eta * delta_b_h\n",
        "                \n",
        "                delta_w_out = grad_w_out + self.l2 * self.w_out\n",
        "                delta_b_out = grad_b_out # bias is not regularized\n",
        "                self.w_out -= self.eta * delta_w_out\n",
        "                self.b_out -= self.eta * delta_b_out\n",
        "                \n",
        "            ############\n",
        "            # Evaluation\n",
        "            ############\n",
        "\n",
        "            # evaluation after each epoch during training\n",
        "            z_h, a_h, z_out, a_out = self._forward(X_train)\n",
        "            z_h_val, a_h_val, z_out_val, a_out_val = self._forward(X_valid)\n",
        "            \n",
        "            cost = self._compute_cost(y_enc=y_train_enc, output=a_out) / a_out.shape[0]\n",
        "            cost_val = self._compute_cost(y_enc=y_valid_enc, output=a_out_val) / a_out_val.shape[0]\n",
        "            \n",
        "            y_train_pred = self.predict(X_train)\n",
        "            y_valid_pred = self.predict(X_valid)\n",
        "            \n",
        "            train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float) / X_train.shape[0])\n",
        "            valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(np.float) / X_valid.shape[0])\n",
        "            \n",
        "            sys.stderr.write('\\r%0*d/%d | Cost: %.2f | Train/Valid Acc: %.2f%%/%.2f%%'\n",
        "                            % (epoch_strlen, i+1, self.epochs, cost, train_acc*100, valid_acc*100))\n",
        "            sys.stderr.flush()\n",
        "            \n",
        "            self.eval_['train_cost'].append(cost)\n",
        "            self.eval_['val_cost'].append(cost_val)\n",
        "            self.eval_['train_acc'].append(train_acc)\n",
        "            self.eval_['valid_acc'].append(valid_acc)\n",
        "            \n",
        "        return self\n",
        "    \n",
        "    def plot_training(self):\n",
        "        \"\"\"\n",
        "        Plots training loss and accuracy\n",
        "        \"\"\"\n",
        "        fig, ax = plt.subplots(ncols=2, figsize=(12,7))\n",
        "        \n",
        "        # plotting loss\n",
        "        ax[0].plot(range(self.epochs), self.eval_['train_cost'], label='Train')\n",
        "        ax[0].plot(range(self.epochs), self.eval_['val_cost'], label='Validation')\n",
        "        ax[0].set_xlabel('Epochs')\n",
        "        ax[0].set_ylabel('Cost')\n",
        "        ax[0].set_title('Training Cost')\n",
        "        ax[0].legend(loc='best')\n",
        "        \n",
        "        ax[1].plot(range(self.epochs), self.eval_['train_acc'], label='Train')\n",
        "        ax[1].plot(range(self.epochs), self.eval_['valid_acc'], label='Validation')\n",
        "        ax[1].set_xlabel('Epochs')\n",
        "        ax[1].set_ylabel('Cost')\n",
        "        ax[1].set_title('Training Accuracy')\n",
        "        ax[0].legend(loc='best')\n",
        "        \n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqFKnmu0AqUu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "d7c2e61e-653b-467a-e823-41a2d22856b1"
      },
      "source": [
        "X = data.drop(columns='target')\n",
        "y = data.target\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, \n",
        "                                                  y,\n",
        "                                                 stratify=y)\n",
        "# scale data\n",
        "scale = StandardScaler()\n",
        "X_train = scale.fit_transform(X_train)\n",
        "X_val = scale.transform(X_val)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
            "  return self.fit(X, **fit_params).transform(X)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF497z6SAu_Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f956f0d1-2a1f-486c-80ec-b312f35f6219"
      },
      "source": [
        "mlp = NeuralNetMLP(n_hidden=30,\n",
        "                  eta=0.001)\n",
        "mlp.fit(X_train=X_train, \n",
        "        y_train=y_train.values, \n",
        "        X_valid=X_val, \n",
        "        y_valid=y_val.values);"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100/100 | Cost: 0.68 | Train/Valid Acc: 87.67%/80.26%"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5b2WoKSAy8q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "fcb34c9e-b8bf-4272-ca8a-51ec919a5430"
      },
      "source": [
        "mlp.plot_training()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtQAAAG5CAYAAACur6PpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niATh/wOs5ZRbFrZRpwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGT1oRzXw3H9",
        "colab_type": "text"
      },
      "source": [
        "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
        "\n",
        "- Use the Heart Disease Dataset (binary classification)\n",
        "- Use an appropriate loss function for a binary classification task\n",
        "- Use an appropriate activation function on the final layer of your network. \n",
        "- Train your model using verbose output for ease of grading.\n",
        "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
        "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
        "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
        "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWw4IYxLxKwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(dense_layers=2,\n",
        "                 dense_nodes=5,\n",
        "                 dropout=False,\n",
        "                 dropout_pct=0.0,\n",
        "                 activation='sigmoid',\n",
        "                 weight_initializer='glorot_uniform',\n",
        "                 optimizer=SGD,\n",
        "                 lr=0.0001,\n",
        "                 input_shape=(X_train.shape[1],)):\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # add input layer\n",
        "    model.add(Dense(dense_nodes, \n",
        "                    input_shape=input_shape,\n",
        "                    kernel_initializer=weight_initializer,\n",
        "                    activation=activation))\n",
        "    \n",
        "    # add dense layers and drop out\n",
        "    for _ in range(dense_layers):\n",
        "        # dense\n",
        "        model.add(Dense(dense_nodes,\n",
        "                        kernel_initializer=weight_initializer,\n",
        "                        activation=activation))\n",
        "        # dropout\n",
        "        if dropout:\n",
        "            model.add(Dropout(rate=dropout_pct))\n",
        "\n",
        "    # add final activation layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # optimizer\n",
        "    optimizer=optimizer(lr=lr)\n",
        "    # compile model\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
        "              \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyj4PQN1iyeJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "03bf5511-ad03-4b7a-b67a-4e4c1c5559a2"
      },
      "source": [
        "epochs = 20\n",
        "batch_size = 20\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, \n",
        "                               epochs=epochs,\n",
        "                               batch_size=batch_size,\n",
        "                               verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "param_grid = {}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=param_grid, \n",
        "                    cv=5,\n",
        "                    n_jobs=1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Best: 0.5462555054263396 using {}\n",
            "Means: 0.5462555054263396, Stdev: 0.04121360135968761 with: {}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whRKbmH1BBR3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "1e28c53b-72ab-44cc-b709-d381371ed453"
      },
      "source": [
        "# define the grid search parameters\n",
        "param_grid = {'batch_size': [10, 20, 40, 60, 80],\n",
        "              'epochs': [30, 60, 90]}\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, \n",
        "                               epochs=epochs,\n",
        "                               batch_size=batch_size,\n",
        "                               verbose=0)\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=param_grid, \n",
        "                    cv=5,\n",
        "                    n_jobs=1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.5462555133036054 using {'batch_size': 10, 'epochs': 60}\n",
            "Means: 0.47136564480575693, Stdev: 0.05493819525050912 with: {'batch_size': 10, 'epochs': 30}\n",
            "Means: 0.5462555133036054, Stdev: 0.04121360050569371 with: {'batch_size': 10, 'epochs': 60}\n",
            "Means: 0.5286343710144186, Stdev: 0.05493820196957742 with: {'batch_size': 10, 'epochs': 90}\n",
            "Means: 0.5154185022026432, Stdev: 0.06000334989435976 with: {'batch_size': 20, 'epochs': 30}\n",
            "Means: 0.5154185022026432, Stdev: 0.06000334989435976 with: {'batch_size': 20, 'epochs': 60}\n",
            "Means: 0.5418502217084826, Stdev: 0.04568032013509382 with: {'batch_size': 20, 'epochs': 90}\n",
            "Means: 0.5198237823757306, Stdev: 0.049660813700694674 with: {'batch_size': 40, 'epochs': 30}\n",
            "Means: 0.5286343523059123, Stdev: 0.05493819394482335 with: {'batch_size': 40, 'epochs': 60}\n",
            "Means: 0.4493392094772818, Stdev: 0.09943036642322883 with: {'batch_size': 40, 'epochs': 90}\n",
            "Means: 0.4757709303616427, Stdev: 0.047666891047933466 with: {'batch_size': 60, 'epochs': 30}\n",
            "Means: 0.5418502256471155, Stdev: 0.04568031240305735 with: {'batch_size': 60, 'epochs': 60}\n",
            "Means: 0.4889867945127025, Stdev: 0.060965914109049106 with: {'batch_size': 60, 'epochs': 90}\n",
            "Means: 0.48458150252371635, Stdev: 0.06000335976046087 with: {'batch_size': 80, 'epochs': 30}\n",
            "Means: 0.5330396534850419, Stdev: 0.052407188084920626 with: {'batch_size': 80, 'epochs': 60}\n",
            "Means: 0.5330396534850419, Stdev: 0.052407188084920626 with: {'batch_size': 80, 'epochs': 90}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIUlcdXuBEre",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "6f779611-cb7f-4258-bbdc-5a3057fd503a"
      },
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, \n",
        "                               epochs=30,\n",
        "                               batch_size=10,\n",
        "                               verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "param_grid = {'optimizer': [SGD, Adam, Nadam],\n",
        "              'lr': [.01, .001, .0001, .00001]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.8370044023980128 using {'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.5462555105465624, Stdev: 0.05819241760350823 with: {'lr': 0.01, 'optimizer': <class 'keras.optimizers.SGD'>}\n",
            "Means: 0.8370044023980128, Stdev: 0.0269996523350443 with: {'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.8193832567609879, Stdev: 0.016700274224222007 with: {'lr': 0.01, 'optimizer': <class 'keras.optimizers.Nadam'>}\n",
            "Means: 0.5022026467822197, Stdev: 0.07430395526019908 with: {'lr': 0.001, 'optimizer': <class 'keras.optimizers.SGD'>}\n",
            "Means: 0.585903087901649, Stdev: 0.08444557497607072 with: {'lr': 0.001, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.6607929591565406, Stdev: 0.1414733364616387 with: {'lr': 0.001, 'optimizer': <class 'keras.optimizers.Nadam'>}\n",
            "Means: 0.47577092950827227, Stdev: 0.07027717421782265 with: {'lr': 0.0001, 'optimizer': <class 'keras.optimizers.SGD'>}\n",
            "Means: 0.590308373326247, Stdev: 0.017526699941877444 with: {'lr': 0.0001, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.5462555105465624, Stdev: 0.05819241760350823 with: {'lr': 0.0001, 'optimizer': <class 'keras.optimizers.Nadam'>}\n",
            "Means: 0.4537444994313076, Stdev: 0.05819241998839535 with: {'lr': 1e-05, 'optimizer': <class 'keras.optimizers.SGD'>}\n",
            "Means: 0.5462555105465624, Stdev: 0.05819241760350823 with: {'lr': 1e-05, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.4537444994313076, Stdev: 0.05819241998839535 with: {'lr': 1e-05, 'optimizer': <class 'keras.optimizers.Nadam'>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In11km_JBIvi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "64ba9342-f9d4-4e49-8b31-ec35808fe59b"
      },
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, \n",
        "                               epochs=30,\n",
        "                               batch_size=10,\n",
        "                               verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "param_grid = {'activation': ['sigmoid', 'tanh', 'relu'],\n",
        "              'optimizer': [Adam],\n",
        "              'lr': [.01]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.8105726911633025 using {'activation': 'sigmoid', 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.8105726911633025, Stdev: 0.016266519981147354 with: {'activation': 'sigmoid', 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.7753303972634975, Stdev: 0.010858007755473128 with: {'activation': 'tanh', 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.7665198237885462, Stdev: 0.04025422480815702 with: {'activation': 'relu', 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLjYuut6BLri",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "32c454b3-7417-494b-e298-6a1d6d6c95a5"
      },
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, \n",
        "                               epochs=30,\n",
        "                               batch_size=10,\n",
        "                               verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "param_grid = {'weight_initializer': ['glorot_uniform', \n",
        "                                     'random_uniform', \n",
        "                                     'random_normal'],\n",
        "              'activation' : ['relu'],\n",
        "             'optimizer' : [Adam],\n",
        "             'lr' : [0.01]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.8149779733057064 using {'activation': 'relu', 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_normal'}\n",
            "Means: 0.7797356786181748, Stdev: 0.016764981302985246 with: {'activation': 'relu', 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'glorot_uniform'}\n",
            "Means: 0.748898679464399, Stdev: 0.10210013619976542 with: {'activation': 'relu', 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.8149779733057064, Stdev: 0.019275199218382055 with: {'activation': 'relu', 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_normal'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199B1dfDBOht",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "dd643e4c-34e2-4a3b-d36d-6d61d6236a6e"
      },
      "source": [
        "model = KerasClassifier(build_fn=create_model, \n",
        "                               epochs=30,\n",
        "                               batch_size=10,\n",
        "                               verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "param_grid = {'dropout' : [True, False],\n",
        "             'dropout_pct' : [0.1, 0.2, 0.3],\n",
        "              'weight_initializer': ['random_uniform'],\n",
        "              'activation' : ['relu'],\n",
        "             'optimizer' : [Adam],\n",
        "             'lr' : [0.01]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.8017621116491142 using {'activation': 'relu', 'dropout': False, 'dropout_pct': 0.3, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7004405278466347, Stdev: 0.1642119583336267 with: {'activation': 'relu', 'dropout': True, 'dropout_pct': 0.1, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.6431718095808827, Stdev: 0.08363885297967802 with: {'activation': 'relu', 'dropout': True, 'dropout_pct': 0.2, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7797356780930238, Stdev: 0.025610553383369525 with: {'activation': 'relu', 'dropout': True, 'dropout_pct': 0.3, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7797356767801461, Stdev: 0.007014808141534128 with: {'activation': 'relu', 'dropout': False, 'dropout_pct': 0.1, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.6740088100475362, Stdev: 0.14604271988046555 with: {'activation': 'relu', 'dropout': False, 'dropout_pct': 0.2, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.8017621116491142, Stdev: 0.01749393574715601 with: {'activation': 'relu', 'dropout': False, 'dropout_pct': 0.3, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ3SednKBRPg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "acde0042-4a1f-4646-fb60-3600971fce8e"
      },
      "source": [
        "model = KerasClassifier(build_fn=create_model, \n",
        "                               epochs=30,\n",
        "                               batch_size=10,\n",
        "                               verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "param_grid = {'dense_layers' : [1,2],\n",
        "              'dense_nodes' : [5, 10, 15, 20, 25],\n",
        "              'weight_initializer': ['random_uniform'],\n",
        "              'activation' : ['relu'],\n",
        "             'optimizer' : [Adam],\n",
        "             'lr' : [0.01]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.8193832606996209 using {'activation': 'relu', 'dense_layers': 1, 'dense_nodes': 15, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7180616734836596, Stdev: 0.10822463974075425 with: {'activation': 'relu', 'dense_layers': 1, 'dense_nodes': 5, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7797356859702896, Stdev: 0.007014801428347454 with: {'activation': 'relu', 'dense_layers': 1, 'dense_nodes': 10, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.8193832606996209, Stdev: 0.043370870710620506 with: {'activation': 'relu', 'dense_layers': 1, 'dense_nodes': 15, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7885462534060037, Stdev: 0.027692449746085254 with: {'activation': 'relu', 'dense_layers': 1, 'dense_nodes': 20, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.8105726798725549, Stdev: 0.007426669260516094 with: {'activation': 'relu', 'dense_layers': 1, 'dense_nodes': 25, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7400881073023254, Stdev: 0.12569193183313607 with: {'activation': 'relu', 'dense_layers': 2, 'dense_nodes': 5, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.8149779680541959, Stdev: 0.0192751972048611 with: {'activation': 'relu', 'dense_layers': 2, 'dense_nodes': 10, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7753303938500157, Stdev: 0.0299875813216957 with: {'activation': 'relu', 'dense_layers': 2, 'dense_nodes': 15, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.8017621129619917, Stdev: 0.027744408059280645 with: {'activation': 'relu', 'dense_layers': 2, 'dense_nodes': 20, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7709251143333671, Stdev: 0.021120967637500487 with: {'activation': 'relu', 'dense_layers': 2, 'dense_nodes': 25, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkxZKdYvBUH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}