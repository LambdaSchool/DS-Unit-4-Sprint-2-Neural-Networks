{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS43SC.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/derek-shing/DS-Unit-4-Sprint-3-Neural-Networks/blob/master/DS_Unit_4_Sprint_3_Neural_Nets_Spring_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6SKlgYrpcym",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks Sprint Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrEbRrjVphPM",
        "colab_type": "text"
      },
      "source": [
        "## 1) Define the following terms:\n",
        "\n",
        "- Neuron\n",
        "- Input Layer\n",
        "- Hidden Layer\n",
        "- Output Layer\n",
        "- Activation\n",
        "- Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5EksLqnp4oB",
        "colab_type": "text"
      },
      "source": [
        "**Neuron**\n",
        "\n",
        "Neuron is node in the Neural Network. It take input from previous layer , and pass it to next layer based on activiation function\n",
        "\n",
        "**Input Layer**\n",
        "\n",
        "It is the first layer of the Neural Network that interact with the input. It's function is taking the data as an input and pass it to the layer after.\n",
        "\n",
        "**Hidden Layer**\n",
        "\n",
        "hidden layer is the layers between input and output layers. Increase the number of hidden layer can increase the complexity of the Neural Network and hence the ability to fit non- linear pattern data and the risk of overfitting at the same time.\n",
        "\n",
        "**Output Layer**\n",
        "\n",
        "Output layer is the final layer of the Neural Network. It represent the expected format of the output of the algorithm. For example , 0/1 in a binary classification Neural Network.\n",
        "\n",
        "**Activation**\n",
        "\n",
        "Each Neuron in a each is connected to all the Neuron at the layer after. Activation is the function determine which connection need to be activate and pass the signal to those activated Neuron.\n",
        "\n",
        "**Backpropagation**\n",
        "\n",
        "Backpropagation is a technique of updating the weight in the Neural Network. It start by calcualating the error in the output layer, and go backward to the first hidden layer to examine reason of causing the error. Since the error can be caused by the weight in each hidden layer or the input of them, we can reduce the error in each iternation by allocate change in weight in different layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri_gRA2Jp728",
        "colab_type": "text"
      },
      "source": [
        "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
        "\n",
        "| x1 | x2 | x3 | y |\n",
        "|----|----|----|---|\n",
        "| 1  | 1  | 1  | 1 |\n",
        "| 1  | 0  | 1  | 0 |\n",
        "| 0  | 1  | 1  | 0 |\n",
        "| 0  | 0  | 1  | 0 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig6ZTH8tpQ19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "inputs = np.array([\n",
        "    [1, 1, 1],\n",
        "    [1, 0, 1],\n",
        "    [0, 1, 1],\n",
        "    [0, 0, 1]\n",
        "])\n",
        "\n",
        "correct_outputs = [[1], [0], [0], [0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy3X14L7WkMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Perceptron(object):\n",
        "  \n",
        "  def sigmoid(self,x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "  def sigmoid_derivative(self,x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "  \n",
        "  def __init__(self, rate = 0.01, niter = 1000):\n",
        "    self.rate = rate\n",
        "    self.niter = niter\n",
        "    self.weights = 2 * np.random.random((3, 1)) - 1\n",
        "    \n",
        "  def fit(self, X, y):\n",
        "\n",
        "    for iteration in range(self.niter):\n",
        "      # Weighted sum of inputs/weights\n",
        "      weighted_sum = np.dot(X, self.weights)\n",
        "      # Activate\n",
        "      self.activated_output = self.sigmoid(weighted_sum)\n",
        "      # Calculate error\n",
        "      error = correct_outputs - self.activated_output\n",
        "      # Calculate weight adjustments \n",
        "      adjustments = error * self.sigmoid(self.activated_output)\n",
        "      # Update weights\n",
        "      self.weights = self.weights + np.dot(inputs.T, adjustments)\n",
        "      \n",
        "    print('final weight',self.weights)\n",
        "    return self\n",
        "  \n",
        "  def output_layer(self):\n",
        "    print('Output after training')\n",
        "    print(self.activated_output)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwUsKlnzXBW5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "48bf56ab-9a1c-4771-8725-01278e366dec"
      },
      "source": [
        "p=Perceptron()\n",
        "p.fit(inputs,correct_outputs)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final weight [[  9.22394561]\n",
            " [  9.22394561]\n",
            " [-13.81795682]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Perceptron at 0x7f4d2fcaa470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yinvFWckYkp0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "99250139-5058-4f74-bf7a-1aa16fd6f280"
      },
      "source": [
        "#print classification result\n",
        "\n",
        "#compare the correct result of [[1], [0], [0], [0]]\n",
        "p.output_layer()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output after training\n",
            "[[9.90329164e-01]\n",
            " [1.00209386e-02]\n",
            " [1.00209386e-02]\n",
            " [1.00057303e-06]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86HyRi8Osr3U",
        "colab_type": "text"
      },
      "source": [
        "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
        "- Your network must have one hidden layer. \n",
        "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
        "- Train your model on the Heart Disease dataset from UCI:\n",
        "\n",
        "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
        "\n",
        "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNfiajv3v4Ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####\n",
        "import pandas as pd\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
        "X = df.drop(columns='target')\n",
        "y = df['target']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyVHBPsBaWps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7c052d48-25b9-4c64-df13-d213ad43dc1d"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
              "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
              "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
              "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
              "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
              "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
              "\n",
              "   ca  thal  \n",
              "0   0     1  \n",
              "1   0     2  \n",
              "2   0     2  \n",
              "3   0     2  \n",
              "4   0     2  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPig91ftaZp2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9fe4ed7-41c5-4f0b-bd83-0c87f9ffdd53"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((303, 13), (303,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmaFYQhUe1CP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "83f91ca4-3e44-4122-97db-fc52f9920b77"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "scaler = preprocessing.StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
            "  return self.fit(X, **fit_params).transform(X)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpiinFM1almm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd83de20-14ab-4c3d-a442-dc1a57321ba3"
      },
      "source": [
        "y = np.array(y).reshape((-1,1))\n",
        "y.shape"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(303, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQhA-uBHaran",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self,X,y):\n",
        "    # Set up NN architecture/layout\n",
        "    self.inputs = X.shape[1]\n",
        "    self.hiddenNodes = 3\n",
        "    self.outputNodes = 1\n",
        "    \n",
        "    # Initialize weights for each layer\n",
        "    self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)\n",
        "    self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
        "    \n",
        "  \n",
        "  def sigmoid(self, s):\n",
        "    return 1/(1+np.exp(-s))\n",
        "    \n",
        "  def feed_forward(self, X):\n",
        "    \"\"\"Calculate the NN inference using feed forward.\"\"\"\n",
        "    # Weighted sum of inputs and hidden layer\n",
        "    self.hidden_sum = np.dot(X, self.weights1)\n",
        "    # Activations of weighted sum\n",
        "    self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "    # Weighted sum between hidden and output\n",
        "    self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
        "    # Final activation of output\n",
        "    self.activated_output = self.sigmoid(self.output_sum)\n",
        "    return self.activated_output\n",
        "  \n",
        "  def sigmoid(self, s):\n",
        "    return 1/(1+np.exp(-s))\n",
        "  \n",
        "  def sigmoidPrime(self, s):\n",
        "    return s * (1 - s)\n",
        "  \n",
        "  def backward(self, X, y, o):\n",
        "    # backward propgate through the network\n",
        "    self.o_error = y - o # error in output\n",
        "    self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to error\n",
        "\n",
        "    self.z2_error = self.o_delta.dot(self.weights2.T) # z2 error: how much our hidden layer weights contributed to output error\n",
        "    self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden) # applying derivative of sigmoid to z2 error\n",
        "\n",
        "    self.weights1 += X.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights\n",
        "    self.weights2 += self.activated_hidden.T.dot(self.o_delta) # adjusting second set (hidden --> output) weights\n",
        "    \n",
        "  def train (self, X, y):\n",
        "    o = self.feed_forward(X)\n",
        "    self.backward(X, y, o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73Xefz5HbZMw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "820ea821-1059-4248-90fe-c667b13a460c"
      },
      "source": [
        "print('Initial weight\\n')\n",
        "nn = NeuralNetwork(X,y)\n",
        "\n",
        "print(\"Layer 1 weights: \\n\", nn.weights1)\n",
        "print(\"Layer 2 weights: \\n\", nn.weights2)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial weight\n",
            "\n",
            "Layer 1 weights: \n",
            " [[-0.45246223  0.77704979  1.04557117]\n",
            " [-0.34214171 -0.92604662 -0.51296532]\n",
            " [ 0.71010924  0.09247887  0.63007494]\n",
            " [ 1.76293747  0.23095372 -0.80893689]\n",
            " [ 1.05742351  0.05136086  0.87244716]\n",
            " [ 1.06619854 -0.9590081   1.3820046 ]\n",
            " [ 0.90512196 -0.60390437  0.30444912]\n",
            " [ 0.25720749  0.02393181  0.87191399]\n",
            " [ 1.43735633  0.00730637  1.33088133]\n",
            " [ 0.98820261  0.23229616  0.17618092]\n",
            " [-1.15256537 -1.50076839  0.1650228 ]\n",
            " [-0.85592892 -0.03963614 -0.53466274]\n",
            " [-1.7884965   0.35730454 -0.41453181]]\n",
            "Layer 2 weights: \n",
            " [[ 0.08019871]\n",
            " [-0.89312027]\n",
            " [-0.33481725]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-pSfKEbb3cD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "cc276b56-f176-4f35-ed45-add50064ced3"
      },
      "source": [
        "for i in range(1000): # trains the NN 1,000 times\n",
        "  if i== 999:\n",
        "    print('+---------- EPOCH', i+1, '-----------+')\n",
        "    #print(\"Input: \\n\", X) \n",
        "    #print(\"Actual Output: \\n\", y)  \n",
        "    #print(\"Predicted Output: \\n\" + str(nn.feed_forward(X))) \n",
        "    print(\"final weight 1:\\n\",nn.weights1)\n",
        "    print(\"final weight 2:\\n\",nn.weights2)\n",
        "    print(\"Loss: \\n\" + str(np.mean(np.square(y - nn.feed_forward(X))))) # mean sum squared loss\n",
        "    print(\"\\n\")\n",
        "  nn.train(X, y)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------- EPOCH 1000 -----------+\n",
            "final weight 1:\n",
            " [[ -4.59173083  -3.76707964   0.04231524]\n",
            " [ -7.48941177   2.04649339 -10.16663307]\n",
            " [  3.33480597  -1.79194923  15.88784664]\n",
            " [  4.74636402   3.58610028  -5.98770361]\n",
            " [  3.33976222   7.69313371   4.1530919 ]\n",
            " [  3.19565716   5.89416739   4.96256623]\n",
            " [  4.5121025   -3.30031171  -2.49014567]\n",
            " [  0.37424746  -2.35289202   6.49853049]\n",
            " [ -1.12129506   3.00708631  -1.25198321]\n",
            " [ -0.17664894  10.38351448  -4.44600492]\n",
            " [ -2.9749837   -9.46512152   3.17717657]\n",
            " [ -0.72467127   1.44138735 -12.5465926 ]\n",
            " [-12.28683744   2.3008704  -12.92571533]]\n",
            "final weight 2:\n",
            " [[  4.64521405]\n",
            " [-24.94257139]\n",
            " [ 21.56903186]]\n",
            "Loss: \n",
            "0.08425154954293343\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGT1oRzXw3H9",
        "colab_type": "text"
      },
      "source": [
        "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
        "\n",
        "- Use the Heart Disease Dataset (binary classification)\n",
        "- Use an appropriate loss function for a binary classification task\n",
        "- Use an appropriate activation function on the final layer of your network. \n",
        "- Train your model using verbose output for ease of grading.\n",
        "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
        "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
        "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
        "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWw4IYxLxKwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyj4PQN1iyeJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69d94b26-b8ee-485e-9693-c3b8f92f4301"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import pandas as pd"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I100aDUTee_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train,y_test = train_test_split(X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNyTq1-wfap7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(16, input_dim=13, activation='relu'))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRWcLpQsehDS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2091
        },
        "outputId": "1627e13d-188f-46d1-83e6-967f50951dde"
      },
      "source": [
        "model = create_model()\n",
        "model.fit(X, y, validation_split=0.20, epochs=60, batch_size=60)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 242 samples, validate on 61 samples\n",
            "Epoch 1/60\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6456 - acc: 0.6116 - val_loss: 1.0075 - val_acc: 0.1639\n",
            "Epoch 2/60\n",
            "242/242 [==============================] - 0s 58us/step - loss: 0.6251 - acc: 0.6488 - val_loss: 0.9890 - val_acc: 0.1639\n",
            "Epoch 3/60\n",
            "242/242 [==============================] - 0s 40us/step - loss: 0.6061 - acc: 0.6860 - val_loss: 0.9881 - val_acc: 0.2295\n",
            "Epoch 4/60\n",
            "242/242 [==============================] - 0s 53us/step - loss: 0.5877 - acc: 0.7149 - val_loss: 0.9929 - val_acc: 0.2295\n",
            "Epoch 5/60\n",
            "242/242 [==============================] - 0s 43us/step - loss: 0.5711 - acc: 0.7479 - val_loss: 0.9982 - val_acc: 0.2295\n",
            "Epoch 6/60\n",
            "242/242 [==============================] - 0s 47us/step - loss: 0.5563 - acc: 0.7645 - val_loss: 1.0067 - val_acc: 0.2459\n",
            "Epoch 7/60\n",
            "242/242 [==============================] - 0s 45us/step - loss: 0.5408 - acc: 0.7686 - val_loss: 1.0115 - val_acc: 0.2295\n",
            "Epoch 8/60\n",
            "242/242 [==============================] - 0s 51us/step - loss: 0.5265 - acc: 0.7686 - val_loss: 1.0123 - val_acc: 0.2295\n",
            "Epoch 9/60\n",
            "242/242 [==============================] - 0s 47us/step - loss: 0.5129 - acc: 0.7810 - val_loss: 1.0189 - val_acc: 0.2787\n",
            "Epoch 10/60\n",
            "242/242 [==============================] - 0s 46us/step - loss: 0.5001 - acc: 0.7893 - val_loss: 1.0182 - val_acc: 0.2787\n",
            "Epoch 11/60\n",
            "242/242 [==============================] - 0s 48us/step - loss: 0.4874 - acc: 0.7975 - val_loss: 1.0152 - val_acc: 0.3115\n",
            "Epoch 12/60\n",
            "242/242 [==============================] - 0s 46us/step - loss: 0.4754 - acc: 0.8099 - val_loss: 0.9990 - val_acc: 0.3279\n",
            "Epoch 13/60\n",
            "242/242 [==============================] - 0s 49us/step - loss: 0.4641 - acc: 0.8182 - val_loss: 0.9795 - val_acc: 0.3770\n",
            "Epoch 14/60\n",
            "242/242 [==============================] - 0s 51us/step - loss: 0.4529 - acc: 0.8182 - val_loss: 0.9586 - val_acc: 0.3934\n",
            "Epoch 15/60\n",
            "242/242 [==============================] - 0s 50us/step - loss: 0.4430 - acc: 0.8182 - val_loss: 0.9375 - val_acc: 0.3934\n",
            "Epoch 16/60\n",
            "242/242 [==============================] - 0s 50us/step - loss: 0.4340 - acc: 0.8264 - val_loss: 0.9346 - val_acc: 0.3934\n",
            "Epoch 17/60\n",
            "242/242 [==============================] - 0s 69us/step - loss: 0.4252 - acc: 0.8264 - val_loss: 0.9387 - val_acc: 0.3934\n",
            "Epoch 18/60\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.4168 - acc: 0.8306 - val_loss: 0.9489 - val_acc: 0.4098\n",
            "Epoch 19/60\n",
            "242/242 [==============================] - 0s 48us/step - loss: 0.4096 - acc: 0.8388 - val_loss: 0.9511 - val_acc: 0.4262\n",
            "Epoch 20/60\n",
            "242/242 [==============================] - 0s 47us/step - loss: 0.4019 - acc: 0.8471 - val_loss: 0.9480 - val_acc: 0.4262\n",
            "Epoch 21/60\n",
            "242/242 [==============================] - 0s 49us/step - loss: 0.3953 - acc: 0.8471 - val_loss: 0.9215 - val_acc: 0.4262\n",
            "Epoch 22/60\n",
            "242/242 [==============================] - 0s 55us/step - loss: 0.3896 - acc: 0.8471 - val_loss: 0.9044 - val_acc: 0.4262\n",
            "Epoch 23/60\n",
            "242/242 [==============================] - 0s 50us/step - loss: 0.3840 - acc: 0.8554 - val_loss: 0.8970 - val_acc: 0.4426\n",
            "Epoch 24/60\n",
            "242/242 [==============================] - 0s 48us/step - loss: 0.3787 - acc: 0.8595 - val_loss: 0.8983 - val_acc: 0.4426\n",
            "Epoch 25/60\n",
            "242/242 [==============================] - 0s 49us/step - loss: 0.3737 - acc: 0.8636 - val_loss: 0.8970 - val_acc: 0.4590\n",
            "Epoch 26/60\n",
            "242/242 [==============================] - 0s 46us/step - loss: 0.3704 - acc: 0.8678 - val_loss: 0.8825 - val_acc: 0.4590\n",
            "Epoch 27/60\n",
            "242/242 [==============================] - 0s 46us/step - loss: 0.3674 - acc: 0.8678 - val_loss: 0.8694 - val_acc: 0.4918\n",
            "Epoch 28/60\n",
            "242/242 [==============================] - 0s 50us/step - loss: 0.3636 - acc: 0.8636 - val_loss: 0.8589 - val_acc: 0.5082\n",
            "Epoch 29/60\n",
            "242/242 [==============================] - 0s 46us/step - loss: 0.3599 - acc: 0.8636 - val_loss: 0.8465 - val_acc: 0.5082\n",
            "Epoch 30/60\n",
            "242/242 [==============================] - 0s 44us/step - loss: 0.3565 - acc: 0.8760 - val_loss: 0.8219 - val_acc: 0.5246\n",
            "Epoch 31/60\n",
            "242/242 [==============================] - 0s 62us/step - loss: 0.3536 - acc: 0.8719 - val_loss: 0.8079 - val_acc: 0.5410\n",
            "Epoch 32/60\n",
            "242/242 [==============================] - 0s 43us/step - loss: 0.3501 - acc: 0.8719 - val_loss: 0.8043 - val_acc: 0.5574\n",
            "Epoch 33/60\n",
            "242/242 [==============================] - 0s 46us/step - loss: 0.3469 - acc: 0.8719 - val_loss: 0.8015 - val_acc: 0.5410\n",
            "Epoch 34/60\n",
            "242/242 [==============================] - 0s 53us/step - loss: 0.3435 - acc: 0.8760 - val_loss: 0.8025 - val_acc: 0.5410\n",
            "Epoch 35/60\n",
            "242/242 [==============================] - 0s 55us/step - loss: 0.3406 - acc: 0.8760 - val_loss: 0.8071 - val_acc: 0.5410\n",
            "Epoch 36/60\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.3374 - acc: 0.8802 - val_loss: 0.8146 - val_acc: 0.5410\n",
            "Epoch 37/60\n",
            "242/242 [==============================] - 0s 47us/step - loss: 0.3343 - acc: 0.8802 - val_loss: 0.8249 - val_acc: 0.5410\n",
            "Epoch 38/60\n",
            "242/242 [==============================] - 0s 50us/step - loss: 0.3309 - acc: 0.8802 - val_loss: 0.8356 - val_acc: 0.5410\n",
            "Epoch 39/60\n",
            "242/242 [==============================] - 0s 79us/step - loss: 0.3280 - acc: 0.8802 - val_loss: 0.8470 - val_acc: 0.5410\n",
            "Epoch 40/60\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.3246 - acc: 0.8802 - val_loss: 0.8529 - val_acc: 0.5410\n",
            "Epoch 41/60\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.3214 - acc: 0.8802 - val_loss: 0.8654 - val_acc: 0.5246\n",
            "Epoch 42/60\n",
            "242/242 [==============================] - 0s 59us/step - loss: 0.3187 - acc: 0.8760 - val_loss: 0.8811 - val_acc: 0.5082\n",
            "Epoch 43/60\n",
            "242/242 [==============================] - 0s 48us/step - loss: 0.3160 - acc: 0.8760 - val_loss: 0.8939 - val_acc: 0.5082\n",
            "Epoch 44/60\n",
            "242/242 [==============================] - 0s 49us/step - loss: 0.3133 - acc: 0.8760 - val_loss: 0.9036 - val_acc: 0.5082\n",
            "Epoch 45/60\n",
            "242/242 [==============================] - 0s 52us/step - loss: 0.3114 - acc: 0.8802 - val_loss: 0.9304 - val_acc: 0.5082\n",
            "Epoch 46/60\n",
            "242/242 [==============================] - 0s 44us/step - loss: 0.3100 - acc: 0.8802 - val_loss: 0.9464 - val_acc: 0.4918\n",
            "Epoch 47/60\n",
            "242/242 [==============================] - 0s 39us/step - loss: 0.3085 - acc: 0.8843 - val_loss: 0.9499 - val_acc: 0.4918\n",
            "Epoch 48/60\n",
            "242/242 [==============================] - 0s 48us/step - loss: 0.3073 - acc: 0.8843 - val_loss: 0.9353 - val_acc: 0.4918\n",
            "Epoch 49/60\n",
            "242/242 [==============================] - 0s 56us/step - loss: 0.3059 - acc: 0.8843 - val_loss: 0.9252 - val_acc: 0.4918\n",
            "Epoch 50/60\n",
            "242/242 [==============================] - 0s 46us/step - loss: 0.3041 - acc: 0.8884 - val_loss: 0.9183 - val_acc: 0.4918\n",
            "Epoch 51/60\n",
            "242/242 [==============================] - 0s 60us/step - loss: 0.3021 - acc: 0.8884 - val_loss: 0.9205 - val_acc: 0.4918\n",
            "Epoch 52/60\n",
            "242/242 [==============================] - 0s 44us/step - loss: 0.3002 - acc: 0.8884 - val_loss: 0.9262 - val_acc: 0.4918\n",
            "Epoch 53/60\n",
            "242/242 [==============================] - 0s 43us/step - loss: 0.2985 - acc: 0.8884 - val_loss: 0.9327 - val_acc: 0.4918\n",
            "Epoch 54/60\n",
            "242/242 [==============================] - 0s 43us/step - loss: 0.2970 - acc: 0.8884 - val_loss: 0.9409 - val_acc: 0.4918\n",
            "Epoch 55/60\n",
            "242/242 [==============================] - 0s 44us/step - loss: 0.2955 - acc: 0.8926 - val_loss: 0.9457 - val_acc: 0.4918\n",
            "Epoch 56/60\n",
            "242/242 [==============================] - 0s 47us/step - loss: 0.2943 - acc: 0.8926 - val_loss: 0.9322 - val_acc: 0.5082\n",
            "Epoch 57/60\n",
            "242/242 [==============================] - 0s 55us/step - loss: 0.2928 - acc: 0.8926 - val_loss: 0.9260 - val_acc: 0.5082\n",
            "Epoch 58/60\n",
            "242/242 [==============================] - 0s 48us/step - loss: 0.2917 - acc: 0.8926 - val_loss: 0.9234 - val_acc: 0.5082\n",
            "Epoch 59/60\n",
            "242/242 [==============================] - 0s 55us/step - loss: 0.2903 - acc: 0.8926 - val_loss: 0.9224 - val_acc: 0.5082\n",
            "Epoch 60/60\n",
            "242/242 [==============================] - 0s 52us/step - loss: 0.2892 - acc: 0.8926 - val_loss: 0.9205 - val_acc: 0.5082\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4d0d7f4e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atE5HFBtgfXD",
        "colab_type": "text"
      },
      "source": [
        "### Use GridSearchCV to tune batch size and epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILlh4P9bgTtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
        "param_grid = {'batch_size': [40,60,80],\n",
        "              'epochs': [20,40],\n",
        "              #'activation':['relu','sigmoid']\n",
        "             }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji33C3t0hKdo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 19723
        },
        "outputId": "dcf44c69-6d12-40da-b279-c0af00a5d490"
      },
      "source": [
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
        "grid_result = grid.fit(X, y)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 0.7293 - acc: 0.5149\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.6946 - acc: 0.5842\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 53us/step - loss: 0.6613 - acc: 0.6485\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.6351 - acc: 0.6980\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 71us/step - loss: 0.6108 - acc: 0.7525\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 68us/step - loss: 0.5897 - acc: 0.7624\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 67us/step - loss: 0.5704 - acc: 0.7871\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 68us/step - loss: 0.5522 - acc: 0.8218\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.5364 - acc: 0.8119\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.5216 - acc: 0.8218\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.5079 - acc: 0.8366\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 53us/step - loss: 0.4935 - acc: 0.8416\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.4815 - acc: 0.8366\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.4699 - acc: 0.8416\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.4581 - acc: 0.8465\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.4472 - acc: 0.8465\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 68us/step - loss: 0.4358 - acc: 0.8515\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.4258 - acc: 0.8416\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.4157 - acc: 0.8465\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 53us/step - loss: 0.4070 - acc: 0.8465\n",
            "101/101 [==============================] - 0s 1ms/step\n",
            "202/202 [==============================] - 0s 34us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 0.6551 - acc: 0.6436\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 68us/step - loss: 0.6373 - acc: 0.6436\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.6223 - acc: 0.6683\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.6071 - acc: 0.7030\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 49us/step - loss: 0.5932 - acc: 0.7228\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.5793 - acc: 0.7574\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 62us/step - loss: 0.5664 - acc: 0.7574\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.5533 - acc: 0.7673\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 60us/step - loss: 0.5412 - acc: 0.7772\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 72us/step - loss: 0.5290 - acc: 0.7970\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.5182 - acc: 0.8119\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.5071 - acc: 0.8119\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.4966 - acc: 0.8119\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.4857 - acc: 0.8168\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 60us/step - loss: 0.4763 - acc: 0.8168\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 69us/step - loss: 0.4666 - acc: 0.8168\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.4588 - acc: 0.8168\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.4495 - acc: 0.8218\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.4405 - acc: 0.8366\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 53us/step - loss: 0.4320 - acc: 0.8317\n",
            "101/101 [==============================] - 0s 1ms/step\n",
            "202/202 [==============================] - 0s 33us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 0.8070 - acc: 0.2772\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.7527 - acc: 0.3465\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.7024 - acc: 0.5149\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.6581 - acc: 0.6782\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.6163 - acc: 0.8119\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.5799 - acc: 0.8366\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.5456 - acc: 0.8614\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.5140 - acc: 0.8713\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.4842 - acc: 0.8812\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.4577 - acc: 0.8911\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.4352 - acc: 0.8911\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.4146 - acc: 0.8911\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 62us/step - loss: 0.3955 - acc: 0.8911\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.3794 - acc: 0.8911\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.3646 - acc: 0.8960\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.3519 - acc: 0.9059\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.3410 - acc: 0.9059\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 66us/step - loss: 0.3310 - acc: 0.9010\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 53us/step - loss: 0.3228 - acc: 0.9010\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 61us/step - loss: 0.3164 - acc: 0.8960\n",
            "101/101 [==============================] - 0s 1ms/step\n",
            "202/202 [==============================] - 0s 40us/step\n",
            "Epoch 1/40\n",
            "202/202 [==============================] - 0s 2ms/step - loss: 0.5543 - acc: 0.6683\n",
            "Epoch 2/40\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.5401 - acc: 0.6733\n",
            "Epoch 3/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.5283 - acc: 0.6832\n",
            "Epoch 4/40\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.5178 - acc: 0.6931\n",
            "Epoch 5/40\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.5064 - acc: 0.7030\n",
            "Epoch 6/40\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.4959 - acc: 0.7178\n",
            "Epoch 7/40\n",
            "202/202 [==============================] - 0s 53us/step - loss: 0.4875 - acc: 0.7228\n",
            "Epoch 8/40\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.4804 - acc: 0.7277\n",
            "Epoch 9/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.4742 - acc: 0.7327\n",
            "Epoch 10/40\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.4679 - acc: 0.7426\n",
            "Epoch 11/40\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.4619 - acc: 0.7525\n",
            "Epoch 12/40\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.4564 - acc: 0.7574\n",
            "Epoch 13/40\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.4511 - acc: 0.7574\n",
            "Epoch 14/40\n",
            "202/202 [==============================] - 0s 49us/step - loss: 0.4464 - acc: 0.7673\n",
            "Epoch 15/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.4413 - acc: 0.7772\n",
            "Epoch 16/40\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.4369 - acc: 0.7723\n",
            "Epoch 17/40\n",
            "202/202 [==============================] - 0s 60us/step - loss: 0.4327 - acc: 0.7772\n",
            "Epoch 18/40\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.4291 - acc: 0.7871\n",
            "Epoch 19/40\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.4256 - acc: 0.7921\n",
            "Epoch 20/40\n",
            "202/202 [==============================] - 0s 65us/step - loss: 0.4222 - acc: 0.8020\n",
            "Epoch 21/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.4184 - acc: 0.8020\n",
            "Epoch 22/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.4145 - acc: 0.8069\n",
            "Epoch 23/40\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.4111 - acc: 0.8069\n",
            "Epoch 24/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.4078 - acc: 0.8119\n",
            "Epoch 25/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.4044 - acc: 0.8168\n",
            "Epoch 26/40\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.4009 - acc: 0.8218\n",
            "Epoch 27/40\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.3978 - acc: 0.8168\n",
            "Epoch 28/40\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.3937 - acc: 0.8267\n",
            "Epoch 29/40\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.3907 - acc: 0.8317\n",
            "Epoch 30/40\n",
            "202/202 [==============================] - 0s 60us/step - loss: 0.3873 - acc: 0.8267\n",
            "Epoch 31/40\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.3864 - acc: 0.8317\n",
            "Epoch 32/40\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.3867 - acc: 0.8317\n",
            "Epoch 33/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.3861 - acc: 0.8366\n",
            "Epoch 34/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.3831 - acc: 0.8366\n",
            "Epoch 35/40\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.3790 - acc: 0.8317\n",
            "Epoch 36/40\n",
            "202/202 [==============================] - 0s 62us/step - loss: 0.3755 - acc: 0.8317\n",
            "Epoch 37/40\n",
            "202/202 [==============================] - 0s 65us/step - loss: 0.3730 - acc: 0.8366\n",
            "Epoch 38/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.3702 - acc: 0.8366\n",
            "Epoch 39/40\n",
            "202/202 [==============================] - 0s 74us/step - loss: 0.3675 - acc: 0.8366\n",
            "Epoch 40/40\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.3646 - acc: 0.8465\n",
            "101/101 [==============================] - 0s 2ms/step\n",
            "202/202 [==============================] - 0s 25us/step\n",
            "Epoch 1/40\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7180 - acc: 0.5941\n",
            "Epoch 2/40\n",
            "202/202 [==============================] - 0s 67us/step - loss: 0.6856 - acc: 0.6188\n",
            "Epoch 3/40\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.6583 - acc: 0.6535\n",
            "Epoch 4/40\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.6365 - acc: 0.6634\n",
            "Epoch 5/40\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.6133 - acc: 0.6881\n",
            "Epoch 6/40\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.5954 - acc: 0.7079\n",
            "Epoch 7/40\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.5782 - acc: 0.6980\n",
            "Epoch 8/40\n",
            "202/202 [==============================] - 0s 53us/step - loss: 0.5638 - acc: 0.7178\n",
            "Epoch 9/40\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.5506 - acc: 0.7228\n",
            "Epoch 10/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.5387 - acc: 0.7376\n",
            "Epoch 11/40\n",
            "202/202 [==============================] - 0s 61us/step - loss: 0.5282 - acc: 0.7475\n",
            "Epoch 12/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.5185 - acc: 0.7673\n",
            "Epoch 13/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.5091 - acc: 0.7624\n",
            "Epoch 14/40\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.5000 - acc: 0.7772\n",
            "Epoch 15/40\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.4907 - acc: 0.7871\n",
            "Epoch 16/40\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.4822 - acc: 0.7822\n",
            "Epoch 17/40\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.4740 - acc: 0.7871\n",
            "Epoch 18/40\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.4660 - acc: 0.7970\n",
            "Epoch 19/40\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.4584 - acc: 0.7970\n",
            "Epoch 20/40\n",
            "202/202 [==============================] - 0s 68us/step - loss: 0.4509 - acc: 0.7970\n",
            "Epoch 21/40\n",
            "202/202 [==============================] - 0s 74us/step - loss: 0.4439 - acc: 0.8069\n",
            "Epoch 22/40\n",
            "202/202 [==============================] - 0s 65us/step - loss: 0.4378 - acc: 0.8020\n",
            "Epoch 23/40\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.4312 - acc: 0.8119\n",
            "Epoch 24/40\n",
            "202/202 [==============================] - 0s 70us/step - loss: 0.4253 - acc: 0.8069\n",
            "Epoch 25/40\n",
            "202/202 [==============================] - 0s 68us/step - loss: 0.4209 - acc: 0.8267\n",
            "Epoch 26/40\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.4177 - acc: 0.8267\n",
            "Epoch 27/40\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.4130 - acc: 0.8267\n",
            "Epoch 28/40\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.4068 - acc: 0.8317\n",
            "Epoch 29/40\n",
            "202/202 [==============================] - 0s 69us/step - loss: 0.4001 - acc: 0.8317\n",
            "Epoch 30/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.3943 - acc: 0.8366\n",
            "Epoch 31/40\n",
            "202/202 [==============================] - 0s 60us/step - loss: 0.3889 - acc: 0.8366\n",
            "Epoch 32/40\n",
            "202/202 [==============================] - 0s 63us/step - loss: 0.3841 - acc: 0.8416\n",
            "Epoch 33/40\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.3791 - acc: 0.8564\n",
            "Epoch 34/40\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.3747 - acc: 0.8614\n",
            "Epoch 35/40\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.3700 - acc: 0.8663\n",
            "Epoch 36/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.3661 - acc: 0.8614\n",
            "Epoch 37/40\n",
            "202/202 [==============================] - 0s 67us/step - loss: 0.3622 - acc: 0.8614\n",
            "Epoch 38/40\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.3587 - acc: 0.8663\n",
            "Epoch 39/40\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.3554 - acc: 0.8663\n",
            "Epoch 40/40\n",
            "202/202 [==============================] - 0s 90us/step - loss: 0.3520 - acc: 0.8663\n",
            "101/101 [==============================] - 0s 2ms/step\n",
            "202/202 [==============================] - 0s 36us/step\n",
            "Epoch 1/40\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6216 - acc: 0.7822\n",
            "Epoch 2/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.5940 - acc: 0.7970\n",
            "Epoch 3/40\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.5688 - acc: 0.8218\n",
            "Epoch 4/40\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.5454 - acc: 0.8218\n",
            "Epoch 5/40\n",
            "202/202 [==============================] - 0s 60us/step - loss: 0.5245 - acc: 0.8168\n",
            "Epoch 6/40\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.5075 - acc: 0.8168\n",
            "Epoch 7/40\n",
            "202/202 [==============================] - 0s 60us/step - loss: 0.4915 - acc: 0.8168\n",
            "Epoch 8/40\n",
            "202/202 [==============================] - 0s 61us/step - loss: 0.4759 - acc: 0.8168\n",
            "Epoch 9/40\n",
            "202/202 [==============================] - 0s 53us/step - loss: 0.4620 - acc: 0.8168\n",
            "Epoch 10/40\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.4492 - acc: 0.8168\n",
            "Epoch 11/40\n",
            "202/202 [==============================] - 0s 59us/step - loss: 0.4376 - acc: 0.8168\n",
            "Epoch 12/40\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.4258 - acc: 0.8168\n",
            "Epoch 13/40\n",
            "202/202 [==============================] - 0s 53us/step - loss: 0.4164 - acc: 0.8168\n",
            "Epoch 14/40\n",
            "202/202 [==============================] - 0s 99us/step - loss: 0.4062 - acc: 0.8168\n",
            "Epoch 15/40\n",
            "202/202 [==============================] - 0s 66us/step - loss: 0.3971 - acc: 0.8168\n",
            "Epoch 16/40\n",
            "202/202 [==============================] - 0s 63us/step - loss: 0.3885 - acc: 0.8168\n",
            "Epoch 17/40\n",
            "202/202 [==============================] - 0s 53us/step - loss: 0.3801 - acc: 0.8168\n",
            "Epoch 18/40\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.3733 - acc: 0.8168\n",
            "Epoch 19/40\n",
            "202/202 [==============================] - 0s 53us/step - loss: 0.3664 - acc: 0.8168\n",
            "Epoch 20/40\n",
            "202/202 [==============================] - 0s 61us/step - loss: 0.3612 - acc: 0.8168\n",
            "Epoch 21/40\n",
            "202/202 [==============================] - 0s 61us/step - loss: 0.3553 - acc: 0.8168\n",
            "Epoch 22/40\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.3502 - acc: 0.8168\n",
            "Epoch 23/40\n",
            "202/202 [==============================] - 0s 65us/step - loss: 0.3447 - acc: 0.8168\n",
            "Epoch 24/40\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.3386 - acc: 0.8267\n",
            "Epoch 25/40\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.3336 - acc: 0.8267\n",
            "Epoch 26/40\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.3286 - acc: 0.8366\n",
            "Epoch 27/40\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.3237 - acc: 0.8366\n",
            "Epoch 28/40\n",
            "202/202 [==============================] - 0s 53us/step - loss: 0.3199 - acc: 0.8416\n",
            "Epoch 29/40\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.3155 - acc: 0.8416\n",
            "Epoch 30/40\n",
            "202/202 [==============================] - 0s 67us/step - loss: 0.3118 - acc: 0.8416\n",
            "Epoch 31/40\n",
            "202/202 [==============================] - 0s 69us/step - loss: 0.3082 - acc: 0.8416\n",
            "Epoch 32/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.3047 - acc: 0.8465\n",
            "Epoch 33/40\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.3016 - acc: 0.8564\n",
            "Epoch 34/40\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.2986 - acc: 0.8663\n",
            "Epoch 35/40\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.2952 - acc: 0.8713\n",
            "Epoch 36/40\n",
            "202/202 [==============================] - 0s 59us/step - loss: 0.2918 - acc: 0.8812\n",
            "Epoch 37/40\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.2877 - acc: 0.8911\n",
            "Epoch 38/40\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.2835 - acc: 0.8911\n",
            "Epoch 39/40\n",
            "202/202 [==============================] - 0s 53us/step - loss: 0.2797 - acc: 0.8911\n",
            "Epoch 40/40\n",
            "202/202 [==============================] - 0s 53us/step - loss: 0.2762 - acc: 0.8911\n",
            "101/101 [==============================] - 0s 2ms/step\n",
            "202/202 [==============================] - 0s 39us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8275 - acc: 0.3762\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 49us/step - loss: 0.7858 - acc: 0.3960\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.7459 - acc: 0.4356\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.7087 - acc: 0.5198\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.6750 - acc: 0.6287\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 38us/step - loss: 0.6448 - acc: 0.6634\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.6189 - acc: 0.7178\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.5951 - acc: 0.7525\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 42us/step - loss: 0.5740 - acc: 0.7624\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 42us/step - loss: 0.5563 - acc: 0.7624\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.5392 - acc: 0.7624\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.5243 - acc: 0.7723\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 38us/step - loss: 0.5109 - acc: 0.7673\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.4998 - acc: 0.7772\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.4886 - acc: 0.7822\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.4784 - acc: 0.7871\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.4694 - acc: 0.7970\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 49us/step - loss: 0.4611 - acc: 0.8020\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.4528 - acc: 0.8020\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.4454 - acc: 0.7970\n",
            "101/101 [==============================] - 0s 2ms/step\n",
            "202/202 [==============================] - 0s 29us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7561 - acc: 0.4257\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.7315 - acc: 0.4505\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 41us/step - loss: 0.7058 - acc: 0.5149\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.6827 - acc: 0.5792\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.6623 - acc: 0.6139\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 38us/step - loss: 0.6429 - acc: 0.6683\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.6257 - acc: 0.6980\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 41us/step - loss: 0.6098 - acc: 0.7079\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 37us/step - loss: 0.5946 - acc: 0.7228\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.5790 - acc: 0.7327\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.5653 - acc: 0.7574\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.5514 - acc: 0.7822\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.5388 - acc: 0.8069\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.5259 - acc: 0.8168\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.5142 - acc: 0.8218\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.5034 - acc: 0.8317\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 38us/step - loss: 0.4928 - acc: 0.8416\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 41us/step - loss: 0.4824 - acc: 0.8515\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.4726 - acc: 0.8416\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.4628 - acc: 0.8317\n",
            "101/101 [==============================] - 0s 2ms/step\n",
            "202/202 [==============================] - 0s 43us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7419 - acc: 0.5000\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 41us/step - loss: 0.7093 - acc: 0.5495\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.6766 - acc: 0.5990\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.6460 - acc: 0.6386\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.6174 - acc: 0.6931\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.5911 - acc: 0.7228\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.5656 - acc: 0.7574\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.5438 - acc: 0.7772\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.5224 - acc: 0.7871\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 49us/step - loss: 0.5041 - acc: 0.8119\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.4864 - acc: 0.8168\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.4697 - acc: 0.8168\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.4558 - acc: 0.8168\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 42us/step - loss: 0.4415 - acc: 0.8267\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.4298 - acc: 0.8317\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.4177 - acc: 0.8267\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.4071 - acc: 0.8317\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.3982 - acc: 0.8317\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 41us/step - loss: 0.3890 - acc: 0.8317\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.3812 - acc: 0.8317\n",
            "101/101 [==============================] - 0s 3ms/step\n",
            "202/202 [==============================] - 0s 30us/step\n",
            "Epoch 1/40\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8075 - acc: 0.3168\n",
            "Epoch 2/40\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.7707 - acc: 0.3317\n",
            "Epoch 3/40\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.7350 - acc: 0.4158\n",
            "Epoch 4/40\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.7035 - acc: 0.5050\n",
            "Epoch 5/40\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.6740 - acc: 0.5990\n",
            "Epoch 6/40\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.6489 - acc: 0.6386\n",
            "Epoch 7/40\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.6280 - acc: 0.6881\n",
            "Epoch 8/40\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.6068 - acc: 0.7129\n",
            "Epoch 9/40\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.5886 - acc: 0.7327\n",
            "Epoch 10/40\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.5711 - acc: 0.7525\n",
            "Epoch 11/40\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.5555 - acc: 0.7772\n",
            "Epoch 12/40\n",
            "202/202 [==============================] - 0s 49us/step - loss: 0.5405 - acc: 0.7871\n",
            "Epoch 13/40\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.5261 - acc: 0.8020\n",
            "Epoch 14/40\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.5132 - acc: 0.8119\n",
            "Epoch 15/40\n",
            "202/202 [==============================] - 0s 42us/step - loss: 0.5008 - acc: 0.8168\n",
            "Epoch 16/40\n",
            "202/202 [==============================] - 0s 38us/step - loss: 0.4888 - acc: 0.8317\n",
            "Epoch 17/40\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.4771 - acc: 0.8317\n",
            "Epoch 18/40\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.4663 - acc: 0.8515\n",
            "Epoch 19/40\n",
            "202/202 [==============================] - 0s 42us/step - loss: 0.4563 - acc: 0.8564\n",
            "Epoch 20/40\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.4466 - acc: 0.8564\n",
            "Epoch 21/40\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.4372 - acc: 0.8663\n",
            "Epoch 22/40\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.4282 - acc: 0.8663\n",
            "Epoch 23/40\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.4199 - acc: 0.8663\n",
            "Epoch 24/40\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.4126 - acc: 0.8663\n",
            "Epoch 25/40\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.4049 - acc: 0.8663\n",
            "Epoch 26/40\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.3979 - acc: 0.8663\n",
            "Epoch 27/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.3923 - acc: 0.8663\n",
            "Epoch 28/40\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.3858 - acc: 0.8663\n",
            "Epoch 29/40\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.3803 - acc: 0.8663\n",
            "Epoch 30/40\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.3747 - acc: 0.8663\n",
            "Epoch 31/40\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.3698 - acc: 0.8663\n",
            "Epoch 32/40\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.3649 - acc: 0.8713\n",
            "Epoch 33/40\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.3603 - acc: 0.8762\n",
            "Epoch 34/40\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.3560 - acc: 0.8812\n",
            "Epoch 35/40\n",
            "202/202 [==============================] - 0s 62us/step - loss: 0.3518 - acc: 0.8861\n",
            "Epoch 36/40\n",
            "202/202 [==============================] - 0s 49us/step - loss: 0.3480 - acc: 0.8861\n",
            "Epoch 37/40\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.3446 - acc: 0.8861\n",
            "Epoch 38/40\n",
            "202/202 [==============================] - 0s 74us/step - loss: 0.3412 - acc: 0.8861\n",
            "Epoch 39/40\n",
            "202/202 [==============================] - 0s 49us/step - loss: 0.3381 - acc: 0.8861\n",
            "Epoch 40/40\n",
            "202/202 [==============================] - 0s 61us/step - loss: 0.3351 - acc: 0.8911\n",
            "101/101 [==============================] - 0s 3ms/step\n",
            "202/202 [==============================] - 0s 27us/step\n",
            "Epoch 1/40\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8107 - acc: 0.3168\n",
            "Epoch 2/40\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.7809 - acc: 0.4109\n",
            "Epoch 3/40\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.7528 - acc: 0.4406\n",
            "Epoch 4/40\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.7269 - acc: 0.4703\n",
            "Epoch 5/40\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.7039 - acc: 0.5198\n",
            "Epoch 6/40\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.6825 - acc: 0.5644\n",
            "Epoch 7/40\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.6621 - acc: 0.5941\n",
            "Epoch 8/40\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.6433 - acc: 0.6089\n",
            "Epoch 9/40\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.6256 - acc: 0.6436\n",
            "Epoch 10/40\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.6086 - acc: 0.6634\n",
            "Epoch 11/40\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.5921 - acc: 0.6931\n",
            "Epoch 12/40\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.5769 - acc: 0.7228\n",
            "Epoch 13/40\n",
            "202/202 [==============================] - 0s 63us/step - loss: 0.5622 - acc: 0.7376\n",
            "Epoch 14/40\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.5480 - acc: 0.7525\n",
            "Epoch 15/40\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.5347 - acc: 0.7525\n",
            "Epoch 16/40\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.5216 - acc: 0.7624\n",
            "Epoch 17/40\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.5089 - acc: 0.7673\n",
            "Epoch 18/40\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.4971 - acc: 0.7723\n",
            "Epoch 19/40\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.4856 - acc: 0.7723\n",
            "Epoch 20/40\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.4743 - acc: 0.7772\n",
            "Epoch 21/40\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.4642 - acc: 0.7772\n",
            "Epoch 22/40\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.4544 - acc: 0.7871\n",
            "Epoch 23/40\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.4453 - acc: 0.7871\n",
            "Epoch 24/40\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.4370 - acc: 0.7921\n",
            "Epoch 25/40\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.4285 - acc: 0.7921\n",
            "Epoch 26/40\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.4210 - acc: 0.7921\n",
            "Epoch 27/40\n",
            "202/202 [==============================] - 0s 49us/step - loss: 0.4138 - acc: 0.7970\n",
            "Epoch 28/40\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.4069 - acc: 0.7970\n",
            "Epoch 29/40\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.4007 - acc: 0.8020\n",
            "Epoch 30/40\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.3946 - acc: 0.8069\n",
            "Epoch 31/40\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.3884 - acc: 0.8119\n",
            "Epoch 32/40\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.3828 - acc: 0.8218\n",
            "Epoch 33/40\n",
            "202/202 [==============================] - 0s 49us/step - loss: 0.3778 - acc: 0.8218\n",
            "Epoch 34/40\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.3729 - acc: 0.8218\n",
            "Epoch 35/40\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.3683 - acc: 0.8218\n",
            "Epoch 36/40\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.3640 - acc: 0.8168\n",
            "Epoch 37/40\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.3596 - acc: 0.8267\n",
            "Epoch 38/40\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.3558 - acc: 0.8366\n",
            "Epoch 39/40\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.3515 - acc: 0.8465\n",
            "Epoch 40/40\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.3481 - acc: 0.8465\n",
            "101/101 [==============================] - 0s 3ms/step\n",
            "202/202 [==============================] - 0s 29us/step\n",
            "Epoch 1/40\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9093 - acc: 0.2228\n",
            "Epoch 2/40\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.8502 - acc: 0.3020\n",
            "Epoch 3/40\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.7954 - acc: 0.3614\n",
            "Epoch 4/40\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.7448 - acc: 0.4505\n",
            "Epoch 5/40\n",
            "202/202 [==============================] - 0s 53us/step - loss: 0.7001 - acc: 0.5396\n",
            "Epoch 6/40\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.6594 - acc: 0.5990\n",
            "Epoch 7/40\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.6243 - acc: 0.6485\n",
            "Epoch 8/40\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.5913 - acc: 0.6881\n",
            "Epoch 9/40\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.5627 - acc: 0.7475\n",
            "Epoch 10/40\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.5380 - acc: 0.7624\n",
            "Epoch 11/40\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.5171 - acc: 0.7921\n",
            "Epoch 12/40\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.4972 - acc: 0.8119\n",
            "Epoch 13/40\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.4793 - acc: 0.8218\n",
            "Epoch 14/40\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.4643 - acc: 0.8267\n",
            "Epoch 15/40\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.4504 - acc: 0.8267\n",
            "Epoch 16/40\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.4380 - acc: 0.8218\n",
            "Epoch 17/40\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.4266 - acc: 0.8218\n",
            "Epoch 18/40\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.4165 - acc: 0.8267\n",
            "Epoch 19/40\n",
            "202/202 [==============================] - 0s 53us/step - loss: 0.4069 - acc: 0.8267\n",
            "Epoch 20/40\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.3982 - acc: 0.8317\n",
            "Epoch 21/40\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.3904 - acc: 0.8317\n",
            "Epoch 22/40\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.3832 - acc: 0.8366\n",
            "Epoch 23/40\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.3768 - acc: 0.8366\n",
            "Epoch 24/40\n",
            "202/202 [==============================] - 0s 49us/step - loss: 0.3709 - acc: 0.8366\n",
            "Epoch 25/40\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.3654 - acc: 0.8317\n",
            "Epoch 26/40\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.3605 - acc: 0.8317\n",
            "Epoch 27/40\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.3560 - acc: 0.8317\n",
            "Epoch 28/40\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.3520 - acc: 0.8317\n",
            "Epoch 29/40\n",
            "202/202 [==============================] - 0s 59us/step - loss: 0.3483 - acc: 0.8317\n",
            "Epoch 30/40\n",
            "202/202 [==============================] - 0s 49us/step - loss: 0.3443 - acc: 0.8317\n",
            "Epoch 31/40\n",
            "202/202 [==============================] - 0s 63us/step - loss: 0.3411 - acc: 0.8317\n",
            "Epoch 32/40\n",
            "202/202 [==============================] - 0s 49us/step - loss: 0.3378 - acc: 0.8317\n",
            "Epoch 33/40\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.3349 - acc: 0.8317\n",
            "Epoch 34/40\n",
            "202/202 [==============================] - 0s 41us/step - loss: 0.3321 - acc: 0.8317\n",
            "Epoch 35/40\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.3294 - acc: 0.8366\n",
            "Epoch 36/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.3268 - acc: 0.8317\n",
            "Epoch 37/40\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.3243 - acc: 0.8317\n",
            "Epoch 38/40\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.3219 - acc: 0.8317\n",
            "Epoch 39/40\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.3193 - acc: 0.8317\n",
            "Epoch 40/40\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.3168 - acc: 0.8317\n",
            "101/101 [==============================] - 0s 3ms/step\n",
            "202/202 [==============================] - 0s 33us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8634 - acc: 0.3465\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.8277 - acc: 0.3960\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.7908 - acc: 0.4257\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.7569 - acc: 0.4554\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.7257 - acc: 0.5050\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 41us/step - loss: 0.6972 - acc: 0.5396\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 35us/step - loss: 0.6705 - acc: 0.6089\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.6481 - acc: 0.6436\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 36us/step - loss: 0.6276 - acc: 0.6584\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 37us/step - loss: 0.6076 - acc: 0.7030\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.5906 - acc: 0.7426\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 42us/step - loss: 0.5739 - acc: 0.7475\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.5590 - acc: 0.7673\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.5463 - acc: 0.7921\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 34us/step - loss: 0.5324 - acc: 0.7970\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.5207 - acc: 0.8069\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 41us/step - loss: 0.5108 - acc: 0.8168\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 32us/step - loss: 0.5004 - acc: 0.8119\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 36us/step - loss: 0.4912 - acc: 0.8168\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.4819 - acc: 0.8267\n",
            "101/101 [==============================] - 0s 3ms/step\n",
            "202/202 [==============================] - 0s 30us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.7665 - acc: 0.4257\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 42us/step - loss: 0.7492 - acc: 0.4356\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 34us/step - loss: 0.7312 - acc: 0.4653\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 37us/step - loss: 0.7148 - acc: 0.5099\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.6980 - acc: 0.5347\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.6825 - acc: 0.5545\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 35us/step - loss: 0.6671 - acc: 0.5941\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.6527 - acc: 0.6337\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.6388 - acc: 0.6485\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 35us/step - loss: 0.6250 - acc: 0.6782\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 38us/step - loss: 0.6122 - acc: 0.7030\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.5992 - acc: 0.7624\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 36us/step - loss: 0.5869 - acc: 0.7822\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 37us/step - loss: 0.5751 - acc: 0.7921\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.5633 - acc: 0.7970\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.5519 - acc: 0.7970\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 42us/step - loss: 0.5412 - acc: 0.7970\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 42us/step - loss: 0.5300 - acc: 0.8168\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 42us/step - loss: 0.5197 - acc: 0.8168\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 42us/step - loss: 0.5098 - acc: 0.8317\n",
            "101/101 [==============================] - 0s 4ms/step\n",
            "202/202 [==============================] - 0s 29us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.7955 - acc: 0.3366\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.7684 - acc: 0.3911\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 33us/step - loss: 0.7401 - acc: 0.4257\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 32us/step - loss: 0.7132 - acc: 0.4802\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 33us/step - loss: 0.6870 - acc: 0.5446\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.6633 - acc: 0.6089\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 37us/step - loss: 0.6402 - acc: 0.6485\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 36us/step - loss: 0.6194 - acc: 0.7079\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 38us/step - loss: 0.5988 - acc: 0.7723\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.5809 - acc: 0.7822\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.5631 - acc: 0.7921\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 49us/step - loss: 0.5479 - acc: 0.7970\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 33us/step - loss: 0.5327 - acc: 0.8020\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.5185 - acc: 0.7970\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 42us/step - loss: 0.5046 - acc: 0.8168\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 34us/step - loss: 0.4917 - acc: 0.8317\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.4799 - acc: 0.8366\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.4683 - acc: 0.8366\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.4570 - acc: 0.8366\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.4466 - acc: 0.8416\n",
            "101/101 [==============================] - 0s 4ms/step\n",
            "202/202 [==============================] - 0s 22us/step\n",
            "Epoch 1/40\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8723 - acc: 0.2673\n",
            "Epoch 2/40\n",
            "202/202 [==============================] - 0s 33us/step - loss: 0.8470 - acc: 0.2772\n",
            "Epoch 3/40\n",
            "202/202 [==============================] - 0s 32us/step - loss: 0.8208 - acc: 0.2723\n",
            "Epoch 4/40\n",
            "202/202 [==============================] - 0s 32us/step - loss: 0.7963 - acc: 0.2970\n",
            "Epoch 5/40\n",
            "202/202 [==============================] - 0s 35us/step - loss: 0.7738 - acc: 0.3218\n",
            "Epoch 6/40\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.7521 - acc: 0.3762\n",
            "Epoch 7/40\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.7323 - acc: 0.4257\n",
            "Epoch 8/40\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.7136 - acc: 0.4752\n",
            "Epoch 9/40\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.6952 - acc: 0.5099\n",
            "Epoch 10/40\n",
            "202/202 [==============================] - 0s 34us/step - loss: 0.6784 - acc: 0.5495\n",
            "Epoch 11/40\n",
            "202/202 [==============================] - 0s 42us/step - loss: 0.6623 - acc: 0.5941\n",
            "Epoch 12/40\n",
            "202/202 [==============================] - 0s 33us/step - loss: 0.6470 - acc: 0.6238\n",
            "Epoch 13/40\n",
            "202/202 [==============================] - 0s 42us/step - loss: 0.6320 - acc: 0.6584\n",
            "Epoch 14/40\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.6182 - acc: 0.6980\n",
            "Epoch 15/40\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.6046 - acc: 0.7228\n",
            "Epoch 16/40\n",
            "202/202 [==============================] - 0s 35us/step - loss: 0.5921 - acc: 0.7327\n",
            "Epoch 17/40\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.5790 - acc: 0.7475\n",
            "Epoch 18/40\n",
            "202/202 [==============================] - 0s 37us/step - loss: 0.5676 - acc: 0.7525\n",
            "Epoch 19/40\n",
            "202/202 [==============================] - 0s 41us/step - loss: 0.5562 - acc: 0.7574\n",
            "Epoch 20/40\n",
            "202/202 [==============================] - 0s 36us/step - loss: 0.5457 - acc: 0.7673\n",
            "Epoch 21/40\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.5349 - acc: 0.7772\n",
            "Epoch 22/40\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.5254 - acc: 0.7871\n",
            "Epoch 23/40\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.5159 - acc: 0.7921\n",
            "Epoch 24/40\n",
            "202/202 [==============================] - 0s 60us/step - loss: 0.5070 - acc: 0.8020\n",
            "Epoch 25/40\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.4984 - acc: 0.8069\n",
            "Epoch 26/40\n",
            "202/202 [==============================] - 0s 41us/step - loss: 0.4902 - acc: 0.8168\n",
            "Epoch 27/40\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.4829 - acc: 0.8218\n",
            "Epoch 28/40\n",
            "202/202 [==============================] - 0s 42us/step - loss: 0.4750 - acc: 0.8267\n",
            "Epoch 29/40\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.4681 - acc: 0.8267\n",
            "Epoch 30/40\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.4611 - acc: 0.8267\n",
            "Epoch 31/40\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.4545 - acc: 0.8416\n",
            "Epoch 32/40\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.4481 - acc: 0.8416\n",
            "Epoch 33/40\n",
            "202/202 [==============================] - 0s 38us/step - loss: 0.4418 - acc: 0.8416\n",
            "Epoch 34/40\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.4358 - acc: 0.8465\n",
            "Epoch 35/40\n",
            "202/202 [==============================] - 0s 38us/step - loss: 0.4298 - acc: 0.8465\n",
            "Epoch 36/40\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.4244 - acc: 0.8465\n",
            "Epoch 37/40\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.4191 - acc: 0.8465\n",
            "Epoch 38/40\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.4135 - acc: 0.8515\n",
            "Epoch 39/40\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.4083 - acc: 0.8465\n",
            "Epoch 40/40\n",
            "202/202 [==============================] - 0s 32us/step - loss: 0.4036 - acc: 0.8515\n",
            "101/101 [==============================] - 0s 4ms/step\n",
            "202/202 [==============================] - 0s 42us/step\n",
            "Epoch 1/40\n",
            "202/202 [==============================] - 1s 6ms/step - loss: 0.9374 - acc: 0.3366\n",
            "Epoch 2/40\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.9135 - acc: 0.3564\n",
            "Epoch 3/40\n",
            "202/202 [==============================] - 0s 30us/step - loss: 0.8903 - acc: 0.3465\n",
            "Epoch 4/40\n",
            "202/202 [==============================] - 0s 33us/step - loss: 0.8684 - acc: 0.3614\n",
            "Epoch 5/40\n",
            "202/202 [==============================] - 0s 30us/step - loss: 0.8487 - acc: 0.3564\n",
            "Epoch 6/40\n",
            "202/202 [==============================] - 0s 33us/step - loss: 0.8288 - acc: 0.3713\n",
            "Epoch 7/40\n",
            "202/202 [==============================] - 0s 35us/step - loss: 0.8106 - acc: 0.3812\n",
            "Epoch 8/40\n",
            "202/202 [==============================] - 0s 33us/step - loss: 0.7947 - acc: 0.3861\n",
            "Epoch 9/40\n",
            "202/202 [==============================] - 0s 32us/step - loss: 0.7787 - acc: 0.4109\n",
            "Epoch 10/40\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.7649 - acc: 0.4208\n",
            "Epoch 11/40\n",
            "202/202 [==============================] - 0s 41us/step - loss: 0.7512 - acc: 0.4307\n",
            "Epoch 12/40\n",
            "202/202 [==============================] - 0s 41us/step - loss: 0.7387 - acc: 0.4554\n",
            "Epoch 13/40\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.7275 - acc: 0.4802\n",
            "Epoch 14/40\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.7164 - acc: 0.5050\n",
            "Epoch 15/40\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.7065 - acc: 0.5198\n",
            "Epoch 16/40\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.6970 - acc: 0.5446\n",
            "Epoch 17/40\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.6882 - acc: 0.5594\n",
            "Epoch 18/40\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.6798 - acc: 0.5792\n",
            "Epoch 19/40\n",
            "202/202 [==============================] - 0s 38us/step - loss: 0.6712 - acc: 0.5941\n",
            "Epoch 20/40\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.6639 - acc: 0.6188\n",
            "Epoch 21/40\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.6563 - acc: 0.6287\n",
            "Epoch 22/40\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.6490 - acc: 0.6485\n",
            "Epoch 23/40\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.6423 - acc: 0.6485\n",
            "Epoch 24/40\n",
            "202/202 [==============================] - 0s 49us/step - loss: 0.6360 - acc: 0.6584\n",
            "Epoch 25/40\n",
            "202/202 [==============================] - 0s 41us/step - loss: 0.6294 - acc: 0.6634\n",
            "Epoch 26/40\n",
            "202/202 [==============================] - 0s 49us/step - loss: 0.6229 - acc: 0.6881\n",
            "Epoch 27/40\n",
            "202/202 [==============================] - 0s 42us/step - loss: 0.6162 - acc: 0.6931\n",
            "Epoch 28/40\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.6097 - acc: 0.7079\n",
            "Epoch 29/40\n",
            "202/202 [==============================] - 0s 97us/step - loss: 0.6030 - acc: 0.7475\n",
            "Epoch 30/40\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.5962 - acc: 0.7624\n",
            "Epoch 31/40\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.5892 - acc: 0.7673\n",
            "Epoch 32/40\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.5818 - acc: 0.7673\n",
            "Epoch 33/40\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.5748 - acc: 0.7723\n",
            "Epoch 34/40\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.5667 - acc: 0.7723\n",
            "Epoch 35/40\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.5591 - acc: 0.7723\n",
            "Epoch 36/40\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.5510 - acc: 0.7723\n",
            "Epoch 37/40\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.5431 - acc: 0.7822\n",
            "Epoch 38/40\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.5346 - acc: 0.7871\n",
            "Epoch 39/40\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.5269 - acc: 0.7921\n",
            "Epoch 40/40\n",
            "202/202 [==============================] - 0s 36us/step - loss: 0.5183 - acc: 0.8069\n",
            "101/101 [==============================] - 0s 5ms/step\n",
            "202/202 [==============================] - 0s 28us/step\n",
            "Epoch 1/40\n",
            "202/202 [==============================] - 1s 6ms/step - loss: 0.7119 - acc: 0.4307\n",
            "Epoch 2/40\n",
            "202/202 [==============================] - 0s 39us/step - loss: 0.6883 - acc: 0.5248\n",
            "Epoch 3/40\n",
            "202/202 [==============================] - 0s 37us/step - loss: 0.6659 - acc: 0.5743\n",
            "Epoch 4/40\n",
            "202/202 [==============================] - 0s 38us/step - loss: 0.6444 - acc: 0.6683\n",
            "Epoch 5/40\n",
            "202/202 [==============================] - 0s 36us/step - loss: 0.6236 - acc: 0.6881\n",
            "Epoch 6/40\n",
            "202/202 [==============================] - 0s 35us/step - loss: 0.6048 - acc: 0.7426\n",
            "Epoch 7/40\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.5859 - acc: 0.7921\n",
            "Epoch 8/40\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.5690 - acc: 0.8020\n",
            "Epoch 9/40\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.5520 - acc: 0.8168\n",
            "Epoch 10/40\n",
            "202/202 [==============================] - 0s 35us/step - loss: 0.5364 - acc: 0.8267\n",
            "Epoch 11/40\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.5212 - acc: 0.8317\n",
            "Epoch 12/40\n",
            "202/202 [==============================] - 0s 40us/step - loss: 0.5068 - acc: 0.8366\n",
            "Epoch 13/40\n",
            "202/202 [==============================] - 0s 37us/step - loss: 0.4933 - acc: 0.8366\n",
            "Epoch 14/40\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.4809 - acc: 0.8366\n",
            "Epoch 15/40\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.4679 - acc: 0.8416\n",
            "Epoch 16/40\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.4560 - acc: 0.8416\n",
            "Epoch 17/40\n",
            "202/202 [==============================] - 0s 37us/step - loss: 0.4453 - acc: 0.8416\n",
            "Epoch 18/40\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.4343 - acc: 0.8416\n",
            "Epoch 19/40\n",
            "202/202 [==============================] - 0s 41us/step - loss: 0.4243 - acc: 0.8416\n",
            "Epoch 20/40\n",
            "202/202 [==============================] - 0s 42us/step - loss: 0.4148 - acc: 0.8416\n",
            "Epoch 21/40\n",
            "202/202 [==============================] - 0s 49us/step - loss: 0.4061 - acc: 0.8416\n",
            "Epoch 22/40\n",
            "202/202 [==============================] - 0s 69us/step - loss: 0.3975 - acc: 0.8416\n",
            "Epoch 23/40\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.3891 - acc: 0.8465\n",
            "Epoch 24/40\n",
            "202/202 [==============================] - 0s 49us/step - loss: 0.3817 - acc: 0.8515\n",
            "Epoch 25/40\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.3748 - acc: 0.8515\n",
            "Epoch 26/40\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.3684 - acc: 0.8515\n",
            "Epoch 27/40\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.3622 - acc: 0.8515\n",
            "Epoch 28/40\n",
            "202/202 [==============================] - 0s 36us/step - loss: 0.3560 - acc: 0.8564\n",
            "Epoch 29/40\n",
            "202/202 [==============================] - 0s 42us/step - loss: 0.3505 - acc: 0.8564\n",
            "Epoch 30/40\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.3450 - acc: 0.8614\n",
            "Epoch 31/40\n",
            "202/202 [==============================] - 0s 49us/step - loss: 0.3401 - acc: 0.8614\n",
            "Epoch 32/40\n",
            "202/202 [==============================] - 0s 46us/step - loss: 0.3352 - acc: 0.8614\n",
            "Epoch 33/40\n",
            "202/202 [==============================] - 0s 43us/step - loss: 0.3307 - acc: 0.8614\n",
            "Epoch 34/40\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.3263 - acc: 0.8614\n",
            "Epoch 35/40\n",
            "202/202 [==============================] - 0s 42us/step - loss: 0.3218 - acc: 0.8614\n",
            "Epoch 36/40\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.3177 - acc: 0.8614\n",
            "Epoch 37/40\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.3140 - acc: 0.8614\n",
            "Epoch 38/40\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.3100 - acc: 0.8663\n",
            "Epoch 39/40\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.3062 - acc: 0.8663\n",
            "Epoch 40/40\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.3027 - acc: 0.8713\n",
            "101/101 [==============================] - 0s 5ms/step\n",
            "202/202 [==============================] - 0s 37us/step\n",
            "Epoch 1/20\n",
            "303/303 [==============================] - 1s 4ms/step - loss: 0.7524 - acc: 0.4389\n",
            "Epoch 2/20\n",
            "303/303 [==============================] - 0s 48us/step - loss: 0.7166 - acc: 0.5083\n",
            "Epoch 3/20\n",
            "303/303 [==============================] - 0s 49us/step - loss: 0.6857 - acc: 0.5512\n",
            "Epoch 4/20\n",
            "303/303 [==============================] - 0s 49us/step - loss: 0.6612 - acc: 0.5908\n",
            "Epoch 5/20\n",
            "303/303 [==============================] - 0s 46us/step - loss: 0.6395 - acc: 0.6337\n",
            "Epoch 6/20\n",
            "303/303 [==============================] - 0s 47us/step - loss: 0.6194 - acc: 0.6469\n",
            "Epoch 7/20\n",
            "303/303 [==============================] - 0s 48us/step - loss: 0.6015 - acc: 0.6766\n",
            "Epoch 8/20\n",
            "303/303 [==============================] - 0s 48us/step - loss: 0.5835 - acc: 0.6964\n",
            "Epoch 9/20\n",
            "303/303 [==============================] - 0s 49us/step - loss: 0.5660 - acc: 0.7129\n",
            "Epoch 10/20\n",
            "303/303 [==============================] - 0s 55us/step - loss: 0.5497 - acc: 0.7162\n",
            "Epoch 11/20\n",
            "303/303 [==============================] - 0s 53us/step - loss: 0.5346 - acc: 0.7426\n",
            "Epoch 12/20\n",
            "303/303 [==============================] - 0s 47us/step - loss: 0.5191 - acc: 0.7492\n",
            "Epoch 13/20\n",
            "303/303 [==============================] - 0s 48us/step - loss: 0.5036 - acc: 0.7624\n",
            "Epoch 14/20\n",
            "303/303 [==============================] - 0s 48us/step - loss: 0.4896 - acc: 0.7657\n",
            "Epoch 15/20\n",
            "303/303 [==============================] - 0s 49us/step - loss: 0.4754 - acc: 0.7822\n",
            "Epoch 16/20\n",
            "303/303 [==============================] - 0s 48us/step - loss: 0.4621 - acc: 0.8020\n",
            "Epoch 17/20\n",
            "303/303 [==============================] - 0s 49us/step - loss: 0.4499 - acc: 0.8086\n",
            "Epoch 18/20\n",
            "303/303 [==============================] - 0s 49us/step - loss: 0.4378 - acc: 0.8119\n",
            "Epoch 19/20\n",
            "303/303 [==============================] - 0s 54us/step - loss: 0.4275 - acc: 0.8053\n",
            "Epoch 20/20\n",
            "303/303 [==============================] - 0s 47us/step - loss: 0.4170 - acc: 0.8086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGGBYVe2f2QE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "0e384514-e93f-4f52-947f-649a98e15aea"
      },
      "source": [
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.5808580840381459 using {'batch_size': 40, 'epochs': 20}\n",
            "Means: 0.5808580840381459, Stdev: 0.13389743896896858 with: {'batch_size': 40, 'epochs': 20}\n",
            "Means: 0.5148514764438762, Stdev: 0.23485737879077612 with: {'batch_size': 40, 'epochs': 40}\n",
            "Means: 0.3861386085254918, Stdev: 0.2752166482191622 with: {'batch_size': 60, 'epochs': 20}\n",
            "Means: 0.5379537902649479, Stdev: 0.27203208549432273 with: {'batch_size': 60, 'epochs': 40}\n",
            "Means: 0.47194718867048574, Stdev: 0.3014341542125554 with: {'batch_size': 80, 'epochs': 20}\n",
            "Means: 0.5049504937708574, Stdev: 0.24103710497898198 with: {'batch_size': 80, 'epochs': 40}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}