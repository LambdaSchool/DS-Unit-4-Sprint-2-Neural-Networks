{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Johan_Mazorra_LS_DS13_Unit_4_Sprint_Challenge_2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsmazorra/DS-Unit-4-Sprint-2-Neural-Networks/blob/master/DS-Unit-4-Sprint-2-Neural-Networks/Johan_Mazorra_LS_DS13_Unit_4_Sprint_Challenge_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQQUVQsK5HXO",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2*\n",
        "\n",
        "# Sprint Challenge - Neural Network Foundations\n",
        "\n",
        "Table of Problems\n",
        "\n",
        "1. [Defining Neural Networks](#Q1)\n",
        "2. [Simple Perceptron](#Q2)\n",
        "    - Perceptron\n",
        "    - Multilayer Perceptron\n",
        "    - Analyze and Compare\n",
        "4. [Keras MMP](#Q3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEXPYgVR5HXR",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"Q1\"></a>\n",
        "## 1. Defining Neural Networks \n",
        "\n",
        "Write *your own* definitions for the following terms:\n",
        "\n",
        "- **Neuron:** also known as a node, this is a function within a neural network which recieves input values either from the raw original data, or from one or more previous nodes. It applies weight coefficients and a bias intercept to those input values, and the sum of those weights and biases applied to each prior input are run through an activation function to relativize the output of the node compared to all other outputs in its layer. This final relativized output is the activation value, which is then input to one or many subsequent nodes, or is interpreted as an output value of the neural network in the case of the output layer.\n",
        "- **Input Layer:** the first layer of nodes in a neural network, recieving the normalized or transformed version of the original data, which enters the neural network as a vector. Each node corresponds to a feature of the observation. The input layer does itself have weights, biases, and an activation function, but it only outputs activation values to the next layer. It does not recieve activation values. It only recieves the vectorized raw data.\n",
        "- **Hidden Layer:** a layer of nodes which does not recieve inputs from the original data. It only recieves activation values from previous nodes of the neural network. However, nodes in a hidden layer also do not present a final output value from the neural network. They recieve activation values, and must also pass activation values on to other nodes in subsequent layers. Hidden layer nodes are therefore intermediating nodes. They also have weights, biases, and an activation function.\n",
        "- **Output Layer:** a layer of nodes which does not recive input activation values from the original data, but from previous nodes in the neural network; but unlike a hidden layer, it's output activation values are the final outputs of the neural network. These output activation values are interpreted as probablilities, each node either representing a class within the target, a target itself, or some medley of the two. Nodes in the output layer also have wights, biases, and activation functions, but their output activation values are the final interpretation of the neural network upon that data observation. No further nodes recieve activation values from the output layer.\n",
        "- **Activation Function:** a function which relates the nodes that are members of the same layer. Each node being a function that presents to the activation function its weighted sum, the activation function then returns to the node an activation value that is calculated relative to all of the other nodes encapsulated by that activation function. It is that activation value that is then passed from one node to the next layer, or as an output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXm5WF7u5HXV",
        "colab_type": "text"
      },
      "source": [
        "Explain how back propagation works as if you were explaining it to a five year-old. Use your own words, but feel free to reference external materials for this question. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTQ9rXlHVdUd",
        "colab_type": "text"
      },
      "source": [
        "Do you remember a few years ago, when you were 2, and the only thing that came out of your mouth was a question about this or that? Well sometimes, you would point at blue or green things, and ask: Is that red? And I would have to kindly tell you, \"No, silly. That's green!\" Or \"No, that's blue\", or \"Yes, it is red!\" if it was actually red. Anyway, you and I kept doing this until you finally understood that cardinals, stop signs, fire hydrants, fire fighters, and bricks were red; and that grass, violets, the midday sky, and your eyes were not red. That's what back propagation is. It's a neural network, in the computer's brain, changing its guesses about the world when it gets it wrong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7pUE3TR5HXg",
        "colab_type": "text"
      },
      "source": [
        "Remember our Simple Perceptron Class from Monday. In a simple prediction describe the process of making a prediction. How do you go from inputs to predicted output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9VZOJV2P_Kg",
        "colab_type": "text"
      },
      "source": [
        "1] The raw data is vectorized, usually into a float value from between 1 and 0\n",
        "\n",
        "2] A single observation is fed into the input layer as a vector, usually with each utilized feature being passed to a single input node.\n",
        "\n",
        "3] Input nodes take the input value, multipy it by a weight, add a bias to that product, and pass that weighted sum into the activation function for the input layer.\n",
        "\n",
        "4] The acivation function of the input layer uses a compression-type function to compress sometimes radically varying input values to within 1 and 0, relative to the range of other weighted sums it recieves from nodes of that layer. It then returns to the respective node its \"squishified\", or [1,0] compressed weighted sum (the activation value).\n",
        "\n",
        "5] The node then passes its activation value on to other nodes in the next layer, as an input to that node's weight/bias function.\n",
        "\n",
        "6] If the next node in the network, recieving that prior activation value, is a hidden layer node, steps 3-5 are repeated for that node and that layer, and it's activation value is passed onto one or many nodes in the subsequent layer.\n",
        "\n",
        "7] If the next node in the network, recieving that prior activation value, is an output layer node, steps 3-5 are repeated for that node and that layer, but it's activation value is not passed into another node. It is output to us, the consumer of that data, assigned to a target or target class, as a probability that the observation it came from represents a result in that target or that target class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K6_35U-5HXo",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"Q2\"></a>\n",
        "## 2. Simple Perceptron\n",
        "\n",
        "In this question, you will build two neural networks using Tensorflow Keras. After you build these two models, compare the results of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYZczs9N5HXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Our Dataset\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "xx, yy = np.meshgrid(np.linspace(-3, 3, 50),\n",
        "                     np.linspace(-3, 3, 50))\n",
        "rng = np.random.RandomState(0)\n",
        "\n",
        "\"Use this X & y in the following 2 models\"\n",
        "X = rng.randn(300, 2)\n",
        "y = np.array(np.logical_xor(X[:, 0] > 0, X[:, 1] > 0), \n",
        "             dtype=int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcRU4QXaQXZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5d325adf-cac5-44c2-c926-d4d8779a2421"
      },
      "source": [
        "# I'm going to see how these arrays look.\n",
        "X[:2]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.76405235, 0.40015721],\n",
              "       [0.97873798, 2.2408932 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbjRGKz6Qaht",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0883b7e4-e112-415a-943f-8216d500697d"
      },
      "source": [
        "y[:2]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8nMiJiWQdOz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28979909-af56-452d-8d9d-fb0ea31ac393"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((300, 2), (300,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REVo56C_5HXu",
        "colab_type": "text"
      },
      "source": [
        "### Simple Perceptron\n",
        "Construct a simple perceptron using Keras. You model should have 1 dense layer with a single neuron and a sigmoid activation function. Your model should be called `model1` and make sure to save the results of your fit statement to a variable called `h1`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZPl0ckt5HXv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "cd06830c-833d-4efe-da28-ee3853e06957"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "\n",
        "stop = EarlyStopping(monitor='accuracy', min_delta=0.01, patience=8)\n",
        "\n",
        "model1 = Sequential([\n",
        "      Dense(1,activation='sigmoid', input_dim=2)\n",
        "])\n",
        "\n",
        "model1.compile(optimizer='sgd', loss='binary_crossentropy', metrics='accuracy')\n",
        "\n",
        "h1 = model1.fit(X,y,epochs=100,callbacks=[stop])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.7590 - accuracy: 0.4833\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7560 - accuracy: 0.4867\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7529 - accuracy: 0.4867\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7501 - accuracy: 0.4933\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7471 - accuracy: 0.4967\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7444 - accuracy: 0.4967\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7416 - accuracy: 0.5033\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7390 - accuracy: 0.5067\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7368 - accuracy: 0.5033\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7342 - accuracy: 0.5100\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7321 - accuracy: 0.5133\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7299 - accuracy: 0.5133\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7279 - accuracy: 0.5133\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7260 - accuracy: 0.5133\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.7241 - accuracy: 0.5133\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7227 - accuracy: 0.5100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWlXNPAQ5HX3",
        "colab_type": "text"
      },
      "source": [
        "### Multi-Layer Perceptron\n",
        "Now construct a multi-layer perceptron using. Here are some architecture suggestions: \n",
        "- 2 Hidden Layers\n",
        "- 5-32 Neurons in the Hidden Layers\n",
        "- Your pick of activation function and optimizer\n",
        "- Incorporate the Callback function below into your model\n",
        "\n",
        "Your model should be called `model2` and make sure to save the results of your fit statement to a variable called `h2`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrWzZw725HX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback): \n",
        "    def on_epoch_end(self, epoch, logs={}): \n",
        "        if(logs.get('accuracy') > .99999):   \n",
        "            self.model.stop_training = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-CyeWVK5HX-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "68611f20-bbdf-4b8f-bdbf-e3a91e28e00c"
      },
      "source": [
        "model2 = Sequential([\n",
        "      Dense(32,activation='relu', input_dim=2),\n",
        "      Dense(8,activation='relu'),\n",
        "      Dense(1,activation='sigmoid')\n",
        "])\n",
        "\n",
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n",
        "\n",
        "h2 = model2.fit(X,y,epochs=100,callbacks=[myCallback()])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7081 - accuracy: 0.5333\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6774 - accuracy: 0.5700\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6485 - accuracy: 0.7000\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6229 - accuracy: 0.8133\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.5975 - accuracy: 0.8733\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5728 - accuracy: 0.9167\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5487 - accuracy: 0.9267\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5252 - accuracy: 0.9533\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.9633\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4802 - accuracy: 0.9667\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4590 - accuracy: 0.9733\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4389 - accuracy: 0.9767\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4191 - accuracy: 0.9733\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4004 - accuracy: 0.9733\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3825 - accuracy: 0.9800\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3656 - accuracy: 0.9800\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3496 - accuracy: 0.9733\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3347 - accuracy: 0.9733\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3202 - accuracy: 0.9733\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3067 - accuracy: 0.9733\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2946 - accuracy: 0.9767\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2824 - accuracy: 0.9767\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2714 - accuracy: 0.9800\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2609 - accuracy: 0.9800\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2514 - accuracy: 0.9867\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2422 - accuracy: 0.9867\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2335 - accuracy: 0.9833\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2260 - accuracy: 0.9833\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2179 - accuracy: 0.9833\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2108 - accuracy: 0.9867\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2054 - accuracy: 0.9833\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1979 - accuracy: 0.9900\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1926 - accuracy: 0.9867\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1867 - accuracy: 0.9900\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1816 - accuracy: 0.9900\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1768 - accuracy: 0.9933\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1714 - accuracy: 0.9933\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1671 - accuracy: 0.9900\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1632 - accuracy: 0.9833\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1603 - accuracy: 0.9833\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1556 - accuracy: 0.9867\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1519 - accuracy: 0.9867\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1486 - accuracy: 0.9867\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1446 - accuracy: 0.9900\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1419 - accuracy: 0.9900\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1385 - accuracy: 0.9967\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1355 - accuracy: 0.9933\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1332 - accuracy: 0.9967\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1302 - accuracy: 0.9967\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1275 - accuracy: 0.9967\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1253 - accuracy: 0.9967\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1230 - accuracy: 0.9967\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1210 - accuracy: 0.9900\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1187 - accuracy: 0.9900\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1167 - accuracy: 0.9867\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1145 - accuracy: 0.9867\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1124 - accuracy: 0.9933\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1105 - accuracy: 0.9967\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1090 - accuracy: 0.9967\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1067 - accuracy: 0.9967\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1055 - accuracy: 0.9967\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1039 - accuracy: 0.9933\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1019 - accuracy: 0.9967\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1007 - accuracy: 0.9967\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0990 - accuracy: 0.9967\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0975 - accuracy: 0.9967\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0965 - accuracy: 0.9967\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0949 - accuracy: 0.9967\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0943 - accuracy: 0.9900\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0922 - accuracy: 0.9933\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0917 - accuracy: 0.9933\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0908 - accuracy: 0.9933\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0882 - accuracy: 0.9967\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0877 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PKeERXWR4G-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Nice, got 100% accuracy in 74 epochs."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyJ-AIJn5HYX",
        "colab_type": "text"
      },
      "source": [
        "### Analyze and Compare\n",
        "\n",
        "**Before you Start**: You will need to install an additional library for this next segment. Install the package `mlxtend` into the environment you are using for the sprint challenge.\n",
        "\n",
        "\n",
        "The cells below generate decision boundary plots of your models (`model1` & `model2`). Review the plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmGRpSDiSw8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import mlxtend"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nvPiY4U5HYY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "ce9b7356-32e0-41ab-eae6-9694330a1af8"
      },
      "source": [
        "# Do Not change anything in this cell\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "h = .02  # step size in the mesh\n",
        "\n",
        "# create a mesh to plot in\n",
        "x_min, x_max = X[:, 0].min() - .2, X[:, 0].max() + .2\n",
        "y_min, y_max = X[:, 1].min() - .2, X[:, 1].max() + .2\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "\n",
        "\n",
        "for clf, hist, name, grd in zip([model1,model2], [h1, h2],['Perceptron', 'Multi-Layer Perceptron'],[1,2]):\n",
        "\n",
        "    ax = plt.subplot(1,2, grd)\n",
        "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
        "    title = f\"{name} with {hist.history['accuracy'][-1]:,.2f} Accuracy\"\n",
        "    plt.title(title)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/mlxtend/plotting/decision_regions.py:244: MatplotlibDeprecationWarning: Passing unsupported keyword arguments to axis() will raise a TypeError in 3.3.\n",
            "  ax.axis(xmin=xx.min(), xmax=xx.max(), y_min=yy.min(), y_max=yy.max())\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GMi1Nzv5HYd",
        "colab_type": "text"
      },
      "source": [
        "Why does the Perceptron (`model1`) only achieve ~70% accuracy? What is the architectural property of the Multi-Layer Perceptron that allows it more accurately learn the relationship between X and y? \n",
        "\n",
        "Why might this property be useful in more complex data such as images?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWeGEFTd9wPP",
        "colab_type": "text"
      },
      "source": [
        "The Single-Layer Perceptron performs much worse than the Multi-Layer Perceptron because it is unable to learn or apply meta-linear decision boundaries to the dataset, which is clearly quadrant in its characteristic.\n",
        "The ability of the MLP to recognize meta-linear and multi-linear decision boundaries is useful for image classification because in natural images there are many lines and curves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1DtQ0ik5HYj",
        "colab_type": "text"
      },
      "source": [
        "## 3. Keras MMP <a id=\"Q3\"></a>\n",
        "\n",
        "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
        "Use the Heart Disease Dataset (binary classification)\n",
        "Use an appropriate loss function for a binary classification task\n",
        "Use an appropriate activation function on the final layer of your network.\n",
        "Train your model using verbose output for ease of grading.\n",
        "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
        "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
        "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
        "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "outputHidden": false,
        "id": "8YfmKBs65HYk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "29a52eee-7f3e-4f1c-85fb-e0b1c3b5e2e5"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
        "df = df.sample(frac=1)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(303, 14)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>165</td>\n",
              "      <td>289</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>124</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>157</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>182</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>140</td>\n",
              "      <td>298</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>122</td>\n",
              "      <td>1</td>\n",
              "      <td>4.2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>67</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>237</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>71</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>53</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>142</td>\n",
              "      <td>226</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>111</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
              "231   57    1   0       165   289    1  ...      0      1.0      1   3     3       0\n",
              "162   41    1   1       120   157    0  ...      0      0.0      2   0     2       1\n",
              "250   51    1   0       140   298    0  ...      1      4.2      1   3     3       0\n",
              "272   67    1   0       120   237    0  ...      0      1.0      1   0     2       0\n",
              "95    53    1   0       142   226    0  ...      1      0.0      2   0     3       1\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krK4Yo535HY4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ac34635a-1a07-4dae-a5c9-40030a2cbcdd"
      },
      "source": [
        "# Your Code Here\n",
        "df.target.value_counts(normalize=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    0.544554\n",
              "0    0.455446\n",
              "Name: target, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrXAJgYnTSdL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13a646fa-c0a6-47f5-a02d-a2cecf9bb46e"
      },
      "source": [
        "# Ok, I have to beat this baseline of 55%.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop(columns='target')\n",
        "y = df['target']\n",
        "X.shape,y.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((303, 13), (303,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxZ8SAy1Tg-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=8\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbG2HPYjTztA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "edd65edc-1fa4-4037-855a-ceff8a5c7cff"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>94</td>\n",
              "      <td>199</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>179</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>58</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>114</td>\n",
              "      <td>318</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>4.4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>120</td>\n",
              "      <td>178</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>96</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>154</td>\n",
              "      <td>232</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>164</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>108</td>\n",
              "      <td>267</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>167</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  sex  cp  trestbps  chol  ...  exang  oldpeak  slope  ca  thal\n",
              "124   39    0   2        94   199  ...      0      0.0      2   0     2\n",
              "291   58    1   0       114   318  ...      0      4.4      0   3     1\n",
              "136   60    0   2       120   178  ...      0      0.0      2   0     2\n",
              "287   57    1   1       154   232  ...      0      0.0      2   1     2\n",
              "123   54    0   2       108   267  ...      0      0.0      2   0     2\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDrZVUY2T2pn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "432257de-398f-4cce-96aa-017cc67a9827"
      },
      "source": [
        "# Let's scale and vectorize the data.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "X_test_std = scaler.transform(X_test)\n",
        "print(X_train_std[:2])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.6822868  -1.46385011  1.04009595 -2.09146154 -0.84286138 -0.39036003\n",
            "   0.86473673  1.2598261  -0.70272837 -0.89413249  0.98186126 -0.71269665\n",
            "  -0.5146532 ]\n",
            " [ 0.40209873  0.68313005 -0.94965283 -0.96971133  1.32764046 -0.39036003\n",
            "   2.75001862 -0.42077301 -0.70272837  2.92791598 -2.29552392  2.22717702\n",
            "  -2.17526754]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXNO1z8cT-sN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train0, X_val, y_train0, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=8\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cGvsXURUJFJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7761e82f-3afa-4621-e22e-0fb6e7889519"
      },
      "source": [
        "# Let's create the model now.\n",
        "stop = EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=16)\n",
        "\n",
        "model3 = Sequential([\n",
        "      Dense(32,activation='relu', input_dim=13),\n",
        "      Dense(64,activation='relu'),\n",
        "      Dense(16,activation='relu'),\n",
        "      Dense(4,activation='relu'),\n",
        "      Dense(1,activation='sigmoid')\n",
        "])\n",
        "\n",
        "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n",
        "\n",
        "result3 = model3.fit(\n",
        "    X_train0,y_train0,\n",
        "    validation_data=(X_val,y_val),\n",
        "    epochs=100\n",
        "    )"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 29.4491 - accuracy: 0.4508 - val_loss: 4.8277 - val_accuracy: 0.3061\n",
            "Epoch 2/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 6.8696 - accuracy: 0.5337 - val_loss: 6.3152 - val_accuracy: 0.5510\n",
            "Epoch 3/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 4.1825 - accuracy: 0.5492 - val_loss: 0.7815 - val_accuracy: 0.5510\n",
            "Epoch 4/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6583 - accuracy: 0.5492 - val_loss: 0.6192 - val_accuracy: 0.5510\n",
            "Epoch 5/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6882 - accuracy: 0.5492 - val_loss: 0.5980 - val_accuracy: 0.5510\n",
            "Epoch 6/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6453 - accuracy: 0.5492 - val_loss: 0.6174 - val_accuracy: 0.5510\n",
            "Epoch 7/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6893 - accuracy: 0.5492 - val_loss: 0.5938 - val_accuracy: 0.5510\n",
            "Epoch 8/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6519 - accuracy: 0.5803 - val_loss: 0.6427 - val_accuracy: 0.5918\n",
            "Epoch 9/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6683 - accuracy: 0.5233 - val_loss: 0.6482 - val_accuracy: 0.5918\n",
            "Epoch 10/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6719 - accuracy: 0.5078 - val_loss: 0.6433 - val_accuracy: 0.5918\n",
            "Epoch 11/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6714 - accuracy: 0.5181 - val_loss: 0.6244 - val_accuracy: 0.6122\n",
            "Epoch 12/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6604 - accuracy: 0.5544 - val_loss: 0.5985 - val_accuracy: 0.6531\n",
            "Epoch 13/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6510 - accuracy: 0.6269 - val_loss: 0.5780 - val_accuracy: 0.6939\n",
            "Epoch 14/100\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.6325 - accuracy: 0.6788 - val_loss: 0.5894 - val_accuracy: 0.6327\n",
            "Epoch 15/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6817 - accuracy: 0.7047 - val_loss: 0.6350 - val_accuracy: 0.6735\n",
            "Epoch 16/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6287 - accuracy: 0.7202 - val_loss: 0.5865 - val_accuracy: 0.6939\n",
            "Epoch 17/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6344 - accuracy: 0.6839 - val_loss: 0.5849 - val_accuracy: 0.6939\n",
            "Epoch 18/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6286 - accuracy: 0.6788 - val_loss: 0.5890 - val_accuracy: 0.6735\n",
            "Epoch 19/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6547 - accuracy: 0.5751 - val_loss: 0.6704 - val_accuracy: 0.5102\n",
            "Epoch 20/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6996 - accuracy: 0.4663 - val_loss: 0.6613 - val_accuracy: 0.5510\n",
            "Epoch 21/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6707 - accuracy: 0.5130 - val_loss: 0.6001 - val_accuracy: 0.6939\n",
            "Epoch 22/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6333 - accuracy: 0.6373 - val_loss: 0.5679 - val_accuracy: 0.6939\n",
            "Epoch 23/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6435 - accuracy: 0.6321 - val_loss: 0.6028 - val_accuracy: 0.6939\n",
            "Epoch 24/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6396 - accuracy: 0.6218 - val_loss: 0.5726 - val_accuracy: 0.6939\n",
            "Epoch 25/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6215 - accuracy: 0.6736 - val_loss: 0.5763 - val_accuracy: 0.6735\n",
            "Epoch 26/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6255 - accuracy: 0.7150 - val_loss: 0.6103 - val_accuracy: 0.6735\n",
            "Epoch 27/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6347 - accuracy: 0.7202 - val_loss: 0.6037 - val_accuracy: 0.6735\n",
            "Epoch 28/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6229 - accuracy: 0.6943 - val_loss: 0.5783 - val_accuracy: 0.6939\n",
            "Epoch 29/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6202 - accuracy: 0.6943 - val_loss: 0.5845 - val_accuracy: 0.6939\n",
            "Epoch 30/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6178 - accuracy: 0.7047 - val_loss: 0.5849 - val_accuracy: 0.6735\n",
            "Epoch 31/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6650 - accuracy: 0.5337 - val_loss: 0.6775 - val_accuracy: 0.4898\n",
            "Epoch 32/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6902 - accuracy: 0.4715 - val_loss: 0.6878 - val_accuracy: 0.4694\n",
            "Epoch 33/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6930 - accuracy: 0.4663 - val_loss: 0.6890 - val_accuracy: 0.4694\n",
            "Epoch 34/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6942 - accuracy: 0.4560 - val_loss: 0.6893 - val_accuracy: 0.4694\n",
            "Epoch 35/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6942 - accuracy: 0.4560 - val_loss: 0.6890 - val_accuracy: 0.4694\n",
            "Epoch 36/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6940 - accuracy: 0.4560 - val_loss: 0.6885 - val_accuracy: 0.4694\n",
            "Epoch 37/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6936 - accuracy: 0.4611 - val_loss: 0.6875 - val_accuracy: 0.4694\n",
            "Epoch 38/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6926 - accuracy: 0.4663 - val_loss: 0.6862 - val_accuracy: 0.4694\n",
            "Epoch 39/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6904 - accuracy: 0.4715 - val_loss: 0.6850 - val_accuracy: 0.4694\n",
            "Epoch 40/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6878 - accuracy: 0.4715 - val_loss: 0.6799 - val_accuracy: 0.4898\n",
            "Epoch 41/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6844 - accuracy: 0.4819 - val_loss: 0.6755 - val_accuracy: 0.4898\n",
            "Epoch 42/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6801 - accuracy: 0.4870 - val_loss: 0.6688 - val_accuracy: 0.5510\n",
            "Epoch 43/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6765 - accuracy: 0.4974 - val_loss: 0.6444 - val_accuracy: 0.5918\n",
            "Epoch 44/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6688 - accuracy: 0.5181 - val_loss: 0.6259 - val_accuracy: 0.5918\n",
            "Epoch 45/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6576 - accuracy: 0.5699 - val_loss: 0.5903 - val_accuracy: 0.6735\n",
            "Epoch 46/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6231 - accuracy: 0.6788 - val_loss: 0.6026 - val_accuracy: 0.7143\n",
            "Epoch 47/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6389 - accuracy: 0.7306 - val_loss: 0.6869 - val_accuracy: 0.6531\n",
            "Epoch 48/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6831 - accuracy: 0.6891 - val_loss: 0.5931 - val_accuracy: 0.6735\n",
            "Epoch 49/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6164 - accuracy: 0.6943 - val_loss: 0.5940 - val_accuracy: 0.6735\n",
            "Epoch 50/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6168 - accuracy: 0.6995 - val_loss: 0.5938 - val_accuracy: 0.6735\n",
            "Epoch 51/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6567 - accuracy: 0.5544 - val_loss: 0.6821 - val_accuracy: 0.4898\n",
            "Epoch 52/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6901 - accuracy: 0.4663 - val_loss: 0.6917 - val_accuracy: 0.4490\n",
            "Epoch 53/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6962 - accuracy: 0.4456 - val_loss: 0.6891 - val_accuracy: 0.4694\n",
            "Epoch 54/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6936 - accuracy: 0.4560 - val_loss: 0.6883 - val_accuracy: 0.4694\n",
            "Epoch 55/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6926 - accuracy: 0.4611 - val_loss: 0.6871 - val_accuracy: 0.4694\n",
            "Epoch 56/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6917 - accuracy: 0.4663 - val_loss: 0.6855 - val_accuracy: 0.4694\n",
            "Epoch 57/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6895 - accuracy: 0.4663 - val_loss: 0.6833 - val_accuracy: 0.4898\n",
            "Epoch 58/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6857 - accuracy: 0.4767 - val_loss: 0.6777 - val_accuracy: 0.4898\n",
            "Epoch 59/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6801 - accuracy: 0.4974 - val_loss: 0.6678 - val_accuracy: 0.5102\n",
            "Epoch 60/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6742 - accuracy: 0.4974 - val_loss: 0.6501 - val_accuracy: 0.5918\n",
            "Epoch 61/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6682 - accuracy: 0.5078 - val_loss: 0.6310 - val_accuracy: 0.5918\n",
            "Epoch 62/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6625 - accuracy: 0.5544 - val_loss: 0.6092 - val_accuracy: 0.5918\n",
            "Epoch 63/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6465 - accuracy: 0.6166 - val_loss: 0.5946 - val_accuracy: 0.6735\n",
            "Epoch 64/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6664 - accuracy: 0.5181 - val_loss: 0.6782 - val_accuracy: 0.5102\n",
            "Epoch 65/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6885 - accuracy: 0.4767 - val_loss: 0.6865 - val_accuracy: 0.4694\n",
            "Epoch 66/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6921 - accuracy: 0.4611 - val_loss: 0.6883 - val_accuracy: 0.4694\n",
            "Epoch 67/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6927 - accuracy: 0.4508 - val_loss: 0.6886 - val_accuracy: 0.5510\n",
            "Epoch 68/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6926 - accuracy: 0.5492 - val_loss: 0.6885 - val_accuracy: 0.5510\n",
            "Epoch 69/100\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.6925 - accuracy: 0.5492 - val_loss: 0.6879 - val_accuracy: 0.5510\n",
            "Epoch 70/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6923 - accuracy: 0.5492 - val_loss: 0.6873 - val_accuracy: 0.5510\n",
            "Epoch 71/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6918 - accuracy: 0.5492 - val_loss: 0.6864 - val_accuracy: 0.5510\n",
            "Epoch 72/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6912 - accuracy: 0.5492 - val_loss: 0.6851 - val_accuracy: 0.5510\n",
            "Epoch 73/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6900 - accuracy: 0.5492 - val_loss: 0.6840 - val_accuracy: 0.5510\n",
            "Epoch 74/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6890 - accuracy: 0.5492 - val_loss: 0.6832 - val_accuracy: 0.5510\n",
            "Epoch 75/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6858 - accuracy: 0.5492 - val_loss: 0.6776 - val_accuracy: 0.5510\n",
            "Epoch 76/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6801 - accuracy: 0.5492 - val_loss: 0.6676 - val_accuracy: 0.5510\n",
            "Epoch 77/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6738 - accuracy: 0.5492 - val_loss: 0.6519 - val_accuracy: 0.5510\n",
            "Epoch 78/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6686 - accuracy: 0.5492 - val_loss: 0.6280 - val_accuracy: 0.5510\n",
            "Epoch 79/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6604 - accuracy: 0.5492 - val_loss: 0.6107 - val_accuracy: 0.5510\n",
            "Epoch 80/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6428 - accuracy: 0.5492 - val_loss: 0.5713 - val_accuracy: 0.5510\n",
            "Epoch 81/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6289 - accuracy: 0.5492 - val_loss: 0.5718 - val_accuracy: 0.5510\n",
            "Epoch 82/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6818 - accuracy: 0.5492 - val_loss: 0.5861 - val_accuracy: 0.5510\n",
            "Epoch 83/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6225 - accuracy: 0.5492 - val_loss: 0.5965 - val_accuracy: 0.5510\n",
            "Epoch 84/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6328 - accuracy: 0.5492 - val_loss: 0.5660 - val_accuracy: 0.5510\n",
            "Epoch 85/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.7152 - accuracy: 0.5440 - val_loss: 0.6029 - val_accuracy: 0.6939\n",
            "Epoch 86/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6306 - accuracy: 0.6580 - val_loss: 0.6141 - val_accuracy: 0.6122\n",
            "Epoch 87/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6545 - accuracy: 0.5803 - val_loss: 0.6138 - val_accuracy: 0.6122\n",
            "Epoch 88/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6462 - accuracy: 0.6010 - val_loss: 0.5690 - val_accuracy: 0.5510\n",
            "Epoch 89/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6163 - accuracy: 0.5492 - val_loss: 0.5624 - val_accuracy: 0.5510\n",
            "Epoch 90/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6193 - accuracy: 0.5492 - val_loss: 0.5939 - val_accuracy: 0.5510\n",
            "Epoch 91/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6490 - accuracy: 0.5492 - val_loss: 0.6137 - val_accuracy: 0.5510\n",
            "Epoch 92/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6201 - accuracy: 0.6218 - val_loss: 0.5626 - val_accuracy: 0.7143\n",
            "Epoch 93/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6184 - accuracy: 0.6839 - val_loss: 0.5593 - val_accuracy: 0.6939\n",
            "Epoch 94/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6063 - accuracy: 0.6995 - val_loss: 0.6239 - val_accuracy: 0.7143\n",
            "Epoch 95/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6867 - accuracy: 0.6891 - val_loss: 0.5703 - val_accuracy: 0.6939\n",
            "Epoch 96/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6107 - accuracy: 0.6684 - val_loss: 0.6081 - val_accuracy: 0.6327\n",
            "Epoch 97/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6460 - accuracy: 0.6010 - val_loss: 0.5972 - val_accuracy: 0.6735\n",
            "Epoch 98/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6231 - accuracy: 0.6528 - val_loss: 0.5721 - val_accuracy: 0.6939\n",
            "Epoch 99/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6687 - accuracy: 0.6891 - val_loss: 0.5851 - val_accuracy: 0.7143\n",
            "Epoch 100/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.5983 - accuracy: 0.7358 - val_loss: 0.5659 - val_accuracy: 0.6939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Riy_ZfdIUS5i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f80f99b-f696-49d9-ce39-ad12a7649b14"
      },
      "source": [
        "max(result3.history['val_accuracy'])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7142857313156128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF-1kCBQUZnV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d2074ef3-17af-45b5-8259-e01341fe4bc6"
      },
      "source": [
        "# We beat the dummy baseline with 71% accuracy, but we can do much better.\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Nadam, Adadelta\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(8)\n",
        "\n",
        "def create_model(learning_rate=0.01, optimizer=Adam()):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(32,activation='relu', input_dim=13))\n",
        "  model.add(Dense(64,activation='relu'))\n",
        "  model.add(Dense(16,activation='relu'))\n",
        "  model.add(Dense(4,activation='relu'))\n",
        "  model.add(Dense(1,activation='relu'))\n",
        "  optim = optimizer(learning_rate=learning_rate)\n",
        "  model.compile(optimizer=optim, loss='binary_crossentropy', metrics='accuracy')\n",
        "  return model\n",
        "\n",
        "model4 = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "para_grid = {\n",
        "    'batch_size': [8, 16, 32, 64], 'epochs': [20],\n",
        "    'learning_rate': [0.001, 0.01, 0.1, 1],\n",
        "    'optimizer': [Adam, SGD, Nadam, Adadelta],\n",
        "    }\n",
        "\n",
        "grid = GridSearchCV(estimator=model4, param_grid=para_grid, n_jobs=1)\n",
        "grid_result = grid.fit(X_train_std, y_train)\n",
        "\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.8596938729286194 using {'batch_size': 8, 'epochs': 20, 'learning_rate': 0.001, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>}\n",
            "Means: 0.8596938729286194, Stdev: 0.03190306617064138 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 0.001, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>}\n",
            "Means: 0.6121598720550537, Stdev: 0.1516241113922098 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 0.001, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>}\n",
            "Means: 0.7693877577781677, Stdev: 0.12029311686570886 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 0.001, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>}\n",
            "Means: 0.4380102038383484, Stdev: 0.059561691098861974 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 0.001, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adadelta.Adadelta'>}\n",
            "Means: 0.7556122362613678, Stdev: 0.15213209216003606 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 0.01, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>}\n",
            "Means: 0.5849489808082581, Stdev: 0.18118240451302564 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 0.01, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>}\n",
            "Means: 0.8470237970352172, Stdev: 0.0444154391585211 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 0.01, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>}\n",
            "Means: 0.4585884392261505, Stdev: 0.04043507302270235 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 0.01, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adadelta.Adadelta'>}\n",
            "Means: 0.483588433265686, Stdev: 0.06420466605601195 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 0.1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>}\n",
            "Means: 0.5252551019191742, Stdev: 0.06126792790519694 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 0.1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>}\n",
            "Means: 0.479676878452301, Stdev: 0.06307575514183264 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 0.1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>}\n",
            "Means: 0.5169217705726623, Stdev: 0.12335454641037717 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 0.1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adadelta.Adadelta'>}\n",
            "Means: 0.4914115726947784, Stdev: 0.0657101222594898 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>}\n",
            "Means: 0.4914115726947784, Stdev: 0.0657101222594898 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>}\n",
            "Means: 0.45467687845230104, Stdev: 0.04834661114625799 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>}\n",
            "Means: 0.7554421722888947, Stdev: 0.1773823388886335 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adadelta.Adadelta'>}\n",
            "Means: 0.8391156435012818, Stdev: 0.05631704024105097 with: {'batch_size': 16, 'epochs': 20, 'learning_rate': 0.001, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>}\n",
            "Means: 0.6907312870025635, Stdev: 0.06839905905988104 with: {'batch_size': 16, 'epochs': 20, 'learning_rate': 0.001, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>}\n",
            "Means: 0.8142857193946839, Stdev: 0.04619891798123129 with: {'batch_size': 16, 'epochs': 20, 'learning_rate': 0.001, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>}\n",
            "Means: 0.45442177057266236, Stdev: 0.041310597208134 with: {'batch_size': 16, 'epochs': 20, 'learning_rate': 0.001, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adadelta.Adadelta'>}\n",
            "Means: 0.7515306293964386, Stdev: 0.14738595445307998 with: {'batch_size': 16, 'epochs': 20, 'learning_rate': 0.01, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>}\n",
            "Means: 0.826360559463501, Stdev: 0.025366088300362324 with: {'batch_size': 16, 'epochs': 20, 'learning_rate': 0.01, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>}\n",
            "Means: 0.7056122422218323, Stdev: 0.1531742331700091 with: {'batch_size': 16, 'epochs': 20, 'learning_rate': 0.01, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>}\n",
            "Means: 0.4629251718521118, Stdev: 0.039349153166867226 with: {'batch_size': 16, 'epochs': 20, 'learning_rate': 0.01, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adadelta.Adadelta'>}\n",
            "Means: 0.4880102097988129, Stdev: 0.06517533347575448 with: {'batch_size': 16, 'epochs': 20, 'learning_rate': 0.1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>}\n",
            "Means: 0.483588433265686, Stdev: 0.06420466605601195 with: {'batch_size': 16, 'epochs': 20, 'learning_rate': 0.1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>}\n",
            "Means: 0.4380102097988129, Stdev: 0.02342745334564438 with: {'batch_size': 16, 'epochs': 20, 'learning_rate': 0.1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>}\n",
            "Means: 0.5336734771728515, Stdev: 0.12795440884954534 with: {'batch_size': 16, 'epochs': 20, 'learning_rate': 0.1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adadelta.Adadelta'>}\n",
            "Means: 0.5252551019191742, Stdev: 0.06126792790519694 with: {'batch_size': 16, 'epochs': 20, 'learning_rate': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>}\n",
            "Means: 0.4963435411453247, Stdev: 0.06616803589863401 with: {'batch_size': 16, 'epochs': 20, 'learning_rate': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>}\n",
            "Means: 0.5130102097988128, Stdev: 0.06497933212749389 with: {'batch_size': 16, 'epochs': 20, 'learning_rate': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>}\n",
            "Means: 0.8096938848495483, Stdev: 0.028769415393181532 with: {'batch_size': 16, 'epochs': 20, 'learning_rate': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adadelta.Adadelta'>}\n",
            "Means: 0.8059523820877075, Stdev: 0.026566290658209257 with: {'batch_size': 32, 'epochs': 20, 'learning_rate': 0.001, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>}\n",
            "Means: 0.5244897961616516, Stdev: 0.07832994726277744 with: {'batch_size': 32, 'epochs': 20, 'learning_rate': 0.001, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>}\n",
            "Means: 0.6960884392261505, Stdev: 0.1928944476085656 with: {'batch_size': 32, 'epochs': 20, 'learning_rate': 0.001, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>}\n",
            "Means: 0.4502550959587097, Stdev: 0.05552661891904621 with: {'batch_size': 32, 'epochs': 20, 'learning_rate': 0.001, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adadelta.Adadelta'>}\n",
            "Means: 0.7516156554222106, Stdev: 0.1617715796499059 with: {'batch_size': 32, 'epochs': 20, 'learning_rate': 0.01, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>}\n",
            "Means: 0.6752551019191741, Stdev: 0.18180272248830673 with: {'batch_size': 32, 'epochs': 20, 'learning_rate': 0.01, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>}\n",
            "Means: 0.7180272161960601, Stdev: 0.16896570122644194 with: {'batch_size': 32, 'epochs': 20, 'learning_rate': 0.01, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>}\n",
            "Means: 0.45025510191917417, Stdev: 0.043783841475797815 with: {'batch_size': 32, 'epochs': 20, 'learning_rate': 0.01, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adadelta.Adadelta'>}\n",
            "Means: 0.5497449040412903, Stdev: 0.04378384153367893 with: {'batch_size': 32, 'epochs': 20, 'learning_rate': 0.1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>}\n",
            "Means: 0.5497449040412903, Stdev: 0.04378384153367893 with: {'batch_size': 32, 'epochs': 20, 'learning_rate': 0.1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>}\n",
            "Means: 0.5130102097988128, Stdev: 0.06497933212749389 with: {'batch_size': 32, 'epochs': 20, 'learning_rate': 0.1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>}\n",
            "Means: 0.47542516589164735, Stdev: 0.0916406308041532 with: {'batch_size': 32, 'epochs': 20, 'learning_rate': 0.1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adadelta.Adadelta'>}\n",
            "Means: 0.5247449040412903, Stdev: 0.06147576743328075 with: {'batch_size': 32, 'epochs': 20, 'learning_rate': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>}\n",
            "Means: 0.5247449040412903, Stdev: 0.06147576743328075 with: {'batch_size': 32, 'epochs': 20, 'learning_rate': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>}\n",
            "Means: 0.479676878452301, Stdev: 0.06307575514183264 with: {'batch_size': 32, 'epochs': 20, 'learning_rate': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>}\n",
            "Means: 0.8143707513809204, Stdev: 0.05063601871782114 with: {'batch_size': 32, 'epochs': 20, 'learning_rate': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adadelta.Adadelta'>}\n",
            "Means: 0.6937074780464172, Stdev: 0.14944678777466838 with: {'batch_size': 64, 'epochs': 20, 'learning_rate': 0.001, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>}\n",
            "Means: 0.6490646243095398, Stdev: 0.07082450181982432 with: {'batch_size': 64, 'epochs': 20, 'learning_rate': 0.001, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>}\n",
            "Means: 0.5863095283508301, Stdev: 0.13842325365070535 with: {'batch_size': 64, 'epochs': 20, 'learning_rate': 0.001, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>}\n",
            "Means: 0.4543367326259613, Stdev: 0.04049761795252836 with: {'batch_size': 64, 'epochs': 20, 'learning_rate': 0.001, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adadelta.Adadelta'>}\n",
            "Means: 0.786139440536499, Stdev: 0.13427594927127426 with: {'batch_size': 64, 'epochs': 20, 'learning_rate': 0.01, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>}\n",
            "Means: 0.756462574005127, Stdev: 0.06973899515642892 with: {'batch_size': 64, 'epochs': 20, 'learning_rate': 0.01, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>}\n",
            "Means: 0.7268707454204559, Stdev: 0.1358982353139129 with: {'batch_size': 64, 'epochs': 20, 'learning_rate': 0.01, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>}\n",
            "Means: 0.45841837525367735, Stdev: 0.05912305224888741 with: {'batch_size': 64, 'epochs': 20, 'learning_rate': 0.01, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adadelta.Adadelta'>}\n",
            "Means: 0.4963435411453247, Stdev: 0.06616803589863401 with: {'batch_size': 64, 'epochs': 20, 'learning_rate': 0.1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>}\n",
            "Means: 0.5130102097988128, Stdev: 0.06497933212749389 with: {'batch_size': 64, 'epochs': 20, 'learning_rate': 0.1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>}\n",
            "Means: 0.5619897961616516, Stdev: 0.023427450338446343 with: {'batch_size': 64, 'epochs': 20, 'learning_rate': 0.1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>}\n",
            "Means: 0.47950680255889894, Stdev: 0.0769181432226642 with: {'batch_size': 64, 'epochs': 20, 'learning_rate': 0.1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adadelta.Adadelta'>}\n",
            "Means: 0.4880102097988129, Stdev: 0.06517533347575448 with: {'batch_size': 64, 'epochs': 20, 'learning_rate': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>}\n",
            "Means: 0.5497449040412903, Stdev: 0.04378384153367893 with: {'batch_size': 64, 'epochs': 20, 'learning_rate': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>}\n",
            "Means: 0.5497449040412903, Stdev: 0.04378384153367893 with: {'batch_size': 64, 'epochs': 20, 'learning_rate': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>}\n",
            "Means: 0.6616496562957763, Stdev: 0.19029471222688898 with: {'batch_size': 64, 'epochs': 20, 'learning_rate': 1, 'optimizer': <class 'tensorflow.python.keras.optimizer_v2.adadelta.Adadelta'>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2kj3_aBXJyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This GridSearch alleges that Adam (the optimizer I'm currently using) is the best optimizer. \n",
        "# It returns an accuracy of 85.96% (basically 86%) at a learning rate of 0.001, and a batch size of 8.\n",
        "# I'm going to try it again with the recommended batch size."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqiXR9JEUzhs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "5175ebd2-fe05-436e-f1b4-3394e4929067"
      },
      "source": [
        "def create_model(learning_rate=0.01, optimizer=Adam()):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(32,activation='relu', input_dim=13))\n",
        "  model.add(Dense(64,activation='relu'))\n",
        "  model.add(Dense(16,activation='relu'))\n",
        "  model.add(Dense(4,activation='relu'))\n",
        "  model.add(Dense(1,activation='relu'))\n",
        "  optim = Adam(learning_rate=learning_rate)\n",
        "  model.compile(optimizer=optim, loss='binary_crossentropy', metrics='accuracy')\n",
        "  return model\n",
        "\n",
        "model4 = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "para_grid = {\n",
        "    'batch_size': [8], 'epochs': [20],\n",
        "    'learning_rate': [0.0001, 0.0005, 0.001, 0.005, 0.01],\n",
        "    }\n",
        "\n",
        "grid = GridSearchCV(estimator=model4, param_grid=para_grid, n_jobs=1)\n",
        "grid_result = grid.fit(X_train_std, y_train)\n",
        "\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.8307823181152344 using {'batch_size': 8, 'epochs': 20, 'learning_rate': 0.005}\n",
            "Means: 0.6034013748168945, Stdev: 0.12457741877020581 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 0.0001}\n",
            "Means: 0.7227040767669678, Stdev: 0.14473837916655716 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 0.0005}\n",
            "Means: 0.7326530635356903, Stdev: 0.1686911491028175 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 0.001}\n",
            "Means: 0.8307823181152344, Stdev: 0.029191576890177602 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 0.005}\n",
            "Means: 0.6933673441410064, Stdev: 0.1403023480313341 with: {'batch_size': 8, 'epochs': 20, 'learning_rate': 0.01}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XgSxksTYtQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This one actually did worse than the last one with 83.07% accuracy using the adam optimizer, batch size of 8, and learning rate of 0.005.\n",
        "# Now I'm going to do the final test accuracy."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4FBn1EuZB89",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a593944-6b64-4c03-babd-06586b001971"
      },
      "source": [
        "stop = EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=16)\n",
        "\n",
        "model4 = Sequential([\n",
        "      Dense(32,activation='relu', input_dim=13),\n",
        "      Dense(64,activation='relu'),\n",
        "      Dense(16,activation='relu'),\n",
        "      Dense(4,activation='relu'),\n",
        "      Dense(1,activation='sigmoid')\n",
        "])\n",
        "\n",
        "optim = Nadam(learning_rate=0.005)\n",
        "\n",
        "model4.compile(optimizer=optim, loss='binary_crossentropy', metrics='accuracy')\n",
        "\n",
        "result4 = model3.fit(\n",
        "    X_train,y_train, batch_size=8, \n",
        "    validation_data=(X_test,y_test),\n",
        "    epochs=100 ,callbacks=[stop]\n",
        "    )"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.6008 - accuracy: 0.7355 - val_loss: 0.8024 - val_accuracy: 0.6230\n",
            "Epoch 2/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.6040 - accuracy: 0.7025 - val_loss: 0.6928 - val_accuracy: 0.4918\n",
            "Epoch 3/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.6939 - accuracy: 0.4876 - val_loss: 0.7000 - val_accuracy: 0.4754\n",
            "Epoch 4/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.6448 - accuracy: 0.5785 - val_loss: 0.6740 - val_accuracy: 0.6066\n",
            "Epoch 5/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.5838 - accuracy: 0.7314 - val_loss: 0.7005 - val_accuracy: 0.6393\n",
            "Epoch 6/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.6036 - accuracy: 0.6901 - val_loss: 0.6956 - val_accuracy: 0.4918\n",
            "Epoch 7/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.6598 - accuracy: 0.5785 - val_loss: 0.7128 - val_accuracy: 0.6230\n",
            "Epoch 8/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.5898 - accuracy: 0.6570 - val_loss: 0.7680 - val_accuracy: 0.6393\n",
            "Epoch 9/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.5795 - accuracy: 0.7479 - val_loss: 0.6869 - val_accuracy: 0.6230\n",
            "Epoch 10/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.6077 - accuracy: 0.6653 - val_loss: 0.6658 - val_accuracy: 0.6230\n",
            "Epoch 11/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.5802 - accuracy: 0.7314 - val_loss: 0.6610 - val_accuracy: 0.6230\n",
            "Epoch 12/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.5927 - accuracy: 0.7273 - val_loss: 0.6760 - val_accuracy: 0.6230\n",
            "Epoch 13/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.5736 - accuracy: 0.7066 - val_loss: 0.6718 - val_accuracy: 0.5738\n",
            "Epoch 14/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.5936 - accuracy: 0.6901 - val_loss: 0.6939 - val_accuracy: 0.6230\n",
            "Epoch 15/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.5860 - accuracy: 0.7066 - val_loss: 0.6595 - val_accuracy: 0.5902\n",
            "Epoch 16/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.5916 - accuracy: 0.6942 - val_loss: 0.6586 - val_accuracy: 0.6066\n",
            "Epoch 17/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.5746 - accuracy: 0.7231 - val_loss: 0.7219 - val_accuracy: 0.6393\n",
            "Epoch 18/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.5790 - accuracy: 0.7397 - val_loss: 0.6569 - val_accuracy: 0.5902\n",
            "Epoch 19/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.6291 - accuracy: 0.6157 - val_loss: 0.6566 - val_accuracy: 0.6557\n",
            "Epoch 20/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.5746 - accuracy: 0.7231 - val_loss: 0.6778 - val_accuracy: 0.6230\n",
            "Epoch 21/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.5774 - accuracy: 0.7355 - val_loss: 0.6637 - val_accuracy: 0.6393\n",
            "Epoch 22/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.5593 - accuracy: 0.7314 - val_loss: 0.6653 - val_accuracy: 0.6230\n",
            "Epoch 23/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.5533 - accuracy: 0.7314 - val_loss: 0.6899 - val_accuracy: 0.6393\n",
            "Epoch 24/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.5747 - accuracy: 0.7314 - val_loss: 0.7057 - val_accuracy: 0.6885\n",
            "Epoch 25/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.5751 - accuracy: 0.7273 - val_loss: 0.6914 - val_accuracy: 0.6557\n",
            "Epoch 26/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.5504 - accuracy: 0.7314 - val_loss: 0.7035 - val_accuracy: 0.6393\n",
            "Epoch 27/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.5767 - accuracy: 0.7149 - val_loss: 0.6462 - val_accuracy: 0.6721\n",
            "Epoch 28/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.5504 - accuracy: 0.7273 - val_loss: 0.6607 - val_accuracy: 0.6230\n",
            "Epoch 29/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.5506 - accuracy: 0.7479 - val_loss: 0.6521 - val_accuracy: 0.6230\n",
            "Epoch 30/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.5601 - accuracy: 0.7314 - val_loss: 0.6451 - val_accuracy: 0.6393\n",
            "Epoch 31/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.5800 - accuracy: 0.7066 - val_loss: 0.6481 - val_accuracy: 0.6066\n",
            "Epoch 32/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.5554 - accuracy: 0.7273 - val_loss: 0.6593 - val_accuracy: 0.6393\n",
            "Epoch 33/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.5457 - accuracy: 0.7438 - val_loss: 0.6533 - val_accuracy: 0.6393\n",
            "Epoch 34/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.5575 - accuracy: 0.7066 - val_loss: 0.6473 - val_accuracy: 0.6721\n",
            "Epoch 35/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.5421 - accuracy: 0.7355 - val_loss: 0.6513 - val_accuracy: 0.6066\n",
            "Epoch 36/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.5790 - accuracy: 0.7314 - val_loss: 0.6382 - val_accuracy: 0.6230\n",
            "Epoch 37/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.5393 - accuracy: 0.7562 - val_loss: 0.6499 - val_accuracy: 0.5902\n",
            "Epoch 38/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.6233 - accuracy: 0.6612 - val_loss: 0.6808 - val_accuracy: 0.5574\n",
            "Epoch 39/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.5822 - accuracy: 0.6942 - val_loss: 0.6498 - val_accuracy: 0.5574\n",
            "Epoch 40/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.5351 - accuracy: 0.7645 - val_loss: 0.8446 - val_accuracy: 0.6557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgxP9HdaaODk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad7b0be0-4f2c-4b6e-c2f0-e903147d70f0"
      },
      "source": [
        "max(result4.history['accuracy'])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7644628286361694"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIt_kX2xZPb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finally got a 76.45& accuracy after 40 epochs this time using the Nadam optimizer, learning rate of 0.005, and the batch size of 8."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}