{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Johan_Mazorra_LS_DS13_422_Backprop_Assignment.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Unit 4 Sprint 2",
      "language": "python",
      "name": "unit4sprint2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nteract": {
      "version": "0.22.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsmazorra/DS-Unit-4-Sprint-2-Neural-Networks/blob/master/DS-Unit-4-Sprint-2-Neural-Networks/Johan_Mazorra_LS_DS13_422_Backprop_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NGGrt9EYlCqY"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "\n",
        "# Backpropagation Practice\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 2*\n",
        "\n",
        "Using TensorFlow Keras, Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
        "\n",
        "| x1 | x2 | x3 | y |\n",
        "|----|----|----|---|\n",
        "| 0  | 0  | 1  | 0 |\n",
        "| 0  | 1  | 1  | 1 |\n",
        "| 1  | 0  | 1  | 1 |\n",
        "| 0  | 1  | 0  | 1 |\n",
        "| 1  | 0  | 0  | 1 |\n",
        "| 1  | 1  | 1  | 0 |\n",
        "| 0  | 0  | 0  | 0 |\n",
        "\n",
        "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn.\n",
        "\n",
        "This is your \"Hello World!\" of TensorFlow.\n",
        "\n",
        "### Example TensorFlow Starter Code\n",
        "\n",
        "```python \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(3, activation='sigmoid', input_dim=2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "results = model.fit(X,y, epochs=100)\n",
        "\n",
        "```\n",
        "\n",
        "### Additional Written Tasks:\n",
        "1. Investigate the various [loss functions](https://www.tensorflow.org/api_docs/python/tf/keras/losses). Which is best suited for the task at hand (predicting 1 / 0) and why? \n",
        "2. What is the difference between a loss function and a metric? Why might we need both in Keras? \n",
        "3. Investigate the various [optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers). Stochastic Gradient Descent (`sgd`) is not the learning algorithm dejour anyone. Why is that? What do newer optimizers such as `adam` have to offer? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nEREYT-3wI1f",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# XOR Gate as numpy array.\n",
        "\n",
        "X = np.array([[0, 0, 1],\n",
        "                [0, 1, 1],\n",
        "                [1, 0, 1],\n",
        "                [0, 1, 0],\n",
        "                [1, 0, 0],\n",
        "                [1, 1, 1],\n",
        "                [0, 0, 0]])\n",
        "y = np.array([[0], [1], [1], [1], [1], [0], [0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKcyesciKaah",
        "colab_type": "code",
        "outputId": "ee013bc7-6cf1-4901-b172-9422c799c5c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        " \n",
        "model = Sequential([\n",
        "    Dense(3, activation='relu', input_dim=3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        " \n",
        "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['acc'])\n",
        " \n",
        "results = model.fit(X,y, epochs=200)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6981 - acc: 0.7143\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6975 - acc: 0.7143\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6970 - acc: 0.7143\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6965 - acc: 0.7143\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6960 - acc: 0.7143\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6955 - acc: 0.7143\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6950 - acc: 0.7143\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6945 - acc: 0.7143\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6940 - acc: 0.7143\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6935 - acc: 0.7143\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.7143\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6925 - acc: 0.7143\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6921 - acc: 0.7143\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6916 - acc: 0.7143\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6911 - acc: 0.7143\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6907 - acc: 0.7143\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6902 - acc: 0.7143\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6897 - acc: 0.7143\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6893 - acc: 0.7143\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6888 - acc: 0.7143\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6884 - acc: 0.7143\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6880 - acc: 0.7143\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6875 - acc: 0.7143\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6871 - acc: 0.7143\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6867 - acc: 0.7143\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6863 - acc: 0.7143\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6858 - acc: 0.7143\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6854 - acc: 0.7143\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6850 - acc: 0.7143\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6846 - acc: 0.7143\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6842 - acc: 0.7143\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6838 - acc: 0.7143\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6834 - acc: 0.7143\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6830 - acc: 0.7143\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6826 - acc: 0.7143\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6822 - acc: 0.7143\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6819 - acc: 0.7143\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6815 - acc: 0.7143\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6811 - acc: 0.7143\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6807 - acc: 0.7143\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6804 - acc: 0.7143\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6800 - acc: 0.7143\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6796 - acc: 0.7143\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6793 - acc: 0.7143\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6789 - acc: 0.7143\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6785 - acc: 0.7143\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6782 - acc: 0.7143\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6778 - acc: 0.7143\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6775 - acc: 0.7143\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6771 - acc: 0.7143\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6768 - acc: 0.7143\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6765 - acc: 0.7143\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6761 - acc: 0.7143\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6758 - acc: 0.7143\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6755 - acc: 0.7143\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6751 - acc: 0.7143\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6748 - acc: 0.7143\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6745 - acc: 0.7143\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6742 - acc: 0.7143\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6740 - acc: 0.7143\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6738 - acc: 0.7143\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6736 - acc: 0.7143\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6735 - acc: 0.7143\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6733 - acc: 0.7143\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6731 - acc: 0.7143\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6729 - acc: 0.7143\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6727 - acc: 0.7143\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6725 - acc: 0.7143\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6723 - acc: 0.7143\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6721 - acc: 0.7143\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6720 - acc: 0.7143\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6718 - acc: 0.7143\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6716 - acc: 0.7143\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6714 - acc: 0.7143\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6712 - acc: 0.7143\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6711 - acc: 0.7143\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6709 - acc: 0.7143\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6707 - acc: 0.7143\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6705 - acc: 0.7143\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6704 - acc: 0.7143\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6702 - acc: 0.7143\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6700 - acc: 0.7143\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6699 - acc: 0.7143\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6697 - acc: 0.7143\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6695 - acc: 0.7143\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6694 - acc: 0.7143\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6692 - acc: 0.7143\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6690 - acc: 0.7143\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6689 - acc: 0.7143\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6687 - acc: 0.7143\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6686 - acc: 0.7143\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6684 - acc: 0.7143\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6682 - acc: 0.7143\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6681 - acc: 0.7143\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6679 - acc: 0.7143\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6678 - acc: 0.7143\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6676 - acc: 0.7143\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6675 - acc: 0.7143\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6673 - acc: 0.7143\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6672 - acc: 0.7143\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6670 - acc: 0.7143\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6669 - acc: 0.7143\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6667 - acc: 0.7143\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6666 - acc: 0.7143\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6664 - acc: 0.7143\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6663 - acc: 0.7143\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6661 - acc: 0.7143\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6660 - acc: 0.7143\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6658 - acc: 0.7143\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6657 - acc: 0.7143\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6656 - acc: 0.7143\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6654 - acc: 0.7143\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6653 - acc: 0.7143\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6651 - acc: 0.7143\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6650 - acc: 0.7143\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6649 - acc: 0.7143\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6647 - acc: 0.7143\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6646 - acc: 0.7143\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6644 - acc: 0.7143\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6643 - acc: 0.7143\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6642 - acc: 0.7143\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6640 - acc: 0.7143\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6639 - acc: 0.7143\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6638 - acc: 0.7143\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6636 - acc: 0.7143\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6635 - acc: 0.7143\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6634 - acc: 0.7143\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6633 - acc: 0.7143\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6631 - acc: 0.7143\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6630 - acc: 0.7143\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6629 - acc: 0.7143\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6627 - acc: 0.7143\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6626 - acc: 0.7143\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6625 - acc: 0.7143\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6624 - acc: 0.7143\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6622 - acc: 0.7143\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6621 - acc: 0.7143\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6620 - acc: 0.7143\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6619 - acc: 0.7143\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6618 - acc: 0.7143\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6616 - acc: 0.7143\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6615 - acc: 0.7143\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6614 - acc: 0.7143\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6613 - acc: 0.7143\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6612 - acc: 0.7143\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6610 - acc: 0.7143\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6609 - acc: 0.7143\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6608 - acc: 0.7143\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6607 - acc: 0.7143\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6606 - acc: 0.7143\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6605 - acc: 0.7143\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6604 - acc: 0.7143\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6602 - acc: 0.7143\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6601 - acc: 0.7143\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6600 - acc: 0.7143\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6599 - acc: 0.7143\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6598 - acc: 0.7143\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6597 - acc: 0.7143\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6596 - acc: 0.7143\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6595 - acc: 0.7143\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6594 - acc: 0.7143\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6592 - acc: 0.7143\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6591 - acc: 0.7143\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6590 - acc: 0.7143\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6589 - acc: 0.7143\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6588 - acc: 0.7143\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6587 - acc: 0.7143\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6586 - acc: 0.7143\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6585 - acc: 0.7143\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6584 - acc: 0.7143\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6583 - acc: 0.7143\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6582 - acc: 0.7143\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6581 - acc: 0.7143\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6580 - acc: 0.7143\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6579 - acc: 0.7143\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6578 - acc: 0.7143\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6577 - acc: 0.7143\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6575 - acc: 0.7143\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6574 - acc: 0.7143\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6573 - acc: 0.7143\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6573 - acc: 0.7143\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6571 - acc: 0.7143\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6571 - acc: 0.7143\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6569 - acc: 0.7143\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6568 - acc: 0.7143\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6567 - acc: 0.7143\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6566 - acc: 0.7143\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6565 - acc: 0.7143\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6564 - acc: 0.7143\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6564 - acc: 0.7143\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6563 - acc: 0.7143\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6562 - acc: 0.7143\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6561 - acc: 0.7143\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6560 - acc: 0.7143\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6559 - acc: 0.7143\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6558 - acc: 0.7143\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6557 - acc: 0.7143\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6556 - acc: 0.7143\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6555 - acc: 0.7143\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6554 - acc: 0.7143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4V4qb7nKaap",
        "colab_type": "code",
        "outputId": "a62609cf-db63-41b0-b133-3b2680a59f65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.predict(np.array([[0,1,0]]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.6427657]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sihIYDHU28K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "12512b32-4d7f-4b28-892a-6a99c6f8545d"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 4         \n",
            "=================================================================\n",
            "Total params: 16\n",
            "Trainable params: 16\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alq15qOGKaaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork: \n",
        "    \n",
        "    def __init__(self, inputs, hiddenNodes, outputNodes):\n",
        "        self.inputs = inputs\n",
        "        self.hiddenNodes = hiddenNodes\n",
        "        self.outputNodes = outputNodes\n",
        "        \n",
        "        # Initialize Weights\n",
        "        \n",
        "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)\n",
        "        \n",
        "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
        "        \n",
        "    def sigmoid(self, s):\n",
        "        return 1 / (1+np.exp(-s))\n",
        "    \n",
        "    def sigmoidPrime(self, s):\n",
        "        sx = self.sigmoid(s)\n",
        "        return sx * (1-sx)\n",
        "    \n",
        "    def feed_forward(self, X):\n",
        "        \"\"\"\n",
        "        Calculate the NN inference using feed forward.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Weight Sum\n",
        "        self.hidden_sum = np.dot(X, self.weights1)\n",
        "        \n",
        "        # Activation\n",
        "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "        \n",
        "        # Weighted Sum 2\n",
        "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
        "        \n",
        "        # Final Output\n",
        "        self.activated_output = self.sigmoid(self.output_sum)\n",
        "        \n",
        "        return self.activated_output\n",
        "    \n",
        "    def backward(self, X,y,o):\n",
        "        \"\"\"\n",
        "        Back prop thru the network\n",
        "        \"\"\"\n",
        "        \n",
        "        self.o_error = y - o\n",
        "        \n",
        "        # Apply derivative of sigmoid to error\n",
        "        self.o_delta = self.o_error * self.sigmoidPrime(self.output_sum)\n",
        "        \n",
        "        # z2 error: how much were our output layer weights off\n",
        "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
        "        \n",
        "        # z2 delta: how much were the weights off?\n",
        "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)\n",
        "\n",
        "        self.weights1 += X.T.dot(self.z2_delta) #Adjust first set (input => hidden) weights\n",
        "        self.weights2 += self.activated_hidden.T.dot(self.o_delta) #adjust second set (hidden => output) weights\n",
        "        \n",
        "    def train(self, X, y):\n",
        "        o = self.feed_forward(X)\n",
        "        self.backward(X,y,o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQXOL16qKaa1",
        "colab_type": "code",
        "outputId": "87cef169-5561-4a24-d604-db77dac1972a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nn = NeuralNetwork(3, 5, 1)\n",
        "\n",
        "for i in range(10000):\n",
        "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 == 0):\n",
        "        print('+' + '-----' * 2 + f'EPOCH {i+1}' + '-----' * 2 + '+')\n",
        "        print('Input: \\n', X)\n",
        "        print('Acutal Output: \\n', y)\n",
        "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
        "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
        "        \n",
        "    nn.train(X,y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------EPOCH 1----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Acutal Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.91626592]\n",
            " [0.89837543]\n",
            " [0.92222469]\n",
            " [0.76990463]\n",
            " [0.83767764]\n",
            " [0.9167813 ]\n",
            " [0.80785486]]\n",
            "Loss: \n",
            " 0.34690423366117484\n",
            "+----------EPOCH 2----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Acutal Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.87621109]\n",
            " [0.86018151]\n",
            " [0.88939841]\n",
            " [0.7311155 ]\n",
            " [0.80016077]\n",
            " [0.88740198]\n",
            " [0.75954653]]\n",
            "Loss: \n",
            " 0.32516508695817736\n",
            "+----------EPOCH 3----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Acutal Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.81128861]\n",
            " [0.80304508]\n",
            " [0.83773967]\n",
            " [0.68219454]\n",
            " [0.75003952]\n",
            " [0.84383446]\n",
            " [0.69457742]]\n",
            "Loss: \n",
            " 0.2973262568934478\n",
            "+----------EPOCH 4----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Acutal Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.71666411]\n",
            " [0.72565305]\n",
            " [0.76343004]\n",
            " [0.62592452]\n",
            " [0.68883929]\n",
            " [0.78475782]\n",
            " [0.61564014]]\n",
            "Loss: \n",
            " 0.268064301572772\n",
            "+----------EPOCH 5----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Acutal Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.61254618]\n",
            " [0.64464772]\n",
            " [0.68069535]\n",
            " [0.5742972 ]\n",
            " [0.62935307]\n",
            " [0.72215948]\n",
            " [0.54011897]]\n",
            "Loss: \n",
            " 0.2478983373343023\n",
            "+----------EPOCH 1000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Acutal Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.05795297]\n",
            " [0.92242006]\n",
            " [0.92011616]\n",
            " [0.94657615]\n",
            " [0.949654  ]\n",
            " [0.99150155]\n",
            " [0.07831589]]\n",
            "Loss: \n",
            " 0.14433659405799135\n",
            "+----------EPOCH 2000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Acutal Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.04544409]\n",
            " [0.94335619]\n",
            " [0.94151624]\n",
            " [0.96374211]\n",
            " [0.96652596]\n",
            " [0.99563411]\n",
            " [0.05375937]]\n",
            "Loss: \n",
            " 0.1436152188261655\n",
            "+----------EPOCH 3000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Acutal Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.03913518]\n",
            " [0.95264011]\n",
            " [0.95098401]\n",
            " [0.9708713 ]\n",
            " [0.97354705]\n",
            " [0.99700133]\n",
            " [0.04359597]]\n",
            "Loss: \n",
            " 0.14337679967551514\n",
            "+----------EPOCH 4000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Acutal Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.03500779]\n",
            " [0.9581918 ]\n",
            " [0.95662624]\n",
            " [0.97477594]\n",
            " [0.97740408]\n",
            " [0.99767909]\n",
            " [0.03803426]]\n",
            "Loss: \n",
            " 0.14325882260951986\n",
            "+----------EPOCH 5000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Acutal Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.03199826]\n",
            " [0.96196706]\n",
            " [0.96044119]\n",
            " [0.97717043]\n",
            " [0.97978619]\n",
            " [0.99807915]\n",
            " [0.03460436]]\n",
            "Loss: \n",
            " 0.14318921846958552\n",
            "+----------EPOCH 6000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Acutal Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.02968106]\n",
            " [0.96470086]\n",
            " [0.96318016]\n",
            " [0.97870038]\n",
            " [0.98133006]\n",
            " [0.99833694]\n",
            " [0.03237866]]\n",
            "Loss: \n",
            " 0.14314428023891462\n",
            "+----------EPOCH 7000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Acutal Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.02785197]\n",
            " [0.96672576]\n",
            " [0.96518248]\n",
            " [0.97965603]\n",
            " [0.98232329]\n",
            " [0.99850882]\n",
            " [0.03094079]]\n",
            "Loss: \n",
            " 0.14311409991339016\n",
            "+----------EPOCH 8000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Acutal Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.02641124]\n",
            " [0.96819306]\n",
            " [0.96660062]\n",
            " [0.98016783]\n",
            " [0.98289837]\n",
            " [0.99862015]\n",
            " [0.03010017]]\n",
            "Loss: \n",
            " 0.14309410709249287\n",
            "+----------EPOCH 9000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Acutal Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.0253315 ]\n",
            " [0.96913262]\n",
            " [0.96745968]\n",
            " [0.98026064]\n",
            " [0.98308892]\n",
            " [0.99867922]\n",
            " [0.02980656]]\n",
            "Loss: \n",
            " 0.14308251352503887\n",
            "+----------EPOCH 10000----------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Acutal Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.0246805 ]\n",
            " [0.96941315]\n",
            " [0.96761193]\n",
            " [0.97982271]\n",
            " [0.98280751]\n",
            " [0.99867601]\n",
            " [0.03017076]]\n",
            "Loss: \n",
            " 0.1430800595794612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny-uN6KnKaa5",
        "colab_type": "code",
        "outputId": "aaac9f71-6964-475e-8231-1d4eec7722b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We're not getting close enough to 0, let's try with Tensorflow now.\n",
        "\n",
        "model = Sequential(\n",
        "    [\n",
        "        Dense(2, activation=\"relu\"),\n",
        "        Dense(3, activation=\"relu\"),\n",
        "        Dense(4),\n",
        "    ]\n",
        ")\n",
        " \n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['acc'])\n",
        " \n",
        "results = model.fit(X,y, epochs=300)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6274 - acc: 0.4286\n",
            "Epoch 2/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6235 - acc: 0.2857\n",
            "Epoch 3/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6197 - acc: 0.2857\n",
            "Epoch 4/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6159 - acc: 0.2857\n",
            "Epoch 5/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6122 - acc: 0.2857\n",
            "Epoch 6/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6085 - acc: 0.2857\n",
            "Epoch 7/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6049 - acc: 0.4286\n",
            "Epoch 8/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6014 - acc: 0.4286\n",
            "Epoch 9/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5979 - acc: 0.4286\n",
            "Epoch 10/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5946 - acc: 0.4286\n",
            "Epoch 11/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5913 - acc: 0.4286\n",
            "Epoch 12/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5881 - acc: 0.4286\n",
            "Epoch 13/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5849 - acc: 0.4286\n",
            "Epoch 14/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5818 - acc: 0.4286\n",
            "Epoch 15/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5788 - acc: 0.4286\n",
            "Epoch 16/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5757 - acc: 0.4286\n",
            "Epoch 17/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5728 - acc: 0.4286\n",
            "Epoch 18/300\n",
            "1/1 [==============================] - 0s 946us/step - loss: 0.5699 - acc: 0.4286\n",
            "Epoch 19/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5670 - acc: 0.4286\n",
            "Epoch 20/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5642 - acc: 0.4286\n",
            "Epoch 21/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5615 - acc: 0.4286\n",
            "Epoch 22/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5588 - acc: 0.4286\n",
            "Epoch 23/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5561 - acc: 0.4286\n",
            "Epoch 24/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5535 - acc: 0.4286\n",
            "Epoch 25/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5509 - acc: 0.4286\n",
            "Epoch 26/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5484 - acc: 0.4286\n",
            "Epoch 27/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5459 - acc: 0.4286\n",
            "Epoch 28/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5434 - acc: 0.4286\n",
            "Epoch 29/300\n",
            "1/1 [==============================] - 0s 889us/step - loss: 0.5410 - acc: 0.4286\n",
            "Epoch 30/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5387 - acc: 0.4286\n",
            "Epoch 31/300\n",
            "1/1 [==============================] - 0s 907us/step - loss: 0.5363 - acc: 0.4286\n",
            "Epoch 32/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5341 - acc: 0.4286\n",
            "Epoch 33/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5318 - acc: 0.4286\n",
            "Epoch 34/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5296 - acc: 0.4286\n",
            "Epoch 35/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5275 - acc: 0.4286\n",
            "Epoch 36/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5253 - acc: 0.4286\n",
            "Epoch 37/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5232 - acc: 0.4286\n",
            "Epoch 38/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5212 - acc: 0.4286\n",
            "Epoch 39/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5191 - acc: 0.4286\n",
            "Epoch 40/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5172 - acc: 0.4286\n",
            "Epoch 41/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5152 - acc: 0.4286\n",
            "Epoch 42/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5133 - acc: 0.4286\n",
            "Epoch 43/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5114 - acc: 0.4286\n",
            "Epoch 44/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5095 - acc: 0.4286\n",
            "Epoch 45/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5076 - acc: 0.4286\n",
            "Epoch 46/300\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5058 - acc: 0.4286\n",
            "Epoch 47/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5040 - acc: 0.4286\n",
            "Epoch 48/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5022 - acc: 0.4286\n",
            "Epoch 49/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5005 - acc: 0.4286\n",
            "Epoch 50/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4987 - acc: 0.4286\n",
            "Epoch 51/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4970 - acc: 0.4286\n",
            "Epoch 52/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4953 - acc: 0.4286\n",
            "Epoch 53/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4937 - acc: 0.4286\n",
            "Epoch 54/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4921 - acc: 0.4286\n",
            "Epoch 55/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4904 - acc: 0.4286\n",
            "Epoch 56/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4889 - acc: 0.4286\n",
            "Epoch 57/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4873 - acc: 0.4286\n",
            "Epoch 58/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4857 - acc: 0.4286\n",
            "Epoch 59/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4842 - acc: 0.4286\n",
            "Epoch 60/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4826 - acc: 0.4286\n",
            "Epoch 61/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4810 - acc: 0.4286\n",
            "Epoch 62/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4794 - acc: 0.4286\n",
            "Epoch 63/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4778 - acc: 0.4286\n",
            "Epoch 64/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4762 - acc: 0.4286\n",
            "Epoch 65/300\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.4746 - acc: 0.4286\n",
            "Epoch 66/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4731 - acc: 0.4286\n",
            "Epoch 67/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4715 - acc: 0.4286\n",
            "Epoch 68/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4700 - acc: 0.4286\n",
            "Epoch 69/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4685 - acc: 0.4286\n",
            "Epoch 70/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4670 - acc: 0.4286\n",
            "Epoch 71/300\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.4655 - acc: 0.4286\n",
            "Epoch 72/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4640 - acc: 0.4286\n",
            "Epoch 73/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4625 - acc: 0.4286\n",
            "Epoch 74/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4610 - acc: 0.4286\n",
            "Epoch 75/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4596 - acc: 0.4286\n",
            "Epoch 76/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4582 - acc: 0.4286\n",
            "Epoch 77/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4567 - acc: 0.4286\n",
            "Epoch 78/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4553 - acc: 0.4286\n",
            "Epoch 79/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4539 - acc: 0.4286\n",
            "Epoch 80/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4525 - acc: 0.4286\n",
            "Epoch 81/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4512 - acc: 0.4286\n",
            "Epoch 82/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4498 - acc: 0.4286\n",
            "Epoch 83/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4484 - acc: 0.4286\n",
            "Epoch 84/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4471 - acc: 0.4286\n",
            "Epoch 85/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4458 - acc: 0.4286\n",
            "Epoch 86/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4444 - acc: 0.4286\n",
            "Epoch 87/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4431 - acc: 0.4286\n",
            "Epoch 88/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4418 - acc: 0.4286\n",
            "Epoch 89/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4405 - acc: 0.4286\n",
            "Epoch 90/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4392 - acc: 0.4286\n",
            "Epoch 91/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4380 - acc: 0.4286\n",
            "Epoch 92/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4367 - acc: 0.4286\n",
            "Epoch 93/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4355 - acc: 0.4286\n",
            "Epoch 94/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4342 - acc: 0.4286\n",
            "Epoch 95/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4330 - acc: 0.4286\n",
            "Epoch 96/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4318 - acc: 0.4286\n",
            "Epoch 97/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4306 - acc: 0.4286\n",
            "Epoch 98/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4294 - acc: 0.4286\n",
            "Epoch 99/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4282 - acc: 0.4286\n",
            "Epoch 100/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4270 - acc: 0.4286\n",
            "Epoch 101/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4258 - acc: 0.4286\n",
            "Epoch 102/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4246 - acc: 0.4286\n",
            "Epoch 103/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4235 - acc: 0.4286\n",
            "Epoch 104/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4223 - acc: 0.4286\n",
            "Epoch 105/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4212 - acc: 0.4286\n",
            "Epoch 106/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4201 - acc: 0.4286\n",
            "Epoch 107/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4189 - acc: 0.4286\n",
            "Epoch 108/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4178 - acc: 0.4286\n",
            "Epoch 109/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4167 - acc: 0.4286\n",
            "Epoch 110/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4156 - acc: 0.4286\n",
            "Epoch 111/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4145 - acc: 0.4286\n",
            "Epoch 112/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4134 - acc: 0.4286\n",
            "Epoch 113/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4123 - acc: 0.4286\n",
            "Epoch 114/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4112 - acc: 0.4286\n",
            "Epoch 115/300\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.4102 - acc: 0.4286\n",
            "Epoch 116/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4091 - acc: 0.4286\n",
            "Epoch 117/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4081 - acc: 0.4286\n",
            "Epoch 118/300\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.4070 - acc: 0.4286\n",
            "Epoch 119/300\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.4060 - acc: 0.4286\n",
            "Epoch 120/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4049 - acc: 0.4286\n",
            "Epoch 121/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4039 - acc: 0.4286\n",
            "Epoch 122/300\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.4029 - acc: 0.4286\n",
            "Epoch 123/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4019 - acc: 0.4286\n",
            "Epoch 124/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4009 - acc: 0.4286\n",
            "Epoch 125/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3999 - acc: 0.4286\n",
            "Epoch 126/300\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3989 - acc: 0.4286\n",
            "Epoch 127/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3979 - acc: 0.4286\n",
            "Epoch 128/300\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3969 - acc: 0.4286\n",
            "Epoch 129/300\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3959 - acc: 0.4286\n",
            "Epoch 130/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3949 - acc: 0.4286\n",
            "Epoch 131/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3940 - acc: 0.4286\n",
            "Epoch 132/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3930 - acc: 0.4286\n",
            "Epoch 133/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3920 - acc: 0.4286\n",
            "Epoch 134/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3911 - acc: 0.4286\n",
            "Epoch 135/300\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3901 - acc: 0.4286\n",
            "Epoch 136/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3892 - acc: 0.4286\n",
            "Epoch 137/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3883 - acc: 0.4286\n",
            "Epoch 138/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3873 - acc: 0.4286\n",
            "Epoch 139/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3864 - acc: 0.4286\n",
            "Epoch 140/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3855 - acc: 0.4286\n",
            "Epoch 141/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3846 - acc: 0.4286\n",
            "Epoch 142/300\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3836 - acc: 0.4286\n",
            "Epoch 143/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3827 - acc: 0.4286\n",
            "Epoch 144/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3818 - acc: 0.4286\n",
            "Epoch 145/300\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3809 - acc: 0.4286\n",
            "Epoch 146/300\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.3801 - acc: 0.4286\n",
            "Epoch 147/300\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.3792 - acc: 0.4286\n",
            "Epoch 148/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3783 - acc: 0.4286\n",
            "Epoch 149/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3774 - acc: 0.4286\n",
            "Epoch 150/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3765 - acc: 0.4286\n",
            "Epoch 151/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3757 - acc: 0.4286\n",
            "Epoch 152/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3748 - acc: 0.4286\n",
            "Epoch 153/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3739 - acc: 0.4286\n",
            "Epoch 154/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3731 - acc: 0.4286\n",
            "Epoch 155/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3722 - acc: 0.4286\n",
            "Epoch 156/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3714 - acc: 0.4286\n",
            "Epoch 157/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3705 - acc: 0.4286\n",
            "Epoch 158/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3697 - acc: 0.4286\n",
            "Epoch 159/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3689 - acc: 0.4286\n",
            "Epoch 160/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3680 - acc: 0.4286\n",
            "Epoch 161/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3672 - acc: 0.4286\n",
            "Epoch 162/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3664 - acc: 0.4286\n",
            "Epoch 163/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3656 - acc: 0.4286\n",
            "Epoch 164/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3647 - acc: 0.4286\n",
            "Epoch 165/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3639 - acc: 0.4286\n",
            "Epoch 166/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3631 - acc: 0.4286\n",
            "Epoch 167/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3623 - acc: 0.4286\n",
            "Epoch 168/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3615 - acc: 0.4286\n",
            "Epoch 169/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3607 - acc: 0.4286\n",
            "Epoch 170/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3599 - acc: 0.4286\n",
            "Epoch 171/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3592 - acc: 0.4286\n",
            "Epoch 172/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3584 - acc: 0.4286\n",
            "Epoch 173/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3576 - acc: 0.4286\n",
            "Epoch 174/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3568 - acc: 0.4286\n",
            "Epoch 175/300\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3560 - acc: 0.4286\n",
            "Epoch 176/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3553 - acc: 0.4286\n",
            "Epoch 177/300\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3545 - acc: 0.4286\n",
            "Epoch 178/300\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3537 - acc: 0.4286\n",
            "Epoch 179/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3530 - acc: 0.4286\n",
            "Epoch 180/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3522 - acc: 0.4286\n",
            "Epoch 181/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3515 - acc: 0.4286\n",
            "Epoch 182/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3507 - acc: 0.4286\n",
            "Epoch 183/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3500 - acc: 0.4286\n",
            "Epoch 184/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3493 - acc: 0.4286\n",
            "Epoch 185/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3485 - acc: 0.4286\n",
            "Epoch 186/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3478 - acc: 0.4286\n",
            "Epoch 187/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3471 - acc: 0.4286\n",
            "Epoch 188/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3463 - acc: 0.4286\n",
            "Epoch 189/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3456 - acc: 0.4286\n",
            "Epoch 190/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3449 - acc: 0.4286\n",
            "Epoch 191/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3442 - acc: 0.4286\n",
            "Epoch 192/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3435 - acc: 0.4286\n",
            "Epoch 193/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3428 - acc: 0.4286\n",
            "Epoch 194/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3420 - acc: 0.4286\n",
            "Epoch 195/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3413 - acc: 0.4286\n",
            "Epoch 196/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3406 - acc: 0.4286\n",
            "Epoch 197/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3399 - acc: 0.4286\n",
            "Epoch 198/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3393 - acc: 0.4286\n",
            "Epoch 199/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3386 - acc: 0.4286\n",
            "Epoch 200/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3379 - acc: 0.4286\n",
            "Epoch 201/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3372 - acc: 0.4286\n",
            "Epoch 202/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3365 - acc: 0.4286\n",
            "Epoch 203/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3358 - acc: 0.4286\n",
            "Epoch 204/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3352 - acc: 0.4286\n",
            "Epoch 205/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3345 - acc: 0.4286\n",
            "Epoch 206/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3338 - acc: 0.4286\n",
            "Epoch 207/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3332 - acc: 0.4286\n",
            "Epoch 208/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3325 - acc: 0.4286\n",
            "Epoch 209/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3318 - acc: 0.4286\n",
            "Epoch 210/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3312 - acc: 0.4286\n",
            "Epoch 211/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3305 - acc: 0.4286\n",
            "Epoch 212/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3299 - acc: 0.4286\n",
            "Epoch 213/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3292 - acc: 0.4286\n",
            "Epoch 214/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3286 - acc: 0.4286\n",
            "Epoch 215/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3279 - acc: 0.4286\n",
            "Epoch 216/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3273 - acc: 0.4286\n",
            "Epoch 217/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3267 - acc: 0.4286\n",
            "Epoch 218/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3260 - acc: 0.4286\n",
            "Epoch 219/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3254 - acc: 0.4286\n",
            "Epoch 220/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3248 - acc: 0.4286\n",
            "Epoch 221/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3241 - acc: 0.4286\n",
            "Epoch 222/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3235 - acc: 0.4286\n",
            "Epoch 223/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3229 - acc: 0.4286\n",
            "Epoch 224/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3223 - acc: 0.4286\n",
            "Epoch 225/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3217 - acc: 0.4286\n",
            "Epoch 226/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3211 - acc: 0.4286\n",
            "Epoch 227/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3204 - acc: 0.4286\n",
            "Epoch 228/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3198 - acc: 0.4286\n",
            "Epoch 229/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3192 - acc: 0.4286\n",
            "Epoch 230/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3186 - acc: 0.4286\n",
            "Epoch 231/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3180 - acc: 0.4286\n",
            "Epoch 232/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3174 - acc: 0.4286\n",
            "Epoch 233/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3168 - acc: 0.4286\n",
            "Epoch 234/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3162 - acc: 0.4286\n",
            "Epoch 235/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3157 - acc: 0.4286\n",
            "Epoch 236/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3151 - acc: 0.4286\n",
            "Epoch 237/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3145 - acc: 0.4286\n",
            "Epoch 238/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3139 - acc: 0.4286\n",
            "Epoch 239/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3133 - acc: 0.4286\n",
            "Epoch 240/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3128 - acc: 0.4286\n",
            "Epoch 241/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3122 - acc: 0.4286\n",
            "Epoch 242/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3116 - acc: 0.4286\n",
            "Epoch 243/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3110 - acc: 0.4286\n",
            "Epoch 244/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3105 - acc: 0.4286\n",
            "Epoch 245/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3099 - acc: 0.4286\n",
            "Epoch 246/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3093 - acc: 0.4286\n",
            "Epoch 247/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3088 - acc: 0.4286\n",
            "Epoch 248/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3082 - acc: 0.4286\n",
            "Epoch 249/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3077 - acc: 0.4286\n",
            "Epoch 250/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3071 - acc: 0.4286\n",
            "Epoch 251/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3066 - acc: 0.4286\n",
            "Epoch 252/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3060 - acc: 0.4286\n",
            "Epoch 253/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3055 - acc: 0.4286\n",
            "Epoch 254/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3049 - acc: 0.4286\n",
            "Epoch 255/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3044 - acc: 0.4286\n",
            "Epoch 256/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3039 - acc: 0.4286\n",
            "Epoch 257/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3033 - acc: 0.4286\n",
            "Epoch 258/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3028 - acc: 0.4286\n",
            "Epoch 259/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3022 - acc: 0.4286\n",
            "Epoch 260/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3017 - acc: 0.4286\n",
            "Epoch 261/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3012 - acc: 0.4286\n",
            "Epoch 262/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3007 - acc: 0.4286\n",
            "Epoch 263/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3001 - acc: 0.4286\n",
            "Epoch 264/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2996 - acc: 0.4286\n",
            "Epoch 265/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2991 - acc: 0.4286\n",
            "Epoch 266/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2986 - acc: 0.4286\n",
            "Epoch 267/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2981 - acc: 0.4286\n",
            "Epoch 268/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2976 - acc: 0.4286\n",
            "Epoch 269/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2970 - acc: 0.4286\n",
            "Epoch 270/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2965 - acc: 0.4286\n",
            "Epoch 271/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2960 - acc: 0.4286\n",
            "Epoch 272/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2955 - acc: 0.4286\n",
            "Epoch 273/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2950 - acc: 0.4286\n",
            "Epoch 274/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2945 - acc: 0.4286\n",
            "Epoch 275/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2940 - acc: 0.4286\n",
            "Epoch 276/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2935 - acc: 0.4286\n",
            "Epoch 277/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2930 - acc: 0.4286\n",
            "Epoch 278/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2925 - acc: 0.4286\n",
            "Epoch 279/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2920 - acc: 0.4286\n",
            "Epoch 280/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2915 - acc: 0.4286\n",
            "Epoch 281/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2911 - acc: 0.4286\n",
            "Epoch 282/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2906 - acc: 0.4286\n",
            "Epoch 283/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2901 - acc: 0.4286\n",
            "Epoch 284/300\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2896 - acc: 0.4286\n",
            "Epoch 285/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2891 - acc: 0.4286\n",
            "Epoch 286/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2886 - acc: 0.4286\n",
            "Epoch 287/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2882 - acc: 0.4286\n",
            "Epoch 288/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2877 - acc: 0.4286\n",
            "Epoch 289/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2872 - acc: 0.4286\n",
            "Epoch 290/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2867 - acc: 0.4286\n",
            "Epoch 291/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2863 - acc: 0.4286\n",
            "Epoch 292/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2858 - acc: 0.4286\n",
            "Epoch 293/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2853 - acc: 0.4286\n",
            "Epoch 294/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2849 - acc: 0.4286\n",
            "Epoch 295/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2844 - acc: 0.4286\n",
            "Epoch 296/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2840 - acc: 0.4286\n",
            "Epoch 297/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2835 - acc: 0.4286\n",
            "Epoch 298/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2830 - acc: 0.4286\n",
            "Epoch 299/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2826 - acc: 0.4286\n",
            "Epoch 300/300\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2821 - acc: 0.4286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4REFkW1jKabC",
        "colab_type": "code",
        "outputId": "c1e590d0-8b5a-4d8b-bb0d-3510e5612bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.predict(np.array([[0,1,1]]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.47602713, 0.42985243, 0.13609731, 0.39202452]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln5ExI_JNAMq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "0c9fe3c4-2d04-43b1-f79e-e1f947257ef9"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              multiple                  8         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             multiple                  9         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             multiple                  16        \n",
            "=================================================================\n",
            "Total params: 33\n",
            "Trainable params: 33\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1_nvu4QKabG",
        "colab_type": "text"
      },
      "source": [
        "### Build a Tensor Keras Perceptron\n",
        "\n",
        "Try to match the architecture we used on Monday - inputs nodes and one output node. Apply this architecture to the XOR-ish dataset above. \n",
        "\n",
        "After fitting your model answer these questions: \n",
        "\n",
        "Are you able to achieve the same results as a bigger architecture from the first part of the assignment? Why is this disparity the case? What properties of the XOR dataset would cause this disparity? \n",
        "\n",
        "Now extrapolate this behavior on a much larger dataset in terms of features. What kind of architecture decisions could we make to avoid the problems the XOR dataset presents at scale? \n",
        "\n",
        "*Note:* The bias term is baked in by default in the Dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyqsQO85KabH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Compare "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8b-r70o8p2Dm"
      },
      "source": [
        "## Try building/training a more complex MLP on a bigger dataset.\n",
        "\n",
        "Use TensorFlow Keras & the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the canonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
        "\n",
        "If you need inspiration, the Internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n",
        "\n",
        "\n",
        "### Parts\n",
        "1. Gathering & Transforming the Data\n",
        "2. Making MNIST a Binary Problem\n",
        "3. Estimating your Neural Network (the part you focus on)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIGkU_42KabK",
        "colab_type": "text"
      },
      "source": [
        "### Gathering the Data \n",
        "\n",
        "`keras` has a handy method to pull the mnist dataset for you. You'll notice that each observation is a 28x28 arrary which represents an image. Although most Neural Network frameworks can handle higher dimensional data, that is more overhead than necessary for us. We need to flatten the image to one long row which will be 784 values (28X28). Basically, you will be appending each row to one another to make on really long row. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfUDc3FzKabL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M448QBVSN11q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_X5zG18Nw0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT40hh_oKabY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0], img_rows * img_cols)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows * img_cols)\n",
        "\n",
        "# Normalize Our Data\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg8DQgkwKabb",
        "colab_type": "code",
        "outputId": "39c2b0d9-a7b7-4a80-b8ca-df27396d7b5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Now the data should be in a format you're more familiar with\n",
        "x_train.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92mfv6ULKabj",
        "colab_type": "text"
      },
      "source": [
        "### Making MNIST a Binary Problem \n",
        "MNIST is multiclass classification problem; however we haven't covered all the necessary techniques to handle this yet. You would need to one-hot encode the target, use a different loss metric, and use softmax activations for the last layer. This is all stuff we'll cover later this week, but let us simplify the problem for now: Zero or all else."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "715t_xhBKabk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "y_temp = np.zeros(y_train.shape)\n",
        "y_temp[np.where(y_train == 0.0)[0]] = 1\n",
        "y_train = y_temp\n",
        "\n",
        "y_temp = np.zeros(y_test.shape)\n",
        "y_temp[np.where(y_test == 0.0)[0]] = 1\n",
        "y_test = y_temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VFeH5cPKabn",
        "colab_type": "code",
        "outputId": "1be9ccbf-f69a-42f6-d438-cc9028889ae8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# A Nice Binary target for ya to work with\n",
        "y_train"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 0., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPwm3JEEKabr",
        "colab_type": "text"
      },
      "source": [
        "### Estimating Your `net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5MOPtYdk1HgA",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # Other self variables (dataset shape, etc)\n",
        "        self.inputs = 784\n",
        "        self.hiddenNodes = 500\n",
        "        self.outputNodes = 1\n",
        "        \n",
        "        # Initial weights 784*500, input to hidden\n",
        "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)\n",
        "        self.weights1 = self.weights1 / self.weights1.shape[0]\n",
        "        \n",
        "        # 500 * 1 (second set), hidden to output\n",
        "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
        "        self.weights2 = self.weights2 / self.weights2.shape[0]\n",
        "        \n",
        "    def sigmoid(self, s):\n",
        "        return 1 / (1 + np.exp(-s))\n",
        "    \n",
        "    def sigmoidPrime(self, s):\n",
        "        ss = self.sigmoid(s)\n",
        "        return ss * (1 - ss)\n",
        "    \n",
        "    def feed_forward(self, X):\n",
        "        \n",
        "        # Weighted sum\n",
        "        self.hidden_sum = np.dot(X, self.weights1)\n",
        "        \n",
        "        # Activate\n",
        "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "        \n",
        "        # Weighted sum of activated hidden (the output layer)\n",
        "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
        "        \n",
        "        # Final activation of output (prediction)\n",
        "        self.activated_output = self.sigmoid(self.output_sum)\n",
        "        \n",
        "        return self.activated_output\n",
        "    \n",
        "    def backward(self, X, y, o):\n",
        "        # Backprop through network\n",
        "        \n",
        "        self.o_error = y - o # error in output\n",
        "        \n",
        "        # apply derivative of sigmoid to error\n",
        "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
        "        \n",
        "        # z2 error: how much our output layer weight was off\n",
        "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
        "        \n",
        "        # z2 delta: how much were the weights off by?\n",
        "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)\n",
        "        \n",
        "        # adjust first set (input -> hidden) weights\n",
        "        self.weights1 += self.learning_rate * np.reshape(X, (-1, 1)).dot(np.reshape(self.z2_delta, (-1, 1)).T)\n",
        "        \n",
        "        # adjust second set\n",
        "        self.weights2 += self.learning_rate * np.reshape(self.activated_hidden, (-1, 1)).dot(np.reshape(self.o_delta, (-1, 1)).T)\n",
        "        \n",
        "    def train(self, X, y):\n",
        "        o = self.feed_forward(X)\n",
        "        self.backward(X, y, o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ExDrscLKab2",
        "colab_type": "code",
        "outputId": "7d3bd1e7-ea38-49d4-9630-e57361a501a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Let's train it.\n",
        "\n",
        "learning_rate = 0.1\n",
        "nn = NeuralNetwork(learning_rate)\n",
        "epochs = 500\n",
        "batch_size = 100\n",
        "\n",
        "# Number of epochs/iterations.\n",
        "losses = []\n",
        "for i in range(epochs):\n",
        "    loss = 0\n",
        "    for k in range(batch_size):\n",
        "        col = np.random.randint(x_train.shape[0])\n",
        "        input_vector = x_train[col, :]\n",
        "        target_vector = y_train[col]\n",
        "        nn.train(input_vector, target_vector)\n",
        "        loss += np.linalg.norm(nn.sigmoid(np.dot(nn.weights2.T, nn.sigmoid(np.dot(nn.weights1.T, input_vector)))) - target_vector, 2)\n",
        "    if (i+1) % 10 == 0:\n",
        "        print('+' + '----' * 2 + f'EPOCH {i + 1}' + '----' * 2 + '+')\n",
        "        print(\"Loss: \\n\", str(loss/batch_size))\n",
        "    losses.append(loss)                              "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------EPOCH 10--------+\n",
            "Loss: \n",
            " 0.1336722017861669\n",
            "+--------EPOCH 20--------+\n",
            "Loss: \n",
            " 0.06243527074521262\n",
            "+--------EPOCH 30--------+\n",
            "Loss: \n",
            " 0.0325582410542625\n",
            "+--------EPOCH 40--------+\n",
            "Loss: \n",
            " 0.024187450413923787\n",
            "+--------EPOCH 50--------+\n",
            "Loss: \n",
            " 0.03259835546301298\n",
            "+--------EPOCH 60--------+\n",
            "Loss: \n",
            " 0.014834252532438825\n",
            "+--------EPOCH 70--------+\n",
            "Loss: \n",
            " 0.01242410926666373\n",
            "+--------EPOCH 80--------+\n",
            "Loss: \n",
            " 0.02317149270737995\n",
            "+--------EPOCH 90--------+\n",
            "Loss: \n",
            " 0.0076761236324916025\n",
            "+--------EPOCH 100--------+\n",
            "Loss: \n",
            " 0.008219038261693611\n",
            "+--------EPOCH 110--------+\n",
            "Loss: \n",
            " 0.009374761157237774\n",
            "+--------EPOCH 120--------+\n",
            "Loss: \n",
            " 0.00370493324749053\n",
            "+--------EPOCH 130--------+\n",
            "Loss: \n",
            " 0.012195018782982563\n",
            "+--------EPOCH 140--------+\n",
            "Loss: \n",
            " 0.011650256972371802\n",
            "+--------EPOCH 150--------+\n",
            "Loss: \n",
            " 0.008539358131035983\n",
            "+--------EPOCH 160--------+\n",
            "Loss: \n",
            " 0.006707806689498402\n",
            "+--------EPOCH 170--------+\n",
            "Loss: \n",
            " 0.011561347114337716\n",
            "+--------EPOCH 180--------+\n",
            "Loss: \n",
            " 0.003241783479221934\n",
            "+--------EPOCH 190--------+\n",
            "Loss: \n",
            " 0.013123281101466466\n",
            "+--------EPOCH 200--------+\n",
            "Loss: \n",
            " 0.008225712572597547\n",
            "+--------EPOCH 210--------+\n",
            "Loss: \n",
            " 0.013010605366312489\n",
            "+--------EPOCH 220--------+\n",
            "Loss: \n",
            " 0.003252587117316177\n",
            "+--------EPOCH 230--------+\n",
            "Loss: \n",
            " 0.011992548622527733\n",
            "+--------EPOCH 240--------+\n",
            "Loss: \n",
            " 0.009073300276714241\n",
            "+--------EPOCH 250--------+\n",
            "Loss: \n",
            " 0.0031419360247219776\n",
            "+--------EPOCH 260--------+\n",
            "Loss: \n",
            " 0.0073390442943070296\n",
            "+--------EPOCH 270--------+\n",
            "Loss: \n",
            " 0.007454286469194691\n",
            "+--------EPOCH 280--------+\n",
            "Loss: \n",
            " 0.006439630660656808\n",
            "+--------EPOCH 290--------+\n",
            "Loss: \n",
            " 0.00520669238329266\n",
            "+--------EPOCH 300--------+\n",
            "Loss: \n",
            " 0.019419924050509425\n",
            "+--------EPOCH 310--------+\n",
            "Loss: \n",
            " 0.009586951290452216\n",
            "+--------EPOCH 320--------+\n",
            "Loss: \n",
            " 0.001735886384056006\n",
            "+--------EPOCH 330--------+\n",
            "Loss: \n",
            " 0.006612073572531384\n",
            "+--------EPOCH 340--------+\n",
            "Loss: \n",
            " 0.011805688126844288\n",
            "+--------EPOCH 350--------+\n",
            "Loss: \n",
            " 0.010619298015795402\n",
            "+--------EPOCH 360--------+\n",
            "Loss: \n",
            " 0.002847823238270762\n",
            "+--------EPOCH 370--------+\n",
            "Loss: \n",
            " 0.013448697571923454\n",
            "+--------EPOCH 380--------+\n",
            "Loss: \n",
            " 0.008451168241893127\n",
            "+--------EPOCH 390--------+\n",
            "Loss: \n",
            " 0.004860869874143071\n",
            "+--------EPOCH 400--------+\n",
            "Loss: \n",
            " 0.01270712694855427\n",
            "+--------EPOCH 410--------+\n",
            "Loss: \n",
            " 0.004129786865288931\n",
            "+--------EPOCH 420--------+\n",
            "Loss: \n",
            " 0.0071595658445672335\n",
            "+--------EPOCH 430--------+\n",
            "Loss: \n",
            " 0.004163374541019668\n",
            "+--------EPOCH 440--------+\n",
            "Loss: \n",
            " 0.011397890708268877\n",
            "+--------EPOCH 450--------+\n",
            "Loss: \n",
            " 0.008446766946717463\n",
            "+--------EPOCH 460--------+\n",
            "Loss: \n",
            " 0.008598255873430131\n",
            "+--------EPOCH 470--------+\n",
            "Loss: \n",
            " 0.01173343417155891\n",
            "+--------EPOCH 480--------+\n",
            "Loss: \n",
            " 0.016811039747901803\n",
            "+--------EPOCH 490--------+\n",
            "Loss: \n",
            " 0.013073180245922588\n",
            "+--------EPOCH 500--------+\n",
            "Loss: \n",
            " 0.007557038649481852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FwlRJSfBlCvy"
      },
      "source": [
        "## Stretch Goals: \n",
        "\n",
        "- Make MNIST a multiclass problem using cross entropy & soft-max\n",
        "- Implement Cross Validation model evaluation on your MNIST implementation \n",
        "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
        " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
        "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
      ]
    }
  ]
}