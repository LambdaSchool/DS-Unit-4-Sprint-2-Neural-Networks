{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ryp-TVm4njD"
   },
   "source": [
    "# Your Mission, should you choose to accept it...\n",
    "\n",
    "To hyperparameter tune and extract every ounce of accuracy out of this telecom customer churn dataset: <https://drive.google.com/file/d/1dfbAsM9DwA7tYhInyflIpZnYs7VT-0AQ/view> \n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Load the data\n",
    "- Clean the data if necessary (it will be)\n",
    "- Create and fit a baseline Keras MLP model to the data.\n",
    "- Hyperparameter tune (at least) the following parameters:\n",
    " - batch_size\n",
    " - training epochs\n",
    " - optimizer\n",
    " - learning rate (if applicable to optimizer)\n",
    " - momentum (if applicable to optimizer)\n",
    " - activation functions\n",
    " - network weight initialization\n",
    " - dropout regularization\n",
    " - number of neurons in the hidden layer\n",
    " \n",
    " You must use Grid Search and Cross Validation for your initial pass of the above hyperparameters\n",
    " \n",
    " Try and get the maximum accuracy possible out of this data! You'll save big telecoms millions! Doesn't that sound great?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNJ-tOBs4jM1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New pandarallel memory created - Size: 2000 MB\n",
      "Pandarallel will run on 4 workers\n",
      "(7043, 53) (7043,)\n",
      "nulls:  0 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "import category_encoders as ce\n",
    "\n",
    "def clean(dat): \n",
    "    ce.basen.BaseNEncoder(base=3).fit_transform(dat.drop('Churn', axis=1))\n",
    "    return (ce.basen.BaseNEncoder(base=3).fit_transform(dat.drop('Churn', axis=1)), \n",
    "            dat.Churn.parallel_map({'No': 0, 'Yes': 1}))\n",
    "\n",
    "\n",
    "X, y = clean(pd.read_csv('telco_churn.csv'))\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print(\"nulls: \", X.isna().sum().sum(), y.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4718 samples, validate on 2325 samples\n",
      "Epoch 1/50\n",
      "4718/4718 [==============================] - 3s 585us/step - loss: 0.4657 - mean_squared_error: 0.4657 - val_loss: 0.2320 - val_mean_squared_error: 0.2320\n",
      "Epoch 2/50\n",
      "4718/4718 [==============================] - 1s 183us/step - loss: 0.2099 - mean_squared_error: 0.2099 - val_loss: 0.2136 - val_mean_squared_error: 0.2136\n",
      "Epoch 3/50\n",
      "4718/4718 [==============================] - 1s 176us/step - loss: 0.1852 - mean_squared_error: 0.1852 - val_loss: 0.1660 - val_mean_squared_error: 0.1660\n",
      "Epoch 4/50\n",
      "4718/4718 [==============================] - 1s 174us/step - loss: 0.1892 - mean_squared_error: 0.1892 - val_loss: 0.1539 - val_mean_squared_error: 0.1539\n",
      "Epoch 5/50\n",
      "4718/4718 [==============================] - 1s 182us/step - loss: 0.1983 - mean_squared_error: 0.1983 - val_loss: 0.2184 - val_mean_squared_error: 0.2184\n",
      "Epoch 6/50\n",
      "4718/4718 [==============================] - 1s 188us/step - loss: 0.1739 - mean_squared_error: 0.1739 - val_loss: 0.1543 - val_mean_squared_error: 0.1543\n",
      "Epoch 7/50\n",
      "4718/4718 [==============================] - 1s 189us/step - loss: 0.1627 - mean_squared_error: 0.1627 - val_loss: 0.1519 - val_mean_squared_error: 0.1519\n",
      "Epoch 8/50\n",
      "4718/4718 [==============================] - 1s 185us/step - loss: 0.1627 - mean_squared_error: 0.1627 - val_loss: 0.1497 - val_mean_squared_error: 0.1497\n",
      "Epoch 9/50\n",
      "4718/4718 [==============================] - 1s 175us/step - loss: 0.1606 - mean_squared_error: 0.1606 - val_loss: 0.1503 - val_mean_squared_error: 0.1503\n",
      "Epoch 10/50\n",
      "4718/4718 [==============================] - 1s 191us/step - loss: 0.1573 - mean_squared_error: 0.1573 - val_loss: 0.1500 - val_mean_squared_error: 0.1500\n",
      "Epoch 11/50\n",
      "4718/4718 [==============================] - 1s 207us/step - loss: 0.1603 - mean_squared_error: 0.1603 - val_loss: 0.1625 - val_mean_squared_error: 0.1625\n",
      "Epoch 12/50\n",
      "4718/4718 [==============================] - 1s 220us/step - loss: 0.1572 - mean_squared_error: 0.1572 - val_loss: 0.1522 - val_mean_squared_error: 0.1522\n",
      "Epoch 13/50\n",
      "4718/4718 [==============================] - 1s 192us/step - loss: 0.1486 - mean_squared_error: 0.1486 - val_loss: 0.1878 - val_mean_squared_error: 0.1878\n",
      "Epoch 14/50\n",
      "4718/4718 [==============================] - 1s 186us/step - loss: 0.1499 - mean_squared_error: 0.1499 - val_loss: 0.1548 - val_mean_squared_error: 0.1548\n",
      "Epoch 15/50\n",
      "4718/4718 [==============================] - 1s 198us/step - loss: 0.1487 - mean_squared_error: 0.1487 - val_loss: 0.1538 - val_mean_squared_error: 0.1538\n",
      "Epoch 16/50\n",
      "4718/4718 [==============================] - 1s 185us/step - loss: 0.1457 - mean_squared_error: 0.1457 - val_loss: 0.1424 - val_mean_squared_error: 0.1424\n",
      "Epoch 17/50\n",
      "4718/4718 [==============================] - 1s 195us/step - loss: 0.1427 - mean_squared_error: 0.1427 - val_loss: 0.1483 - val_mean_squared_error: 0.1483\n",
      "Epoch 18/50\n",
      "4718/4718 [==============================] - 1s 201us/step - loss: 0.1417 - mean_squared_error: 0.1417 - val_loss: 0.1574 - val_mean_squared_error: 0.1574\n",
      "Epoch 19/50\n",
      "4718/4718 [==============================] - 1s 203us/step - loss: 0.1408 - mean_squared_error: 0.1408 - val_loss: 0.1414 - val_mean_squared_error: 0.1414\n",
      "Epoch 20/50\n",
      "4718/4718 [==============================] - 1s 185us/step - loss: 0.1390 - mean_squared_error: 0.1390 - val_loss: 0.1446 - val_mean_squared_error: 0.1446\n",
      "Epoch 21/50\n",
      "4718/4718 [==============================] - 1s 197us/step - loss: 0.1389 - mean_squared_error: 0.1389 - val_loss: 0.1536 - val_mean_squared_error: 0.1536\n",
      "Epoch 22/50\n",
      "4718/4718 [==============================] - 1s 184us/step - loss: 0.1387 - mean_squared_error: 0.1387 - val_loss: 0.1424 - val_mean_squared_error: 0.1424\n",
      "Epoch 23/50\n",
      "4718/4718 [==============================] - 1s 186us/step - loss: 0.1360 - mean_squared_error: 0.1360 - val_loss: 0.1413 - val_mean_squared_error: 0.1413\n",
      "Epoch 24/50\n",
      "4718/4718 [==============================] - 1s 182us/step - loss: 0.1360 - mean_squared_error: 0.1360 - val_loss: 0.1430 - val_mean_squared_error: 0.1430\n",
      "Epoch 25/50\n",
      "4718/4718 [==============================] - 1s 188us/step - loss: 0.1332 - mean_squared_error: 0.1332 - val_loss: 0.1596 - val_mean_squared_error: 0.1596\n",
      "Epoch 26/50\n",
      "4718/4718 [==============================] - 1s 187us/step - loss: 0.1334 - mean_squared_error: 0.1334 - val_loss: 0.1428 - val_mean_squared_error: 0.1428\n",
      "Epoch 27/50\n",
      "4718/4718 [==============================] - 1s 192us/step - loss: 0.1337 - mean_squared_error: 0.1337 - val_loss: 0.1449 - val_mean_squared_error: 0.1449\n",
      "Epoch 28/50\n",
      "4718/4718 [==============================] - 1s 206us/step - loss: 0.1321 - mean_squared_error: 0.1321 - val_loss: 0.1537 - val_mean_squared_error: 0.1537\n",
      "Epoch 29/50\n",
      "4718/4718 [==============================] - 1s 187us/step - loss: 0.1326 - mean_squared_error: 0.1326 - val_loss: 0.1467 - val_mean_squared_error: 0.1467\n",
      "Epoch 30/50\n",
      "4718/4718 [==============================] - 1s 189us/step - loss: 0.1320 - mean_squared_error: 0.1320 - val_loss: 0.1446 - val_mean_squared_error: 0.1446\n",
      "Epoch 31/50\n",
      "4718/4718 [==============================] - 1s 203us/step - loss: 0.1316 - mean_squared_error: 0.1316 - val_loss: 0.1486 - val_mean_squared_error: 0.1486\n",
      "Epoch 32/50\n",
      "4718/4718 [==============================] - 1s 193us/step - loss: 0.1303 - mean_squared_error: 0.1303 - val_loss: 0.1431 - val_mean_squared_error: 0.1431\n",
      "Epoch 33/50\n",
      "4718/4718 [==============================] - 1s 194us/step - loss: 0.1305 - mean_squared_error: 0.1305 - val_loss: 0.1424 - val_mean_squared_error: 0.1424\n",
      "Epoch 34/50\n",
      "4718/4718 [==============================] - 1s 198us/step - loss: 0.1290 - mean_squared_error: 0.1290 - val_loss: 0.1437 - val_mean_squared_error: 0.1437\n",
      "Epoch 35/50\n",
      "4718/4718 [==============================] - 1s 194us/step - loss: 0.1291 - mean_squared_error: 0.1291 - val_loss: 0.1440 - val_mean_squared_error: 0.1440\n",
      "Epoch 36/50\n",
      "4718/4718 [==============================] - 1s 197us/step - loss: 0.1284 - mean_squared_error: 0.1284 - val_loss: 0.1455 - val_mean_squared_error: 0.1455\n",
      "Epoch 37/50\n",
      "4718/4718 [==============================] - 1s 203us/step - loss: 0.1279 - mean_squared_error: 0.1279 - val_loss: 0.1440 - val_mean_squared_error: 0.1440\n",
      "Epoch 38/50\n",
      "4718/4718 [==============================] - 1s 189us/step - loss: 0.1262 - mean_squared_error: 0.1262 - val_loss: 0.1466 - val_mean_squared_error: 0.1466\n",
      "Epoch 39/50\n",
      "4718/4718 [==============================] - 1s 191us/step - loss: 0.1254 - mean_squared_error: 0.1254 - val_loss: 0.1424 - val_mean_squared_error: 0.1424\n",
      "Epoch 40/50\n",
      "4718/4718 [==============================] - 1s 196us/step - loss: 0.1253 - mean_squared_error: 0.1253 - val_loss: 0.1443 - val_mean_squared_error: 0.1443\n",
      "Epoch 41/50\n",
      "4718/4718 [==============================] - 1s 236us/step - loss: 0.1257 - mean_squared_error: 0.1257 - val_loss: 0.1456 - val_mean_squared_error: 0.1456\n",
      "Epoch 42/50\n",
      "4718/4718 [==============================] - 1s 190us/step - loss: 0.1252 - mean_squared_error: 0.1252 - val_loss: 0.1460 - val_mean_squared_error: 0.1460\n",
      "Epoch 43/50\n",
      "4718/4718 [==============================] - 1s 199us/step - loss: 0.1248 - mean_squared_error: 0.1248 - val_loss: 0.1459 - val_mean_squared_error: 0.1459\n",
      "Epoch 44/50\n",
      "4718/4718 [==============================] - 1s 188us/step - loss: 0.1248 - mean_squared_error: 0.1248 - val_loss: 0.1480 - val_mean_squared_error: 0.1480\n",
      "Epoch 45/50\n",
      "4718/4718 [==============================] - 1s 192us/step - loss: 0.1230 - mean_squared_error: 0.1230 - val_loss: 0.1462 - val_mean_squared_error: 0.1462\n",
      "Epoch 46/50\n",
      "4718/4718 [==============================] - 1s 198us/step - loss: 0.1223 - mean_squared_error: 0.1223 - val_loss: 0.1495 - val_mean_squared_error: 0.1495\n",
      "Epoch 47/50\n",
      "4718/4718 [==============================] - 1s 199us/step - loss: 0.1228 - mean_squared_error: 0.1228 - val_loss: 0.1556 - val_mean_squared_error: 0.1556\n",
      "Epoch 48/50\n",
      "4718/4718 [==============================] - 1s 198us/step - loss: 0.1218 - mean_squared_error: 0.1218 - val_loss: 0.1490 - val_mean_squared_error: 0.1490\n",
      "Epoch 49/50\n",
      "4718/4718 [==============================] - 1s 190us/step - loss: 0.1202 - mean_squared_error: 0.1202 - val_loss: 0.1490 - val_mean_squared_error: 0.1490\n",
      "Epoch 50/50\n",
      "4718/4718 [==============================] - 1s 184us/step - loss: 0.1208 - mean_squared_error: 0.1208 - val_loss: 0.1567 - val_mean_squared_error: 0.1567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7314ec8dd8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Important Hyperparameters\n",
    "inputs = X.shape[1]\n",
    "epochs = 22\n",
    "batch_size = 16\n",
    "\n",
    "# Create Model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(inputs,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "# Fit Model\n",
    "model.fit(X, y, validation_split=0.33, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfZRtJ7MCN3x"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Try to implement Random Search Hyperparameter Tuning on this dataset\n",
    "- Try to implement Bayesian Optimiation tuning on this dataset\n",
    "- Practice hyperparameter tuning other datasets that we have looked at. How high can you get MNIST? Above 99%?\n",
    "- Study for the Sprint Challenge\n",
    " - Can you implement both perceptron and MLP models from scratch with forward and backpropagation?\n",
    " - Can you implement both perceptron and MLP models in keras and tune their hyperparameters with cross validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_434_Hyperparameter_Tuning_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
