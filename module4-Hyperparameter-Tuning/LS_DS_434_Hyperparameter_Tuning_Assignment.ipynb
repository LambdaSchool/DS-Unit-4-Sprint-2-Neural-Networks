{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ryp-TVm4njD"
   },
   "source": [
    "# Your Mission, should you choose to accept it...\n",
    "\n",
    "To hyperparameter tune and extract every ounce of accuracy out of this telecom customer churn dataset: <https://drive.google.com/file/d/1dfbAsM9DwA7tYhInyflIpZnYs7VT-0AQ/view> \n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Load the data\n",
    "- Clean the data if necessary (it will be)\n",
    "- Create and fit a baseline Keras MLP model to the data.\n",
    "- Hyperparameter tune (at least) the following parameters:\n",
    " - batch_size\n",
    " - training epochs\n",
    " - optimizer\n",
    " - learning rate (if applicable to optimizer)\n",
    " - momentum (if applicable to optimizer)\n",
    " - activation functions\n",
    " - network weight initialization\n",
    " - dropout regularization\n",
    " - number of neurons in the hidden layer\n",
    " \n",
    " You must use Grid Search and Cross Validation for your initial pass of the above hyperparameters\n",
    " \n",
    " Try and get the maximum accuracy possible out of this data! You'll save big telecoms millions! Doesn't that sound great?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNJ-tOBs4jM1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New pandarallel memory created - Size: 2000 MB\n",
      "Pandarallel will run on 4 workers\n",
      "(7043, 53) (7043,)\n",
      "nulls:  0 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "import category_encoders as ce\n",
    "\n",
    "def clean(dat): \n",
    "    ce.basen.BaseNEncoder(base=3).fit_transform(dat.drop('Churn', axis=1))\n",
    "    return (ce.basen.BaseNEncoder(base=3).fit_transform(dat.drop('Churn', axis=1)), \n",
    "            dat.Churn.parallel_map({'No': 0, 'Yes': 1}))\n",
    "\n",
    "\n",
    "X, y = clean(pd.read_csv('telco_churn.csv'))\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print(\"nulls: \", X.isna().sum().sum(), y.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4718 samples, validate on 2325 samples\n",
      "Epoch 1/22\n",
      "4718/4718 [==============================] - 1s 194us/step - loss: 0.1587 - mean_squared_error: 0.1587 - val_loss: 0.1532 - val_mean_squared_error: 0.1532\n",
      "Epoch 2/22\n",
      "4718/4718 [==============================] - 1s 115us/step - loss: 0.1504 - mean_squared_error: 0.1504 - val_loss: 0.1520 - val_mean_squared_error: 0.1520\n",
      "Epoch 3/22\n",
      "4718/4718 [==============================] - 1s 125us/step - loss: 0.1466 - mean_squared_error: 0.1466 - val_loss: 0.1512 - val_mean_squared_error: 0.1512\n",
      "Epoch 4/22\n",
      "4718/4718 [==============================] - 1s 124us/step - loss: 0.1454 - mean_squared_error: 0.1454 - val_loss: 0.1473 - val_mean_squared_error: 0.1473\n",
      "Epoch 5/22\n",
      "4718/4718 [==============================] - 1s 132us/step - loss: 0.1422 - mean_squared_error: 0.1422 - val_loss: 0.1411 - val_mean_squared_error: 0.1411\n",
      "Epoch 6/22\n",
      "4718/4718 [==============================] - 1s 121us/step - loss: 0.1401 - mean_squared_error: 0.1401 - val_loss: 0.1412 - val_mean_squared_error: 0.1412\n",
      "Epoch 7/22\n",
      "4718/4718 [==============================] - 1s 132us/step - loss: 0.1391 - mean_squared_error: 0.1391 - val_loss: 0.1401 - val_mean_squared_error: 0.1401\n",
      "Epoch 8/22\n",
      "4718/4718 [==============================] - 1s 120us/step - loss: 0.1377 - mean_squared_error: 0.1377 - val_loss: 0.1431 - val_mean_squared_error: 0.1431\n",
      "Epoch 9/22\n",
      "4718/4718 [==============================] - 1s 128us/step - loss: 0.1367 - mean_squared_error: 0.1367 - val_loss: 0.1429 - val_mean_squared_error: 0.1429\n",
      "Epoch 10/22\n",
      "4718/4718 [==============================] - 1s 140us/step - loss: 0.1393 - mean_squared_error: 0.1393 - val_loss: 0.1506 - val_mean_squared_error: 0.1506\n",
      "Epoch 11/22\n",
      "4718/4718 [==============================] - 1s 132us/step - loss: 0.1355 - mean_squared_error: 0.1355 - val_loss: 0.1642 - val_mean_squared_error: 0.1642\n",
      "Epoch 12/22\n",
      "4718/4718 [==============================] - 1s 149us/step - loss: 0.1363 - mean_squared_error: 0.1363 - val_loss: 0.1610 - val_mean_squared_error: 0.1610\n",
      "Epoch 13/22\n",
      "4718/4718 [==============================] - 1s 131us/step - loss: 0.1355 - mean_squared_error: 0.1355 - val_loss: 0.1451 - val_mean_squared_error: 0.1451\n",
      "Epoch 14/22\n",
      "4718/4718 [==============================] - 1s 128us/step - loss: 0.1340 - mean_squared_error: 0.1340 - val_loss: 0.1398 - val_mean_squared_error: 0.1398\n",
      "Epoch 15/22\n",
      "4718/4718 [==============================] - 1s 121us/step - loss: 0.1340 - mean_squared_error: 0.1340 - val_loss: 0.1414 - val_mean_squared_error: 0.1414\n",
      "Epoch 16/22\n",
      "4718/4718 [==============================] - 1s 118us/step - loss: 0.1347 - mean_squared_error: 0.1347 - val_loss: 0.1430 - val_mean_squared_error: 0.1430\n",
      "Epoch 17/22\n",
      "4718/4718 [==============================] - 1s 138us/step - loss: 0.1354 - mean_squared_error: 0.1354 - val_loss: 0.1400 - val_mean_squared_error: 0.1400\n",
      "Epoch 18/22\n",
      "4718/4718 [==============================] - 1s 127us/step - loss: 0.1364 - mean_squared_error: 0.1364 - val_loss: 0.1406 - val_mean_squared_error: 0.1406\n",
      "Epoch 19/22\n",
      "4718/4718 [==============================] - 1s 128us/step - loss: 0.1318 - mean_squared_error: 0.1318 - val_loss: 0.1454 - val_mean_squared_error: 0.1454\n",
      "Epoch 20/22\n",
      "4718/4718 [==============================] - 1s 139us/step - loss: 0.1324 - mean_squared_error: 0.1324 - val_loss: 0.1383 - val_mean_squared_error: 0.1383\n",
      "Epoch 21/22\n",
      "4718/4718 [==============================] - 1s 130us/step - loss: 0.1331 - mean_squared_error: 0.1331 - val_loss: 0.1480 - val_mean_squared_error: 0.1480\n",
      "Epoch 22/22\n",
      "4718/4718 [==============================] - 1s 121us/step - loss: 0.1321 - mean_squared_error: 0.1321 - val_loss: 0.1399 - val_mean_squared_error: 0.1399\n",
      "7043/7043 [==============================] - 0s 23us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13230134519833944, 0.13230134519833944]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Important Hyperparameters\n",
    "inputs = X.shape[1]\n",
    "epochs = 22\n",
    "batch_size = 16\n",
    "\n",
    "# Create Model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(inputs,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(1))\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "# Fit Model\n",
    "model.fit(X, y, validation_split=0.33, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "model.evaluate(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfZRtJ7MCN3x"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Try to implement Random Search Hyperparameter Tuning on this dataset\n",
    "- Try to implement Bayesian Optimiation tuning on this dataset\n",
    "- Practice hyperparameter tuning other datasets that we have looked at. How high can you get MNIST? Above 99%?\n",
    "- Study for the Sprint Challenge\n",
    " - Can you implement both perceptron and MLP models from scratch with forward and backpropagation?\n",
    " - Can you implement both perceptron and MLP models in keras and tune their hyperparameters with cross validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_434_Hyperparameter_Tuning_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
