{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ryp-TVm4njD"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 4*\n",
    "\n",
    "## Your Mission, should you choose to accept it...\n",
    "\n",
    "To hyperparameter tune and extract every ounce of accuracy out of this telecom customer churn dataset: <https://drive.google.com/file/d/1dfbAsM9DwA7tYhInyflIpZnYs7VT-0AQ/view> \n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Load the data\n",
    "- Clean the data if necessary (it will be)\n",
    "- Create and fit a baseline Keras MLP model to the data.\n",
    "- Hyperparameter tune (at least) the following parameters:\n",
    " - batch_size\n",
    " - training epochs\n",
    " - optimizer\n",
    " - learning rate (if applicable to optimizer)\n",
    " - momentum (if applicable to optimizer)\n",
    " - activation functions\n",
    " - network weight initialization\n",
    " - dropout regularization\n",
    " - number of neurons in the hidden layer\n",
    " \n",
    " You must use Grid Search and Cross Validation for your initial pass of the above hyperparameters\n",
    " \n",
    " Try and get the maximum accuracy possible out of this data! You'll save big telecoms millions! Doesn't that sound great?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNJ-tOBs4jM1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn+(1).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     0.73463\n",
       "Yes    0.26537\n",
       "Name: Churn, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets get a percentage count of how often people churn to set a baseline\n",
    "df['Churn'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the target to a 0/1 categorical\n",
    "\n",
    "df['Churn'] = df['Churn'].replace({'Yes':1, 'No':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customerID 7043\n",
      "gender 2\n",
      "SeniorCitizen 2\n",
      "Partner 2\n",
      "Dependents 2\n",
      "tenure 73\n",
      "PhoneService 2\n",
      "MultipleLines 3\n",
      "InternetService 3\n",
      "OnlineSecurity 3\n",
      "OnlineBackup 3\n",
      "DeviceProtection 3\n",
      "TechSupport 3\n",
      "StreamingTV 3\n",
      "StreamingMovies 3\n",
      "Contract 3\n",
      "PaperlessBilling 2\n",
      "PaymentMethod 4\n",
      "MonthlyCharges 1585\n",
      "TotalCharges 6531\n",
      "Churn 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#digging up unit 2 notes on working with tabular data\n",
    "#offhand I know understanding how many unique values there is in a dataset is imortant for feature engineering\n",
    "\n",
    "for col in df.columns: print(col, df[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping Customer ID\n",
    "X = df.drop(columns=['Churn', 'customerID']).values\n",
    "y = df['Churn'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordinal encoding will map the many low cardinality fields into a numerical representation\n",
    "import category_encoders as ce\n",
    "ord_enc = ce.OrdinalEncoder()\n",
    "\n",
    "#scalar encoding will transform our inputs to be prepared for a neural network\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler  = StandardScaler()\n",
    "\n",
    "X = ord_enc.fit_transform(X)\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(19, input_dim=19, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# param_grid = dict(batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.8017796346496696 using {'batch_size': 25, 'epochs': 20}\n",
      "Means: 0.8002650513765442, Stdev: 0.0018325075247239823 with: {'batch_size': 10, 'epochs': 20}\n",
      "Means: 0.8017796346496696, Stdev: 0.005173583521320478 with: {'batch_size': 25, 'epochs': 20}\n",
      "Means: 0.8000757090411643, Stdev: 0.0012796621415408743 with: {'batch_size': 50, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [10, 25, 50],\n",
    "              'epochs': [20]}\n",
    "\n",
    "# Create Grid Search\n",
    "#MAKE SURE JOBS =1, MAKING JOBS =-1 DOES NOT PLAY NICELY WITH THE KERAS WRAPPER IT DOES NOT PARRELIZE WELL\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best: 0.8017796346496696 using {'batch_size': 25, 'epochs': 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.8014009849608102 using {'batch_size': 10, 'epochs': 20}\n",
      "Means: 0.8014009849608102, Stdev: 0.003401740389408124 with: {'batch_size': 10, 'epochs': 20}\n",
      "Means: 0.7968572580037989, Stdev: 0.0026085012557913166 with: {'batch_size': 10, 'epochs': 50}\n",
      "Means: 0.7906096140370591, Stdev: 0.003532912877744876 with: {'batch_size': 10, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [10],\n",
    "              'epochs': [20,50,100]}\n",
    "\n",
    "# Create Grid Search\n",
    "#MAKE SURE JOBS =1, MAKING JOBS =-1 DOES NOT PLAY NICELY WITH THE KERAS WRAPPER IT DOES NOT PARRELIZE WELL\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4753 samples, validate on 529 samples\n",
      "Epoch 1/20\n",
      "4753/4753 [==============================] - 2s 436us/sample - loss: 0.4932 - acc: 0.7684 - val_loss: 0.4490 - val_acc: 0.7902\n",
      "Epoch 2/20\n",
      "4753/4753 [==============================] - 1s 273us/sample - loss: 0.4234 - acc: 0.7965 - val_loss: 0.4414 - val_acc: 0.7977\n",
      "Epoch 3/20\n",
      "4753/4753 [==============================] - 1s 274us/sample - loss: 0.4156 - acc: 0.8033 - val_loss: 0.4394 - val_acc: 0.7958\n",
      "Epoch 4/20\n",
      "4753/4753 [==============================] - 1s 270us/sample - loss: 0.4120 - acc: 0.8062 - val_loss: 0.4405 - val_acc: 0.7902\n",
      "Epoch 5/20\n",
      "4753/4753 [==============================] - 1s 267us/sample - loss: 0.4097 - acc: 0.8106 - val_loss: 0.4386 - val_acc: 0.8015\n",
      "Epoch 6/20\n",
      "4753/4753 [==============================] - 1s 268us/sample - loss: 0.4078 - acc: 0.8081 - val_loss: 0.4376 - val_acc: 0.8034\n",
      "Epoch 7/20\n",
      "4753/4753 [==============================] - 1s 275us/sample - loss: 0.4063 - acc: 0.8136 - val_loss: 0.4367 - val_acc: 0.8034\n",
      "Epoch 8/20\n",
      "4753/4753 [==============================] - 1s 272us/sample - loss: 0.4051 - acc: 0.8127 - val_loss: 0.4399 - val_acc: 0.7996\n",
      "Epoch 9/20\n",
      "4753/4753 [==============================] - 1s 272us/sample - loss: 0.4041 - acc: 0.8117 - val_loss: 0.4430 - val_acc: 0.8015\n",
      "Epoch 10/20\n",
      "4753/4753 [==============================] - 1s 273us/sample - loss: 0.4023 - acc: 0.8121 - val_loss: 0.4428 - val_acc: 0.7996\n",
      "Epoch 11/20\n",
      "4753/4753 [==============================] - 1s 274us/sample - loss: 0.4018 - acc: 0.8146 - val_loss: 0.4383 - val_acc: 0.8053\n",
      "Epoch 12/20\n",
      "4753/4753 [==============================] - 1s 270us/sample - loss: 0.4007 - acc: 0.8151 - val_loss: 0.4399 - val_acc: 0.8015\n",
      "Epoch 13/20\n",
      "4753/4753 [==============================] - 1s 275us/sample - loss: 0.3999 - acc: 0.8159 - val_loss: 0.4372 - val_acc: 0.8053\n",
      "Epoch 14/20\n",
      "4753/4753 [==============================] - 1s 271us/sample - loss: 0.3986 - acc: 0.8134 - val_loss: 0.4395 - val_acc: 0.7977\n",
      "Epoch 15/20\n",
      "4753/4753 [==============================] - 1s 273us/sample - loss: 0.3980 - acc: 0.8121 - val_loss: 0.4388 - val_acc: 0.8015\n",
      "Epoch 16/20\n",
      "4753/4753 [==============================] - 1s 273us/sample - loss: 0.3971 - acc: 0.8155 - val_loss: 0.4444 - val_acc: 0.7977\n",
      "Epoch 17/20\n",
      "4753/4753 [==============================] - 1s 276us/sample - loss: 0.3969 - acc: 0.8157 - val_loss: 0.4399 - val_acc: 0.7996\n",
      "Epoch 18/20\n",
      "4753/4753 [==============================] - 1s 278us/sample - loss: 0.3956 - acc: 0.8144 - val_loss: 0.4456 - val_acc: 0.7921\n",
      "Epoch 19/20\n",
      "4753/4753 [==============================] - 1s 270us/sample - loss: 0.3957 - acc: 0.8149 - val_loss: 0.4432 - val_acc: 0.7940\n",
      "Epoch 20/20\n",
      "4753/4753 [==============================] - 1s 273us/sample - loss: 0.3945 - acc: 0.8165 - val_loss: 0.4399 - val_acc: 0.7940\n"
     ]
    }
   ],
   "source": [
    "#my second pass of models shows an optimal combination of batch size 10 and epochs of 20\n",
    "\n",
    "updated_model = create_model()\n",
    "\n",
    "opt = updated_model.fit(X_train, y_train,\n",
    "                        epochs=20,\n",
    "                        batch_size=10,\n",
    "                        validation_split=.1,\n",
    "                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1761/1761 [==============================] - 0s 57us/sample - loss: 0.4345 - acc: 0.7939\n",
      "Neural Network ACC:  0.7938671\n"
     ]
    }
   ],
   "source": [
    "scores = updated_model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Neural Network ACC: ', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def create_model_with_drops():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    \n",
    "    adam = Adam(lr=0.001)\n",
    "\n",
    "    \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model_with_drops = KerasClassifier(build_fn=create_model_with_drops, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6338 samples, validate on 705 samples\n",
      "Epoch 1/20\n",
      "6338/6338 [==============================] - 3s 480us/sample - loss: 0.5075 - acc: 0.7275 - val_loss: 0.4497 - val_acc: 0.7844\n",
      "Epoch 2/20\n",
      "6338/6338 [==============================] - 2s 305us/sample - loss: 0.4372 - acc: 0.7898 - val_loss: 0.4422 - val_acc: 0.7830\n",
      "Epoch 3/20\n",
      "6338/6338 [==============================] - 2s 311us/sample - loss: 0.4308 - acc: 0.7963 - val_loss: 0.4418 - val_acc: 0.7915\n",
      "Epoch 4/20\n",
      "6338/6338 [==============================] - 2s 303us/sample - loss: 0.4258 - acc: 0.7977 - val_loss: 0.4378 - val_acc: 0.7844\n",
      "Epoch 5/20\n",
      "6338/6338 [==============================] - 2s 307us/sample - loss: 0.4248 - acc: 0.8001 - val_loss: 0.4398 - val_acc: 0.7972\n",
      "Epoch 6/20\n",
      "6338/6338 [==============================] - 2s 302us/sample - loss: 0.4219 - acc: 0.8020 - val_loss: 0.4397 - val_acc: 0.8028\n",
      "Epoch 7/20\n",
      "6338/6338 [==============================] - 2s 305us/sample - loss: 0.4199 - acc: 0.8036 - val_loss: 0.4367 - val_acc: 0.8014\n",
      "Epoch 8/20\n",
      "6338/6338 [==============================] - 2s 310us/sample - loss: 0.4190 - acc: 0.8058 - val_loss: 0.4349 - val_acc: 0.7943\n",
      "Epoch 9/20\n",
      "6338/6338 [==============================] - 2s 303us/sample - loss: 0.4172 - acc: 0.8034 - val_loss: 0.4347 - val_acc: 0.7943\n",
      "Epoch 10/20\n",
      "6338/6338 [==============================] - 2s 297us/sample - loss: 0.4191 - acc: 0.8009 - val_loss: 0.4349 - val_acc: 0.8099\n",
      "Epoch 11/20\n",
      "6338/6338 [==============================] - 2s 303us/sample - loss: 0.4129 - acc: 0.8053 - val_loss: 0.4298 - val_acc: 0.7929\n",
      "Epoch 12/20\n",
      "6338/6338 [==============================] - 2s 301us/sample - loss: 0.4124 - acc: 0.8064 - val_loss: 0.4315 - val_acc: 0.8071\n",
      "Epoch 13/20\n",
      "6338/6338 [==============================] - 2s 297us/sample - loss: 0.4129 - acc: 0.8058 - val_loss: 0.4295 - val_acc: 0.8000\n",
      "Epoch 14/20\n",
      "6338/6338 [==============================] - 2s 297us/sample - loss: 0.4111 - acc: 0.8091 - val_loss: 0.4303 - val_acc: 0.8071\n",
      "Epoch 15/20\n",
      "6338/6338 [==============================] - 2s 298us/sample - loss: 0.4126 - acc: 0.8122 - val_loss: 0.4265 - val_acc: 0.8014\n",
      "Epoch 16/20\n",
      "6338/6338 [==============================] - 2s 297us/sample - loss: 0.4107 - acc: 0.8116 - val_loss: 0.4291 - val_acc: 0.7957\n",
      "Epoch 17/20\n",
      "6338/6338 [==============================] - 2s 299us/sample - loss: 0.4091 - acc: 0.8088 - val_loss: 0.4302 - val_acc: 0.8043\n",
      "Epoch 18/20\n",
      "6338/6338 [==============================] - 2s 300us/sample - loss: 0.4100 - acc: 0.8075 - val_loss: 0.4277 - val_acc: 0.7957\n",
      "Epoch 19/20\n",
      "6338/6338 [==============================] - 2s 301us/sample - loss: 0.4116 - acc: 0.8108 - val_loss: 0.4299 - val_acc: 0.8028\n",
      "Epoch 20/20\n",
      "6338/6338 [==============================] - 2s 304us/sample - loss: 0.4117 - acc: 0.8118 - val_loss: 0.4314 - val_acc: 0.8028\n"
     ]
    }
   ],
   "source": [
    "final_model = model_with_drops.fit(X, y,\n",
    "                         epochs=20,\n",
    "                         batch_size=10,\n",
    "                         validation_split=.1,\n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfZRtJ7MCN3x"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Try to implement Random Search Hyperparameter Tuning on this dataset\n",
    "- Try to implement Bayesian Optimiation tuning on this dataset using hyperas or hyperopt (if you're brave)\n",
    "- Practice hyperparameter tuning other datasets that we have looked at. How high can you get MNIST? Above 99%?\n",
    "- Study for the Sprint Challenge\n",
    " - Can you implement both perceptron and MLP models from scratch with forward and backpropagation?\n",
    " - Can you implement both perceptron and MLP models in keras and tune their hyperparameters with cross validation?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_434_Hyperparameter_Tuning_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
