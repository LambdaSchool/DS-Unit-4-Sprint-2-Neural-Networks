{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_434_Hyperparameter_Tuning_Assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9Ryp-TVm4njD"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "\n",
        "# Hyperparameter Tuning\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 4*\n",
        "\n",
        "## Your Mission, should you choose to accept it...\n",
        "\n",
        "To hyperparameter tune and extract every ounce of accuracy out of this telecom customer churn dataset: [Available Here](https://lambdaschool-data-science.s3.amazonaws.com/telco-churn/WA_Fn-UseC_-Telco-Customer-Churn+(1).csv)\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- Load the data\n",
        "- Clean the data if necessary (it will be)\n",
        "- Create and fit a baseline Keras MLP model to the data.\n",
        "- Hyperparameter tune (at least) the following parameters:\n",
        " - batch_size\n",
        " - training epochs\n",
        " - optimizer\n",
        " - learning rate (if applicable to optimizer)\n",
        " - momentum (if applicable to optimizer)\n",
        " - activation functions\n",
        " - network weight initialization\n",
        " - dropout regularization\n",
        " - number of neurons in the hidden layer\n",
        " \n",
        " You must use Grid Search and Cross Validation for your initial pass of the above hyperparameters\n",
        " \n",
        " Try and get the maximum accuracy possible out of this data! You'll save big telecoms millions! Doesn't that sound great?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J10XDleioEw7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b566626e-960b-41a2-f767-435597f119ff"
      },
      "source": [
        "##### Importing #####\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/WA_Fn-UseC_-Telco-Customer-Churn+(1).csv')\n",
        "df.head(5)\n",
        "\n",
        "##### Cleaning #####\n",
        "\n",
        "# No null values\n",
        "df.isnull().sum().sum()\n",
        "\n",
        "# CustomerID is a unique identifier so drop\n",
        "df = df.drop(columns=['customerID'])\n",
        "\n",
        "# Gender\n",
        "df['gender'].replace({'Male': 1, 'Female': 2}, inplace=True)\n",
        "\n",
        "# Encoding yes/no's \n",
        "binary_cols = ['Partner', 'Dependents', 'PhoneService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'Churn', 'MultipleLines']\n",
        "for col in binary_cols:\n",
        "  df[col].replace({'Yes': 1, 'No': 0, 'No internet service': 0, 'No phone service': 0}, inplace=True)\n",
        "\n",
        "# Ordinal encoding\n",
        "df['InternetService'].replace({'Fiber optic': 3, 'DSL': 2, 'No': 1}, inplace=True)\n",
        "df['Contract'].replace({'Month-to-month': 1, 'One year': 2, 'Two year': 3}, inplace=True)\n",
        "df['PaymentMethod'].replace({'Credit card (automatic)': 4, 'Bank transfer (automatic)': 3, 'Electronic check': 2, 'Mailed check': 1}, inplace=True)\n",
        "\n",
        "# Removing rows with spaces and converting to float\n",
        "df = df[df['TotalCharges'] != ' ']\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'])\n",
        "\n",
        "# Normalizing (row-wise operation as compared to StandardScaler)\n",
        "from sklearn.preprocessing import Normalizer\n",
        "transformer = Normalizer()\n",
        "X = transformer.fit_transform(df.drop(columns=['Churn']))\n",
        "\n",
        "X"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.71529624e-02, 0.00000000e+00, 2.35764812e-02, ...,\n",
              "        4.71529624e-02, 7.03757963e-01, 7.03757963e-01],\n",
              "       [5.28913807e-04, 0.00000000e+00, 0.00000000e+00, ...,\n",
              "        5.28913807e-04, 3.01216413e-02, 9.99382639e-01],\n",
              "       [8.27287359e-03, 0.00000000e+00, 0.00000000e+00, ...,\n",
              "        8.27287359e-03, 4.45494243e-01, 8.94711279e-01],\n",
              "       ...,\n",
              "       [5.74860378e-03, 0.00000000e+00, 2.87430189e-03, ...,\n",
              "        5.74860378e-03, 8.50793360e-02, 9.95801890e-01],\n",
              "       [3.16906801e-03, 3.16906801e-03, 3.16906801e-03, ...,\n",
              "        3.16906801e-03, 2.35778660e-01, 9.71636251e-01],\n",
              "       [1.46078464e-04, 0.00000000e+00, 0.00000000e+00, ...,\n",
              "        4.38235391e-04, 1.54331897e-02, 9.99834045e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zskHEE7gzlmI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7423f6f1-6f91-4199-e632-5a6798379da8"
      },
      "source": [
        "##### Create and fit a baseline Keras MLP model to the data. #####\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "# Before fitting a little more preprocessing...\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df['Churn'], test_size=0.2, random_state=42)\n",
        "y_test = np.asarray(y_test)\n",
        "y_train = np.asarray(y_train)\n",
        "\n",
        "inputs = X_train.shape[1]\n",
        "\n",
        "# All numpy arrays\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5625, 19), (1407, 19), (5625,), (1407,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RyLTUYn6HSd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "901b27a1-e014-47be-9698-ec95ed0b42ce"
      },
      "source": [
        "# Baseline for just guessing majority class\n",
        "\"\"\"\n",
        "No     0.73463\n",
        "Yes    0.26537\n",
        "\n",
        "73% guessing No Churning for every instance\n",
        "\"\"\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNo     0.73463\\nYes    0.26537\\n\\n73% guessing No Churning for every instance\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwJp8bzO2F3u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "4fe07941-ac01-4596-ae27-bf0115fb1e58"
      },
      "source": [
        "# Building the baseline model w/ 19 input nodes (# of features) and 2 softmax probability output nodes for the 2 classes \n",
        "model = Sequential()\n",
        "model.add(Dense(19, activation='relu', input_shape=(inputs,)))\n",
        "model.add(Dense(1, activation='relu'))\n",
        "\n",
        "# Compiling\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fitting\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, verbose=1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 0.5756 - accuracy: 0.7342 - val_loss: 0.5412 - val_accuracy: 0.7342\n",
            "Epoch 2/20\n",
            "176/176 [==============================] - 0s 1ms/step - loss: 0.5340 - accuracy: 0.7444 - val_loss: 0.5267 - val_accuracy: 0.7548\n",
            "Epoch 3/20\n",
            "176/176 [==============================] - 0s 1ms/step - loss: 0.5229 - accuracy: 0.7556 - val_loss: 0.5188 - val_accuracy: 0.7605\n",
            "Epoch 4/20\n",
            "176/176 [==============================] - 0s 1ms/step - loss: 0.5153 - accuracy: 0.7582 - val_loss: 0.5137 - val_accuracy: 0.7605\n",
            "Epoch 5/20\n",
            "176/176 [==============================] - 0s 1ms/step - loss: 0.5092 - accuracy: 0.7600 - val_loss: 0.5063 - val_accuracy: 0.7598\n",
            "Epoch 6/20\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 0.5012 - accuracy: 0.7614 - val_loss: 0.4986 - val_accuracy: 0.7619\n",
            "Epoch 7/20\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 0.4943 - accuracy: 0.7644 - val_loss: 0.4929 - val_accuracy: 0.7647\n",
            "Epoch 8/20\n",
            "176/176 [==============================] - 0s 1ms/step - loss: 0.4877 - accuracy: 0.7676 - val_loss: 0.4874 - val_accuracy: 0.7655\n",
            "Epoch 9/20\n",
            "176/176 [==============================] - 0s 1ms/step - loss: 0.4836 - accuracy: 0.7691 - val_loss: 0.4840 - val_accuracy: 0.7655\n",
            "Epoch 10/20\n",
            "176/176 [==============================] - 0s 1ms/step - loss: 0.4797 - accuracy: 0.7724 - val_loss: 0.4802 - val_accuracy: 0.7676\n",
            "Epoch 11/20\n",
            "176/176 [==============================] - 0s 1ms/step - loss: 0.4779 - accuracy: 0.7723 - val_loss: 0.4802 - val_accuracy: 0.7697\n",
            "Epoch 12/20\n",
            "176/176 [==============================] - 0s 1ms/step - loss: 0.4824 - accuracy: 0.7751 - val_loss: 0.4790 - val_accuracy: 0.7690\n",
            "Epoch 13/20\n",
            "176/176 [==============================] - 0s 1ms/step - loss: 0.4804 - accuracy: 0.7739 - val_loss: 0.4805 - val_accuracy: 0.7704\n",
            "Epoch 14/20\n",
            "176/176 [==============================] - 0s 1ms/step - loss: 0.4759 - accuracy: 0.7744 - val_loss: 0.4790 - val_accuracy: 0.7711\n",
            "Epoch 15/20\n",
            "176/176 [==============================] - 0s 1ms/step - loss: 0.4775 - accuracy: 0.7739 - val_loss: 0.4762 - val_accuracy: 0.7711\n",
            "Epoch 16/20\n",
            "176/176 [==============================] - 0s 1ms/step - loss: 0.4761 - accuracy: 0.7760 - val_loss: 0.4759 - val_accuracy: 0.7719\n",
            "Epoch 17/20\n",
            "176/176 [==============================] - 0s 1ms/step - loss: 0.4779 - accuracy: 0.7758 - val_loss: 0.4748 - val_accuracy: 0.7711\n",
            "Epoch 18/20\n",
            "176/176 [==============================] - 0s 1ms/step - loss: 0.4759 - accuracy: 0.7764 - val_loss: 0.4767 - val_accuracy: 0.7740\n",
            "Epoch 19/20\n",
            "176/176 [==============================] - 0s 1ms/step - loss: 0.4758 - accuracy: 0.7760 - val_loss: 0.4793 - val_accuracy: 0.7719\n",
            "Epoch 20/20\n",
            "176/176 [==============================] - 0s 1ms/step - loss: 0.4800 - accuracy: 0.7751 - val_loss: 0.4812 - val_accuracy: 0.7690\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0f04e43080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaFl6lmx7RXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Hyperparameter tune (at least) the following parameters:\n",
        "\n",
        "batch_size\n",
        "training epochs\n",
        "optimizer\n",
        "learning rate (if applicable to optimizer)\n",
        "momentum (if applicable to optimizer)\n",
        "activation functions\n",
        "network weight initialization\n",
        "dropout regularization\n",
        "number of neurons in the hidden layer\n",
        "You must use Grid Search and Cross Validation for your initial pass of the above hyperparameters\n",
        "\n",
        "Try and get the maximum accuracy possible out of this data! You'll save big telecoms millions! Doesn't that sound great?\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J80Dv4mbIYyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hypertuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model():\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(19, activation='relu', input_shape=(inputs,)))\n",
        "    model.add(Dense(1, activation='relu'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# create model with an sklearn wrapper\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "param_grid = {'batch_size': [10],\n",
        "              'epochs': [100, 200, 300, 400, 500, 600, 700]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD2x2OmMKt2M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5dd56577-474c-4000-ed1c-ba23e87013d9"
      },
      "source": [
        "# Cross validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_score(model, X_train, y_train, cv=3)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.75999999, 0.74346668, 0.72426665])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0z2iiNCL2yz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Batch size: [10, 20, 40, 60, 80, 100] - 10 was the best\n",
        "Epochs: [100, 200, 300, 400, 500, 600, 700] - \n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FfZRtJ7MCN3x"
      },
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- Try to implement Random Search Hyperparameter Tuning on this dataset\n",
        "- Try to implement Bayesian Optimiation tuning on this dataset using hyperas or hyperopt (if you're brave)\n",
        "- Practice hyperparameter tuning other datasets that we have looked at. How high can you get MNIST? Above 99%?\n",
        "- Study for the Sprint Challenge\n",
        " - Can you implement both perceptron and MLP models from scratch with forward and backpropagation?\n",
        " - Can you implement both perceptron and MLP models in keras and tune their hyperparameters with cross validation?"
      ]
    }
  ]
}