{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ryp-TVm4njD"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 4*\n",
    "\n",
    "## Your Mission, should you choose to accept it...\n",
    "\n",
    "To hyperparameter tune and extract every ounce of accuracy out of this telecom customer churn dataset: [Available Here](https://lambdaschool-data-science.s3.amazonaws.com/telco-churn/WA_Fn-UseC_-Telco-Customer-Churn+(1).csv)\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Load the data\n",
    "- Clean the data if necessary (it will be)\n",
    "- Create and fit a baseline Keras MLP model to the data.\n",
    "- Hyperparameter tune (at least) the following parameters:\n",
    " - batch_size\n",
    " - training epochs\n",
    " - optimizer\n",
    " - learning rate (if applicable to optimizer)\n",
    " - momentum (if applicable to optimizer)\n",
    " - activation functions\n",
    " - network weight initialization\n",
    " - dropout regularization\n",
    " - number of neurons in the hidden layer\n",
    " \n",
    " You must use Grid Search and Cross Validation for your initial pass of the above hyperparameters\n",
    " \n",
    " Try and get the maximum accuracy possible out of this data! You'll save big telecoms millions! Doesn't that sound great?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNJ-tOBs4jM1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.activations import relu, elu, sigmoid, linear\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, SGD\n",
    "from tensorflow.keras.losses import binary_crossentropy, hinge, squared_hinge\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZnEM2y725gt"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://lambdaschool-data-science.s3.amazonaws.com/telco-churn/WA_Fn-UseC_-Telco-Customer-Churn+(1).csv', header=0)\n",
    "df = df.drop(columns=['customerID', 'gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seFqHYe1I-2_"
   },
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder()\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "saPX9L2O3GSv"
   },
   "outputs": [],
   "source": [
    "features = df.columns[:-1]\n",
    "target = 'Churn'\n",
    "df[features] = enc.fit_transform(df[features])\n",
    "df[target] = df[target].replace('No', 0).replace('Yes', 1)\n",
    "train, val = train_test_split(df, test_size=.2)\n",
    "X_train, y_train = train[features], train[target]\n",
    "X_val, y_val = val[features], val[target]\n",
    "X_train = enc.fit_transform(X_train)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = enc.fit_transform(X_val)\n",
    "X_val = scaler.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pZwHPSG4G6p"
   },
   "outputs": [],
   "source": [
    "first_neuron = [12, 18, 24, 48, 96]\n",
    "hidden_layer = [6, 9, 12, 24, 48]\n",
    "activation = ['relu']\n",
    "optimizer = [Adam]\n",
    "loss_function = [binary_crossentropy, hinge, squared_hinge]\n",
    "batch_size = [2, 4]\n",
    "epochs = [50, 100, 150]\n",
    "lr = [1, 10, 20]\n",
    "dropout = [.1, .2, .4]\n",
    "last_activation = [sigmoid]\n",
    "weight_initializer = ['glorot_uniform']\n",
    "\n",
    "params = dict(first_neuron=first_neuron, hidden_layer=hidden_layer, activation=activation,\n",
    "              optimizer=optimizer, lr=lr, dropout=dropout, last_activation=last_activation,\n",
    "              weight_initializer=weight_initializer, epochs=epochs, batch_size=batch_size,\n",
    "              loss_function=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Clp4MFL32m3R"
   },
   "outputs": [],
   "source": [
    "def telecom(first_neuron=18, hidden_layer=10, activation='relu',\n",
    "            optimizer=Adam, lr=lr, dropout=0, last_activation=sigmoid,\n",
    "            weight_initializer='uniform', epochs=100, batch_size=100,\n",
    "            loss_function=binary_crossentropy):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(units=first_neuron, \n",
    "                    input_dim=18, activation=activation,\n",
    "                    kernel_initializer=weight_initializer\n",
    "                    ))\n",
    "  \n",
    "    model.add(Dense(units=hidden_layer, \n",
    "                    activation=activation))\n",
    "\n",
    "    model.add(Dense(1, activation=last_activation))\n",
    "\n",
    "    model.compile(loss=loss_function, \n",
    "                  optimizer=Adam\n",
    "                  (lr=lr), \n",
    "                  metrics=['accuracy'])\n",
    "  \n",
    "    return model\n",
    "  \n",
    "model = KerasClassifier(build_fn=telecom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "WWaZIkAQZpyH",
    "outputId": "49adc8fb-0d9f-4df6-da24-25c884e5b5d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  36 tasks      | elapsed: 30.0min\n",
      "[Parallel(n_jobs=-2)]: Done 150 out of 150 | elapsed: 113.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5634 samples\n",
      "Epoch 1/150\n",
      "5634/5634 [==============================] - 3s 466us/sample - loss: 1.0002 - accuracy: 0.7345\n",
      "Epoch 2/150\n",
      "5634/5634 [==============================] - 2s 403us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 3/150\n",
      "5634/5634 [==============================] - 2s 414us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 4/150\n",
      "5634/5634 [==============================] - 2s 406us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 5/150\n",
      "5634/5634 [==============================] - 2s 408us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 6/150\n",
      "5634/5634 [==============================] - 2s 404us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 7/150\n",
      "5634/5634 [==============================] - 2s 419us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 8/150\n",
      "5634/5634 [==============================] - 2s 409us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 9/150\n",
      "5634/5634 [==============================] - 2s 405us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 10/150\n",
      "5634/5634 [==============================] - 2s 414us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 11/150\n",
      "5634/5634 [==============================] - 2s 412us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 12/150\n",
      "5634/5634 [==============================] - 2s 428us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 13/150\n",
      "5634/5634 [==============================] - 2s 411us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 14/150\n",
      "5634/5634 [==============================] - 2s 413us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 15/150\n",
      "5634/5634 [==============================] - 2s 417us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 16/150\n",
      "5634/5634 [==============================] - 2s 426us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 17/150\n",
      "5634/5634 [==============================] - 2s 421us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 18/150\n",
      "5634/5634 [==============================] - 2s 417us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 19/150\n",
      "5634/5634 [==============================] - 2s 417us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 20/150\n",
      "5634/5634 [==============================] - 2s 432us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 21/150\n",
      "5634/5634 [==============================] - 2s 417us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 22/150\n",
      "5634/5634 [==============================] - 2s 435us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 23/150\n",
      "5634/5634 [==============================] - 2s 440us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 24/150\n",
      "5634/5634 [==============================] - 2s 437us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 25/150\n",
      "5634/5634 [==============================] - 2s 429us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 26/150\n",
      "5634/5634 [==============================] - 2s 434us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 27/150\n",
      "5634/5634 [==============================] - 3s 446us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 28/150\n",
      "5634/5634 [==============================] - 2s 432us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 29/150\n",
      "5634/5634 [==============================] - 2s 429us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 30/150\n",
      "5634/5634 [==============================] - 2s 418us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 31/150\n",
      "5634/5634 [==============================] - 3s 463us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 32/150\n",
      "5634/5634 [==============================] - 3s 459us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 33/150\n",
      "5634/5634 [==============================] - 2s 437us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 34/150\n",
      "5634/5634 [==============================] - 2s 443us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 35/150\n",
      "5634/5634 [==============================] - 2s 440us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 36/150\n",
      "5634/5634 [==============================] - 2s 436us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 37/150\n",
      "5634/5634 [==============================] - 2s 428us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 38/150\n",
      "5634/5634 [==============================] - 2s 429us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 39/150\n",
      "5634/5634 [==============================] - 2s 425us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 40/150\n",
      "5634/5634 [==============================] - 2s 440us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 41/150\n",
      "5634/5634 [==============================] - 3s 455us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 42/150\n",
      "5634/5634 [==============================] - 3s 454us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 43/150\n",
      "5634/5634 [==============================] - 3s 453us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 44/150\n",
      "5634/5634 [==============================] - 3s 469us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 45/150\n",
      "5634/5634 [==============================] - 2s 430us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 46/150\n",
      "5634/5634 [==============================] - 2s 427us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 47/150\n",
      "5634/5634 [==============================] - 2s 417us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 48/150\n",
      "5634/5634 [==============================] - 2s 438us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 49/150\n",
      "5634/5634 [==============================] - 2s 422us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 50/150\n",
      "5634/5634 [==============================] - 2s 434us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 51/150\n",
      "5634/5634 [==============================] - 3s 464us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 52/150\n",
      "5634/5634 [==============================] - 3s 454us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 53/150\n",
      "5634/5634 [==============================] - 2s 428us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 54/150\n",
      "5634/5634 [==============================] - 2s 434us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 55/150\n",
      "5634/5634 [==============================] - 3s 464us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 56/150\n",
      "5634/5634 [==============================] - 2s 434us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 57/150\n",
      "5634/5634 [==============================] - 2s 426us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 58/150\n",
      "5634/5634 [==============================] - 2s 443us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 59/150\n",
      "5634/5634 [==============================] - 3s 462us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 60/150\n",
      "5634/5634 [==============================] - 3s 450us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 61/150\n",
      "5634/5634 [==============================] - 2s 436us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 62/150\n",
      "5634/5634 [==============================] - 3s 470us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 63/150\n",
      "5634/5634 [==============================] - 3s 477us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 64/150\n",
      "5634/5634 [==============================] - 3s 466us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 65/150\n",
      "5634/5634 [==============================] - 2s 435us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 66/150\n",
      "5634/5634 [==============================] - 2s 434us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 67/150\n",
      "5634/5634 [==============================] - 3s 464us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 68/150\n",
      "5634/5634 [==============================] - 3s 472us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 69/150\n",
      "5634/5634 [==============================] - 3s 461us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 70/150\n",
      "5634/5634 [==============================] - 3s 446us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 71/150\n",
      "5634/5634 [==============================] - 2s 443us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 72/150\n",
      "5634/5634 [==============================] - 3s 465us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 73/150\n",
      "5634/5634 [==============================] - 3s 454us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 74/150\n",
      "5634/5634 [==============================] - 2s 442us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 75/150\n",
      "5634/5634 [==============================] - 3s 447us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 76/150\n",
      "5634/5634 [==============================] - 3s 470us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 77/150\n",
      "5634/5634 [==============================] - 3s 470us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 78/150\n",
      "5634/5634 [==============================] - 3s 444us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 79/150\n",
      "5634/5634 [==============================] - 2s 443us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 80/150\n",
      "5634/5634 [==============================] - 2s 437us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 81/150\n",
      "5634/5634 [==============================] - 2s 436us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 82/150\n",
      "5634/5634 [==============================] - 2s 433us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 83/150\n",
      "5634/5634 [==============================] - 3s 476us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 84/150\n",
      "5634/5634 [==============================] - 3s 464us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 85/150\n",
      "5634/5634 [==============================] - 3s 463us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 86/150\n",
      "5634/5634 [==============================] - 3s 454us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 87/150\n",
      "5634/5634 [==============================] - 3s 453us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 88/150\n",
      "5634/5634 [==============================] - 3s 446us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 89/150\n",
      "5634/5634 [==============================] - 3s 451us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 90/150\n",
      "5634/5634 [==============================] - 3s 457us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 91/150\n",
      "5634/5634 [==============================] - 3s 448us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 92/150\n",
      "5634/5634 [==============================] - 3s 466us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 93/150\n",
      "5634/5634 [==============================] - 3s 459us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 94/150\n",
      "5634/5634 [==============================] - 3s 458us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 95/150\n",
      "5634/5634 [==============================] - 3s 454us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 96/150\n",
      "5634/5634 [==============================] - 3s 445us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 97/150\n",
      "5634/5634 [==============================] - 2s 439us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 98/150\n",
      "5634/5634 [==============================] - 3s 473us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 99/150\n",
      "5634/5634 [==============================] - 3s 447us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 100/150\n",
      "5634/5634 [==============================] - 3s 451us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 101/150\n",
      "5634/5634 [==============================] - 3s 444us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 102/150\n",
      "5634/5634 [==============================] - 3s 472us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 103/150\n",
      "5634/5634 [==============================] - 2s 429us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 104/150\n",
      "5634/5634 [==============================] - 2s 401us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 105/150\n",
      "5634/5634 [==============================] - 2s 431us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 106/150\n",
      "5634/5634 [==============================] - 3s 478us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 107/150\n",
      "5634/5634 [==============================] - 3s 454us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 108/150\n",
      "5634/5634 [==============================] - 2s 431us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 109/150\n",
      "5634/5634 [==============================] - 2s 430us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 110/150\n",
      "5634/5634 [==============================] - 3s 451us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 111/150\n",
      "5634/5634 [==============================] - 2s 424us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 112/150\n",
      "5634/5634 [==============================] - 2s 432us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 113/150\n",
      "5634/5634 [==============================] - 2s 426us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 114/150\n",
      "5634/5634 [==============================] - 2s 429us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 115/150\n",
      "5634/5634 [==============================] - 2s 435us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 116/150\n",
      "5634/5634 [==============================] - 2s 407us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 117/150\n",
      "5634/5634 [==============================] - 2s 398us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 118/150\n",
      "5634/5634 [==============================] - 2s 404us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 119/150\n",
      "5634/5634 [==============================] - 2s 401us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 120/150\n",
      "5634/5634 [==============================] - 2s 430us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 121/150\n",
      "5634/5634 [==============================] - 2s 435us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 122/150\n",
      "5634/5634 [==============================] - 2s 430us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 123/150\n",
      "5634/5634 [==============================] - 2s 422us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 124/150\n",
      "5634/5634 [==============================] - 2s 426us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 125/150\n",
      "5634/5634 [==============================] - 2s 418us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 126/150\n",
      "5634/5634 [==============================] - 2s 431us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 127/150\n",
      "5634/5634 [==============================] - 2s 434us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 128/150\n",
      "5634/5634 [==============================] - 2s 437us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 129/150\n",
      "5634/5634 [==============================] - 2s 432us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 130/150\n",
      "5634/5634 [==============================] - 2s 431us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 131/150\n",
      "5634/5634 [==============================] - 2s 436us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 132/150\n",
      "5634/5634 [==============================] - 2s 418us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 133/150\n",
      "5634/5634 [==============================] - 2s 426us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 134/150\n",
      "5634/5634 [==============================] - 2s 427us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 135/150\n",
      "5634/5634 [==============================] - 2s 436us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 136/150\n",
      "5634/5634 [==============================] - 2s 438us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 137/150\n",
      "5634/5634 [==============================] - 2s 427us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 138/150\n",
      "5634/5634 [==============================] - 2s 440us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 139/150\n",
      "5634/5634 [==============================] - 2s 423us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 140/150\n",
      "5634/5634 [==============================] - 2s 427us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 141/150\n",
      "5634/5634 [==============================] - 2s 422us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 142/150\n",
      "5634/5634 [==============================] - 2s 431us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 143/150\n",
      "5634/5634 [==============================] - 3s 450us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 144/150\n",
      "5634/5634 [==============================] - 2s 407us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 145/150\n",
      "5634/5634 [==============================] - 3s 460us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 146/150\n",
      "5634/5634 [==============================] - 2s 414us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 147/150\n",
      "5634/5634 [==============================] - 2s 422us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 148/150\n",
      "5634/5634 [==============================] - 2s 419us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 149/150\n",
      "5634/5634 [==============================] - 2s 425us/sample - loss: 1.0000 - accuracy: 0.7348\n",
      "Epoch 150/150\n",
      "5634/5634 [==============================] - 2s 418us/sample - loss: 1.0000 - accuracy: 0.7348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x14606b890>,\n",
       "                   iid='deprecated', n_iter=50, n_jobs=-2,\n",
       "                   param_distributions={'activation': ['relu'],\n",
       "                                        'batch_size': [2, 4],\n",
       "                                        'dropout': [0.1, 0.2, 0.4],\n",
       "                                        'epochs': [50, 100, 150],\n",
       "                                        'first_neuron': [12, 20, 24, 48, 96],\n",
       "                                        'hidden_layer': [6, 10, 12, 24, 48],\n",
       "                                        'last_act...\n",
       "                                        'loss_function': [<function binary_crossentropy at 0x13374ad40>,\n",
       "                                                          <function hinge at 0x13374a9e0>,\n",
       "                                                          <function squared_hinge at 0x13374a950>],\n",
       "                                        'lr': [1, 10, 20],\n",
       "                                        'optimizer': [<class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>],\n",
       "                                        'weight_initializer': ['glorot_uniform']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = RandomizedSearchCV(model,\n",
    "                      params,\n",
    "                      scoring='accuracy',\n",
    "                      n_jobs=-2,\n",
    "                      n_iter=50,\n",
    "                      cv=3,\n",
    "                      verbose=1)\n",
    "\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight_initializer': 'glorot_uniform',\n",
       " 'optimizer': tensorflow.python.keras.optimizer_v2.adam.Adam,\n",
       " 'lr': 10,\n",
       " 'loss_function': <function tensorflow.python.keras.losses.hinge(y_true, y_pred)>,\n",
       " 'last_activation': <function tensorflow.python.keras.activations.sigmoid(x)>,\n",
       " 'hidden_layer': 24,\n",
       " 'first_neuron': 48,\n",
       " 'epochs': 150,\n",
       " 'dropout': 0.1,\n",
       " 'batch_size': 2,\n",
       " 'activation': 'relu'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7362442314518992"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7338537970191625"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "stop = EarlyStopping(monitor='val_accuracy', min_delta=.01, patience=5)\n",
    "\n",
    "def telecom_final(X_train, y_train):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(units=18, \n",
    "                    input_dim=18, activation='relu',\n",
    "                    kernel_initializer='glorot_uniform'\n",
    "                    ))\n",
    "    \n",
    "    model.add(Dense(units=10, \n",
    "                    activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Dense(1, activation=sigmoid))\n",
    "\n",
    "    model.compile(loss=hinge, \n",
    "                  optimizer=Adam\n",
    "                  (lr=.1), \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    out = model.fit(X_train, y_train, epochs=999, batch_size=32, validation_split=.2, callbacks=[stop])\n",
    "  \n",
    "    return model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4507 samples, validate on 1127 samples\n",
      "Epoch 1/999\n",
      "4507/4507 [==============================] - 5s 1ms/sample - loss: 1.0002 - accuracy: 0.7273 - val_loss: 1.0000 - val_accuracy: 0.7471\n",
      "Epoch 2/999\n",
      "4507/4507 [==============================] - 5s 1ms/sample - loss: 1.0000 - accuracy: 0.7273 - val_loss: 1.0000 - val_accuracy: 0.7471\n",
      "Epoch 3/999\n",
      "4507/4507 [==============================] - 4s 996us/sample - loss: 1.0000 - accuracy: 0.7273 - val_loss: 1.0000 - val_accuracy: 0.7471\n",
      "Epoch 4/999\n",
      "4507/4507 [==============================] - 5s 1ms/sample - loss: 1.0000 - accuracy: 0.7273 - val_loss: 1.0000 - val_accuracy: 0.7471\n",
      "Epoch 5/999\n",
      "1368/4507 [========>.....................] - ETA: 2s - loss: 1.0000 - accuracy: 0.7127"
     ]
    }
   ],
   "source": [
    "telecom_final(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfZRtJ7MCN3x"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Try to implement Random Search Hyperparameter Tuning on this dataset\n",
    "- Try to implement Bayesian Optimiation tuning on this dataset using hyperas or hyperopt (if you're brave)\n",
    "- Practice hyperparameter tuning other datasets that we have looked at. How high can you get MNIST? Above 99%?\n",
    "- Study for the Sprint Challenge\n",
    " - Can you implement both perceptron and MLP models from scratch with forward and backpropagation?\n",
    " - Can you implement both perceptron and MLP models in keras and tune their hyperparameters with cross validation?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "U4-S2",
   "language": "python",
   "name": "ds-12-unit-4-sprint-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
