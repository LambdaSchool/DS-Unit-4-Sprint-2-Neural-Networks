{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Perceptron on XOR Gates](#Q2)\n",
    "3. [Multilayer Perceptron](#Q3)\n",
    "4. [Keras MMP](#Q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:** Neurons are what constitute the input, hidden, and output layers of a neural network. Neurons consist of a set of inputs, a set of weights, and an activation function.\n",
    "- **Input Layer:** The input layer is what brings the initial data into the neural network for further processing. It is the very beginning of the neural network pipeline.\n",
    "- **Hidden Layer:** The hidden layer (or layers) is used to transform the inputs into values that are useful for the output layer.\n",
    "- **Output Layer:** The output layer is the last layer in a neural network and produces the desired output for the model.\n",
    "- **Activation:** Activation is achieved when a neuron is activated in a neural net. This is done using an activation function which determines whether a neuron is fired (activated) or not.\n",
    "- **Backpropagation:** Backpropogation is the method in which the neural network adjusts it's parameters after an initial run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perceptron on XOR Gates <a id=\"Q3=2\"></a>\n",
    "\n",
    "Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "|x1\t|x2|x3|\ty|\n",
    "|---|---|---|---|\n",
    "1|\t1|\t1|\t1|\n",
    "1|\t0|\t1|\t0|\n",
    "0|\t1|\t1|\t0|\n",
    "0|\t0|\t1|\t0|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(812)\n",
    "\n",
    "inputs = np.array([\n",
    "    [1,1,1],\n",
    "    [1,0,1],\n",
    "    [0,1,1],\n",
    "    [0,0,1]\n",
    "])\n",
    "\n",
    "ground_truth = [[1], [0], [0], [0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivate(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0099616 ],\n",
       "       [ 0.21185521],\n",
       "       [-0.08502562]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = 2 *np.random.random((3,1)) -1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training\n",
      "[[  7.20886354]\n",
      " [  7.20886371]\n",
      " [-11.10146556]]\n",
      "Output after training\n",
      "[[9.64948649e-01]\n",
      " [2.00042945e-02]\n",
      " [2.00042978e-02]\n",
      " [1.51353405e-05]]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(1000):\n",
    "    \n",
    "    # Weighted sum of inputs/weights\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    #print(weighted_sum)\n",
    "    \n",
    "    # Activate!\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    #print(activated_output)\n",
    "    \n",
    "    # Calculate the error\n",
    "    error = ground_truth - activated_output\n",
    "    \n",
    "    # Adjustments\n",
    "    adjustments = error * sigmoid_derivate(activated_output)\n",
    "    \n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "    #print(weights)\n",
    "    \n",
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights.\n",
    "Your network must have one hidden layer.\n",
    "You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "Train your model on the Heart Disease dataset from UCI:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "num_transform = StandardScaler()\n",
    "\n",
    "import category_encoders as ce\n",
    "cat_transform = ce.one_hot.OneHotEncoder(verbose=0, cols=None, drop_invariant=False, return_df=True, handle_missing='value', handle_unknown='value', use_cat_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create class NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = 30\n",
    "        self.hiddenNodes = 5\n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        # Initial Weights \n",
    "        # 28x28 Matrix Array for the First Layer\n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)\n",
    "        # 28x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "        \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        \"\"\"\n",
    "        \n",
    "        #Weighted sume of inputs and hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        #Acivations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        #Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        \n",
    "        self.o_error = y - o #error in output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o) # apply derivative of sigmoid to error\n",
    "        \n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T) # z2 error: how much our hidden layer weights were off\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        self.weights1 += X.T.dot(self.z2_delta) #Adjust first set (input => hidden) weights\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta) #adjust second set (hidden => output) weights\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age          41\n",
       "sex           2\n",
       "cp            4\n",
       "trestbps     49\n",
       "chol        152\n",
       "fbs           2\n",
       "restecg       3\n",
       "thalach      91\n",
       "exang         2\n",
       "oldpeak      40\n",
       "slope         3\n",
       "ca            5\n",
       "thal          4\n",
       "target        2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform numerical and categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating numerical and categorical variables\n",
    "df_cat = df[['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']]\n",
    "df_num = df[['age', 'trestbps', 'chol', 'thalach', 'oldpeak']]\n",
    "y = df[['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming categorical variables into strings\n",
    "df_cat = df_cat.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "# One hot encode categoricals\n",
    "df_cat = cat_transform.fit_transform(df_cat)\n",
    "\n",
    "# Standard scale numericals\n",
    "df_num = num_transform.fit_transform(df_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((303, 25), (303, 5))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify shape of dataframes for merge\n",
    "df_cat.shape, df_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform df_num into dataframe for merge\n",
    "df_num = pd.DataFrame(df_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Dataframes\n",
    "X = df_cat.merge(df_num, how='outer', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 30)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of X\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate neural network class\n",
    "nn= NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Actual Output: \n",
      "      target\n",
      "0         1\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         1\n",
      "5         1\n",
      "6         1\n",
      "7         1\n",
      "8         1\n",
      "9         1\n",
      "10        1\n",
      "11        1\n",
      "12        1\n",
      "13        1\n",
      "14        1\n",
      "15        1\n",
      "16        1\n",
      "17        1\n",
      "18        1\n",
      "19        1\n",
      "20        1\n",
      "21        1\n",
      "22        1\n",
      "23        1\n",
      "24        1\n",
      "25        1\n",
      "26        1\n",
      "27        1\n",
      "28        1\n",
      "29        1\n",
      "..      ...\n",
      "273       0\n",
      "274       0\n",
      "275       0\n",
      "276       0\n",
      "277       0\n",
      "278       0\n",
      "279       0\n",
      "280       0\n",
      "281       0\n",
      "282       0\n",
      "283       0\n",
      "284       0\n",
      "285       0\n",
      "286       0\n",
      "287       0\n",
      "288       0\n",
      "289       0\n",
      "290       0\n",
      "291       0\n",
      "292       0\n",
      "293       0\n",
      "294       0\n",
      "295       0\n",
      "296       0\n",
      "297       0\n",
      "298       0\n",
      "299       0\n",
      "300       0\n",
      "301       0\n",
      "302       0\n",
      "\n",
      "[303 rows x 1 columns]\n",
      "Predicted Output: \n",
      " [[0.99995674]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99993956]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.98723591]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.66682821]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99993925]\n",
      " [0.99999892]\n",
      " [0.99046508]\n",
      " [0.99499259]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99742536]\n",
      " [0.99813137]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99985965]\n",
      " [0.99999999]\n",
      " [0.9977542 ]\n",
      " [0.99995626]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999946]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.66666453]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99934931]\n",
      " [0.50000259]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99733808]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999997]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99794547]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999996]\n",
      " [0.99999514]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99881259]\n",
      " [0.99999999]\n",
      " [0.98930044]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99833988]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.66682779]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.99999998]\n",
      " [0.99062703]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99234038]\n",
      " [0.99772588]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99788207]\n",
      " [0.98702046]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99951486]\n",
      " [0.99999998]\n",
      " [0.99947589]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99608453]\n",
      " [1.        ]\n",
      " [0.99999944]\n",
      " [0.5       ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.998992  ]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99317477]\n",
      " [0.5       ]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99840471]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99807054]\n",
      " [0.99993956]\n",
      " [0.99688133]\n",
      " [0.99999935]\n",
      " [0.99999998]\n",
      " [0.9923306 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99932919]\n",
      " [0.66670422]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000367]\n",
      " [0.50008154]\n",
      " [0.50011404]\n",
      " [0.50000005]\n",
      " [0.5       ]\n",
      " [0.50006282]\n",
      " [0.5       ]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.99999998]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000058]\n",
      " [0.5000022 ]\n",
      " [0.5       ]\n",
      " [0.50000094]\n",
      " [0.66706828]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000332]\n",
      " [0.5       ]\n",
      " [0.50004897]\n",
      " [0.5       ]\n",
      " [0.50000116]\n",
      " [0.50000197]\n",
      " [0.5       ]\n",
      " [0.50001229]\n",
      " [0.50003974]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000329]\n",
      " [0.50001604]\n",
      " [0.50002412]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000107]\n",
      " [0.50000043]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50012511]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50019095]\n",
      " [0.50000028]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50021821]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50001391]\n",
      " [0.5       ]\n",
      " [0.50000002]\n",
      " [0.50003465]\n",
      " [0.5000008 ]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000001]\n",
      " [0.5       ]\n",
      " [0.50000005]\n",
      " [0.50000014]\n",
      " [0.5       ]\n",
      " [0.50005792]\n",
      " [0.50000798]\n",
      " [0.50000515]\n",
      " [0.5       ]\n",
      " [0.50008271]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50001356]\n",
      " [0.5       ]\n",
      " [0.50000011]\n",
      " [0.50000664]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000003]\n",
      " [0.5       ]\n",
      " [0.50074932]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000002]\n",
      " [0.50001862]\n",
      " [0.50001569]\n",
      " [0.50000002]\n",
      " [0.50000609]\n",
      " [0.5       ]\n",
      " [0.50109823]\n",
      " [0.5       ]\n",
      " [0.50000007]\n",
      " [0.5       ]\n",
      " [0.50053541]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000002]\n",
      " [0.50000123]\n",
      " [0.5000268 ]\n",
      " [0.50000114]\n",
      " [0.5       ]\n",
      " [0.50000002]\n",
      " [0.5       ]\n",
      " [0.5000009 ]\n",
      " [0.99999998]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50005923]\n",
      " [0.50002955]\n",
      " [0.50088907]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50007406]\n",
      " [0.50003997]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5000004 ]\n",
      " [0.50000003]\n",
      " [0.50000002]\n",
      " [0.50003069]\n",
      " [0.50001984]\n",
      " [0.5       ]\n",
      " [0.66719675]\n",
      " [0.50000862]\n",
      " [0.5       ]\n",
      " [0.50004318]\n",
      " [0.50001139]\n",
      " [0.50000001]\n",
      " [0.50000004]]\n",
      "Loss: \n",
      " target    0.129835\n",
      "dtype: float64\n",
      "+---------EPOCH 250---------+\n",
      "Actual Output: \n",
      "      target\n",
      "0         1\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         1\n",
      "5         1\n",
      "6         1\n",
      "7         1\n",
      "8         1\n",
      "9         1\n",
      "10        1\n",
      "11        1\n",
      "12        1\n",
      "13        1\n",
      "14        1\n",
      "15        1\n",
      "16        1\n",
      "17        1\n",
      "18        1\n",
      "19        1\n",
      "20        1\n",
      "21        1\n",
      "22        1\n",
      "23        1\n",
      "24        1\n",
      "25        1\n",
      "26        1\n",
      "27        1\n",
      "28        1\n",
      "29        1\n",
      "..      ...\n",
      "273       0\n",
      "274       0\n",
      "275       0\n",
      "276       0\n",
      "277       0\n",
      "278       0\n",
      "279       0\n",
      "280       0\n",
      "281       0\n",
      "282       0\n",
      "283       0\n",
      "284       0\n",
      "285       0\n",
      "286       0\n",
      "287       0\n",
      "288       0\n",
      "289       0\n",
      "290       0\n",
      "291       0\n",
      "292       0\n",
      "293       0\n",
      "294       0\n",
      "295       0\n",
      "296       0\n",
      "297       0\n",
      "298       0\n",
      "299       0\n",
      "300       0\n",
      "301       0\n",
      "302       0\n",
      "\n",
      "[303 rows x 1 columns]\n",
      "Predicted Output: \n",
      " [[0.99995693]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99994005]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.98731065]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.66682547]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99993938]\n",
      " [0.99999894]\n",
      " [0.990518  ]\n",
      " [0.99502378]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99743015]\n",
      " [0.99814003]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99986108]\n",
      " [0.99999999]\n",
      " [0.99775924]\n",
      " [0.9999563 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999946]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.66666257]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99927034]\n",
      " [0.50000256]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99734136]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999997]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99795384]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999996]\n",
      " [0.99999515]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99881666]\n",
      " [0.99999999]\n",
      " [0.98936365]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99834241]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.66682505]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.99999998]\n",
      " [0.99067681]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.9923904 ]\n",
      " [0.99772946]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99788497]\n",
      " [0.98709031]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99951544]\n",
      " [0.99999998]\n",
      " [0.99947746]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99610628]\n",
      " [1.        ]\n",
      " [0.99999945]\n",
      " [0.5       ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.998993  ]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99321348]\n",
      " [0.5       ]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99840706]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99807319]\n",
      " [0.99994006]\n",
      " [0.99688678]\n",
      " [0.99999929]\n",
      " [0.99999998]\n",
      " [0.99237609]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99932197]\n",
      " [0.66670201]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000366]\n",
      " [0.50008093]\n",
      " [0.50011341]\n",
      " [0.50000005]\n",
      " [0.5       ]\n",
      " [0.50006253]\n",
      " [0.5       ]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.99999998]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000056]\n",
      " [0.50000219]\n",
      " [0.5       ]\n",
      " [0.50000093]\n",
      " [0.66706308]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000326]\n",
      " [0.5       ]\n",
      " [0.50004839]\n",
      " [0.5       ]\n",
      " [0.50000115]\n",
      " [0.50000193]\n",
      " [0.5       ]\n",
      " [0.50001219]\n",
      " [0.50003952]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000328]\n",
      " [0.50001599]\n",
      " [0.50002403]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000105]\n",
      " [0.50000042]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50012376]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50018947]\n",
      " [0.50000028]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50021609]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50001387]\n",
      " [0.5       ]\n",
      " [0.50000002]\n",
      " [0.50003454]\n",
      " [0.50000079]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000001]\n",
      " [0.5       ]\n",
      " [0.50000005]\n",
      " [0.50000014]\n",
      " [0.5       ]\n",
      " [0.50005791]\n",
      " [0.50000797]\n",
      " [0.50000513]\n",
      " [0.5       ]\n",
      " [0.50008185]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50001339]\n",
      " [0.5       ]\n",
      " [0.50000011]\n",
      " [0.50000663]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000003]\n",
      " [0.5       ]\n",
      " [0.50074151]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000002]\n",
      " [0.50001842]\n",
      " [0.50001563]\n",
      " [0.50000002]\n",
      " [0.50000608]\n",
      " [0.5       ]\n",
      " [0.50108632]\n",
      " [0.5       ]\n",
      " [0.50000007]\n",
      " [0.5       ]\n",
      " [0.50052996]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000002]\n",
      " [0.50000121]\n",
      " [0.50002649]\n",
      " [0.50000112]\n",
      " [0.5       ]\n",
      " [0.50000002]\n",
      " [0.5       ]\n",
      " [0.50000089]\n",
      " [0.99999998]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50005898]\n",
      " [0.50002937]\n",
      " [0.50087969]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50007365]\n",
      " [0.50003984]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000039]\n",
      " [0.50000003]\n",
      " [0.50000002]\n",
      " [0.50003044]\n",
      " [0.50001968]\n",
      " [0.5       ]\n",
      " [0.66719096]\n",
      " [0.5000086 ]\n",
      " [0.5       ]\n",
      " [0.5000428 ]\n",
      " [0.5000112 ]\n",
      " [0.50000001]\n",
      " [0.50000004]]\n",
      "Loss: \n",
      " target    0.129835\n",
      "dtype: float64\n",
      "+---------EPOCH 500---------+\n",
      "Actual Output: \n",
      "      target\n",
      "0         1\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         1\n",
      "5         1\n",
      "6         1\n",
      "7         1\n",
      "8         1\n",
      "9         1\n",
      "10        1\n",
      "11        1\n",
      "12        1\n",
      "13        1\n",
      "14        1\n",
      "15        1\n",
      "16        1\n",
      "17        1\n",
      "18        1\n",
      "19        1\n",
      "20        1\n",
      "21        1\n",
      "22        1\n",
      "23        1\n",
      "24        1\n",
      "25        1\n",
      "26        1\n",
      "27        1\n",
      "28        1\n",
      "29        1\n",
      "..      ...\n",
      "273       0\n",
      "274       0\n",
      "275       0\n",
      "276       0\n",
      "277       0\n",
      "278       0\n",
      "279       0\n",
      "280       0\n",
      "281       0\n",
      "282       0\n",
      "283       0\n",
      "284       0\n",
      "285       0\n",
      "286       0\n",
      "287       0\n",
      "288       0\n",
      "289       0\n",
      "290       0\n",
      "291       0\n",
      "292       0\n",
      "293       0\n",
      "294       0\n",
      "295       0\n",
      "296       0\n",
      "297       0\n",
      "298       0\n",
      "299       0\n",
      "300       0\n",
      "301       0\n",
      "302       0\n",
      "\n",
      "[303 rows x 1 columns]\n",
      "Predicted Output: \n",
      " [[0.99995711]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99994054]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.98738515]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.66682278]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.9999395 ]\n",
      " [0.99999897]\n",
      " [0.99056983]\n",
      " [0.99505458]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99743491]\n",
      " [0.99814966]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.9998625 ]\n",
      " [0.99999999]\n",
      " [0.99776422]\n",
      " [0.99995634]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999946]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.66666067]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.9991845 ]\n",
      " [0.50000252]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99734469]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999997]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99796222]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999996]\n",
      " [0.99999516]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99882071]\n",
      " [0.99999999]\n",
      " [0.98942661]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99834497]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.66682237]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.99999998]\n",
      " [0.99072619]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99243952]\n",
      " [0.99773304]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99788787]\n",
      " [0.98715953]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99951603]\n",
      " [0.99999998]\n",
      " [0.99947911]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99612779]\n",
      " [1.        ]\n",
      " [0.99999946]\n",
      " [0.5       ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.998994  ]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.9932517 ]\n",
      " [0.5       ]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.9984094 ]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99807584]\n",
      " [0.99994055]\n",
      " [0.99689222]\n",
      " [0.99999922]\n",
      " [0.99999998]\n",
      " [0.99242097]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99931464]\n",
      " [0.66669985]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000365]\n",
      " [0.50008033]\n",
      " [0.5001128 ]\n",
      " [0.50000005]\n",
      " [0.5       ]\n",
      " [0.50006223]\n",
      " [0.5       ]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.99999998]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000055]\n",
      " [0.50000218]\n",
      " [0.5       ]\n",
      " [0.50000093]\n",
      " [0.667058  ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5000032 ]\n",
      " [0.5       ]\n",
      " [0.50004782]\n",
      " [0.5       ]\n",
      " [0.50000115]\n",
      " [0.5000019 ]\n",
      " [0.5       ]\n",
      " [0.50001209]\n",
      " [0.5000393 ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000327]\n",
      " [0.50001594]\n",
      " [0.50002394]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000103]\n",
      " [0.50000042]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50012244]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.500188  ]\n",
      " [0.50000028]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50021404]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50001383]\n",
      " [0.5       ]\n",
      " [0.50000002]\n",
      " [0.50003444]\n",
      " [0.50000079]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000001]\n",
      " [0.5       ]\n",
      " [0.50000005]\n",
      " [0.50000013]\n",
      " [0.5       ]\n",
      " [0.50005792]\n",
      " [0.50000796]\n",
      " [0.50000512]\n",
      " [0.5       ]\n",
      " [0.500081  ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50001322]\n",
      " [0.5       ]\n",
      " [0.50000011]\n",
      " [0.50000661]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000003]\n",
      " [0.5       ]\n",
      " [0.50073388]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000002]\n",
      " [0.50001822]\n",
      " [0.50001557]\n",
      " [0.50000002]\n",
      " [0.50000606]\n",
      " [0.5       ]\n",
      " [0.50107466]\n",
      " [0.5       ]\n",
      " [0.50000007]\n",
      " [0.5       ]\n",
      " [0.50052458]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000002]\n",
      " [0.50000119]\n",
      " [0.50002618]\n",
      " [0.50000111]\n",
      " [0.5       ]\n",
      " [0.50000002]\n",
      " [0.5       ]\n",
      " [0.50000088]\n",
      " [0.99999998]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50005873]\n",
      " [0.50002918]\n",
      " [0.50087041]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50007324]\n",
      " [0.50003971]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000039]\n",
      " [0.50000003]\n",
      " [0.50000002]\n",
      " [0.50003018]\n",
      " [0.50001952]\n",
      " [0.5       ]\n",
      " [0.66718527]\n",
      " [0.50000857]\n",
      " [0.5       ]\n",
      " [0.50004242]\n",
      " [0.50001103]\n",
      " [0.50000001]\n",
      " [0.50000003]]\n",
      "Loss: \n",
      " target    0.129835\n",
      "dtype: float64\n",
      "+---------EPOCH 750---------+\n",
      "Actual Output: \n",
      "      target\n",
      "0         1\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         1\n",
      "5         1\n",
      "6         1\n",
      "7         1\n",
      "8         1\n",
      "9         1\n",
      "10        1\n",
      "11        1\n",
      "12        1\n",
      "13        1\n",
      "14        1\n",
      "15        1\n",
      "16        1\n",
      "17        1\n",
      "18        1\n",
      "19        1\n",
      "20        1\n",
      "21        1\n",
      "22        1\n",
      "23        1\n",
      "24        1\n",
      "25        1\n",
      "26        1\n",
      "27        1\n",
      "28        1\n",
      "29        1\n",
      "..      ...\n",
      "273       0\n",
      "274       0\n",
      "275       0\n",
      "276       0\n",
      "277       0\n",
      "278       0\n",
      "279       0\n",
      "280       0\n",
      "281       0\n",
      "282       0\n",
      "283       0\n",
      "284       0\n",
      "285       0\n",
      "286       0\n",
      "287       0\n",
      "288       0\n",
      "289       0\n",
      "290       0\n",
      "291       0\n",
      "292       0\n",
      "293       0\n",
      "294       0\n",
      "295       0\n",
      "296       0\n",
      "297       0\n",
      "298       0\n",
      "299       0\n",
      "300       0\n",
      "301       0\n",
      "302       0\n",
      "\n",
      "[303 rows x 1 columns]\n",
      "Predicted Output: \n",
      " [[0.99995729]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99994102]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.98745923]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.66682017]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99993963]\n",
      " [0.999999  ]\n",
      " [0.99062031]\n",
      " [0.99508488]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99743963]\n",
      " [0.99816025]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99986388]\n",
      " [0.99999999]\n",
      " [0.99776912]\n",
      " [0.99995638]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999946]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.66665883]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99909216]\n",
      " [0.50000249]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99734804]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99797059]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999996]\n",
      " [0.99999516]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.9988247 ]\n",
      " [0.99999999]\n",
      " [0.98948918]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99834754]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.66681977]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.99999998]\n",
      " [0.99077504]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99248754]\n",
      " [0.99773661]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99789077]\n",
      " [0.98722787]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99951662]\n",
      " [0.99999998]\n",
      " [0.99948083]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99614899]\n",
      " [1.        ]\n",
      " [0.99999947]\n",
      " [0.5       ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99899499]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99328929]\n",
      " [0.5       ]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99841173]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99807849]\n",
      " [0.99994103]\n",
      " [0.99689763]\n",
      " [0.99999915]\n",
      " [0.99999998]\n",
      " [0.99246509]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99930724]\n",
      " [0.66669777]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000364]\n",
      " [0.50007974]\n",
      " [0.50011222]\n",
      " [0.50000005]\n",
      " [0.5       ]\n",
      " [0.50006194]\n",
      " [0.5       ]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.99999998]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000053]\n",
      " [0.50000217]\n",
      " [0.5       ]\n",
      " [0.50000093]\n",
      " [0.66705304]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000315]\n",
      " [0.5       ]\n",
      " [0.50004726]\n",
      " [0.5       ]\n",
      " [0.50000114]\n",
      " [0.50000186]\n",
      " [0.5       ]\n",
      " [0.50001199]\n",
      " [0.50003908]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000326]\n",
      " [0.50001589]\n",
      " [0.50002385]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000101]\n",
      " [0.50000042]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50012115]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50018654]\n",
      " [0.50000028]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50021206]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50001378]\n",
      " [0.5       ]\n",
      " [0.50000002]\n",
      " [0.50003433]\n",
      " [0.50000079]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000001]\n",
      " [0.5       ]\n",
      " [0.50000005]\n",
      " [0.50000012]\n",
      " [0.5       ]\n",
      " [0.50005792]\n",
      " [0.50000795]\n",
      " [0.5000051 ]\n",
      " [0.5       ]\n",
      " [0.50008017]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50001306]\n",
      " [0.5       ]\n",
      " [0.50000011]\n",
      " [0.50000659]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000003]\n",
      " [0.5       ]\n",
      " [0.50072648]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000002]\n",
      " [0.50001803]\n",
      " [0.50001552]\n",
      " [0.50000001]\n",
      " [0.50000604]\n",
      " [0.5       ]\n",
      " [0.50106329]\n",
      " [0.5       ]\n",
      " [0.50000007]\n",
      " [0.5       ]\n",
      " [0.50051927]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000001]\n",
      " [0.50000117]\n",
      " [0.50002588]\n",
      " [0.5000011 ]\n",
      " [0.5       ]\n",
      " [0.50000002]\n",
      " [0.5       ]\n",
      " [0.50000087]\n",
      " [0.99999998]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50005846]\n",
      " [0.500029  ]\n",
      " [0.50086126]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50007282]\n",
      " [0.50003958]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000039]\n",
      " [0.50000003]\n",
      " [0.50000002]\n",
      " [0.50002994]\n",
      " [0.50001937]\n",
      " [0.5       ]\n",
      " [0.6671797 ]\n",
      " [0.50000855]\n",
      " [0.5       ]\n",
      " [0.50004205]\n",
      " [0.50001086]\n",
      " [0.50000001]\n",
      " [0.50000003]]\n",
      "Loss: \n",
      " target    0.129835\n",
      "dtype: float64\n",
      "+---------EPOCH 1000---------+\n",
      "Actual Output: \n",
      "      target\n",
      "0         1\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         1\n",
      "5         1\n",
      "6         1\n",
      "7         1\n",
      "8         1\n",
      "9         1\n",
      "10        1\n",
      "11        1\n",
      "12        1\n",
      "13        1\n",
      "14        1\n",
      "15        1\n",
      "16        1\n",
      "17        1\n",
      "18        1\n",
      "19        1\n",
      "20        1\n",
      "21        1\n",
      "22        1\n",
      "23        1\n",
      "24        1\n",
      "25        1\n",
      "26        1\n",
      "27        1\n",
      "28        1\n",
      "29        1\n",
      "..      ...\n",
      "273       0\n",
      "274       0\n",
      "275       0\n",
      "276       0\n",
      "277       0\n",
      "278       0\n",
      "279       0\n",
      "280       0\n",
      "281       0\n",
      "282       0\n",
      "283       0\n",
      "284       0\n",
      "285       0\n",
      "286       0\n",
      "287       0\n",
      "288       0\n",
      "289       0\n",
      "290       0\n",
      "291       0\n",
      "292       0\n",
      "293       0\n",
      "294       0\n",
      "295       0\n",
      "296       0\n",
      "297       0\n",
      "298       0\n",
      "299       0\n",
      "300       0\n",
      "301       0\n",
      "302       0\n",
      "\n",
      "[303 rows x 1 columns]\n",
      "Predicted Output: \n",
      " [[0.99995747]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.9999415 ]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.98753301]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.66681762]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99993975]\n",
      " [0.99999902]\n",
      " [0.99066941]\n",
      " [0.99511471]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99744432]\n",
      " [0.99817181]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99986524]\n",
      " [0.99999999]\n",
      " [0.99777395]\n",
      " [0.99995642]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999946]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.66665704]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99899352]\n",
      " [0.50000246]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99735142]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99797893]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999996]\n",
      " [0.99999517]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99882865]\n",
      " [0.99999999]\n",
      " [0.98955145]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99999998]\n",
      " [0.99835012]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.66681723]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.99999998]\n",
      " [0.99082339]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99253446]\n",
      " [0.99774017]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99789367]\n",
      " [0.98729538]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [0.99951721]\n",
      " [0.99999998]\n",
      " [0.99948261]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99616987]\n",
      " [1.        ]\n",
      " [0.99999948]\n",
      " [0.5       ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99899599]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [0.99332628]\n",
      " [0.5       ]\n",
      " [1.        ]\n",
      " [0.99999998]\n",
      " [0.99999999]\n",
      " [0.99999998]\n",
      " [1.        ]\n",
      " [0.99841405]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99808115]\n",
      " [0.9999415 ]\n",
      " [0.99690301]\n",
      " [0.99999908]\n",
      " [0.99999998]\n",
      " [0.99250847]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99929977]\n",
      " [0.66669574]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [0.99999999]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000362]\n",
      " [0.50007917]\n",
      " [0.50011166]\n",
      " [0.50000005]\n",
      " [0.5       ]\n",
      " [0.50006166]\n",
      " [0.5       ]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.99999998]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000052]\n",
      " [0.50000216]\n",
      " [0.5       ]\n",
      " [0.50000093]\n",
      " [0.66704821]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5000031 ]\n",
      " [0.5       ]\n",
      " [0.50004672]\n",
      " [0.5       ]\n",
      " [0.50000113]\n",
      " [0.50000183]\n",
      " [0.5       ]\n",
      " [0.5000119 ]\n",
      " [0.50003887]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000324]\n",
      " [0.50001584]\n",
      " [0.50002377]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000099]\n",
      " [0.50000042]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50011988]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50018511]\n",
      " [0.50000028]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50021016]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50001374]\n",
      " [0.5       ]\n",
      " [0.50000002]\n",
      " [0.50003422]\n",
      " [0.50000079]\n",
      " [1.        ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000001]\n",
      " [0.5       ]\n",
      " [0.50000005]\n",
      " [0.50000012]\n",
      " [0.5       ]\n",
      " [0.50005794]\n",
      " [0.50000794]\n",
      " [0.50000508]\n",
      " [0.5       ]\n",
      " [0.50007936]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5000129 ]\n",
      " [0.5       ]\n",
      " [0.50000011]\n",
      " [0.50000657]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000003]\n",
      " [0.5       ]\n",
      " [0.50071931]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000002]\n",
      " [0.50001784]\n",
      " [0.50001546]\n",
      " [0.50000001]\n",
      " [0.50000603]\n",
      " [0.5       ]\n",
      " [0.50105221]\n",
      " [0.5       ]\n",
      " [0.50000007]\n",
      " [0.5       ]\n",
      " [0.50051405]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000001]\n",
      " [0.50000115]\n",
      " [0.50002559]\n",
      " [0.50000108]\n",
      " [0.5       ]\n",
      " [0.50000002]\n",
      " [0.5       ]\n",
      " [0.50000087]\n",
      " [0.99999998]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50005818]\n",
      " [0.50002881]\n",
      " [0.50085222]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50007241]\n",
      " [0.50003945]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000038]\n",
      " [0.50000003]\n",
      " [0.50000002]\n",
      " [0.5000297 ]\n",
      " [0.50001922]\n",
      " [0.5       ]\n",
      " [0.66717424]\n",
      " [0.50000853]\n",
      " [0.5       ]\n",
      " [0.50004168]\n",
      " [0.50001069]\n",
      " [0.50000001]\n",
      " [0.50000003]]\n",
      "Loss: \n",
      " target    0.129834\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Loss consistently decreases with each iteration!\n",
    "\n",
    "for i in range(1000):\n",
    "    if (i+1 in [1]) or ((i+1) % 250 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        #print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Keras MMP <a id=\"Q4\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Keras modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Reshape, Conv2D, AveragePooling2D, Flatten\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(keras.layers.Dense(12, input_dim=30, activation='relu'))\n",
    "    model.add(keras.layers.Dense(12, activation='relu'))\n",
    "    model.add(keras.layers.Dense(12, activation='relu'))\n",
    "    model.add(keras.layers.Dense(12, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "202/202 [==============================] - 2s 8ms/sample - loss: 0.6521 - acc: 0.6881\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 271us/sample - loss: 0.6081 - acc: 0.7574\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 286us/sample - loss: 0.5706 - acc: 0.7525\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 339us/sample - loss: 0.5376 - acc: 0.7673\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 289us/sample - loss: 0.5120 - acc: 0.7772\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 342us/sample - loss: 0.4888 - acc: 0.8069\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 520us/sample - loss: 0.4672 - acc: 0.8267\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 363us/sample - loss: 0.4488 - acc: 0.8317\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 344us/sample - loss: 0.4304 - acc: 0.8317\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 296us/sample - loss: 0.4110 - acc: 0.8416\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 267us/sample - loss: 0.3897 - acc: 0.8515\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 287us/sample - loss: 0.3722 - acc: 0.8465\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 275us/sample - loss: 0.3588 - acc: 0.8663\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 253us/sample - loss: 0.3450 - acc: 0.8812\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 267us/sample - loss: 0.3334 - acc: 0.8812\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 282us/sample - loss: 0.3252 - acc: 0.8762\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 287us/sample - loss: 0.3155 - acc: 0.8762\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 287us/sample - loss: 0.3088 - acc: 0.8911\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 293us/sample - loss: 0.3027 - acc: 0.8861\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 274us/sample - loss: 0.2957 - acc: 0.8812\n",
      "101/101 [==============================] - 1s 14ms/sample - loss: 0.8211 - acc: 0.6238\n",
      "202/202 [==============================] - 0s 228us/sample - loss: 0.2895 - acc: 0.8861\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 7ms/sample - loss: 0.6790 - acc: 0.5594\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 381us/sample - loss: 0.6585 - acc: 0.6436\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 424us/sample - loss: 0.6384 - acc: 0.6931\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 479us/sample - loss: 0.6179 - acc: 0.7327\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 465us/sample - loss: 0.5935 - acc: 0.7475\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 349us/sample - loss: 0.5670 - acc: 0.7723\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 882us/sample - loss: 0.5381 - acc: 0.7822\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 367us/sample - loss: 0.5083 - acc: 0.7871\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 309us/sample - loss: 0.4831 - acc: 0.7772\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 262us/sample - loss: 0.4593 - acc: 0.7871\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 333us/sample - loss: 0.4362 - acc: 0.7970\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 321us/sample - loss: 0.4178 - acc: 0.7970\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 659us/sample - loss: 0.3992 - acc: 0.8168\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 311us/sample - loss: 0.3859 - acc: 0.8168\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 349us/sample - loss: 0.3786 - acc: 0.8069\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 395us/sample - loss: 0.3678 - acc: 0.8218\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 328us/sample - loss: 0.3570 - acc: 0.8267\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 377us/sample - loss: 0.3481 - acc: 0.8267\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 280us/sample - loss: 0.3377 - acc: 0.8317\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 329us/sample - loss: 0.3271 - acc: 0.8564\n",
      "101/101 [==============================] - 1s 14ms/sample - loss: 0.4361 - acc: 0.8317\n",
      "202/202 [==============================] - 0s 230us/sample - loss: 0.3209 - acc: 0.8663\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 7ms/sample - loss: 0.6855 - acc: 0.5545\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 419us/sample - loss: 0.6567 - acc: 0.8168\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 311us/sample - loss: 0.6205 - acc: 0.8168\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 333us/sample - loss: 0.5727 - acc: 0.8168\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 350us/sample - loss: 0.5173 - acc: 0.8168\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 313us/sample - loss: 0.4655 - acc: 0.8168\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 418us/sample - loss: 0.4270 - acc: 0.8168\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 324us/sample - loss: 0.4043 - acc: 0.8168\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 295us/sample - loss: 0.3907 - acc: 0.8168\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 294us/sample - loss: 0.3782 - acc: 0.8168\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 326us/sample - loss: 0.3681 - acc: 0.8168\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 292us/sample - loss: 0.3591 - acc: 0.8168\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 315us/sample - loss: 0.3499 - acc: 0.8168\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 335us/sample - loss: 0.3418 - acc: 0.8168\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 282us/sample - loss: 0.3330 - acc: 0.8168\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 299us/sample - loss: 0.3244 - acc: 0.8168\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 285us/sample - loss: 0.3172 - acc: 0.8168\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 315us/sample - loss: 0.3106 - acc: 0.8168\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 405us/sample - loss: 0.3041 - acc: 0.8168\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 310us/sample - loss: 0.2979 - acc: 0.8168\n",
      "101/101 [==============================] - 1s 12ms/sample - loss: 1.4284 - acc: 0.0000e+00\n",
      "202/202 [==============================] - 0s 276us/sample - loss: 0.2930 - acc: 0.8168\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 7ms/sample - loss: 0.6765 - acc: 0.6931\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 115us/sample - loss: 0.6675 - acc: 0.7228\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 108us/sample - loss: 0.6588 - acc: 0.7327\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 132us/sample - loss: 0.6494 - acc: 0.7277\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.6402 - acc: 0.7327\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 151us/sample - loss: 0.6310 - acc: 0.7228\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 129us/sample - loss: 0.6209 - acc: 0.7228\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 114us/sample - loss: 0.6100 - acc: 0.7228\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 148us/sample - loss: 0.5981 - acc: 0.7228\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 135us/sample - loss: 0.5857 - acc: 0.7178\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 147us/sample - loss: 0.5725 - acc: 0.7178\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 151us/sample - loss: 0.5584 - acc: 0.7178\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 91us/sample - loss: 0.5443 - acc: 0.7178\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 104us/sample - loss: 0.5297 - acc: 0.7327\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 105us/sample - loss: 0.5136 - acc: 0.7376\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 103us/sample - loss: 0.4972 - acc: 0.7574\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 117us/sample - loss: 0.4812 - acc: 0.7822\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 174us/sample - loss: 0.4661 - acc: 0.7921\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 152us/sample - loss: 0.4515 - acc: 0.8069\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 142us/sample - loss: 0.4382 - acc: 0.8267\n",
      "101/101 [==============================] - 1s 12ms/sample - loss: 0.8952 - acc: 0.3663\n",
      "202/202 [==============================] - 0s 105us/sample - loss: 0.4294 - acc: 0.8416\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 2s 9ms/sample - loss: 0.6609 - acc: 0.7129\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 127us/sample - loss: 0.6457 - acc: 0.7673\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 130us/sample - loss: 0.6316 - acc: 0.7673\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 122us/sample - loss: 0.6183 - acc: 0.7723\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 107us/sample - loss: 0.6046 - acc: 0.7970\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 119us/sample - loss: 0.5905 - acc: 0.8119\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 152us/sample - loss: 0.5760 - acc: 0.8119\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.5607 - acc: 0.8416\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 135us/sample - loss: 0.5433 - acc: 0.8416\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 130us/sample - loss: 0.5264 - acc: 0.8515\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 168us/sample - loss: 0.5075 - acc: 0.8515\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 152us/sample - loss: 0.4883 - acc: 0.8564\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.4695 - acc: 0.8614\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 108us/sample - loss: 0.4497 - acc: 0.8564\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 127us/sample - loss: 0.4296 - acc: 0.8663\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 110us/sample - loss: 0.4117 - acc: 0.8713\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 109us/sample - loss: 0.3940 - acc: 0.8564\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 116us/sample - loss: 0.3764 - acc: 0.8564\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 127us/sample - loss: 0.3602 - acc: 0.8713\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 110us/sample - loss: 0.3452 - acc: 0.8713\n",
      "101/101 [==============================] - 1s 14ms/sample - loss: 0.5025 - acc: 0.7525\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.3344 - acc: 0.8762\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 7ms/sample - loss: 0.5709 - acc: 0.8119\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 157us/sample - loss: 0.5431 - acc: 0.8168\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 237us/sample - loss: 0.5142 - acc: 0.8168\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 137us/sample - loss: 0.4899 - acc: 0.8168\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 119us/sample - loss: 0.4679 - acc: 0.8168\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 159us/sample - loss: 0.4458 - acc: 0.8168\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 124us/sample - loss: 0.4293 - acc: 0.8168\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 148us/sample - loss: 0.4140 - acc: 0.8168\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 107us/sample - loss: 0.4016 - acc: 0.8168\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 104us/sample - loss: 0.3894 - acc: 0.8168\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 121us/sample - loss: 0.3807 - acc: 0.8168\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 161us/sample - loss: 0.3720 - acc: 0.8168\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 113us/sample - loss: 0.3649 - acc: 0.8168\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 114us/sample - loss: 0.3582 - acc: 0.8168\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 109us/sample - loss: 0.3512 - acc: 0.8168\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 97us/sample - loss: 0.3447 - acc: 0.8168\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 118us/sample - loss: 0.3381 - acc: 0.8168\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 128us/sample - loss: 0.3317 - acc: 0.8168\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 113us/sample - loss: 0.3257 - acc: 0.8168\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 119us/sample - loss: 0.3203 - acc: 0.8168\n",
      "101/101 [==============================] - 1s 11ms/sample - loss: 1.3710 - acc: 0.0198\n",
      "202/202 [==============================] - 0s 67us/sample - loss: 0.3156 - acc: 0.8168\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 2s 11ms/sample - loss: 0.7319 - acc: 0.3218\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 142us/sample - loss: 0.7161 - acc: 0.3663\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 139us/sample - loss: 0.7041 - acc: 0.4059\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 113us/sample - loss: 0.6951 - acc: 0.4653\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 91us/sample - loss: 0.6872 - acc: 0.5248\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 141us/sample - loss: 0.6796 - acc: 0.6188\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 155us/sample - loss: 0.6722 - acc: 0.7178\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 118us/sample - loss: 0.6654 - acc: 0.7376\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 177us/sample - loss: 0.6595 - acc: 0.7525\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 175us/sample - loss: 0.6548 - acc: 0.7624\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 113us/sample - loss: 0.6490 - acc: 0.7574\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 89us/sample - loss: 0.6440 - acc: 0.7574\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 136us/sample - loss: 0.6393 - acc: 0.7574\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 106us/sample - loss: 0.6348 - acc: 0.7624\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 113us/sample - loss: 0.6301 - acc: 0.7624\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 163us/sample - loss: 0.6239 - acc: 0.7624\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 84us/sample - loss: 0.6174 - acc: 0.7723\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 185us/sample - loss: 0.6102 - acc: 0.7673\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.6034 - acc: 0.7574\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 143us/sample - loss: 0.5957 - acc: 0.7426\n",
      "101/101 [==============================] - 1s 13ms/sample - loss: 0.8810 - acc: 0.1485\n",
      "202/202 [==============================] - 0s 58us/sample - loss: 0.5907 - acc: 0.7376\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 2s 9ms/sample - loss: 0.7271 - acc: 0.5000\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 103us/sample - loss: 0.7171 - acc: 0.5000\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 148us/sample - loss: 0.7092 - acc: 0.5000\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 92us/sample - loss: 0.7022 - acc: 0.4950\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 132us/sample - loss: 0.6953 - acc: 0.4950\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 132us/sample - loss: 0.6892 - acc: 0.4950\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 92us/sample - loss: 0.6827 - acc: 0.4950\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 85us/sample - loss: 0.6768 - acc: 0.4950\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 120us/sample - loss: 0.6716 - acc: 0.4950\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 74us/sample - loss: 0.6666 - acc: 0.4950\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 129us/sample - loss: 0.6609 - acc: 0.5000\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 85us/sample - loss: 0.6556 - acc: 0.5000\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 126us/sample - loss: 0.6502 - acc: 0.5446\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 91us/sample - loss: 0.6446 - acc: 0.5644\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 96us/sample - loss: 0.6394 - acc: 0.5743\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 89us/sample - loss: 0.6343 - acc: 0.5842\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 105us/sample - loss: 0.6289 - acc: 0.5990\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 81us/sample - loss: 0.6235 - acc: 0.6188\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 113us/sample - loss: 0.6178 - acc: 0.6436\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 72us/sample - loss: 0.6117 - acc: 0.6535\n",
      "101/101 [==============================] - 1s 13ms/sample - loss: 0.6763 - acc: 0.5050\n",
      "202/202 [==============================] - 0s 78us/sample - loss: 0.6065 - acc: 0.6832\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 2s 8ms/sample - loss: 0.6398 - acc: 0.8168\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 94us/sample - loss: 0.6278 - acc: 0.8168\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 75us/sample - loss: 0.6175 - acc: 0.8168\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 87us/sample - loss: 0.6056 - acc: 0.8168\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 98us/sample - loss: 0.5962 - acc: 0.8168\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.5870 - acc: 0.8168\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 73us/sample - loss: 0.5766 - acc: 0.8168\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 79us/sample - loss: 0.5653 - acc: 0.8168\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 81us/sample - loss: 0.5541 - acc: 0.8168\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 89us/sample - loss: 0.5414 - acc: 0.8168\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 82us/sample - loss: 0.5288 - acc: 0.8168\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.5157 - acc: 0.8168\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 91us/sample - loss: 0.5028 - acc: 0.8168\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 83us/sample - loss: 0.4909 - acc: 0.8168\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.4793 - acc: 0.8168\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 86us/sample - loss: 0.4695 - acc: 0.8168\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 120us/sample - loss: 0.4604 - acc: 0.8168\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 71us/sample - loss: 0.4520 - acc: 0.8168\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 87us/sample - loss: 0.4460 - acc: 0.8168\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 84us/sample - loss: 0.4403 - acc: 0.8168\n",
      "101/101 [==============================] - 1s 12ms/sample - loss: 1.5867 - acc: 0.0000e+00\n",
      "202/202 [==============================] - 0s 76us/sample - loss: 0.4361 - acc: 0.8168\n",
      "Epoch 1/20\n",
      "303/303 [==============================] - 2s 6ms/sample - loss: 0.6756 - acc: 0.5446\n",
      "Epoch 2/20\n",
      "303/303 [==============================] - 0s 300us/sample - loss: 0.6422 - acc: 0.6502\n",
      "Epoch 3/20\n",
      "303/303 [==============================] - 0s 504us/sample - loss: 0.6130 - acc: 0.7261\n",
      "Epoch 4/20\n",
      "303/303 [==============================] - 0s 440us/sample - loss: 0.5835 - acc: 0.7459\n",
      "Epoch 5/20\n",
      "303/303 [==============================] - 0s 350us/sample - loss: 0.5529 - acc: 0.7624\n",
      "Epoch 6/20\n",
      "303/303 [==============================] - 0s 341us/sample - loss: 0.5199 - acc: 0.7855\n",
      "Epoch 7/20\n",
      "303/303 [==============================] - 0s 374us/sample - loss: 0.4878 - acc: 0.8086\n",
      "Epoch 8/20\n",
      "303/303 [==============================] - 0s 340us/sample - loss: 0.4544 - acc: 0.8152\n",
      "Epoch 9/20\n",
      "303/303 [==============================] - 0s 299us/sample - loss: 0.4221 - acc: 0.8284\n",
      "Epoch 10/20\n",
      "303/303 [==============================] - 0s 297us/sample - loss: 0.3920 - acc: 0.8350\n",
      "Epoch 11/20\n",
      "303/303 [==============================] - 0s 345us/sample - loss: 0.3712 - acc: 0.8416\n",
      "Epoch 12/20\n",
      "303/303 [==============================] - 0s 315us/sample - loss: 0.3570 - acc: 0.8515\n",
      "Epoch 13/20\n",
      "303/303 [==============================] - 0s 318us/sample - loss: 0.3468 - acc: 0.8647\n",
      "Epoch 14/20\n",
      "303/303 [==============================] - 0s 284us/sample - loss: 0.3349 - acc: 0.8680\n",
      "Epoch 15/20\n",
      "303/303 [==============================] - 0s 293us/sample - loss: 0.3276 - acc: 0.8713\n",
      "Epoch 16/20\n",
      "303/303 [==============================] - 0s 322us/sample - loss: 0.3229 - acc: 0.8746\n",
      "Epoch 17/20\n",
      "303/303 [==============================] - 0s 360us/sample - loss: 0.3151 - acc: 0.8845\n",
      "Epoch 18/20\n",
      "303/303 [==============================] - 0s 382us/sample - loss: 0.3107 - acc: 0.8878\n",
      "Epoch 19/20\n",
      "303/303 [==============================] - 0s 305us/sample - loss: 0.3035 - acc: 0.8812\n",
      "Epoch 20/20\n",
      "303/303 [==============================] - 0s 292us/sample - loss: 0.3002 - acc: 0.8944\n",
      "Best: 0.48514850934346515 using {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.48514850934346515, Stdev: 0.35339738801896864 with: {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.37953796175618965, Stdev: 0.29925823916940475 with: {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.21782179176807404, Stdev: 0.21189045392539263 with: {'batch_size': 100, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "# define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [ 20, 60, 100],\n",
    "              'epochs': [20,40,60]\n",
    "              }\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nteract": {
   "version": "0.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
