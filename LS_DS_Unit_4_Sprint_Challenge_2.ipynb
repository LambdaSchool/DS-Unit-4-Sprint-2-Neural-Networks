{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Perceptron on XOR Gates](#Q2)\n",
    "3. [Multilayer Perceptron](#Q3)\n",
    "4. [Keras MMP](#Q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:**\n",
    "  * Neurons are the building blocks when talking about a neural network.  They can take on different \"shapes\" or forms depending on their purpose.  They are called neurons because a neural network got is loosely based on a brain. The idea is that the information that is fed into the neurons and weights associated with them will either make them fire or not to determine the final output of our classification or regression problem.\n",
    "- **Input Layer:**\n",
    "  * The input layer of a neural network is the initial layer. Where everything begins. It essentially is made up of the nodes that receive the data that will go through all the layers.  The amount of nodes or neurons that make up the input layer is largely determined by the amount of features you have in your data.\n",
    "- **Hidden Layer:**\n",
    "  * The hidden layers in a neural network is where a lot of the magic happens.  Taking the weights associated with the inputs, it usually is also coupled with some activation function that determines if the neuron fires or not. There can be a lot of hidden layers and a lot of neurons within each layer.  It depends on the problem you are trying to solve and the information you have to determine the amount. A neural network with many hidden layers is what people refer to when they talk about 'deep learning'.\n",
    "- **Output Layer:**\n",
    "  * The output layer is always the final layer of a neural network.  For binary or regression problem, there is usually only one neuron in the output layer.  If you are trying to solve a multiple classification problem, you may want multiple neurons in this layer.  Once the inputs have gone through the network, the last layer uses the weights and activation fucntion to determine the 'answer' of what you are trying to find.  For example, in a binary problem, it will output either a 1 or a 0, depending on what the network believes the input is according to the training.\n",
    "- **Activation:**\n",
    "  * An activation function is what determines if a neuron fires or not.  In the output layer, it determines whether an input is something or not something.  There are different types of activation functions, which can be more useful or not, depending on the problem at hand.  What an activation function does, is it normalizes the combination of the weights inputs and bias, and given a certain threshold, determines if a neuron will fire or not.  In a way, activation functions are the gate keepers of a neural network.\n",
    "- **Backpropagation:**\n",
    "  * Backpropagation is a technique that can be used to make a neural network learn and improve upon itself.  Given specific inputs and targets for those inputs. Once inputs go through a neural network, it calculates the amount of error, according to the true target output, and feeds that information back into the network to adjust the weights accordingly.  Using some optimization algorithm, most popularly Gradient Descent, it will adjust the weights until it finds some optimum result to minimize the final error. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perceptron on XOR Gates <a id=\"Q3=2\"></a>\n",
    "\n",
    "Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "|x1\t|x2|x3|\ty|\n",
    "|---|---|---|---|\n",
    "1|\t1|\t1|\t1|\n",
    "1|\t0|\t1|\t0|\n",
    "0|\t1|\t1|\t0|\n",
    "0|\t0|\t1|\t0|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([\n",
    "    [1,1,1],\n",
    "    [1,0,1],\n",
    "    [0,1,1],\n",
    "    [0,0,1]\n",
    "])\n",
    "\n",
    "y = [\n",
    "    [1],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0]\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(X.shape[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    def __init__(self, niter=1000):\n",
    "        self.niter = niter\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        def sigmoid(self, X):\n",
    "            return 1 / (1 + np.exp(-X))\n",
    "\n",
    "        def sigmoid_derivative(self, X):\n",
    "            sx = sigmoid(X)\n",
    "            return sx * (1-sx)\n",
    "\n",
    "        # weights\n",
    "        self.weight = 2 * np.random.random((3,1)) - 1\n",
    "    \n",
    "        for iteration in range(self.niter):\n",
    "            \n",
    "            self.weighted_sum = np.dot(X, self.weight)\n",
    "\n",
    "            self.activated_outputs = sigmoid(self.weighted_sum)\n",
    "\n",
    "            self.error = y - self.activated_outputs\n",
    "\n",
    "            self.adjustments = self.error * sigmoid_derivative(self.activated_outputs)\n",
    "\n",
    "            self.weight += np.dot(X.T, self.adjustments)\n",
    "\n",
    "        print(\"Weights after training: \", self.weight)\n",
    "        print(\"Output after training: \", self.activated_outputs)\n",
    "        return self\n",
    "    \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sigmoid() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-7ed7070789b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-e5b2b79288e8>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweighted_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivated_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweighted_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivated_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sigmoid() missing 1 required positional argument: 'X'"
     ]
    }
   ],
   "source": [
    "nn = Perceptron(1000)\n",
    "\n",
    "nn.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights.\n",
    "Your network must have one hidden layer.\n",
    "You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "Train your model on the Heart Disease dataset from UCI:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
       "       'exang', 'oldpeak', 'slope', 'ca', 'thal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['target']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.481132</td>\n",
       "      <td>0.244292</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.283105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.885496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.564516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.562500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.251142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.816794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.702290</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  sex        cp  trestbps      chol  fbs  restecg   thalach  exang  \\\n",
       "0  0.708333  1.0  1.000000  0.481132  0.244292  1.0      0.0  0.603053    0.0   \n",
       "1  0.166667  1.0  0.666667  0.339623  0.283105  0.0      0.5  0.885496    0.0   \n",
       "2  0.250000  0.0  0.333333  0.339623  0.178082  0.0      0.0  0.770992    0.0   \n",
       "3  0.562500  1.0  0.333333  0.245283  0.251142  0.0      0.5  0.816794    0.0   \n",
       "4  0.583333  0.0  0.000000  0.245283  0.520548  0.0      0.5  0.702290    1.0   \n",
       "\n",
       "    oldpeak  slope   ca      thal  target  \n",
       "0  0.370968    0.0  0.0  0.333333       1  \n",
       "1  0.564516    0.0  0.0  0.666667       1  \n",
       "2  0.225806    1.0  0.0  0.666667       1  \n",
       "3  0.129032    1.0  0.0  0.666667       1  \n",
       "4  0.096774    1.0  0.0  0.666667       1  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert numeric cols to floats and normalize:\n",
    "num_cols = df.drop(columns=['target']).columns\n",
    "scaler = MinMaxScaler()\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols].values)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      "age         303 non-null float32\n",
      "sex         303 non-null float32\n",
      "cp          303 non-null float32\n",
      "trestbps    303 non-null float32\n",
      "chol        303 non-null float32\n",
      "fbs         303 non-null float32\n",
      "restecg     303 non-null float32\n",
      "thalach     303 non-null float32\n",
      "exang       303 non-null float32\n",
      "oldpeak     303 non-null float32\n",
      "slope       303 non-null float32\n",
      "ca          303 non-null float32\n",
      "thal        303 non-null float32\n",
      "target      303 non-null float32\n",
      "dtypes: float32(14)\n",
      "memory usage: 16.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((203, 13), (203,), (100, 13), (100,))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.values\n",
    "x_test = x_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 13\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #Make empty list to store costs:\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 4.723070\n",
      "         Iterations: 200\n",
      "         Function evaluations: 226\n",
      "         Gradient evaluations: 226\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "T = trainer(NN)\n",
    "\n",
    "T.train(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [9.99999999e-01]\n",
      " [9.04079389e-11]\n",
      " [3.32891960e-10]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [9.99999992e-01]\n",
      " [9.08306611e-11]]\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 4.625521\n",
      "         Iterations: 200\n",
      "         Function evaluations: 205\n",
      "         Gradient evaluations: 205\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[9.99999994e-01]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [7.01916988e-15]\n",
      " [1.83984495e-03]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [9.99999998e-01]\n",
      " [8.94461494e-15]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in square\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in square\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.146412\n",
      "         Iterations: 194\n",
      "         Function evaluations: 227\n",
      "         Gradient evaluations: 215\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.14771868e-13]\n",
      " [6.00369424e-10]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.93571559e-13]]\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 3.509505\n",
      "         Iterations: 200\n",
      "         Function evaluations: 229\n",
      "         Gradient evaluations: 229\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [2.95009373e-19]\n",
      " [5.47719457e-13]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.04096028e-18]]\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 3.500572\n",
      "         Iterations: 200\n",
      "         Function evaluations: 225\n",
      "         Gradient evaluations: 225\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [3.07884778e-27]\n",
      " [9.86423385e-18]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.38601197e-26]]\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 3.500202\n",
      "         Iterations: 200\n",
      "         Function evaluations: 214\n",
      "         Gradient evaluations: 214\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 200\n",
      "         Function evaluations: 211\n",
      "         Gradient evaluations: 211\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 2\n",
      "         Function evaluations: 4\n",
      "         Gradient evaluations: 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "+---------EPOCH 100---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00412314e-51]\n",
      " [9.06321279e-23]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [5.16901183e-32]]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "+---------EPOCH 200---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00412314e-51]\n",
      " [9.06321279e-23]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [5.16901183e-32]]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "+---------EPOCH 300---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00412314e-51]\n",
      " [9.06321279e-23]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [5.16901183e-32]]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "+---------EPOCH 400---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00412314e-51]\n",
      " [9.06321279e-23]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [5.16901183e-32]]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "+---------EPOCH 500---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00412314e-51]\n",
      " [9.06321279e-23]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [5.16901183e-32]]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "+---------EPOCH 600---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00412314e-51]\n",
      " [9.06321279e-23]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [5.16901183e-32]]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "+---------EPOCH 700---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00412314e-51]\n",
      " [9.06321279e-23]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [5.16901183e-32]]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "+---------EPOCH 800---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00412314e-51]\n",
      " [9.06321279e-23]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [5.16901183e-32]]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "+---------EPOCH 900---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00412314e-51]\n",
      " [9.06321279e-23]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [5.16901183e-32]]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "+---------EPOCH 1000---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00412314e-51]\n",
      " [9.06321279e-23]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [5.16901183e-32]]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.500012\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    if (epoch+1 in [1,2,3,4,5]) or ((epoch+1) % 100 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {epoch+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', x_train)\n",
    "        print('Actual Output: \\n', y_train[:10])\n",
    "        print('Predicted Output: \\n', str(NN.forward(x_train[:10])))\n",
    "    T.train(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Keras MMP <a id=\"Q4\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 203 samples, validate on 100 samples\n",
      "Epoch 1/50\n",
      "203/203 [==============================] - 1s 2ms/step - loss: 0.7490 - acc: 0.5271 - val_loss: 0.6916 - val_acc: 0.5800\n",
      "Epoch 2/50\n",
      "203/203 [==============================] - 0s 203us/step - loss: 0.7263 - acc: 0.5271 - val_loss: 0.6775 - val_acc: 0.5800\n",
      "Epoch 3/50\n",
      "203/203 [==============================] - 0s 167us/step - loss: 0.7093 - acc: 0.5271 - val_loss: 0.6650 - val_acc: 0.5800\n",
      "Epoch 4/50\n",
      "203/203 [==============================] - 0s 185us/step - loss: 0.6910 - acc: 0.5271 - val_loss: 0.6537 - val_acc: 0.5800\n",
      "Epoch 5/50\n",
      "203/203 [==============================] - 0s 240us/step - loss: 0.6764 - acc: 0.5320 - val_loss: 0.6415 - val_acc: 0.5900\n",
      "Epoch 6/50\n",
      "203/203 [==============================] - 0s 220us/step - loss: 0.6618 - acc: 0.5369 - val_loss: 0.6302 - val_acc: 0.5900\n",
      "Epoch 7/50\n",
      "203/203 [==============================] - 0s 188us/step - loss: 0.6486 - acc: 0.5369 - val_loss: 0.6166 - val_acc: 0.5900\n",
      "Epoch 8/50\n",
      "203/203 [==============================] - 0s 197us/step - loss: 0.6355 - acc: 0.5419 - val_loss: 0.6045 - val_acc: 0.5900\n",
      "Epoch 9/50\n",
      "203/203 [==============================] - 0s 183us/step - loss: 0.6250 - acc: 0.5419 - val_loss: 0.5955 - val_acc: 0.6100\n",
      "Epoch 10/50\n",
      "203/203 [==============================] - 0s 170us/step - loss: 0.6147 - acc: 0.5714 - val_loss: 0.5876 - val_acc: 0.6400\n",
      "Epoch 11/50\n",
      "203/203 [==============================] - 0s 215us/step - loss: 0.6045 - acc: 0.6059 - val_loss: 0.5804 - val_acc: 0.7200\n",
      "Epoch 12/50\n",
      "203/203 [==============================] - 0s 196us/step - loss: 0.5948 - acc: 0.6404 - val_loss: 0.5708 - val_acc: 0.7700\n",
      "Epoch 13/50\n",
      "203/203 [==============================] - 0s 172us/step - loss: 0.5855 - acc: 0.6946 - val_loss: 0.5636 - val_acc: 0.8300\n",
      "Epoch 14/50\n",
      "203/203 [==============================] - 0s 224us/step - loss: 0.5754 - acc: 0.7340 - val_loss: 0.5568 - val_acc: 0.8300\n",
      "Epoch 15/50\n",
      "203/203 [==============================] - 0s 166us/step - loss: 0.5654 - acc: 0.7586 - val_loss: 0.5488 - val_acc: 0.8300\n",
      "Epoch 16/50\n",
      "203/203 [==============================] - 0s 160us/step - loss: 0.5562 - acc: 0.7635 - val_loss: 0.5386 - val_acc: 0.8400\n",
      "Epoch 17/50\n",
      "203/203 [==============================] - 0s 159us/step - loss: 0.5459 - acc: 0.7734 - val_loss: 0.5323 - val_acc: 0.8300\n",
      "Epoch 18/50\n",
      "203/203 [==============================] - 0s 159us/step - loss: 0.5378 - acc: 0.7783 - val_loss: 0.5279 - val_acc: 0.8200\n",
      "Epoch 19/50\n",
      "203/203 [==============================] - 0s 173us/step - loss: 0.5269 - acc: 0.7980 - val_loss: 0.5209 - val_acc: 0.8200\n",
      "Epoch 20/50\n",
      "203/203 [==============================] - 0s 157us/step - loss: 0.5171 - acc: 0.8030 - val_loss: 0.5135 - val_acc: 0.8100\n",
      "Epoch 21/50\n",
      "203/203 [==============================] - 0s 165us/step - loss: 0.5076 - acc: 0.7980 - val_loss: 0.5071 - val_acc: 0.8100\n",
      "Epoch 22/50\n",
      "203/203 [==============================] - 0s 173us/step - loss: 0.4978 - acc: 0.8079 - val_loss: 0.4976 - val_acc: 0.8200\n",
      "Epoch 23/50\n",
      "203/203 [==============================] - 0s 166us/step - loss: 0.4877 - acc: 0.8079 - val_loss: 0.4942 - val_acc: 0.8300\n",
      "Epoch 24/50\n",
      "203/203 [==============================] - 0s 167us/step - loss: 0.4784 - acc: 0.8276 - val_loss: 0.4866 - val_acc: 0.8300\n",
      "Epoch 25/50\n",
      "203/203 [==============================] - 0s 152us/step - loss: 0.4694 - acc: 0.8325 - val_loss: 0.4734 - val_acc: 0.8300\n",
      "Epoch 26/50\n",
      "203/203 [==============================] - 0s 158us/step - loss: 0.4598 - acc: 0.8325 - val_loss: 0.4698 - val_acc: 0.8300\n",
      "Epoch 27/50\n",
      "203/203 [==============================] - 0s 166us/step - loss: 0.4518 - acc: 0.8325 - val_loss: 0.4664 - val_acc: 0.8300\n",
      "Epoch 28/50\n",
      "203/203 [==============================] - 0s 147us/step - loss: 0.4447 - acc: 0.8374 - val_loss: 0.4608 - val_acc: 0.8300\n",
      "Epoch 29/50\n",
      "203/203 [==============================] - 0s 210us/step - loss: 0.4368 - acc: 0.8621 - val_loss: 0.4603 - val_acc: 0.8300\n",
      "Epoch 30/50\n",
      "203/203 [==============================] - 0s 220us/step - loss: 0.4292 - acc: 0.8621 - val_loss: 0.4560 - val_acc: 0.8300\n",
      "Epoch 31/50\n",
      "203/203 [==============================] - 0s 326us/step - loss: 0.4215 - acc: 0.8522 - val_loss: 0.4582 - val_acc: 0.8100\n",
      "Epoch 32/50\n",
      "203/203 [==============================] - 0s 282us/step - loss: 0.4154 - acc: 0.8522 - val_loss: 0.4510 - val_acc: 0.8200\n",
      "Epoch 33/50\n",
      "203/203 [==============================] - 0s 268us/step - loss: 0.4099 - acc: 0.8424 - val_loss: 0.4543 - val_acc: 0.8100\n",
      "Epoch 34/50\n",
      "203/203 [==============================] - 0s 269us/step - loss: 0.4069 - acc: 0.8522 - val_loss: 0.4446 - val_acc: 0.8200\n",
      "Epoch 35/50\n",
      "203/203 [==============================] - 0s 207us/step - loss: 0.4004 - acc: 0.8522 - val_loss: 0.4546 - val_acc: 0.8000\n",
      "Epoch 36/50\n",
      "203/203 [==============================] - 0s 208us/step - loss: 0.3959 - acc: 0.8424 - val_loss: 0.4438 - val_acc: 0.8200\n",
      "Epoch 37/50\n",
      "203/203 [==============================] - 0s 249us/step - loss: 0.3929 - acc: 0.8670 - val_loss: 0.4404 - val_acc: 0.8200\n",
      "Epoch 38/50\n",
      "203/203 [==============================] - 0s 181us/step - loss: 0.3891 - acc: 0.8670 - val_loss: 0.4411 - val_acc: 0.8200\n",
      "Epoch 39/50\n",
      "203/203 [==============================] - 0s 277us/step - loss: 0.3854 - acc: 0.8670 - val_loss: 0.4412 - val_acc: 0.8200\n",
      "Epoch 40/50\n",
      "203/203 [==============================] - 0s 728us/step - loss: 0.3818 - acc: 0.8670 - val_loss: 0.4339 - val_acc: 0.8200\n",
      "Epoch 41/50\n",
      "203/203 [==============================] - 0s 317us/step - loss: 0.3806 - acc: 0.8670 - val_loss: 0.4340 - val_acc: 0.8200\n",
      "Epoch 42/50\n",
      "203/203 [==============================] - 0s 260us/step - loss: 0.3769 - acc: 0.8719 - val_loss: 0.4361 - val_acc: 0.8200\n",
      "Epoch 43/50\n",
      "203/203 [==============================] - 0s 575us/step - loss: 0.3740 - acc: 0.8670 - val_loss: 0.4401 - val_acc: 0.8200\n",
      "Epoch 44/50\n",
      "203/203 [==============================] - 0s 245us/step - loss: 0.3711 - acc: 0.8670 - val_loss: 0.4396 - val_acc: 0.8200\n",
      "Epoch 45/50\n",
      "203/203 [==============================] - 0s 432us/step - loss: 0.3686 - acc: 0.8621 - val_loss: 0.4487 - val_acc: 0.8000\n",
      "Epoch 46/50\n",
      "203/203 [==============================] - 0s 195us/step - loss: 0.3683 - acc: 0.8571 - val_loss: 0.4475 - val_acc: 0.8000\n",
      "Epoch 47/50\n",
      "203/203 [==============================] - 0s 402us/step - loss: 0.3666 - acc: 0.8571 - val_loss: 0.4573 - val_acc: 0.7800\n",
      "Epoch 48/50\n",
      "203/203 [==============================] - 0s 416us/step - loss: 0.3618 - acc: 0.8473 - val_loss: 0.4409 - val_acc: 0.8100\n",
      "Epoch 49/50\n",
      "203/203 [==============================] - 0s 764us/step - loss: 0.3606 - acc: 0.8670 - val_loss: 0.4390 - val_acc: 0.8100\n",
      "Epoch 50/50\n",
      "203/203 [==============================] - 0s 385us/step - loss: 0.3587 - acc: 0.8670 - val_loss: 0.4389 - val_acc: 0.8100\n"
     ]
    }
   ],
   "source": [
    "# Keras imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create Model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape=(13,)))\n",
    "model.add(Dense(13, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Fit Model\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=50, batch_size=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypertune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.7280 - acc: 0.4667\n",
      "Epoch 2/20\n",
      "135/135 [==============================] - 0s 503us/step - loss: 0.7176 - acc: 0.4741\n",
      "Epoch 3/20\n",
      "135/135 [==============================] - 0s 242us/step - loss: 0.7074 - acc: 0.4815\n",
      "Epoch 4/20\n",
      "135/135 [==============================] - 0s 172us/step - loss: 0.6987 - acc: 0.5259\n",
      "Epoch 5/20\n",
      "135/135 [==============================] - 0s 159us/step - loss: 0.6896 - acc: 0.5407\n",
      "Epoch 6/20\n",
      "135/135 [==============================] - 0s 167us/step - loss: 0.6823 - acc: 0.5481\n",
      "Epoch 7/20\n",
      "135/135 [==============================] - 0s 207us/step - loss: 0.6753 - acc: 0.5778\n",
      "Epoch 8/20\n",
      "135/135 [==============================] - 0s 151us/step - loss: 0.6683 - acc: 0.5778\n",
      "Epoch 9/20\n",
      "135/135 [==============================] - 0s 143us/step - loss: 0.6624 - acc: 0.5778\n",
      "Epoch 10/20\n",
      "135/135 [==============================] - 0s 149us/step - loss: 0.6566 - acc: 0.6000\n",
      "Epoch 11/20\n",
      "135/135 [==============================] - 0s 133us/step - loss: 0.6510 - acc: 0.6000\n",
      "Epoch 12/20\n",
      "135/135 [==============================] - 0s 127us/step - loss: 0.6464 - acc: 0.6074\n",
      "Epoch 13/20\n",
      "135/135 [==============================] - 0s 174us/step - loss: 0.6412 - acc: 0.6148\n",
      "Epoch 14/20\n",
      "135/135 [==============================] - 0s 144us/step - loss: 0.6367 - acc: 0.6444\n",
      "Epoch 15/20\n",
      "135/135 [==============================] - 0s 137us/step - loss: 0.6322 - acc: 0.6593\n",
      "Epoch 16/20\n",
      "135/135 [==============================] - 0s 158us/step - loss: 0.6273 - acc: 0.7037\n",
      "Epoch 17/20\n",
      "135/135 [==============================] - 0s 134us/step - loss: 0.6224 - acc: 0.7037\n",
      "Epoch 18/20\n",
      "135/135 [==============================] - 0s 173us/step - loss: 0.6179 - acc: 0.7259\n",
      "Epoch 19/20\n",
      "135/135 [==============================] - 0s 155us/step - loss: 0.6132 - acc: 0.7333\n",
      "Epoch 20/20\n",
      "135/135 [==============================] - 0s 137us/step - loss: 0.6078 - acc: 0.7333\n",
      "68/68 [==============================] - 0s 2ms/step\n",
      "135/135 [==============================] - 0s 169us/step\n",
      "Epoch 1/20\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 0.7326 - acc: 0.4000\n",
      "Epoch 2/20\n",
      "135/135 [==============================] - 0s 197us/step - loss: 0.7249 - acc: 0.3556\n",
      "Epoch 3/20\n",
      "135/135 [==============================] - 0s 190us/step - loss: 0.7158 - acc: 0.3556\n",
      "Epoch 4/20\n",
      "135/135 [==============================] - 0s 199us/step - loss: 0.7088 - acc: 0.3630\n",
      "Epoch 5/20\n",
      "135/135 [==============================] - 0s 185us/step - loss: 0.7026 - acc: 0.4296\n",
      "Epoch 6/20\n",
      "135/135 [==============================] - 0s 183us/step - loss: 0.6948 - acc: 0.5111\n",
      "Epoch 7/20\n",
      "135/135 [==============================] - 0s 210us/step - loss: 0.6882 - acc: 0.5037\n",
      "Epoch 8/20\n",
      "135/135 [==============================] - 0s 158us/step - loss: 0.6805 - acc: 0.5037\n",
      "Epoch 9/20\n",
      "135/135 [==============================] - 0s 181us/step - loss: 0.6739 - acc: 0.5259\n",
      "Epoch 10/20\n",
      "135/135 [==============================] - 0s 175us/step - loss: 0.6669 - acc: 0.5333\n",
      "Epoch 11/20\n",
      "135/135 [==============================] - 0s 180us/step - loss: 0.6594 - acc: 0.5333\n",
      "Epoch 12/20\n",
      "135/135 [==============================] - 0s 227us/step - loss: 0.6530 - acc: 0.5481\n",
      "Epoch 13/20\n",
      "135/135 [==============================] - 0s 212us/step - loss: 0.6466 - acc: 0.5704\n",
      "Epoch 14/20\n",
      "135/135 [==============================] - 0s 233us/step - loss: 0.6401 - acc: 0.6222\n",
      "Epoch 15/20\n",
      "135/135 [==============================] - 0s 238us/step - loss: 0.6343 - acc: 0.6296\n",
      "Epoch 16/20\n",
      "135/135 [==============================] - 0s 198us/step - loss: 0.6285 - acc: 0.6667\n",
      "Epoch 17/20\n",
      "135/135 [==============================] - 0s 168us/step - loss: 0.6221 - acc: 0.6889\n",
      "Epoch 18/20\n",
      "135/135 [==============================] - 0s 202us/step - loss: 0.6157 - acc: 0.7407\n",
      "Epoch 19/20\n",
      "135/135 [==============================] - 0s 191us/step - loss: 0.6096 - acc: 0.7556\n",
      "Epoch 20/20\n",
      "135/135 [==============================] - 0s 231us/step - loss: 0.6026 - acc: 0.7556\n",
      "68/68 [==============================] - 0s 2ms/step\n",
      "135/135 [==============================] - 0s 233us/step\n",
      "Epoch 1/20\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.6886 - acc: 0.5000\n",
      "Epoch 2/20\n",
      "136/136 [==============================] - 0s 195us/step - loss: 0.6810 - acc: 0.5588\n",
      "Epoch 3/20\n",
      "136/136 [==============================] - 0s 206us/step - loss: 0.6739 - acc: 0.5956\n",
      "Epoch 4/20\n",
      "136/136 [==============================] - 0s 209us/step - loss: 0.6670 - acc: 0.6471\n",
      "Epoch 5/20\n",
      "136/136 [==============================] - 0s 250us/step - loss: 0.6607 - acc: 0.6765\n",
      "Epoch 6/20\n",
      "136/136 [==============================] - 0s 118us/step - loss: 0.6544 - acc: 0.7059\n",
      "Epoch 7/20\n",
      "136/136 [==============================] - 0s 241us/step - loss: 0.6489 - acc: 0.6912\n",
      "Epoch 8/20\n",
      "136/136 [==============================] - 0s 221us/step - loss: 0.6432 - acc: 0.7132\n",
      "Epoch 9/20\n",
      "136/136 [==============================] - 0s 175us/step - loss: 0.6380 - acc: 0.7426\n",
      "Epoch 10/20\n",
      "136/136 [==============================] - 0s 181us/step - loss: 0.6337 - acc: 0.7353\n",
      "Epoch 11/20\n",
      "136/136 [==============================] - 0s 217us/step - loss: 0.6283 - acc: 0.7353\n",
      "Epoch 12/20\n",
      "136/136 [==============================] - 0s 204us/step - loss: 0.6236 - acc: 0.7500\n",
      "Epoch 13/20\n",
      "136/136 [==============================] - 0s 169us/step - loss: 0.6190 - acc: 0.7500\n",
      "Epoch 14/20\n",
      "136/136 [==============================] - 0s 187us/step - loss: 0.6138 - acc: 0.7574\n",
      "Epoch 15/20\n",
      "136/136 [==============================] - 0s 158us/step - loss: 0.6092 - acc: 0.7647\n",
      "Epoch 16/20\n",
      "136/136 [==============================] - 0s 149us/step - loss: 0.6042 - acc: 0.7721\n",
      "Epoch 17/20\n",
      "136/136 [==============================] - 0s 168us/step - loss: 0.5990 - acc: 0.7647\n",
      "Epoch 18/20\n",
      "136/136 [==============================] - 0s 145us/step - loss: 0.5943 - acc: 0.7721\n",
      "Epoch 19/20\n",
      "136/136 [==============================] - 0s 180us/step - loss: 0.5890 - acc: 0.7794\n",
      "Epoch 20/20\n",
      "136/136 [==============================] - 0s 162us/step - loss: 0.5837 - acc: 0.7721\n",
      "67/67 [==============================] - 0s 2ms/step\n",
      "136/136 [==============================] - 0s 95us/step\n",
      "Epoch 1/50\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.6677 - acc: 0.6222\n",
      "Epoch 2/50\n",
      "135/135 [==============================] - 0s 214us/step - loss: 0.6614 - acc: 0.6370\n",
      "Epoch 3/50\n",
      "135/135 [==============================] - 0s 794us/step - loss: 0.6554 - acc: 0.6370\n",
      "Epoch 4/50\n",
      "135/135 [==============================] - 0s 596us/step - loss: 0.6499 - acc: 0.6519\n",
      "Epoch 5/50\n",
      "135/135 [==============================] - 0s 227us/step - loss: 0.6435 - acc: 0.6889\n",
      "Epoch 6/50\n",
      "135/135 [==============================] - 0s 280us/step - loss: 0.6374 - acc: 0.7111\n",
      "Epoch 7/50\n",
      "135/135 [==============================] - 0s 317us/step - loss: 0.6319 - acc: 0.6963\n",
      "Epoch 8/50\n",
      "135/135 [==============================] - 0s 347us/step - loss: 0.6257 - acc: 0.7037\n",
      "Epoch 9/50\n",
      "135/135 [==============================] - 0s 234us/step - loss: 0.6198 - acc: 0.7037\n",
      "Epoch 10/50\n",
      "135/135 [==============================] - 0s 181us/step - loss: 0.6137 - acc: 0.7111\n",
      "Epoch 11/50\n",
      "135/135 [==============================] - 0s 197us/step - loss: 0.6082 - acc: 0.7111\n",
      "Epoch 12/50\n",
      "135/135 [==============================] - 0s 194us/step - loss: 0.6021 - acc: 0.7185\n",
      "Epoch 13/50\n",
      "135/135 [==============================] - 0s 205us/step - loss: 0.5961 - acc: 0.7333\n",
      "Epoch 14/50\n",
      "135/135 [==============================] - 0s 210us/step - loss: 0.5903 - acc: 0.7407\n",
      "Epoch 15/50\n",
      "135/135 [==============================] - 0s 243us/step - loss: 0.5853 - acc: 0.7556\n",
      "Epoch 16/50\n",
      "135/135 [==============================] - 0s 170us/step - loss: 0.5783 - acc: 0.7481\n",
      "Epoch 17/50\n",
      "135/135 [==============================] - 0s 252us/step - loss: 0.5726 - acc: 0.7556\n",
      "Epoch 18/50\n",
      "135/135 [==============================] - 0s 227us/step - loss: 0.5665 - acc: 0.7630\n",
      "Epoch 19/50\n",
      "135/135 [==============================] - 0s 162us/step - loss: 0.5609 - acc: 0.7556\n",
      "Epoch 20/50\n",
      "135/135 [==============================] - 0s 231us/step - loss: 0.5551 - acc: 0.7556\n",
      "Epoch 21/50\n",
      "135/135 [==============================] - 0s 164us/step - loss: 0.5495 - acc: 0.7704\n",
      "Epoch 22/50\n",
      "135/135 [==============================] - 0s 196us/step - loss: 0.5429 - acc: 0.7704\n",
      "Epoch 23/50\n",
      "135/135 [==============================] - 0s 201us/step - loss: 0.5373 - acc: 0.7630\n",
      "Epoch 24/50\n",
      "135/135 [==============================] - 0s 343us/step - loss: 0.5316 - acc: 0.7704\n",
      "Epoch 25/50\n",
      "135/135 [==============================] - 0s 366us/step - loss: 0.5258 - acc: 0.7704\n",
      "Epoch 26/50\n",
      "135/135 [==============================] - 0s 261us/step - loss: 0.5200 - acc: 0.7704\n",
      "Epoch 27/50\n",
      "135/135 [==============================] - 0s 257us/step - loss: 0.5148 - acc: 0.7852\n",
      "Epoch 28/50\n",
      "135/135 [==============================] - 0s 266us/step - loss: 0.5093 - acc: 0.7852\n",
      "Epoch 29/50\n",
      "135/135 [==============================] - 0s 309us/step - loss: 0.5042 - acc: 0.7852\n",
      "Epoch 30/50\n",
      "135/135 [==============================] - 0s 195us/step - loss: 0.4988 - acc: 0.7926\n",
      "Epoch 31/50\n",
      "135/135 [==============================] - 0s 187us/step - loss: 0.4938 - acc: 0.8000\n",
      "Epoch 32/50\n",
      "135/135 [==============================] - 0s 241us/step - loss: 0.4895 - acc: 0.8000\n",
      "Epoch 33/50\n",
      "135/135 [==============================] - 0s 209us/step - loss: 0.4834 - acc: 0.8000\n",
      "Epoch 34/50\n",
      "135/135 [==============================] - 0s 186us/step - loss: 0.4785 - acc: 0.8000\n",
      "Epoch 35/50\n",
      "135/135 [==============================] - 0s 152us/step - loss: 0.4739 - acc: 0.8000\n",
      "Epoch 36/50\n",
      "135/135 [==============================] - 0s 208us/step - loss: 0.4692 - acc: 0.8000\n",
      "Epoch 37/50\n",
      "135/135 [==============================] - 0s 192us/step - loss: 0.4644 - acc: 0.8000\n",
      "Epoch 38/50\n",
      "135/135 [==============================] - 0s 144us/step - loss: 0.4603 - acc: 0.8074\n",
      "Epoch 39/50\n",
      "135/135 [==============================] - 0s 177us/step - loss: 0.4558 - acc: 0.8000\n",
      "Epoch 40/50\n",
      "135/135 [==============================] - 0s 238us/step - loss: 0.4517 - acc: 0.8074\n",
      "Epoch 41/50\n",
      "135/135 [==============================] - 0s 257us/step - loss: 0.4473 - acc: 0.8074\n",
      "Epoch 42/50\n",
      "135/135 [==============================] - 0s 276us/step - loss: 0.4430 - acc: 0.8148\n",
      "Epoch 43/50\n",
      "135/135 [==============================] - 0s 232us/step - loss: 0.4394 - acc: 0.8148\n",
      "Epoch 44/50\n",
      "135/135 [==============================] - 0s 187us/step - loss: 0.4354 - acc: 0.8148\n",
      "Epoch 45/50\n",
      "135/135 [==============================] - 0s 178us/step - loss: 0.4317 - acc: 0.8148\n",
      "Epoch 46/50\n",
      "135/135 [==============================] - 0s 227us/step - loss: 0.4279 - acc: 0.8148\n",
      "Epoch 47/50\n",
      "135/135 [==============================] - 0s 179us/step - loss: 0.4243 - acc: 0.8148\n",
      "Epoch 48/50\n",
      "135/135 [==============================] - 0s 255us/step - loss: 0.4209 - acc: 0.8222\n",
      "Epoch 49/50\n",
      "135/135 [==============================] - 0s 193us/step - loss: 0.4176 - acc: 0.8222\n",
      "Epoch 50/50\n",
      "135/135 [==============================] - 0s 178us/step - loss: 0.4146 - acc: 0.8222\n",
      "68/68 [==============================] - 0s 3ms/step\n",
      "135/135 [==============================] - 0s 105us/step\n",
      "Epoch 1/50\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 0.7060 - acc: 0.5111\n",
      "Epoch 2/50\n",
      "135/135 [==============================] - 0s 140us/step - loss: 0.7010 - acc: 0.5037\n",
      "Epoch 3/50\n",
      "135/135 [==============================] - 0s 223us/step - loss: 0.6963 - acc: 0.5111\n",
      "Epoch 4/50\n",
      "135/135 [==============================] - 0s 166us/step - loss: 0.6922 - acc: 0.5111\n",
      "Epoch 5/50\n",
      "135/135 [==============================] - 0s 161us/step - loss: 0.6884 - acc: 0.4963\n",
      "Epoch 6/50\n",
      "135/135 [==============================] - 0s 224us/step - loss: 0.6844 - acc: 0.5111\n",
      "Epoch 7/50\n",
      "135/135 [==============================] - 0s 249us/step - loss: 0.6806 - acc: 0.5259\n",
      "Epoch 8/50\n",
      "135/135 [==============================] - 0s 161us/step - loss: 0.6768 - acc: 0.5259\n",
      "Epoch 9/50\n",
      "135/135 [==============================] - 0s 256us/step - loss: 0.6734 - acc: 0.5333\n",
      "Epoch 10/50\n",
      "135/135 [==============================] - 0s 222us/step - loss: 0.6701 - acc: 0.5407\n",
      "Epoch 11/50\n",
      "135/135 [==============================] - 0s 183us/step - loss: 0.6670 - acc: 0.5407\n",
      "Epoch 12/50\n",
      "135/135 [==============================] - 0s 173us/step - loss: 0.6636 - acc: 0.5556\n",
      "Epoch 13/50\n",
      "135/135 [==============================] - 0s 196us/step - loss: 0.6602 - acc: 0.5630\n",
      "Epoch 14/50\n",
      "135/135 [==============================] - 0s 202us/step - loss: 0.6569 - acc: 0.5556\n",
      "Epoch 15/50\n",
      "135/135 [==============================] - 0s 172us/step - loss: 0.6529 - acc: 0.5630\n",
      "Epoch 16/50\n",
      "135/135 [==============================] - 0s 234us/step - loss: 0.6489 - acc: 0.5704\n",
      "Epoch 17/50\n",
      "135/135 [==============================] - 0s 246us/step - loss: 0.6442 - acc: 0.5852\n",
      "Epoch 18/50\n",
      "135/135 [==============================] - 0s 195us/step - loss: 0.6397 - acc: 0.6000\n",
      "Epoch 19/50\n",
      "135/135 [==============================] - 0s 208us/step - loss: 0.6347 - acc: 0.6370\n",
      "Epoch 20/50\n",
      "135/135 [==============================] - 0s 184us/step - loss: 0.6292 - acc: 0.6889\n",
      "Epoch 21/50\n",
      "135/135 [==============================] - 0s 185us/step - loss: 0.6235 - acc: 0.6963\n",
      "Epoch 22/50\n",
      "135/135 [==============================] - 0s 203us/step - loss: 0.6175 - acc: 0.7185\n",
      "Epoch 23/50\n",
      "135/135 [==============================] - 0s 231us/step - loss: 0.6120 - acc: 0.7333\n",
      "Epoch 24/50\n",
      "135/135 [==============================] - 0s 259us/step - loss: 0.6059 - acc: 0.7481\n",
      "Epoch 25/50\n",
      "135/135 [==============================] - 0s 289us/step - loss: 0.5993 - acc: 0.7481\n",
      "Epoch 26/50\n",
      "135/135 [==============================] - 0s 279us/step - loss: 0.5930 - acc: 0.7481\n",
      "Epoch 27/50\n",
      "135/135 [==============================] - 0s 198us/step - loss: 0.5859 - acc: 0.7481\n",
      "Epoch 28/50\n",
      "135/135 [==============================] - 0s 143us/step - loss: 0.5790 - acc: 0.7556\n",
      "Epoch 29/50\n",
      "135/135 [==============================] - 0s 365us/step - loss: 0.5716 - acc: 0.7556\n",
      "Epoch 30/50\n",
      "135/135 [==============================] - 0s 184us/step - loss: 0.5643 - acc: 0.7630\n",
      "Epoch 31/50\n",
      "135/135 [==============================] - 0s 145us/step - loss: 0.5549 - acc: 0.7630\n",
      "Epoch 32/50\n",
      "135/135 [==============================] - 0s 167us/step - loss: 0.5450 - acc: 0.7704\n",
      "Epoch 33/50\n",
      "135/135 [==============================] - 0s 217us/step - loss: 0.5323 - acc: 0.8148\n",
      "Epoch 34/50\n",
      "135/135 [==============================] - 0s 186us/step - loss: 0.5203 - acc: 0.8074\n",
      "Epoch 35/50\n",
      "135/135 [==============================] - 0s 128us/step - loss: 0.5072 - acc: 0.8074\n",
      "Epoch 36/50\n",
      "135/135 [==============================] - 0s 150us/step - loss: 0.4959 - acc: 0.8222\n",
      "Epoch 37/50\n",
      "135/135 [==============================] - 0s 116us/step - loss: 0.4856 - acc: 0.8222\n",
      "Epoch 38/50\n",
      "135/135 [==============================] - 0s 226us/step - loss: 0.4763 - acc: 0.8222\n",
      "Epoch 39/50\n",
      "135/135 [==============================] - 0s 166us/step - loss: 0.4676 - acc: 0.8222\n",
      "Epoch 40/50\n",
      "135/135 [==============================] - 0s 160us/step - loss: 0.4599 - acc: 0.8222\n",
      "Epoch 41/50\n",
      "135/135 [==============================] - 0s 173us/step - loss: 0.4524 - acc: 0.8222\n",
      "Epoch 42/50\n",
      "135/135 [==============================] - 0s 188us/step - loss: 0.4449 - acc: 0.8296\n",
      "Epoch 43/50\n",
      "135/135 [==============================] - 0s 152us/step - loss: 0.4382 - acc: 0.8296\n",
      "Epoch 44/50\n",
      "135/135 [==============================] - 0s 198us/step - loss: 0.4320 - acc: 0.8296\n",
      "Epoch 45/50\n",
      "135/135 [==============================] - 0s 141us/step - loss: 0.4257 - acc: 0.8370\n",
      "Epoch 46/50\n",
      "135/135 [==============================] - 0s 138us/step - loss: 0.4201 - acc: 0.8370\n",
      "Epoch 47/50\n",
      "135/135 [==============================] - 0s 181us/step - loss: 0.4151 - acc: 0.8370\n",
      "Epoch 48/50\n",
      "135/135 [==============================] - 0s 163us/step - loss: 0.4096 - acc: 0.8444\n",
      "Epoch 49/50\n",
      "135/135 [==============================] - 0s 125us/step - loss: 0.4043 - acc: 0.8370\n",
      "Epoch 50/50\n",
      "135/135 [==============================] - 0s 145us/step - loss: 0.3999 - acc: 0.8370\n",
      "68/68 [==============================] - 0s 2ms/step\n",
      "135/135 [==============================] - 0s 101us/step\n",
      "Epoch 1/50\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 0.7317 - acc: 0.4926\n",
      "Epoch 2/50\n",
      "136/136 [==============================] - 0s 196us/step - loss: 0.7084 - acc: 0.5147\n",
      "Epoch 3/50\n",
      "136/136 [==============================] - 0s 202us/step - loss: 0.6900 - acc: 0.5441\n",
      "Epoch 4/50\n",
      "136/136 [==============================] - 0s 206us/step - loss: 0.6690 - acc: 0.5882\n",
      "Epoch 5/50\n",
      "136/136 [==============================] - 0s 259us/step - loss: 0.6548 - acc: 0.6471\n",
      "Epoch 6/50\n",
      "136/136 [==============================] - 0s 221us/step - loss: 0.6444 - acc: 0.6397\n",
      "Epoch 7/50\n",
      "136/136 [==============================] - 0s 146us/step - loss: 0.6339 - acc: 0.6176\n",
      "Epoch 8/50\n",
      "136/136 [==============================] - 0s 135us/step - loss: 0.6247 - acc: 0.6618\n",
      "Epoch 9/50\n",
      "136/136 [==============================] - 0s 226us/step - loss: 0.6157 - acc: 0.6765\n",
      "Epoch 10/50\n",
      "136/136 [==============================] - 0s 153us/step - loss: 0.6086 - acc: 0.7206\n",
      "Epoch 11/50\n",
      "136/136 [==============================] - 0s 210us/step - loss: 0.6015 - acc: 0.7353\n",
      "Epoch 12/50\n",
      "136/136 [==============================] - 0s 226us/step - loss: 0.5951 - acc: 0.7279\n",
      "Epoch 13/50\n",
      "136/136 [==============================] - 0s 174us/step - loss: 0.5883 - acc: 0.7206\n",
      "Epoch 14/50\n",
      "136/136 [==============================] - 0s 190us/step - loss: 0.5818 - acc: 0.7206\n",
      "Epoch 15/50\n",
      "136/136 [==============================] - 0s 211us/step - loss: 0.5748 - acc: 0.7279\n",
      "Epoch 16/50\n",
      "136/136 [==============================] - 0s 135us/step - loss: 0.5688 - acc: 0.7279\n",
      "Epoch 17/50\n",
      "136/136 [==============================] - 0s 175us/step - loss: 0.5614 - acc: 0.7279\n",
      "Epoch 18/50\n",
      "136/136 [==============================] - 0s 220us/step - loss: 0.5546 - acc: 0.7279\n",
      "Epoch 19/50\n",
      "136/136 [==============================] - 0s 231us/step - loss: 0.5477 - acc: 0.7353\n",
      "Epoch 20/50\n",
      "136/136 [==============================] - 0s 193us/step - loss: 0.5409 - acc: 0.7574\n",
      "Epoch 21/50\n",
      "136/136 [==============================] - 0s 164us/step - loss: 0.5346 - acc: 0.7647\n",
      "Epoch 22/50\n",
      "136/136 [==============================] - 0s 189us/step - loss: 0.5286 - acc: 0.7647\n",
      "Epoch 23/50\n",
      "136/136 [==============================] - 0s 154us/step - loss: 0.5224 - acc: 0.7647\n",
      "Epoch 24/50\n",
      "136/136 [==============================] - 0s 122us/step - loss: 0.5167 - acc: 0.7647\n",
      "Epoch 25/50\n",
      "136/136 [==============================] - 0s 140us/step - loss: 0.5118 - acc: 0.7721\n",
      "Epoch 26/50\n",
      "136/136 [==============================] - 0s 124us/step - loss: 0.5057 - acc: 0.7868\n",
      "Epoch 27/50\n",
      "136/136 [==============================] - 0s 126us/step - loss: 0.5008 - acc: 0.7868\n",
      "Epoch 28/50\n",
      "136/136 [==============================] - 0s 192us/step - loss: 0.4955 - acc: 0.7868\n",
      "Epoch 29/50\n",
      "136/136 [==============================] - 0s 208us/step - loss: 0.4906 - acc: 0.7868\n",
      "Epoch 30/50\n",
      "136/136 [==============================] - 0s 167us/step - loss: 0.4861 - acc: 0.8015\n",
      "Epoch 31/50\n",
      "136/136 [==============================] - 0s 129us/step - loss: 0.4813 - acc: 0.8015\n",
      "Epoch 32/50\n",
      "136/136 [==============================] - 0s 142us/step - loss: 0.4772 - acc: 0.8015\n",
      "Epoch 33/50\n",
      "136/136 [==============================] - 0s 142us/step - loss: 0.4730 - acc: 0.8015\n",
      "Epoch 34/50\n",
      "136/136 [==============================] - 0s 141us/step - loss: 0.4690 - acc: 0.7941\n",
      "Epoch 35/50\n",
      "136/136 [==============================] - 0s 174us/step - loss: 0.4655 - acc: 0.7941\n",
      "Epoch 36/50\n",
      "136/136 [==============================] - 0s 173us/step - loss: 0.4615 - acc: 0.7941\n",
      "Epoch 37/50\n",
      "136/136 [==============================] - 0s 174us/step - loss: 0.4578 - acc: 0.8015\n",
      "Epoch 38/50\n",
      "136/136 [==============================] - 0s 212us/step - loss: 0.4542 - acc: 0.8015\n",
      "Epoch 39/50\n",
      "136/136 [==============================] - 0s 223us/step - loss: 0.4507 - acc: 0.8088\n",
      "Epoch 40/50\n",
      "136/136 [==============================] - 0s 177us/step - loss: 0.4472 - acc: 0.8088\n",
      "Epoch 41/50\n",
      "136/136 [==============================] - 0s 153us/step - loss: 0.4440 - acc: 0.8088\n",
      "Epoch 42/50\n",
      "136/136 [==============================] - 0s 140us/step - loss: 0.4408 - acc: 0.8088\n",
      "Epoch 43/50\n",
      "136/136 [==============================] - 0s 153us/step - loss: 0.4380 - acc: 0.8162\n",
      "Epoch 44/50\n",
      "136/136 [==============================] - 0s 137us/step - loss: 0.4347 - acc: 0.8162\n",
      "Epoch 45/50\n",
      "136/136 [==============================] - 0s 140us/step - loss: 0.4322 - acc: 0.8162\n",
      "Epoch 46/50\n",
      "136/136 [==============================] - 0s 152us/step - loss: 0.4290 - acc: 0.8162\n",
      "Epoch 47/50\n",
      "136/136 [==============================] - 0s 159us/step - loss: 0.4265 - acc: 0.8162\n",
      "Epoch 48/50\n",
      "136/136 [==============================] - 0s 141us/step - loss: 0.4238 - acc: 0.8088\n",
      "Epoch 49/50\n",
      "136/136 [==============================] - 0s 151us/step - loss: 0.4207 - acc: 0.8162\n",
      "Epoch 50/50\n",
      "136/136 [==============================] - 0s 152us/step - loss: 0.4185 - acc: 0.8162\n",
      "67/67 [==============================] - 0s 4ms/step\n",
      "136/136 [==============================] - 0s 117us/step\n",
      "Epoch 1/100\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.8260 - acc: 0.4519\n",
      "Epoch 2/100\n",
      "135/135 [==============================] - 0s 209us/step - loss: 0.7995 - acc: 0.4444\n",
      "Epoch 3/100\n",
      "135/135 [==============================] - 0s 191us/step - loss: 0.7778 - acc: 0.4519\n",
      "Epoch 4/100\n",
      "135/135 [==============================] - 0s 182us/step - loss: 0.7562 - acc: 0.4296\n",
      "Epoch 5/100\n",
      "135/135 [==============================] - 0s 205us/step - loss: 0.7397 - acc: 0.4296\n",
      "Epoch 6/100\n",
      "135/135 [==============================] - 0s 172us/step - loss: 0.7223 - acc: 0.4667\n",
      "Epoch 7/100\n",
      "135/135 [==============================] - 0s 128us/step - loss: 0.7075 - acc: 0.4889\n",
      "Epoch 8/100\n",
      "135/135 [==============================] - 0s 134us/step - loss: 0.6950 - acc: 0.4815\n",
      "Epoch 9/100\n",
      "135/135 [==============================] - 0s 185us/step - loss: 0.6832 - acc: 0.4889\n",
      "Epoch 10/100\n",
      "135/135 [==============================] - 0s 182us/step - loss: 0.6718 - acc: 0.5407\n",
      "Epoch 11/100\n",
      "135/135 [==============================] - 0s 214us/step - loss: 0.6612 - acc: 0.5926\n",
      "Epoch 12/100\n",
      "135/135 [==============================] - 0s 149us/step - loss: 0.6518 - acc: 0.6444\n",
      "Epoch 13/100\n",
      "135/135 [==============================] - 0s 235us/step - loss: 0.6425 - acc: 0.6963\n",
      "Epoch 14/100\n",
      "135/135 [==============================] - 0s 332us/step - loss: 0.6337 - acc: 0.7111\n",
      "Epoch 15/100\n",
      "135/135 [==============================] - 0s 190us/step - loss: 0.6259 - acc: 0.7037\n",
      "Epoch 16/100\n",
      "135/135 [==============================] - 0s 168us/step - loss: 0.6178 - acc: 0.7111\n",
      "Epoch 17/100\n",
      "135/135 [==============================] - 0s 159us/step - loss: 0.6105 - acc: 0.7111\n",
      "Epoch 18/100\n",
      "135/135 [==============================] - 0s 195us/step - loss: 0.6021 - acc: 0.7037\n",
      "Epoch 19/100\n",
      "135/135 [==============================] - 0s 209us/step - loss: 0.5947 - acc: 0.7111\n",
      "Epoch 20/100\n",
      "135/135 [==============================] - 0s 193us/step - loss: 0.5865 - acc: 0.7111\n",
      "Epoch 21/100\n",
      "135/135 [==============================] - 0s 132us/step - loss: 0.5793 - acc: 0.7111\n",
      "Epoch 22/100\n",
      "135/135 [==============================] - 0s 157us/step - loss: 0.5725 - acc: 0.7259\n",
      "Epoch 23/100\n",
      "135/135 [==============================] - 0s 179us/step - loss: 0.5649 - acc: 0.7333\n",
      "Epoch 24/100\n",
      "135/135 [==============================] - 0s 202us/step - loss: 0.5582 - acc: 0.7481\n",
      "Epoch 25/100\n",
      "135/135 [==============================] - 0s 164us/step - loss: 0.5516 - acc: 0.7481\n",
      "Epoch 26/100\n",
      "135/135 [==============================] - 0s 179us/step - loss: 0.5453 - acc: 0.7481\n",
      "Epoch 27/100\n",
      "135/135 [==============================] - 0s 194us/step - loss: 0.5385 - acc: 0.7481\n",
      "Epoch 28/100\n",
      "135/135 [==============================] - 0s 207us/step - loss: 0.5327 - acc: 0.7556\n",
      "Epoch 29/100\n",
      "135/135 [==============================] - 0s 203us/step - loss: 0.5268 - acc: 0.7630\n",
      "Epoch 30/100\n",
      "135/135 [==============================] - 0s 169us/step - loss: 0.5215 - acc: 0.7704\n",
      "Epoch 31/100\n",
      "135/135 [==============================] - 0s 165us/step - loss: 0.5161 - acc: 0.7704\n",
      "Epoch 32/100\n",
      "135/135 [==============================] - 0s 189us/step - loss: 0.5106 - acc: 0.7704\n",
      "Epoch 33/100\n",
      "135/135 [==============================] - 0s 198us/step - loss: 0.5056 - acc: 0.7778\n",
      "Epoch 34/100\n",
      "135/135 [==============================] - 0s 184us/step - loss: 0.5010 - acc: 0.7852\n",
      "Epoch 35/100\n",
      "135/135 [==============================] - 0s 160us/step - loss: 0.4963 - acc: 0.7630\n",
      "Epoch 36/100\n",
      "135/135 [==============================] - 0s 169us/step - loss: 0.4922 - acc: 0.7630\n",
      "Epoch 37/100\n",
      "135/135 [==============================] - 0s 175us/step - loss: 0.4876 - acc: 0.7704\n",
      "Epoch 38/100\n",
      "135/135 [==============================] - 0s 219us/step - loss: 0.4837 - acc: 0.7704\n",
      "Epoch 39/100\n",
      "135/135 [==============================] - 0s 184us/step - loss: 0.4793 - acc: 0.7704\n",
      "Epoch 40/100\n",
      "135/135 [==============================] - 0s 175us/step - loss: 0.4757 - acc: 0.7778\n",
      "Epoch 41/100\n",
      "135/135 [==============================] - 0s 157us/step - loss: 0.4719 - acc: 0.7778\n",
      "Epoch 42/100\n",
      "135/135 [==============================] - 0s 152us/step - loss: 0.4680 - acc: 0.7778\n",
      "Epoch 43/100\n",
      "135/135 [==============================] - 0s 151us/step - loss: 0.4643 - acc: 0.7852\n",
      "Epoch 44/100\n",
      "135/135 [==============================] - 0s 157us/step - loss: 0.4608 - acc: 0.7926\n",
      "Epoch 45/100\n",
      "135/135 [==============================] - 0s 165us/step - loss: 0.4575 - acc: 0.7926\n",
      "Epoch 46/100\n",
      "135/135 [==============================] - 0s 162us/step - loss: 0.4543 - acc: 0.7926\n",
      "Epoch 47/100\n",
      "135/135 [==============================] - 0s 171us/step - loss: 0.4510 - acc: 0.8074\n",
      "Epoch 48/100\n",
      "135/135 [==============================] - 0s 168us/step - loss: 0.4480 - acc: 0.8000\n",
      "Epoch 49/100\n",
      "135/135 [==============================] - 0s 163us/step - loss: 0.4449 - acc: 0.8000\n",
      "Epoch 50/100\n",
      "135/135 [==============================] - 0s 140us/step - loss: 0.4426 - acc: 0.8000\n",
      "Epoch 51/100\n",
      "135/135 [==============================] - 0s 145us/step - loss: 0.4392 - acc: 0.8000\n",
      "Epoch 52/100\n",
      "135/135 [==============================] - 0s 155us/step - loss: 0.4369 - acc: 0.8000\n",
      "Epoch 53/100\n",
      "135/135 [==============================] - 0s 135us/step - loss: 0.4341 - acc: 0.8000\n",
      "Epoch 54/100\n",
      "135/135 [==============================] - 0s 161us/step - loss: 0.4317 - acc: 0.8000\n",
      "Epoch 55/100\n",
      "135/135 [==============================] - 0s 155us/step - loss: 0.4291 - acc: 0.8000\n",
      "Epoch 56/100\n",
      "135/135 [==============================] - 0s 166us/step - loss: 0.4266 - acc: 0.8000\n",
      "Epoch 57/100\n",
      "135/135 [==============================] - 0s 147us/step - loss: 0.4242 - acc: 0.8074\n",
      "Epoch 58/100\n",
      "135/135 [==============================] - 0s 195us/step - loss: 0.4221 - acc: 0.8000\n",
      "Epoch 59/100\n",
      "135/135 [==============================] - 0s 145us/step - loss: 0.4198 - acc: 0.8074\n",
      "Epoch 60/100\n",
      "135/135 [==============================] - 0s 123us/step - loss: 0.4182 - acc: 0.8074\n",
      "Epoch 61/100\n",
      "135/135 [==============================] - 0s 160us/step - loss: 0.4158 - acc: 0.8074\n",
      "Epoch 62/100\n",
      "135/135 [==============================] - 0s 128us/step - loss: 0.4136 - acc: 0.8148\n",
      "Epoch 63/100\n",
      "135/135 [==============================] - 0s 140us/step - loss: 0.4114 - acc: 0.8148\n",
      "Epoch 64/100\n",
      "135/135 [==============================] - 0s 119us/step - loss: 0.4097 - acc: 0.8148\n",
      "Epoch 65/100\n",
      "135/135 [==============================] - 0s 172us/step - loss: 0.4075 - acc: 0.8148\n",
      "Epoch 66/100\n",
      "135/135 [==============================] - 0s 132us/step - loss: 0.4056 - acc: 0.8148\n",
      "Epoch 67/100\n",
      "135/135 [==============================] - 0s 125us/step - loss: 0.4039 - acc: 0.8148\n",
      "Epoch 68/100\n",
      "135/135 [==============================] - 0s 132us/step - loss: 0.4027 - acc: 0.8222\n",
      "Epoch 69/100\n",
      "135/135 [==============================] - 0s 114us/step - loss: 0.4006 - acc: 0.8148\n",
      "Epoch 70/100\n",
      "135/135 [==============================] - 0s 176us/step - loss: 0.3989 - acc: 0.8222\n",
      "Epoch 71/100\n",
      "135/135 [==============================] - 0s 133us/step - loss: 0.3972 - acc: 0.8222\n",
      "Epoch 72/100\n",
      "135/135 [==============================] - 0s 131us/step - loss: 0.3954 - acc: 0.8296\n",
      "Epoch 73/100\n",
      "135/135 [==============================] - 0s 145us/step - loss: 0.3936 - acc: 0.8296\n",
      "Epoch 74/100\n",
      "135/135 [==============================] - 0s 123us/step - loss: 0.3922 - acc: 0.8444\n",
      "Epoch 75/100\n",
      "135/135 [==============================] - 0s 153us/step - loss: 0.3906 - acc: 0.8444\n",
      "Epoch 76/100\n",
      "135/135 [==============================] - 0s 152us/step - loss: 0.3894 - acc: 0.8519\n",
      "Epoch 77/100\n",
      "135/135 [==============================] - 0s 134us/step - loss: 0.3873 - acc: 0.8444\n",
      "Epoch 78/100\n",
      "135/135 [==============================] - 0s 142us/step - loss: 0.3865 - acc: 0.8444\n",
      "Epoch 79/100\n",
      "135/135 [==============================] - 0s 138us/step - loss: 0.3848 - acc: 0.8444\n",
      "Epoch 80/100\n",
      "135/135 [==============================] - 0s 146us/step - loss: 0.3830 - acc: 0.8444\n",
      "Epoch 81/100\n",
      "135/135 [==============================] - 0s 153us/step - loss: 0.3817 - acc: 0.8444\n",
      "Epoch 82/100\n",
      "135/135 [==============================] - 0s 135us/step - loss: 0.3804 - acc: 0.8444\n",
      "Epoch 83/100\n",
      "135/135 [==============================] - 0s 206us/step - loss: 0.3790 - acc: 0.8444\n",
      "Epoch 84/100\n",
      "135/135 [==============================] - 0s 169us/step - loss: 0.3775 - acc: 0.8444\n",
      "Epoch 85/100\n",
      "135/135 [==============================] - 0s 211us/step - loss: 0.3762 - acc: 0.8444\n",
      "Epoch 86/100\n",
      "135/135 [==============================] - 0s 187us/step - loss: 0.3752 - acc: 0.8444\n",
      "Epoch 87/100\n",
      "135/135 [==============================] - 0s 145us/step - loss: 0.3754 - acc: 0.8444\n",
      "Epoch 88/100\n",
      "135/135 [==============================] - 0s 126us/step - loss: 0.3731 - acc: 0.8444\n",
      "Epoch 89/100\n",
      "135/135 [==============================] - 0s 132us/step - loss: 0.3716 - acc: 0.8444\n",
      "Epoch 90/100\n",
      "135/135 [==============================] - 0s 117us/step - loss: 0.3703 - acc: 0.8444\n",
      "Epoch 91/100\n",
      "135/135 [==============================] - 0s 127us/step - loss: 0.3692 - acc: 0.8444\n",
      "Epoch 92/100\n",
      "135/135 [==============================] - 0s 141us/step - loss: 0.3682 - acc: 0.8444\n",
      "Epoch 93/100\n",
      "135/135 [==============================] - 0s 183us/step - loss: 0.3669 - acc: 0.8444\n",
      "Epoch 94/100\n",
      "135/135 [==============================] - 0s 157us/step - loss: 0.3662 - acc: 0.8444\n",
      "Epoch 95/100\n",
      "135/135 [==============================] - 0s 122us/step - loss: 0.3654 - acc: 0.8444\n",
      "Epoch 96/100\n",
      "135/135 [==============================] - 0s 147us/step - loss: 0.3642 - acc: 0.8519\n",
      "Epoch 97/100\n",
      "135/135 [==============================] - 0s 117us/step - loss: 0.3633 - acc: 0.8519\n",
      "Epoch 98/100\n",
      "135/135 [==============================] - 0s 156us/step - loss: 0.3618 - acc: 0.8519\n",
      "Epoch 99/100\n",
      "135/135 [==============================] - 0s 112us/step - loss: 0.3609 - acc: 0.8593\n",
      "Epoch 100/100\n",
      "135/135 [==============================] - 0s 121us/step - loss: 0.3600 - acc: 0.8519\n",
      "68/68 [==============================] - 0s 3ms/step\n",
      "135/135 [==============================] - 0s 131us/step\n",
      "Epoch 1/100\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.6707 - acc: 0.4889\n",
      "Epoch 2/100\n",
      "135/135 [==============================] - 0s 202us/step - loss: 0.6595 - acc: 0.5037\n",
      "Epoch 3/100\n",
      "135/135 [==============================] - 0s 226us/step - loss: 0.6482 - acc: 0.5407\n",
      "Epoch 4/100\n",
      "135/135 [==============================] - 0s 192us/step - loss: 0.6378 - acc: 0.5481\n",
      "Epoch 5/100\n",
      "135/135 [==============================] - 0s 188us/step - loss: 0.6294 - acc: 0.6000\n",
      "Epoch 6/100\n",
      "135/135 [==============================] - 0s 217us/step - loss: 0.6205 - acc: 0.6074\n",
      "Epoch 7/100\n",
      "135/135 [==============================] - 0s 201us/step - loss: 0.6125 - acc: 0.6370\n",
      "Epoch 8/100\n",
      "135/135 [==============================] - 0s 179us/step - loss: 0.6050 - acc: 0.6519\n",
      "Epoch 9/100\n",
      "135/135 [==============================] - 0s 222us/step - loss: 0.5976 - acc: 0.6889\n",
      "Epoch 10/100\n",
      "135/135 [==============================] - 0s 189us/step - loss: 0.5899 - acc: 0.7259\n",
      "Epoch 11/100\n",
      "135/135 [==============================] - 0s 255us/step - loss: 0.5826 - acc: 0.7481\n",
      "Epoch 12/100\n",
      "135/135 [==============================] - 0s 166us/step - loss: 0.5748 - acc: 0.7704\n",
      "Epoch 13/100\n",
      "135/135 [==============================] - 0s 205us/step - loss: 0.5677 - acc: 0.7778\n",
      "Epoch 14/100\n",
      "135/135 [==============================] - 0s 188us/step - loss: 0.5606 - acc: 0.7704\n",
      "Epoch 15/100\n",
      "135/135 [==============================] - 0s 228us/step - loss: 0.5535 - acc: 0.7778\n",
      "Epoch 16/100\n",
      "135/135 [==============================] - 0s 178us/step - loss: 0.5463 - acc: 0.7704\n",
      "Epoch 17/100\n",
      "135/135 [==============================] - 0s 152us/step - loss: 0.5397 - acc: 0.7778\n",
      "Epoch 18/100\n",
      "135/135 [==============================] - 0s 170us/step - loss: 0.5328 - acc: 0.7926\n",
      "Epoch 19/100\n",
      "135/135 [==============================] - 0s 143us/step - loss: 0.5265 - acc: 0.8000\n",
      "Epoch 20/100\n",
      "135/135 [==============================] - 0s 145us/step - loss: 0.5199 - acc: 0.8000\n",
      "Epoch 21/100\n",
      "135/135 [==============================] - 0s 167us/step - loss: 0.5141 - acc: 0.8074\n",
      "Epoch 22/100\n",
      "135/135 [==============================] - 0s 280us/step - loss: 0.5077 - acc: 0.8148\n",
      "Epoch 23/100\n",
      "135/135 [==============================] - 0s 192us/step - loss: 0.5021 - acc: 0.8148\n",
      "Epoch 24/100\n",
      "135/135 [==============================] - 0s 168us/step - loss: 0.4963 - acc: 0.8222\n",
      "Epoch 25/100\n",
      "135/135 [==============================] - 0s 137us/step - loss: 0.4913 - acc: 0.8222\n",
      "Epoch 26/100\n",
      "135/135 [==============================] - 0s 154us/step - loss: 0.4855 - acc: 0.8370\n",
      "Epoch 27/100\n",
      "135/135 [==============================] - 0s 181us/step - loss: 0.4804 - acc: 0.8296\n",
      "Epoch 28/100\n",
      "135/135 [==============================] - 0s 147us/step - loss: 0.4750 - acc: 0.8370\n",
      "Epoch 29/100\n",
      "135/135 [==============================] - 0s 129us/step - loss: 0.4697 - acc: 0.8370\n",
      "Epoch 30/100\n",
      "135/135 [==============================] - 0s 167us/step - loss: 0.4652 - acc: 0.8370\n",
      "Epoch 31/100\n",
      "135/135 [==============================] - 0s 161us/step - loss: 0.4605 - acc: 0.8370\n",
      "Epoch 32/100\n",
      "135/135 [==============================] - 0s 133us/step - loss: 0.4557 - acc: 0.8296\n",
      "Epoch 33/100\n",
      "135/135 [==============================] - 0s 152us/step - loss: 0.4512 - acc: 0.8296\n",
      "Epoch 34/100\n",
      "135/135 [==============================] - 0s 169us/step - loss: 0.4468 - acc: 0.8296\n",
      "Epoch 35/100\n",
      "135/135 [==============================] - 0s 147us/step - loss: 0.4425 - acc: 0.8370\n",
      "Epoch 36/100\n",
      "135/135 [==============================] - 0s 154us/step - loss: 0.4382 - acc: 0.8444\n",
      "Epoch 37/100\n",
      "135/135 [==============================] - 0s 337us/step - loss: 0.4343 - acc: 0.8444\n",
      "Epoch 38/100\n",
      "135/135 [==============================] - 0s 156us/step - loss: 0.4306 - acc: 0.8370\n",
      "Epoch 39/100\n",
      "135/135 [==============================] - 0s 139us/step - loss: 0.4265 - acc: 0.8370\n",
      "Epoch 40/100\n",
      "135/135 [==============================] - 0s 160us/step - loss: 0.4236 - acc: 0.8370\n",
      "Epoch 41/100\n",
      "135/135 [==============================] - 0s 142us/step - loss: 0.4194 - acc: 0.8370\n",
      "Epoch 42/100\n",
      "135/135 [==============================] - 0s 145us/step - loss: 0.4157 - acc: 0.8370\n",
      "Epoch 43/100\n",
      "135/135 [==============================] - 0s 172us/step - loss: 0.4124 - acc: 0.8370\n",
      "Epoch 44/100\n",
      "135/135 [==============================] - 0s 143us/step - loss: 0.4093 - acc: 0.8444\n",
      "Epoch 45/100\n",
      "135/135 [==============================] - 0s 158us/step - loss: 0.4064 - acc: 0.8444\n",
      "Epoch 46/100\n",
      "135/135 [==============================] - 0s 128us/step - loss: 0.4031 - acc: 0.8444\n",
      "Epoch 47/100\n",
      "135/135 [==============================] - 0s 155us/step - loss: 0.4001 - acc: 0.8444\n",
      "Epoch 48/100\n",
      "135/135 [==============================] - 0s 171us/step - loss: 0.3969 - acc: 0.8519\n",
      "Epoch 49/100\n",
      "135/135 [==============================] - 0s 150us/step - loss: 0.3943 - acc: 0.8519\n",
      "Epoch 50/100\n",
      "135/135 [==============================] - 0s 146us/step - loss: 0.3919 - acc: 0.8519\n",
      "Epoch 51/100\n",
      "135/135 [==============================] - 0s 123us/step - loss: 0.3889 - acc: 0.8519\n",
      "Epoch 52/100\n",
      "135/135 [==============================] - 0s 145us/step - loss: 0.3865 - acc: 0.8593\n",
      "Epoch 53/100\n",
      "135/135 [==============================] - 0s 151us/step - loss: 0.3838 - acc: 0.8444\n",
      "Epoch 54/100\n",
      "135/135 [==============================] - 0s 122us/step - loss: 0.3813 - acc: 0.8444\n",
      "Epoch 55/100\n",
      "135/135 [==============================] - 0s 150us/step - loss: 0.3792 - acc: 0.8444\n",
      "Epoch 56/100\n",
      "135/135 [==============================] - 0s 125us/step - loss: 0.3767 - acc: 0.8519\n",
      "Epoch 57/100\n",
      "135/135 [==============================] - 0s 138us/step - loss: 0.3744 - acc: 0.8593\n",
      "Epoch 58/100\n",
      "135/135 [==============================] - 0s 124us/step - loss: 0.3724 - acc: 0.8519\n",
      "Epoch 59/100\n",
      "135/135 [==============================] - 0s 127us/step - loss: 0.3706 - acc: 0.8519\n",
      "Epoch 60/100\n",
      "135/135 [==============================] - 0s 148us/step - loss: 0.3680 - acc: 0.8519\n",
      "Epoch 61/100\n",
      "135/135 [==============================] - 0s 149us/step - loss: 0.3659 - acc: 0.8593\n",
      "Epoch 62/100\n",
      "135/135 [==============================] - 0s 129us/step - loss: 0.3641 - acc: 0.8593\n",
      "Epoch 63/100\n",
      "135/135 [==============================] - 0s 155us/step - loss: 0.3622 - acc: 0.8593\n",
      "Epoch 64/100\n",
      "135/135 [==============================] - 0s 116us/step - loss: 0.3610 - acc: 0.8519\n",
      "Epoch 65/100\n",
      "135/135 [==============================] - 0s 153us/step - loss: 0.3583 - acc: 0.8593\n",
      "Epoch 66/100\n",
      "135/135 [==============================] - 0s 156us/step - loss: 0.3568 - acc: 0.8593\n",
      "Epoch 67/100\n",
      "135/135 [==============================] - 0s 150us/step - loss: 0.3549 - acc: 0.8593\n",
      "Epoch 68/100\n",
      "135/135 [==============================] - 0s 143us/step - loss: 0.3534 - acc: 0.8593\n",
      "Epoch 69/100\n",
      "135/135 [==============================] - 0s 136us/step - loss: 0.3517 - acc: 0.8593\n",
      "Epoch 70/100\n",
      "135/135 [==============================] - 0s 129us/step - loss: 0.3502 - acc: 0.8593\n",
      "Epoch 71/100\n",
      "135/135 [==============================] - 0s 151us/step - loss: 0.3488 - acc: 0.8593\n",
      "Epoch 72/100\n",
      "135/135 [==============================] - 0s 140us/step - loss: 0.3470 - acc: 0.8593\n",
      "Epoch 73/100\n",
      "135/135 [==============================] - 0s 153us/step - loss: 0.3455 - acc: 0.8593\n",
      "Epoch 74/100\n",
      "135/135 [==============================] - 0s 129us/step - loss: 0.3441 - acc: 0.8593\n",
      "Epoch 75/100\n",
      "135/135 [==============================] - 0s 131us/step - loss: 0.3426 - acc: 0.8593\n",
      "Epoch 76/100\n",
      "135/135 [==============================] - 0s 141us/step - loss: 0.3421 - acc: 0.8593\n",
      "Epoch 77/100\n",
      "135/135 [==============================] - 0s 159us/step - loss: 0.3402 - acc: 0.8593\n",
      "Epoch 78/100\n",
      "135/135 [==============================] - 0s 130us/step - loss: 0.3388 - acc: 0.8593\n",
      "Epoch 79/100\n",
      "135/135 [==============================] - 0s 143us/step - loss: 0.3376 - acc: 0.8593\n",
      "Epoch 80/100\n",
      "135/135 [==============================] - 0s 126us/step - loss: 0.3364 - acc: 0.8593\n",
      "Epoch 81/100\n",
      "135/135 [==============================] - 0s 136us/step - loss: 0.3359 - acc: 0.8667\n",
      "Epoch 82/100\n",
      "135/135 [==============================] - 0s 133us/step - loss: 0.3340 - acc: 0.8667\n",
      "Epoch 83/100\n",
      "135/135 [==============================] - 0s 125us/step - loss: 0.3328 - acc: 0.8667\n",
      "Epoch 84/100\n",
      "135/135 [==============================] - 0s 184us/step - loss: 0.3318 - acc: 0.8667\n",
      "Epoch 85/100\n",
      "135/135 [==============================] - 0s 140us/step - loss: 0.3306 - acc: 0.8741\n",
      "Epoch 86/100\n",
      "135/135 [==============================] - 0s 145us/step - loss: 0.3298 - acc: 0.8741\n",
      "Epoch 87/100\n",
      "135/135 [==============================] - 0s 137us/step - loss: 0.3285 - acc: 0.8741\n",
      "Epoch 88/100\n",
      "135/135 [==============================] - 0s 144us/step - loss: 0.3288 - acc: 0.8815\n",
      "Epoch 89/100\n",
      "135/135 [==============================] - 0s 160us/step - loss: 0.3263 - acc: 0.8963\n",
      "Epoch 90/100\n",
      "135/135 [==============================] - 0s 187us/step - loss: 0.3253 - acc: 0.8889\n",
      "Epoch 91/100\n",
      "135/135 [==============================] - 0s 146us/step - loss: 0.3247 - acc: 0.8963\n",
      "Epoch 92/100\n",
      "135/135 [==============================] - 0s 177us/step - loss: 0.3237 - acc: 0.8963\n",
      "Epoch 93/100\n",
      "135/135 [==============================] - 0s 136us/step - loss: 0.3228 - acc: 0.9037\n",
      "Epoch 94/100\n",
      "135/135 [==============================] - 0s 141us/step - loss: 0.3219 - acc: 0.9037\n",
      "Epoch 95/100\n",
      "135/135 [==============================] - 0s 189us/step - loss: 0.3210 - acc: 0.9037\n",
      "Epoch 96/100\n",
      "135/135 [==============================] - 0s 170us/step - loss: 0.3200 - acc: 0.8963\n",
      "Epoch 97/100\n",
      "135/135 [==============================] - 0s 150us/step - loss: 0.3191 - acc: 0.9037\n",
      "Epoch 98/100\n",
      "135/135 [==============================] - 0s 183us/step - loss: 0.3182 - acc: 0.9037\n",
      "Epoch 99/100\n",
      "135/135 [==============================] - 0s 153us/step - loss: 0.3181 - acc: 0.8963\n",
      "Epoch 100/100\n",
      "135/135 [==============================] - 0s 182us/step - loss: 0.3165 - acc: 0.9037\n",
      "68/68 [==============================] - 0s 3ms/step\n",
      "135/135 [==============================] - 0s 64us/step\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 0.7942 - acc: 0.5368\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 185us/step - loss: 0.7666 - acc: 0.5368\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 204us/step - loss: 0.7389 - acc: 0.5294\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 166us/step - loss: 0.7172 - acc: 0.5294\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 175us/step - loss: 0.6991 - acc: 0.5147\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 158us/step - loss: 0.6825 - acc: 0.5294\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 182us/step - loss: 0.6709 - acc: 0.5441\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 144us/step - loss: 0.6608 - acc: 0.5882\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 163us/step - loss: 0.6524 - acc: 0.6250\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 176us/step - loss: 0.6454 - acc: 0.6250\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 162us/step - loss: 0.6393 - acc: 0.6471\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 187us/step - loss: 0.6334 - acc: 0.6618\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 132us/step - loss: 0.6282 - acc: 0.6765\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 179us/step - loss: 0.6228 - acc: 0.6985\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 122us/step - loss: 0.6175 - acc: 0.7206\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 132us/step - loss: 0.6127 - acc: 0.7279\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 163us/step - loss: 0.6073 - acc: 0.7279\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 134us/step - loss: 0.6026 - acc: 0.7353\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 168us/step - loss: 0.5974 - acc: 0.7426\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 160us/step - loss: 0.5926 - acc: 0.7426\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 140us/step - loss: 0.5875 - acc: 0.7647\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 177us/step - loss: 0.5822 - acc: 0.7647\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 132us/step - loss: 0.5773 - acc: 0.7647\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 206us/step - loss: 0.5725 - acc: 0.7647\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 159us/step - loss: 0.5676 - acc: 0.7721\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 138us/step - loss: 0.5625 - acc: 0.7721\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 123us/step - loss: 0.5577 - acc: 0.7868\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 168us/step - loss: 0.5528 - acc: 0.7941\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 156us/step - loss: 0.5478 - acc: 0.7941\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 152us/step - loss: 0.5433 - acc: 0.7941\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 111us/step - loss: 0.5390 - acc: 0.7941\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 149us/step - loss: 0.5343 - acc: 0.8015\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 143us/step - loss: 0.5297 - acc: 0.8015\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 143us/step - loss: 0.5251 - acc: 0.8015\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 173us/step - loss: 0.5206 - acc: 0.8015\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 0s 145us/step - loss: 0.5163 - acc: 0.8088\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 156us/step - loss: 0.5118 - acc: 0.8088\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 159us/step - loss: 0.5078 - acc: 0.8088\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 0s 149us/step - loss: 0.5032 - acc: 0.8088\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 0s 158us/step - loss: 0.4990 - acc: 0.8088\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 150us/step - loss: 0.4954 - acc: 0.8088\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 169us/step - loss: 0.4915 - acc: 0.8088\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 126us/step - loss: 0.4868 - acc: 0.8162\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 139us/step - loss: 0.4830 - acc: 0.8162\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 123us/step - loss: 0.4792 - acc: 0.8088\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 154us/step - loss: 0.4751 - acc: 0.8088\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 153us/step - loss: 0.4714 - acc: 0.8309\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 155us/step - loss: 0.4679 - acc: 0.8309\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 106us/step - loss: 0.4639 - acc: 0.8309\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 135us/step - loss: 0.4603 - acc: 0.8309\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 128us/step - loss: 0.4570 - acc: 0.8309\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 127us/step - loss: 0.4536 - acc: 0.8309\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 110us/step - loss: 0.4503 - acc: 0.8382\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 117us/step - loss: 0.4474 - acc: 0.8382\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 118us/step - loss: 0.4438 - acc: 0.8456\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 0s 162us/step - loss: 0.4407 - acc: 0.8456\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 0s 128us/step - loss: 0.4377 - acc: 0.8456\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 0s 121us/step - loss: 0.4349 - acc: 0.8456\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 0s 118us/step - loss: 0.4322 - acc: 0.8382\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - 0s 130us/step - loss: 0.4293 - acc: 0.8456\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 0s 152us/step - loss: 0.4266 - acc: 0.8456\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 0s 108us/step - loss: 0.4238 - acc: 0.8456\n",
      "Epoch 63/100\n",
      "136/136 [==============================] - 0s 116us/step - loss: 0.4216 - acc: 0.8456\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 0s 103us/step - loss: 0.4188 - acc: 0.8456\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 0s 139us/step - loss: 0.4166 - acc: 0.8603\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 0s 129us/step - loss: 0.4141 - acc: 0.8603\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 0s 106us/step - loss: 0.4117 - acc: 0.8603\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 0s 108us/step - loss: 0.4094 - acc: 0.8603\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 0s 148us/step - loss: 0.4072 - acc: 0.8603\n",
      "Epoch 70/100\n",
      "136/136 [==============================] - 0s 140us/step - loss: 0.4052 - acc: 0.8603\n",
      "Epoch 71/100\n",
      "136/136 [==============================] - 0s 114us/step - loss: 0.4031 - acc: 0.8603\n",
      "Epoch 72/100\n",
      "136/136 [==============================] - 0s 111us/step - loss: 0.4007 - acc: 0.8676\n",
      "Epoch 73/100\n",
      "136/136 [==============================] - 0s 133us/step - loss: 0.3990 - acc: 0.8676\n",
      "Epoch 74/100\n",
      "136/136 [==============================] - 0s 140us/step - loss: 0.3967 - acc: 0.8676\n",
      "Epoch 75/100\n",
      "136/136 [==============================] - 0s 119us/step - loss: 0.3951 - acc: 0.8676\n",
      "Epoch 76/100\n",
      "136/136 [==============================] - 0s 158us/step - loss: 0.3931 - acc: 0.8676\n",
      "Epoch 77/100\n",
      "136/136 [==============================] - 0s 125us/step - loss: 0.3910 - acc: 0.8750\n",
      "Epoch 78/100\n",
      "136/136 [==============================] - 0s 125us/step - loss: 0.3893 - acc: 0.8824\n",
      "Epoch 79/100\n",
      "136/136 [==============================] - 0s 146us/step - loss: 0.3873 - acc: 0.8824\n",
      "Epoch 80/100\n",
      "136/136 [==============================] - 0s 123us/step - loss: 0.3860 - acc: 0.8824\n",
      "Epoch 81/100\n",
      "136/136 [==============================] - 0s 114us/step - loss: 0.3840 - acc: 0.8824\n",
      "Epoch 82/100\n",
      "136/136 [==============================] - 0s 115us/step - loss: 0.3825 - acc: 0.8824\n",
      "Epoch 83/100\n",
      "136/136 [==============================] - 0s 135us/step - loss: 0.3809 - acc: 0.8824\n",
      "Epoch 84/100\n",
      "136/136 [==============================] - 0s 119us/step - loss: 0.3792 - acc: 0.8824\n",
      "Epoch 85/100\n",
      "136/136 [==============================] - 0s 119us/step - loss: 0.3777 - acc: 0.8824\n",
      "Epoch 86/100\n",
      "136/136 [==============================] - 0s 123us/step - loss: 0.3762 - acc: 0.8824\n",
      "Epoch 87/100\n",
      "136/136 [==============================] - 0s 111us/step - loss: 0.3753 - acc: 0.8824\n",
      "Epoch 88/100\n",
      "136/136 [==============================] - 0s 151us/step - loss: 0.3737 - acc: 0.8824\n",
      "Epoch 89/100\n",
      "136/136 [==============================] - 0s 117us/step - loss: 0.3720 - acc: 0.8824\n",
      "Epoch 90/100\n",
      "136/136 [==============================] - 0s 110us/step - loss: 0.3711 - acc: 0.8824\n",
      "Epoch 91/100\n",
      "136/136 [==============================] - 0s 121us/step - loss: 0.3695 - acc: 0.8824\n",
      "Epoch 92/100\n",
      "136/136 [==============================] - 0s 194us/step - loss: 0.3679 - acc: 0.8824\n",
      "Epoch 93/100\n",
      "136/136 [==============================] - 0s 131us/step - loss: 0.3667 - acc: 0.8824\n",
      "Epoch 94/100\n",
      "136/136 [==============================] - 0s 117us/step - loss: 0.3654 - acc: 0.8824\n",
      "Epoch 95/100\n",
      "136/136 [==============================] - 0s 108us/step - loss: 0.3646 - acc: 0.8824\n",
      "Epoch 96/100\n",
      "136/136 [==============================] - 0s 104us/step - loss: 0.3631 - acc: 0.8824\n",
      "Epoch 97/100\n",
      "136/136 [==============================] - 0s 136us/step - loss: 0.3620 - acc: 0.8824\n",
      "Epoch 98/100\n",
      "136/136 [==============================] - 0s 120us/step - loss: 0.3608 - acc: 0.8824\n",
      "Epoch 99/100\n",
      "136/136 [==============================] - 0s 110us/step - loss: 0.3597 - acc: 0.8824\n",
      "Epoch 100/100\n",
      "136/136 [==============================] - 0s 115us/step - loss: 0.3594 - acc: 0.8824\n",
      "67/67 [==============================] - 0s 3ms/step\n",
      "136/136 [==============================] - 0s 60us/step\n",
      "Epoch 1/20\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.7724 - acc: 0.4370\n",
      "Epoch 2/20\n",
      "135/135 [==============================] - 0s 111us/step - loss: 0.7631 - acc: 0.4370\n",
      "Epoch 3/20\n",
      "135/135 [==============================] - 0s 131us/step - loss: 0.7554 - acc: 0.4370\n",
      "Epoch 4/20\n",
      "135/135 [==============================] - 0s 141us/step - loss: 0.7474 - acc: 0.4296\n",
      "Epoch 5/20\n",
      "135/135 [==============================] - 0s 152us/step - loss: 0.7399 - acc: 0.4222\n",
      "Epoch 6/20\n",
      "135/135 [==============================] - 0s 129us/step - loss: 0.7341 - acc: 0.4148\n",
      "Epoch 7/20\n",
      "135/135 [==============================] - 0s 142us/step - loss: 0.7275 - acc: 0.4296\n",
      "Epoch 8/20\n",
      "135/135 [==============================] - 0s 131us/step - loss: 0.7218 - acc: 0.4370\n",
      "Epoch 9/20\n",
      "135/135 [==============================] - 0s 143us/step - loss: 0.7157 - acc: 0.4593\n",
      "Epoch 10/20\n",
      "135/135 [==============================] - 0s 149us/step - loss: 0.7103 - acc: 0.4741\n",
      "Epoch 11/20\n",
      "135/135 [==============================] - 0s 126us/step - loss: 0.7048 - acc: 0.5185\n",
      "Epoch 12/20\n",
      "135/135 [==============================] - 0s 195us/step - loss: 0.6995 - acc: 0.5333\n",
      "Epoch 13/20\n",
      "135/135 [==============================] - 0s 130us/step - loss: 0.6943 - acc: 0.5333\n",
      "Epoch 14/20\n",
      "135/135 [==============================] - 0s 137us/step - loss: 0.6894 - acc: 0.5630\n",
      "Epoch 15/20\n",
      "135/135 [==============================] - 0s 126us/step - loss: 0.6847 - acc: 0.5852\n",
      "Epoch 16/20\n",
      "135/135 [==============================] - 0s 91us/step - loss: 0.6801 - acc: 0.6074\n",
      "Epoch 17/20\n",
      "135/135 [==============================] - 0s 107us/step - loss: 0.6759 - acc: 0.6074\n",
      "Epoch 18/20\n",
      "135/135 [==============================] - 0s 93us/step - loss: 0.6715 - acc: 0.6222\n",
      "Epoch 19/20\n",
      "135/135 [==============================] - 0s 138us/step - loss: 0.6674 - acc: 0.6074\n",
      "Epoch 20/20\n",
      "135/135 [==============================] - 0s 116us/step - loss: 0.6634 - acc: 0.5926\n",
      "68/68 [==============================] - 0s 5ms/step\n",
      "135/135 [==============================] - 0s 86us/step\n",
      "Epoch 1/20\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.6735 - acc: 0.4963\n",
      "Epoch 2/20\n",
      "135/135 [==============================] - 0s 251us/step - loss: 0.6625 - acc: 0.5037\n",
      "Epoch 3/20\n",
      "135/135 [==============================] - 0s 235us/step - loss: 0.6521 - acc: 0.5037\n",
      "Epoch 4/20\n",
      "135/135 [==============================] - 0s 245us/step - loss: 0.6424 - acc: 0.5259\n",
      "Epoch 5/20\n",
      "135/135 [==============================] - 0s 193us/step - loss: 0.6333 - acc: 0.5556\n",
      "Epoch 6/20\n",
      "135/135 [==============================] - 0s 170us/step - loss: 0.6236 - acc: 0.5852\n",
      "Epoch 7/20\n",
      "135/135 [==============================] - 0s 140us/step - loss: 0.6163 - acc: 0.6222\n",
      "Epoch 8/20\n",
      "135/135 [==============================] - 0s 205us/step - loss: 0.6077 - acc: 0.6444\n",
      "Epoch 9/20\n",
      "135/135 [==============================] - 0s 130us/step - loss: 0.5999 - acc: 0.6667\n",
      "Epoch 10/20\n",
      "135/135 [==============================] - 0s 157us/step - loss: 0.5926 - acc: 0.6593\n",
      "Epoch 11/20\n",
      "135/135 [==============================] - 0s 178us/step - loss: 0.5852 - acc: 0.6741\n",
      "Epoch 12/20\n",
      "135/135 [==============================] - 0s 121us/step - loss: 0.5786 - acc: 0.6815\n",
      "Epoch 13/20\n",
      "135/135 [==============================] - 0s 145us/step - loss: 0.5711 - acc: 0.7037\n",
      "Epoch 14/20\n",
      "135/135 [==============================] - 0s 119us/step - loss: 0.5643 - acc: 0.7111\n",
      "Epoch 15/20\n",
      "135/135 [==============================] - 0s 119us/step - loss: 0.5577 - acc: 0.7407\n",
      "Epoch 16/20\n",
      "135/135 [==============================] - 0s 147us/step - loss: 0.5511 - acc: 0.7481\n",
      "Epoch 17/20\n",
      "135/135 [==============================] - 0s 151us/step - loss: 0.5444 - acc: 0.7778\n",
      "Epoch 18/20\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.5385 - acc: 0.7852\n",
      "Epoch 19/20\n",
      "135/135 [==============================] - 0s 114us/step - loss: 0.5323 - acc: 0.8148\n",
      "Epoch 20/20\n",
      "135/135 [==============================] - 0s 97us/step - loss: 0.5264 - acc: 0.8222\n",
      "68/68 [==============================] - 0s 4ms/step\n",
      "135/135 [==============================] - 0s 79us/step\n",
      "Epoch 1/20\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 0.6799 - acc: 0.5368\n",
      "Epoch 2/20\n",
      "136/136 [==============================] - 0s 172us/step - loss: 0.6730 - acc: 0.5588\n",
      "Epoch 3/20\n",
      "136/136 [==============================] - 0s 136us/step - loss: 0.6675 - acc: 0.5588\n",
      "Epoch 4/20\n",
      "136/136 [==============================] - 0s 141us/step - loss: 0.6616 - acc: 0.5882\n",
      "Epoch 5/20\n",
      "136/136 [==============================] - 0s 108us/step - loss: 0.6560 - acc: 0.5956\n",
      "Epoch 6/20\n",
      "136/136 [==============================] - 0s 138us/step - loss: 0.6508 - acc: 0.6324\n",
      "Epoch 7/20\n",
      "136/136 [==============================] - 0s 124us/step - loss: 0.6459 - acc: 0.6544\n",
      "Epoch 8/20\n",
      "136/136 [==============================] - 0s 138us/step - loss: 0.6407 - acc: 0.6912\n",
      "Epoch 9/20\n",
      "136/136 [==============================] - 0s 140us/step - loss: 0.6362 - acc: 0.7059\n",
      "Epoch 10/20\n",
      "136/136 [==============================] - 0s 116us/step - loss: 0.6314 - acc: 0.7279\n",
      "Epoch 11/20\n",
      "136/136 [==============================] - 0s 144us/step - loss: 0.6267 - acc: 0.7206\n",
      "Epoch 12/20\n",
      "136/136 [==============================] - 0s 141us/step - loss: 0.6217 - acc: 0.7353\n",
      "Epoch 13/20\n",
      "136/136 [==============================] - 0s 114us/step - loss: 0.6171 - acc: 0.7574\n",
      "Epoch 14/20\n",
      "136/136 [==============================] - 0s 145us/step - loss: 0.6121 - acc: 0.7500\n",
      "Epoch 15/20\n",
      "136/136 [==============================] - 0s 117us/step - loss: 0.6075 - acc: 0.7574\n",
      "Epoch 16/20\n",
      "136/136 [==============================] - 0s 129us/step - loss: 0.6031 - acc: 0.7647\n",
      "Epoch 17/20\n",
      "136/136 [==============================] - 0s 125us/step - loss: 0.5987 - acc: 0.7647\n",
      "Epoch 18/20\n",
      "136/136 [==============================] - 0s 109us/step - loss: 0.5941 - acc: 0.7574\n",
      "Epoch 19/20\n",
      "136/136 [==============================] - 0s 131us/step - loss: 0.5897 - acc: 0.7574\n",
      "Epoch 20/20\n",
      "136/136 [==============================] - 0s 123us/step - loss: 0.5849 - acc: 0.7647\n",
      "67/67 [==============================] - 0s 4ms/step\n",
      "136/136 [==============================] - 0s 58us/step\n",
      "Epoch 1/50\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.7454 - acc: 0.4370\n",
      "Epoch 2/50\n",
      "135/135 [==============================] - 0s 106us/step - loss: 0.7365 - acc: 0.4222\n",
      "Epoch 3/50\n",
      "135/135 [==============================] - 0s 125us/step - loss: 0.7290 - acc: 0.4222\n",
      "Epoch 4/50\n",
      "135/135 [==============================] - 0s 151us/step - loss: 0.7209 - acc: 0.4296\n",
      "Epoch 5/50\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.7151 - acc: 0.4444\n",
      "Epoch 6/50\n",
      "135/135 [==============================] - 0s 145us/step - loss: 0.7081 - acc: 0.4370\n",
      "Epoch 7/50\n",
      "135/135 [==============================] - 0s 135us/step - loss: 0.7011 - acc: 0.4444\n",
      "Epoch 8/50\n",
      "135/135 [==============================] - 0s 125us/step - loss: 0.6953 - acc: 0.4667\n",
      "Epoch 9/50\n",
      "135/135 [==============================] - 0s 145us/step - loss: 0.6890 - acc: 0.5111\n",
      "Epoch 10/50\n",
      "135/135 [==============================] - 0s 150us/step - loss: 0.6839 - acc: 0.5704\n",
      "Epoch 11/50\n",
      "135/135 [==============================] - 0s 114us/step - loss: 0.6778 - acc: 0.5926\n",
      "Epoch 12/50\n",
      "135/135 [==============================] - 0s 148us/step - loss: 0.6730 - acc: 0.6667\n",
      "Epoch 13/50\n",
      "135/135 [==============================] - 0s 167us/step - loss: 0.6682 - acc: 0.6889\n",
      "Epoch 14/50\n",
      "135/135 [==============================] - 0s 160us/step - loss: 0.6633 - acc: 0.7111\n",
      "Epoch 15/50\n",
      "135/135 [==============================] - 0s 126us/step - loss: 0.6588 - acc: 0.7630\n",
      "Epoch 16/50\n",
      "135/135 [==============================] - 0s 136us/step - loss: 0.6541 - acc: 0.7704\n",
      "Epoch 17/50\n",
      "135/135 [==============================] - 0s 116us/step - loss: 0.6484 - acc: 0.7630\n",
      "Epoch 18/50\n",
      "135/135 [==============================] - 0s 126us/step - loss: 0.6431 - acc: 0.7704\n",
      "Epoch 19/50\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.6377 - acc: 0.7556\n",
      "Epoch 20/50\n",
      "135/135 [==============================] - 0s 84us/step - loss: 0.6320 - acc: 0.7630\n",
      "Epoch 21/50\n",
      "135/135 [==============================] - 0s 109us/step - loss: 0.6262 - acc: 0.7630\n",
      "Epoch 22/50\n",
      "135/135 [==============================] - 0s 151us/step - loss: 0.6207 - acc: 0.7556\n",
      "Epoch 23/50\n",
      "135/135 [==============================] - 0s 91us/step - loss: 0.6150 - acc: 0.7481\n",
      "Epoch 24/50\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.6096 - acc: 0.7407\n",
      "Epoch 25/50\n",
      "135/135 [==============================] - 0s 92us/step - loss: 0.6042 - acc: 0.7333\n",
      "Epoch 26/50\n",
      "135/135 [==============================] - 0s 109us/step - loss: 0.5984 - acc: 0.7259\n",
      "Epoch 27/50\n",
      "135/135 [==============================] - 0s 104us/step - loss: 0.5929 - acc: 0.7185\n",
      "Epoch 28/50\n",
      "135/135 [==============================] - 0s 111us/step - loss: 0.5871 - acc: 0.7259\n",
      "Epoch 29/50\n",
      "135/135 [==============================] - 0s 91us/step - loss: 0.5816 - acc: 0.7481\n",
      "Epoch 30/50\n",
      "135/135 [==============================] - 0s 108us/step - loss: 0.5762 - acc: 0.7481\n",
      "Epoch 31/50\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.5702 - acc: 0.7481\n",
      "Epoch 32/50\n",
      "135/135 [==============================] - 0s 104us/step - loss: 0.5645 - acc: 0.7556\n",
      "Epoch 33/50\n",
      "135/135 [==============================] - 0s 89us/step - loss: 0.5591 - acc: 0.7630\n",
      "Epoch 34/50\n",
      "135/135 [==============================] - 0s 105us/step - loss: 0.5535 - acc: 0.7704\n",
      "Epoch 35/50\n",
      "135/135 [==============================] - 0s 111us/step - loss: 0.5483 - acc: 0.7704\n",
      "Epoch 36/50\n",
      "135/135 [==============================] - 0s 109us/step - loss: 0.5433 - acc: 0.7704\n",
      "Epoch 37/50\n",
      "135/135 [==============================] - 0s 119us/step - loss: 0.5380 - acc: 0.7778\n",
      "Epoch 38/50\n",
      "135/135 [==============================] - 0s 118us/step - loss: 0.5331 - acc: 0.7778\n",
      "Epoch 39/50\n",
      "135/135 [==============================] - 0s 91us/step - loss: 0.5279 - acc: 0.7852\n",
      "Epoch 40/50\n",
      "135/135 [==============================] - 0s 93us/step - loss: 0.5226 - acc: 0.8000\n",
      "Epoch 41/50\n",
      "135/135 [==============================] - 0s 118us/step - loss: 0.5176 - acc: 0.8000\n",
      "Epoch 42/50\n",
      "135/135 [==============================] - 0s 99us/step - loss: 0.5123 - acc: 0.8000\n",
      "Epoch 43/50\n",
      "135/135 [==============================] - 0s 115us/step - loss: 0.5073 - acc: 0.8000\n",
      "Epoch 44/50\n",
      "135/135 [==============================] - 0s 108us/step - loss: 0.5017 - acc: 0.8000\n",
      "Epoch 45/50\n",
      "135/135 [==============================] - 0s 95us/step - loss: 0.4969 - acc: 0.8000\n",
      "Epoch 46/50\n",
      "135/135 [==============================] - 0s 91us/step - loss: 0.4920 - acc: 0.8074\n",
      "Epoch 47/50\n",
      "135/135 [==============================] - 0s 115us/step - loss: 0.4874 - acc: 0.8148\n",
      "Epoch 48/50\n",
      "135/135 [==============================] - 0s 113us/step - loss: 0.4829 - acc: 0.8222\n",
      "Epoch 49/50\n",
      "135/135 [==============================] - 0s 126us/step - loss: 0.4784 - acc: 0.8148\n",
      "Epoch 50/50\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.4745 - acc: 0.8148\n",
      "68/68 [==============================] - 0s 6ms/step\n",
      "135/135 [==============================] - 0s 103us/step\n",
      "Epoch 1/50\n",
      "135/135 [==============================] - 1s 7ms/step - loss: 0.8699 - acc: 0.5037\n",
      "Epoch 2/50\n",
      "135/135 [==============================] - 0s 108us/step - loss: 0.8546 - acc: 0.5037\n",
      "Epoch 3/50\n",
      "135/135 [==============================] - 0s 146us/step - loss: 0.8400 - acc: 0.5037\n",
      "Epoch 4/50\n",
      "135/135 [==============================] - 0s 137us/step - loss: 0.8255 - acc: 0.5037\n",
      "Epoch 5/50\n",
      "135/135 [==============================] - 0s 123us/step - loss: 0.8124 - acc: 0.5037\n",
      "Epoch 6/50\n",
      "135/135 [==============================] - 0s 150us/step - loss: 0.8012 - acc: 0.5037\n",
      "Epoch 7/50\n",
      "135/135 [==============================] - 0s 135us/step - loss: 0.7909 - acc: 0.5037\n",
      "Epoch 8/50\n",
      "135/135 [==============================] - 0s 126us/step - loss: 0.7801 - acc: 0.5037\n",
      "Epoch 9/50\n",
      "135/135 [==============================] - 0s 143us/step - loss: 0.7717 - acc: 0.4963\n",
      "Epoch 10/50\n",
      "135/135 [==============================] - 0s 128us/step - loss: 0.7632 - acc: 0.4963\n",
      "Epoch 11/50\n",
      "135/135 [==============================] - 0s 143us/step - loss: 0.7546 - acc: 0.4963\n",
      "Epoch 12/50\n",
      "135/135 [==============================] - 0s 151us/step - loss: 0.7468 - acc: 0.4963\n",
      "Epoch 13/50\n",
      "135/135 [==============================] - 0s 144us/step - loss: 0.7384 - acc: 0.4963\n",
      "Epoch 14/50\n",
      "135/135 [==============================] - 0s 147us/step - loss: 0.7304 - acc: 0.4889\n",
      "Epoch 15/50\n",
      "135/135 [==============================] - 0s 122us/step - loss: 0.7227 - acc: 0.4815\n",
      "Epoch 16/50\n",
      "135/135 [==============================] - 0s 123us/step - loss: 0.7153 - acc: 0.4741\n",
      "Epoch 17/50\n",
      "135/135 [==============================] - 0s 91us/step - loss: 0.7071 - acc: 0.4815\n",
      "Epoch 18/50\n",
      "135/135 [==============================] - 0s 168us/step - loss: 0.6995 - acc: 0.4815\n",
      "Epoch 19/50\n",
      "135/135 [==============================] - 0s 147us/step - loss: 0.6920 - acc: 0.4889\n",
      "Epoch 20/50\n",
      "135/135 [==============================] - 0s 118us/step - loss: 0.6838 - acc: 0.5037\n",
      "Epoch 21/50\n",
      "135/135 [==============================] - 0s 162us/step - loss: 0.6765 - acc: 0.5185\n",
      "Epoch 22/50\n",
      "135/135 [==============================] - 0s 163us/step - loss: 0.6692 - acc: 0.5556\n",
      "Epoch 23/50\n",
      "135/135 [==============================] - 0s 115us/step - loss: 0.6620 - acc: 0.5852\n",
      "Epoch 24/50\n",
      "135/135 [==============================] - 0s 115us/step - loss: 0.6545 - acc: 0.6296\n",
      "Epoch 25/50\n",
      "135/135 [==============================] - 0s 157us/step - loss: 0.6476 - acc: 0.6519\n",
      "Epoch 26/50\n",
      "135/135 [==============================] - 0s 125us/step - loss: 0.6402 - acc: 0.6741\n",
      "Epoch 27/50\n",
      "135/135 [==============================] - 0s 149us/step - loss: 0.6330 - acc: 0.6815\n",
      "Epoch 28/50\n",
      "135/135 [==============================] - 0s 95us/step - loss: 0.6265 - acc: 0.6815\n",
      "Epoch 29/50\n",
      "135/135 [==============================] - 0s 97us/step - loss: 0.6196 - acc: 0.6815\n",
      "Epoch 30/50\n",
      "135/135 [==============================] - 0s 150us/step - loss: 0.6125 - acc: 0.7111\n",
      "Epoch 31/50\n",
      "135/135 [==============================] - 0s 104us/step - loss: 0.6057 - acc: 0.7185\n",
      "Epoch 32/50\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.5993 - acc: 0.7333\n",
      "Epoch 33/50\n",
      "135/135 [==============================] - 0s 125us/step - loss: 0.5923 - acc: 0.7407\n",
      "Epoch 34/50\n",
      "135/135 [==============================] - 0s 166us/step - loss: 0.5861 - acc: 0.7481\n",
      "Epoch 35/50\n",
      "135/135 [==============================] - 0s 141us/step - loss: 0.5798 - acc: 0.7481\n",
      "Epoch 36/50\n",
      "135/135 [==============================] - 0s 115us/step - loss: 0.5734 - acc: 0.7556\n",
      "Epoch 37/50\n",
      "135/135 [==============================] - 0s 111us/step - loss: 0.5672 - acc: 0.7556\n",
      "Epoch 38/50\n",
      "135/135 [==============================] - 0s 137us/step - loss: 0.5615 - acc: 0.7630\n",
      "Epoch 39/50\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.5562 - acc: 0.7556\n",
      "Epoch 40/50\n",
      "135/135 [==============================] - 0s 125us/step - loss: 0.5500 - acc: 0.7704\n",
      "Epoch 41/50\n",
      "135/135 [==============================] - 0s 116us/step - loss: 0.5445 - acc: 0.7704\n",
      "Epoch 42/50\n",
      "135/135 [==============================] - 0s 115us/step - loss: 0.5392 - acc: 0.7630\n",
      "Epoch 43/50\n",
      "135/135 [==============================] - 0s 115us/step - loss: 0.5339 - acc: 0.7704\n",
      "Epoch 44/50\n",
      "135/135 [==============================] - 0s 108us/step - loss: 0.5288 - acc: 0.7704\n",
      "Epoch 45/50\n",
      "135/135 [==============================] - 0s 140us/step - loss: 0.5236 - acc: 0.7778\n",
      "Epoch 46/50\n",
      "135/135 [==============================] - 0s 118us/step - loss: 0.5182 - acc: 0.7852\n",
      "Epoch 47/50\n",
      "135/135 [==============================] - 0s 132us/step - loss: 0.5134 - acc: 0.7926\n",
      "Epoch 48/50\n",
      "135/135 [==============================] - 0s 123us/step - loss: 0.5084 - acc: 0.8000\n",
      "Epoch 49/50\n",
      "135/135 [==============================] - 0s 127us/step - loss: 0.5037 - acc: 0.8074\n",
      "Epoch 50/50\n",
      "135/135 [==============================] - 0s 118us/step - loss: 0.4990 - acc: 0.8074\n",
      "68/68 [==============================] - 0s 5ms/step\n",
      "135/135 [==============================] - 0s 140us/step\n",
      "Epoch 1/50\n",
      "136/136 [==============================] - 1s 7ms/step - loss: 0.7078 - acc: 0.4632\n",
      "Epoch 2/50\n",
      "136/136 [==============================] - 0s 112us/step - loss: 0.7009 - acc: 0.4926\n",
      "Epoch 3/50\n",
      "136/136 [==============================] - 0s 140us/step - loss: 0.6930 - acc: 0.5662\n",
      "Epoch 4/50\n",
      "136/136 [==============================] - 0s 137us/step - loss: 0.6865 - acc: 0.6250\n",
      "Epoch 5/50\n",
      "136/136 [==============================] - 0s 128us/step - loss: 0.6808 - acc: 0.6176\n",
      "Epoch 6/50\n",
      "136/136 [==============================] - 0s 148us/step - loss: 0.6746 - acc: 0.6250\n",
      "Epoch 7/50\n",
      "136/136 [==============================] - 0s 115us/step - loss: 0.6692 - acc: 0.6324\n",
      "Epoch 8/50\n",
      "136/136 [==============================] - 0s 140us/step - loss: 0.6640 - acc: 0.6471\n",
      "Epoch 9/50\n",
      "136/136 [==============================] - 0s 139us/step - loss: 0.6593 - acc: 0.6397\n",
      "Epoch 10/50\n",
      "136/136 [==============================] - 0s 121us/step - loss: 0.6550 - acc: 0.6691\n",
      "Epoch 11/50\n",
      "136/136 [==============================] - 0s 155us/step - loss: 0.6509 - acc: 0.6912\n",
      "Epoch 12/50\n",
      "136/136 [==============================] - 0s 162us/step - loss: 0.6465 - acc: 0.6985\n",
      "Epoch 13/50\n",
      "136/136 [==============================] - 0s 124us/step - loss: 0.6430 - acc: 0.6985\n",
      "Epoch 14/50\n",
      "136/136 [==============================] - 0s 158us/step - loss: 0.6392 - acc: 0.6985\n",
      "Epoch 15/50\n",
      "136/136 [==============================] - 0s 112us/step - loss: 0.6354 - acc: 0.7132\n",
      "Epoch 16/50\n",
      "136/136 [==============================] - 0s 176us/step - loss: 0.6317 - acc: 0.7426\n",
      "Epoch 17/50\n",
      "136/136 [==============================] - 0s 104us/step - loss: 0.6280 - acc: 0.7353\n",
      "Epoch 18/50\n",
      "136/136 [==============================] - 0s 166us/step - loss: 0.6243 - acc: 0.7426\n",
      "Epoch 19/50\n",
      "136/136 [==============================] - 0s 153us/step - loss: 0.6204 - acc: 0.7574\n",
      "Epoch 20/50\n",
      "136/136 [==============================] - 0s 127us/step - loss: 0.6170 - acc: 0.7647\n",
      "Epoch 21/50\n",
      "136/136 [==============================] - 0s 137us/step - loss: 0.6132 - acc: 0.7721\n",
      "Epoch 22/50\n",
      "136/136 [==============================] - 0s 126us/step - loss: 0.6094 - acc: 0.7721\n",
      "Epoch 23/50\n",
      "136/136 [==============================] - 0s 142us/step - loss: 0.6059 - acc: 0.7721\n",
      "Epoch 24/50\n",
      "136/136 [==============================] - 0s 115us/step - loss: 0.6024 - acc: 0.7647\n",
      "Epoch 25/50\n",
      "136/136 [==============================] - 0s 148us/step - loss: 0.5988 - acc: 0.7647\n",
      "Epoch 26/50\n",
      "136/136 [==============================] - 0s 105us/step - loss: 0.5950 - acc: 0.7721\n",
      "Epoch 27/50\n",
      "136/136 [==============================] - 0s 89us/step - loss: 0.5914 - acc: 0.7794\n",
      "Epoch 28/50\n",
      "136/136 [==============================] - 0s 157us/step - loss: 0.5878 - acc: 0.7868\n",
      "Epoch 29/50\n",
      "136/136 [==============================] - 0s 105us/step - loss: 0.5841 - acc: 0.7868\n",
      "Epoch 30/50\n",
      "136/136 [==============================] - 0s 127us/step - loss: 0.5804 - acc: 0.7941\n",
      "Epoch 31/50\n",
      "136/136 [==============================] - 0s 119us/step - loss: 0.5767 - acc: 0.7868\n",
      "Epoch 32/50\n",
      "136/136 [==============================] - 0s 125us/step - loss: 0.5730 - acc: 0.8015\n",
      "Epoch 33/50\n",
      "136/136 [==============================] - 0s 125us/step - loss: 0.5690 - acc: 0.8088\n",
      "Epoch 34/50\n",
      "136/136 [==============================] - 0s 106us/step - loss: 0.5654 - acc: 0.8235\n",
      "Epoch 35/50\n",
      "136/136 [==============================] - 0s 132us/step - loss: 0.5618 - acc: 0.8235\n",
      "Epoch 36/50\n",
      "136/136 [==============================] - 0s 131us/step - loss: 0.5582 - acc: 0.8235\n",
      "Epoch 37/50\n",
      "136/136 [==============================] - 0s 103us/step - loss: 0.5545 - acc: 0.8235\n",
      "Epoch 38/50\n",
      "136/136 [==============================] - 0s 126us/step - loss: 0.5509 - acc: 0.8235\n",
      "Epoch 39/50\n",
      "136/136 [==============================] - 0s 109us/step - loss: 0.5473 - acc: 0.8235\n",
      "Epoch 40/50\n",
      "136/136 [==============================] - 0s 123us/step - loss: 0.5437 - acc: 0.8235\n",
      "Epoch 41/50\n",
      "136/136 [==============================] - 0s 109us/step - loss: 0.5403 - acc: 0.8235\n",
      "Epoch 42/50\n",
      "136/136 [==============================] - 0s 107us/step - loss: 0.5367 - acc: 0.8088\n",
      "Epoch 43/50\n",
      "136/136 [==============================] - 0s 133us/step - loss: 0.5331 - acc: 0.8162\n",
      "Epoch 44/50\n",
      "136/136 [==============================] - 0s 104us/step - loss: 0.5299 - acc: 0.8162\n",
      "Epoch 45/50\n",
      "136/136 [==============================] - 0s 103us/step - loss: 0.5265 - acc: 0.8162\n",
      "Epoch 46/50\n",
      "136/136 [==============================] - 0s 87us/step - loss: 0.5231 - acc: 0.8235\n",
      "Epoch 47/50\n",
      "136/136 [==============================] - 0s 68us/step - loss: 0.5195 - acc: 0.8382\n",
      "Epoch 48/50\n",
      "136/136 [==============================] - 0s 71us/step - loss: 0.5165 - acc: 0.8382\n",
      "Epoch 49/50\n",
      "136/136 [==============================] - 0s 119us/step - loss: 0.5128 - acc: 0.8309\n",
      "Epoch 50/50\n",
      "136/136 [==============================] - 0s 114us/step - loss: 0.5096 - acc: 0.8382\n",
      "67/67 [==============================] - 0s 7ms/step\n",
      "136/136 [==============================] - 0s 70us/step\n",
      "Epoch 1/100\n",
      "135/135 [==============================] - 1s 7ms/step - loss: 0.6953 - acc: 0.4741\n",
      "Epoch 2/100\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.6903 - acc: 0.4815\n",
      "Epoch 3/100\n",
      "135/135 [==============================] - 0s 188us/step - loss: 0.6858 - acc: 0.4889\n",
      "Epoch 4/100\n",
      "135/135 [==============================] - 0s 136us/step - loss: 0.6810 - acc: 0.5185\n",
      "Epoch 5/100\n",
      "135/135 [==============================] - 0s 115us/step - loss: 0.6759 - acc: 0.5852\n",
      "Epoch 6/100\n",
      "135/135 [==============================] - 0s 138us/step - loss: 0.6716 - acc: 0.6000\n",
      "Epoch 7/100\n",
      "135/135 [==============================] - 0s 117us/step - loss: 0.6671 - acc: 0.6148\n",
      "Epoch 8/100\n",
      "135/135 [==============================] - 0s 149us/step - loss: 0.6629 - acc: 0.6222\n",
      "Epoch 9/100\n",
      "135/135 [==============================] - 0s 118us/step - loss: 0.6592 - acc: 0.6000\n",
      "Epoch 10/100\n",
      "135/135 [==============================] - 0s 148us/step - loss: 0.6552 - acc: 0.6074\n",
      "Epoch 11/100\n",
      "135/135 [==============================] - 0s 102us/step - loss: 0.6510 - acc: 0.6593\n",
      "Epoch 12/100\n",
      "135/135 [==============================] - 0s 182us/step - loss: 0.6473 - acc: 0.7259\n",
      "Epoch 13/100\n",
      "135/135 [==============================] - 0s 193us/step - loss: 0.6440 - acc: 0.7333\n",
      "Epoch 14/100\n",
      "135/135 [==============================] - 0s 180us/step - loss: 0.6408 - acc: 0.7481\n",
      "Epoch 15/100\n",
      "135/135 [==============================] - 0s 231us/step - loss: 0.6370 - acc: 0.7481\n",
      "Epoch 16/100\n",
      "135/135 [==============================] - 0s 273us/step - loss: 0.6334 - acc: 0.7556\n",
      "Epoch 17/100\n",
      "135/135 [==============================] - 0s 158us/step - loss: 0.6295 - acc: 0.7556\n",
      "Epoch 18/100\n",
      "135/135 [==============================] - 0s 128us/step - loss: 0.6262 - acc: 0.7630\n",
      "Epoch 19/100\n",
      "135/135 [==============================] - 0s 141us/step - loss: 0.6217 - acc: 0.7556\n",
      "Epoch 20/100\n",
      "135/135 [==============================] - 0s 134us/step - loss: 0.6177 - acc: 0.7556\n",
      "Epoch 21/100\n",
      "135/135 [==============================] - 0s 107us/step - loss: 0.6138 - acc: 0.7556\n",
      "Epoch 22/100\n",
      "135/135 [==============================] - 0s 152us/step - loss: 0.6099 - acc: 0.7556\n",
      "Epoch 23/100\n",
      "135/135 [==============================] - 0s 116us/step - loss: 0.6061 - acc: 0.7556\n",
      "Epoch 24/100\n",
      "135/135 [==============================] - 0s 132us/step - loss: 0.6021 - acc: 0.7556\n",
      "Epoch 25/100\n",
      "135/135 [==============================] - 0s 96us/step - loss: 0.5978 - acc: 0.7556\n",
      "Epoch 26/100\n",
      "135/135 [==============================] - 0s 128us/step - loss: 0.5941 - acc: 0.7556\n",
      "Epoch 27/100\n",
      "135/135 [==============================] - 0s 127us/step - loss: 0.5898 - acc: 0.7556\n",
      "Epoch 28/100\n",
      "135/135 [==============================] - 0s 137us/step - loss: 0.5860 - acc: 0.7556\n",
      "Epoch 29/100\n",
      "135/135 [==============================] - 0s 101us/step - loss: 0.5818 - acc: 0.7556\n",
      "Epoch 30/100\n",
      "135/135 [==============================] - 0s 147us/step - loss: 0.5777 - acc: 0.7556\n",
      "Epoch 31/100\n",
      "135/135 [==============================] - 0s 118us/step - loss: 0.5734 - acc: 0.7556\n",
      "Epoch 32/100\n",
      "135/135 [==============================] - 0s 150us/step - loss: 0.5692 - acc: 0.7556\n",
      "Epoch 33/100\n",
      "135/135 [==============================] - 0s 125us/step - loss: 0.5650 - acc: 0.7556\n",
      "Epoch 34/100\n",
      "135/135 [==============================] - 0s 127us/step - loss: 0.5605 - acc: 0.7556\n",
      "Epoch 35/100\n",
      "135/135 [==============================] - 0s 124us/step - loss: 0.5567 - acc: 0.7630\n",
      "Epoch 36/100\n",
      "135/135 [==============================] - 0s 110us/step - loss: 0.5522 - acc: 0.7704\n",
      "Epoch 37/100\n",
      "135/135 [==============================] - 0s 132us/step - loss: 0.5482 - acc: 0.7704\n",
      "Epoch 38/100\n",
      "135/135 [==============================] - 0s 111us/step - loss: 0.5438 - acc: 0.7704\n",
      "Epoch 39/100\n",
      "135/135 [==============================] - 0s 111us/step - loss: 0.5397 - acc: 0.7630\n",
      "Epoch 40/100\n",
      "135/135 [==============================] - 0s 149us/step - loss: 0.5359 - acc: 0.7704\n",
      "Epoch 41/100\n",
      "135/135 [==============================] - 0s 114us/step - loss: 0.5315 - acc: 0.7704\n",
      "Epoch 42/100\n",
      "135/135 [==============================] - 0s 120us/step - loss: 0.5276 - acc: 0.7704\n",
      "Epoch 43/100\n",
      "135/135 [==============================] - 0s 172us/step - loss: 0.5235 - acc: 0.7704\n",
      "Epoch 44/100\n",
      "135/135 [==============================] - 0s 127us/step - loss: 0.5195 - acc: 0.7704\n",
      "Epoch 45/100\n",
      "135/135 [==============================] - 0s 118us/step - loss: 0.5155 - acc: 0.7778\n",
      "Epoch 46/100\n",
      "135/135 [==============================] - 0s 132us/step - loss: 0.5119 - acc: 0.7852\n",
      "Epoch 47/100\n",
      "135/135 [==============================] - 0s 107us/step - loss: 0.5077 - acc: 0.7852\n",
      "Epoch 48/100\n",
      "135/135 [==============================] - 0s 160us/step - loss: 0.5038 - acc: 0.7852\n",
      "Epoch 49/100\n",
      "135/135 [==============================] - 0s 110us/step - loss: 0.5004 - acc: 0.7852\n",
      "Epoch 50/100\n",
      "135/135 [==============================] - 0s 117us/step - loss: 0.4966 - acc: 0.7852\n",
      "Epoch 51/100\n",
      "135/135 [==============================] - 0s 121us/step - loss: 0.4931 - acc: 0.7852\n",
      "Epoch 52/100\n",
      "135/135 [==============================] - 0s 124us/step - loss: 0.4896 - acc: 0.7852\n",
      "Epoch 53/100\n",
      "135/135 [==============================] - 0s 136us/step - loss: 0.4862 - acc: 0.7852\n",
      "Epoch 54/100\n",
      "135/135 [==============================] - 0s 120us/step - loss: 0.4830 - acc: 0.7852\n",
      "Epoch 55/100\n",
      "135/135 [==============================] - 0s 117us/step - loss: 0.4792 - acc: 0.7852\n",
      "Epoch 56/100\n",
      "135/135 [==============================] - 0s 125us/step - loss: 0.4759 - acc: 0.7852\n",
      "Epoch 57/100\n",
      "135/135 [==============================] - 0s 143us/step - loss: 0.4724 - acc: 0.7852\n",
      "Epoch 58/100\n",
      "135/135 [==============================] - 0s 114us/step - loss: 0.4689 - acc: 0.7852\n",
      "Epoch 59/100\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.4658 - acc: 0.7852\n",
      "Epoch 60/100\n",
      "135/135 [==============================] - 0s 130us/step - loss: 0.4625 - acc: 0.7852\n",
      "Epoch 61/100\n",
      "135/135 [==============================] - 0s 110us/step - loss: 0.4595 - acc: 0.7926\n",
      "Epoch 62/100\n",
      "135/135 [==============================] - 0s 113us/step - loss: 0.4563 - acc: 0.7926\n",
      "Epoch 63/100\n",
      "135/135 [==============================] - 0s 107us/step - loss: 0.4532 - acc: 0.7926\n",
      "Epoch 64/100\n",
      "135/135 [==============================] - 0s 172us/step - loss: 0.4503 - acc: 0.7926\n",
      "Epoch 65/100\n",
      "135/135 [==============================] - 0s 124us/step - loss: 0.4473 - acc: 0.7926\n",
      "Epoch 66/100\n",
      "135/135 [==============================] - 0s 113us/step - loss: 0.4444 - acc: 0.7926\n",
      "Epoch 67/100\n",
      "135/135 [==============================] - 0s 113us/step - loss: 0.4416 - acc: 0.7926\n",
      "Epoch 68/100\n",
      "135/135 [==============================] - 0s 97us/step - loss: 0.4389 - acc: 0.7926\n",
      "Epoch 69/100\n",
      "135/135 [==============================] - 0s 109us/step - loss: 0.4361 - acc: 0.7926\n",
      "Epoch 70/100\n",
      "135/135 [==============================] - 0s 91us/step - loss: 0.4336 - acc: 0.8000\n",
      "Epoch 71/100\n",
      "135/135 [==============================] - 0s 98us/step - loss: 0.4309 - acc: 0.8074\n",
      "Epoch 72/100\n",
      "135/135 [==============================] - 0s 99us/step - loss: 0.4285 - acc: 0.8074\n",
      "Epoch 73/100\n",
      "135/135 [==============================] - 0s 135us/step - loss: 0.4260 - acc: 0.8074\n",
      "Epoch 74/100\n",
      "135/135 [==============================] - 0s 105us/step - loss: 0.4235 - acc: 0.8148\n",
      "Epoch 75/100\n",
      "135/135 [==============================] - 0s 151us/step - loss: 0.4210 - acc: 0.8148\n",
      "Epoch 76/100\n",
      "135/135 [==============================] - 0s 110us/step - loss: 0.4191 - acc: 0.8148\n",
      "Epoch 77/100\n",
      "135/135 [==============================] - 0s 108us/step - loss: 0.4167 - acc: 0.8148\n",
      "Epoch 78/100\n",
      "135/135 [==============================] - 0s 104us/step - loss: 0.4140 - acc: 0.8148\n",
      "Epoch 79/100\n",
      "135/135 [==============================] - 0s 88us/step - loss: 0.4117 - acc: 0.8222\n",
      "Epoch 80/100\n",
      "135/135 [==============================] - 0s 107us/step - loss: 0.4096 - acc: 0.8222\n",
      "Epoch 81/100\n",
      "135/135 [==============================] - 0s 102us/step - loss: 0.4073 - acc: 0.8222\n",
      "Epoch 82/100\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.4053 - acc: 0.8222\n",
      "Epoch 83/100\n",
      "135/135 [==============================] - 0s 92us/step - loss: 0.4031 - acc: 0.8222\n",
      "Epoch 84/100\n",
      "135/135 [==============================] - 0s 95us/step - loss: 0.4015 - acc: 0.8222\n",
      "Epoch 85/100\n",
      "135/135 [==============================] - 0s 98us/step - loss: 0.3988 - acc: 0.8296\n",
      "Epoch 86/100\n",
      "135/135 [==============================] - 0s 96us/step - loss: 0.3971 - acc: 0.8296\n",
      "Epoch 87/100\n",
      "135/135 [==============================] - 0s 89us/step - loss: 0.3949 - acc: 0.8296\n",
      "Epoch 88/100\n",
      "135/135 [==============================] - 0s 110us/step - loss: 0.3931 - acc: 0.8296\n",
      "Epoch 89/100\n",
      "135/135 [==============================] - 0s 110us/step - loss: 0.3908 - acc: 0.8296\n",
      "Epoch 90/100\n",
      "135/135 [==============================] - 0s 114us/step - loss: 0.3888 - acc: 0.8296\n",
      "Epoch 91/100\n",
      "135/135 [==============================] - 0s 113us/step - loss: 0.3865 - acc: 0.8370\n",
      "Epoch 92/100\n",
      "135/135 [==============================] - 0s 97us/step - loss: 0.3844 - acc: 0.8370\n",
      "Epoch 93/100\n",
      "135/135 [==============================] - 0s 106us/step - loss: 0.3827 - acc: 0.8444\n",
      "Epoch 94/100\n",
      "135/135 [==============================] - 0s 109us/step - loss: 0.3805 - acc: 0.8444\n",
      "Epoch 95/100\n",
      "135/135 [==============================] - 0s 108us/step - loss: 0.3789 - acc: 0.8519\n",
      "Epoch 96/100\n",
      "135/135 [==============================] - 0s 117us/step - loss: 0.3770 - acc: 0.8519\n",
      "Epoch 97/100\n",
      "135/135 [==============================] - 0s 83us/step - loss: 0.3750 - acc: 0.8519\n",
      "Epoch 98/100\n",
      "135/135 [==============================] - 0s 108us/step - loss: 0.3735 - acc: 0.8593\n",
      "Epoch 99/100\n",
      "135/135 [==============================] - 0s 90us/step - loss: 0.3719 - acc: 0.8593\n",
      "Epoch 100/100\n",
      "135/135 [==============================] - 0s 97us/step - loss: 0.3700 - acc: 0.8593\n",
      "68/68 [==============================] - 0s 5ms/step\n",
      "135/135 [==============================] - 0s 64us/step\n",
      "Epoch 1/100\n",
      "135/135 [==============================] - 1s 8ms/step - loss: 0.7905 - acc: 0.4889\n",
      "Epoch 2/100\n",
      "135/135 [==============================] - 0s 113us/step - loss: 0.7730 - acc: 0.4741\n",
      "Epoch 3/100\n",
      "135/135 [==============================] - 0s 153us/step - loss: 0.7602 - acc: 0.4741\n",
      "Epoch 4/100\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.7460 - acc: 0.4593\n",
      "Epoch 5/100\n",
      "135/135 [==============================] - 0s 134us/step - loss: 0.7354 - acc: 0.4593\n",
      "Epoch 6/100\n",
      "135/135 [==============================] - 0s 120us/step - loss: 0.7251 - acc: 0.4519\n",
      "Epoch 7/100\n",
      "135/135 [==============================] - 0s 142us/step - loss: 0.7156 - acc: 0.4519\n",
      "Epoch 8/100\n",
      "135/135 [==============================] - 0s 127us/step - loss: 0.7068 - acc: 0.4296\n",
      "Epoch 9/100\n",
      "135/135 [==============================] - 0s 101us/step - loss: 0.6982 - acc: 0.4815\n",
      "Epoch 10/100\n",
      "135/135 [==============================] - 0s 133us/step - loss: 0.6910 - acc: 0.4889\n",
      "Epoch 11/100\n",
      "135/135 [==============================] - 0s 113us/step - loss: 0.6849 - acc: 0.5185\n",
      "Epoch 12/100\n",
      "135/135 [==============================] - 0s 168us/step - loss: 0.6772 - acc: 0.5407\n",
      "Epoch 13/100\n",
      "135/135 [==============================] - 0s 105us/step - loss: 0.6708 - acc: 0.5852\n",
      "Epoch 14/100\n",
      "135/135 [==============================] - 0s 92us/step - loss: 0.6639 - acc: 0.6222\n",
      "Epoch 15/100\n",
      "135/135 [==============================] - 0s 127us/step - loss: 0.6579 - acc: 0.6222\n",
      "Epoch 16/100\n",
      "135/135 [==============================] - 0s 122us/step - loss: 0.6507 - acc: 0.6667\n",
      "Epoch 17/100\n",
      "135/135 [==============================] - 0s 107us/step - loss: 0.6448 - acc: 0.6889\n",
      "Epoch 18/100\n",
      "135/135 [==============================] - 0s 118us/step - loss: 0.6381 - acc: 0.7185\n",
      "Epoch 19/100\n",
      "135/135 [==============================] - 0s 129us/step - loss: 0.6317 - acc: 0.7185\n",
      "Epoch 20/100\n",
      "135/135 [==============================] - 0s 108us/step - loss: 0.6253 - acc: 0.7259\n",
      "Epoch 21/100\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.6192 - acc: 0.7333\n",
      "Epoch 22/100\n",
      "135/135 [==============================] - 0s 108us/step - loss: 0.6133 - acc: 0.7630\n",
      "Epoch 23/100\n",
      "135/135 [==============================] - 0s 133us/step - loss: 0.6072 - acc: 0.7704\n",
      "Epoch 24/100\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.6011 - acc: 0.7630\n",
      "Epoch 25/100\n",
      "135/135 [==============================] - 0s 133us/step - loss: 0.5954 - acc: 0.7704\n",
      "Epoch 26/100\n",
      "135/135 [==============================] - 0s 125us/step - loss: 0.5898 - acc: 0.7778\n",
      "Epoch 27/100\n",
      "135/135 [==============================] - 0s 126us/step - loss: 0.5837 - acc: 0.8000\n",
      "Epoch 28/100\n",
      "135/135 [==============================] - 0s 97us/step - loss: 0.5782 - acc: 0.8148\n",
      "Epoch 29/100\n",
      "135/135 [==============================] - 0s 98us/step - loss: 0.5728 - acc: 0.8148\n",
      "Epoch 30/100\n",
      "135/135 [==============================] - 0s 161us/step - loss: 0.5672 - acc: 0.8148\n",
      "Epoch 31/100\n",
      "135/135 [==============================] - 0s 105us/step - loss: 0.5620 - acc: 0.8148\n",
      "Epoch 32/100\n",
      "135/135 [==============================] - 0s 88us/step - loss: 0.5565 - acc: 0.8148\n",
      "Epoch 33/100\n",
      "135/135 [==============================] - 0s 131us/step - loss: 0.5511 - acc: 0.8148\n",
      "Epoch 34/100\n",
      "135/135 [==============================] - 0s 122us/step - loss: 0.5462 - acc: 0.8148\n",
      "Epoch 35/100\n",
      "135/135 [==============================] - 0s 113us/step - loss: 0.5408 - acc: 0.8148\n",
      "Epoch 36/100\n",
      "135/135 [==============================] - 0s 105us/step - loss: 0.5359 - acc: 0.8148\n",
      "Epoch 37/100\n",
      "135/135 [==============================] - 0s 86us/step - loss: 0.5307 - acc: 0.8148\n",
      "Epoch 38/100\n",
      "135/135 [==============================] - 0s 147us/step - loss: 0.5256 - acc: 0.8222\n",
      "Epoch 39/100\n",
      "135/135 [==============================] - 0s 99us/step - loss: 0.5206 - acc: 0.8222\n",
      "Epoch 40/100\n",
      "135/135 [==============================] - 0s 108us/step - loss: 0.5157 - acc: 0.8222\n",
      "Epoch 41/100\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.5107 - acc: 0.8222\n",
      "Epoch 42/100\n",
      "135/135 [==============================] - 0s 118us/step - loss: 0.5059 - acc: 0.8222\n",
      "Epoch 43/100\n",
      "135/135 [==============================] - 0s 92us/step - loss: 0.5011 - acc: 0.8222\n",
      "Epoch 44/100\n",
      "135/135 [==============================] - 0s 144us/step - loss: 0.4963 - acc: 0.8148\n",
      "Epoch 45/100\n",
      "135/135 [==============================] - 0s 110us/step - loss: 0.4916 - acc: 0.8148\n",
      "Epoch 46/100\n",
      "135/135 [==============================] - 0s 118us/step - loss: 0.4876 - acc: 0.8148\n",
      "Epoch 47/100\n",
      "135/135 [==============================] - 0s 105us/step - loss: 0.4827 - acc: 0.8148\n",
      "Epoch 48/100\n",
      "135/135 [==============================] - 0s 118us/step - loss: 0.4780 - acc: 0.8222\n",
      "Epoch 49/100\n",
      "135/135 [==============================] - 0s 95us/step - loss: 0.4737 - acc: 0.8222\n",
      "Epoch 50/100\n",
      "135/135 [==============================] - 0s 101us/step - loss: 0.4696 - acc: 0.8296\n",
      "Epoch 51/100\n",
      "135/135 [==============================] - 0s 117us/step - loss: 0.4654 - acc: 0.8370\n",
      "Epoch 52/100\n",
      "135/135 [==============================] - 0s 130us/step - loss: 0.4611 - acc: 0.8370\n",
      "Epoch 53/100\n",
      "135/135 [==============================] - 0s 95us/step - loss: 0.4571 - acc: 0.8296\n",
      "Epoch 54/100\n",
      "135/135 [==============================] - 0s 106us/step - loss: 0.4531 - acc: 0.8370\n",
      "Epoch 55/100\n",
      "135/135 [==============================] - 0s 118us/step - loss: 0.4492 - acc: 0.8296\n",
      "Epoch 56/100\n",
      "135/135 [==============================] - 0s 109us/step - loss: 0.4453 - acc: 0.8296\n",
      "Epoch 57/100\n",
      "135/135 [==============================] - 0s 107us/step - loss: 0.4416 - acc: 0.8296\n",
      "Epoch 58/100\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.4377 - acc: 0.8296\n",
      "Epoch 59/100\n",
      "135/135 [==============================] - 0s 104us/step - loss: 0.4338 - acc: 0.8296\n",
      "Epoch 60/100\n",
      "135/135 [==============================] - 0s 107us/step - loss: 0.4302 - acc: 0.8370\n",
      "Epoch 61/100\n",
      "135/135 [==============================] - 0s 99us/step - loss: 0.4272 - acc: 0.8370\n",
      "Epoch 62/100\n",
      "135/135 [==============================] - 0s 115us/step - loss: 0.4228 - acc: 0.8370\n",
      "Epoch 63/100\n",
      "135/135 [==============================] - 0s 97us/step - loss: 0.4191 - acc: 0.8370\n",
      "Epoch 64/100\n",
      "135/135 [==============================] - 0s 141us/step - loss: 0.4159 - acc: 0.8370\n",
      "Epoch 65/100\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.4126 - acc: 0.8444\n",
      "Epoch 66/100\n",
      "135/135 [==============================] - 0s 91us/step - loss: 0.4092 - acc: 0.8593\n",
      "Epoch 67/100\n",
      "135/135 [==============================] - 0s 105us/step - loss: 0.4062 - acc: 0.8593\n",
      "Epoch 68/100\n",
      "135/135 [==============================] - 0s 97us/step - loss: 0.4030 - acc: 0.8593\n",
      "Epoch 69/100\n",
      "135/135 [==============================] - 0s 98us/step - loss: 0.3998 - acc: 0.8519\n",
      "Epoch 70/100\n",
      "135/135 [==============================] - 0s 97us/step - loss: 0.3970 - acc: 0.8519\n",
      "Epoch 71/100\n",
      "135/135 [==============================] - 0s 114us/step - loss: 0.3943 - acc: 0.8444\n",
      "Epoch 72/100\n",
      "135/135 [==============================] - 0s 106us/step - loss: 0.3914 - acc: 0.8519\n",
      "Epoch 73/100\n",
      "135/135 [==============================] - 0s 110us/step - loss: 0.3885 - acc: 0.8519\n",
      "Epoch 74/100\n",
      "135/135 [==============================] - 0s 113us/step - loss: 0.3860 - acc: 0.8593\n",
      "Epoch 75/100\n",
      "135/135 [==============================] - 0s 124us/step - loss: 0.3834 - acc: 0.8519\n",
      "Epoch 76/100\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.3812 - acc: 0.8593\n",
      "Epoch 77/100\n",
      "135/135 [==============================] - 0s 116us/step - loss: 0.3787 - acc: 0.8593\n",
      "Epoch 78/100\n",
      "135/135 [==============================] - 0s 95us/step - loss: 0.3762 - acc: 0.8741\n",
      "Epoch 79/100\n",
      "135/135 [==============================] - 0s 132us/step - loss: 0.3743 - acc: 0.8741\n",
      "Epoch 80/100\n",
      "135/135 [==============================] - 0s 126us/step - loss: 0.3720 - acc: 0.8741\n",
      "Epoch 81/100\n",
      "135/135 [==============================] - 0s 96us/step - loss: 0.3703 - acc: 0.8815\n",
      "Epoch 82/100\n",
      "135/135 [==============================] - 0s 90us/step - loss: 0.3680 - acc: 0.8741\n",
      "Epoch 83/100\n",
      "135/135 [==============================] - 0s 97us/step - loss: 0.3658 - acc: 0.8815\n",
      "Epoch 84/100\n",
      "135/135 [==============================] - 0s 98us/step - loss: 0.3645 - acc: 0.8889\n",
      "Epoch 85/100\n",
      "135/135 [==============================] - 0s 99us/step - loss: 0.3625 - acc: 0.8889\n",
      "Epoch 86/100\n",
      "135/135 [==============================] - 0s 93us/step - loss: 0.3605 - acc: 0.8889\n",
      "Epoch 87/100\n",
      "135/135 [==============================] - 0s 104us/step - loss: 0.3590 - acc: 0.8889\n",
      "Epoch 88/100\n",
      "135/135 [==============================] - 0s 113us/step - loss: 0.3573 - acc: 0.8889\n",
      "Epoch 89/100\n",
      "135/135 [==============================] - 0s 128us/step - loss: 0.3558 - acc: 0.8889\n",
      "Epoch 90/100\n",
      "135/135 [==============================] - 0s 113us/step - loss: 0.3544 - acc: 0.8889\n",
      "Epoch 91/100\n",
      "135/135 [==============================] - 0s 98us/step - loss: 0.3535 - acc: 0.8889\n",
      "Epoch 92/100\n",
      "135/135 [==============================] - 0s 108us/step - loss: 0.3513 - acc: 0.8889\n",
      "Epoch 93/100\n",
      "135/135 [==============================] - 0s 107us/step - loss: 0.3499 - acc: 0.8889\n",
      "Epoch 94/100\n",
      "135/135 [==============================] - 0s 126us/step - loss: 0.3485 - acc: 0.8889\n",
      "Epoch 95/100\n",
      "135/135 [==============================] - 0s 110us/step - loss: 0.3472 - acc: 0.8889\n",
      "Epoch 96/100\n",
      "135/135 [==============================] - 0s 101us/step - loss: 0.3458 - acc: 0.8889\n",
      "Epoch 97/100\n",
      "135/135 [==============================] - 0s 141us/step - loss: 0.3449 - acc: 0.8963\n",
      "Epoch 98/100\n",
      "135/135 [==============================] - 0s 124us/step - loss: 0.3433 - acc: 0.8889\n",
      "Epoch 99/100\n",
      "135/135 [==============================] - 0s 126us/step - loss: 0.3419 - acc: 0.8889\n",
      "Epoch 100/100\n",
      "135/135 [==============================] - 0s 96us/step - loss: 0.3407 - acc: 0.8889\n",
      "68/68 [==============================] - 0s 5ms/step\n",
      "135/135 [==============================] - 0s 59us/step\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 1s 8ms/step - loss: 0.6950 - acc: 0.5515\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 106us/step - loss: 0.6862 - acc: 0.5588\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 149us/step - loss: 0.6764 - acc: 0.5956\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 115us/step - loss: 0.6679 - acc: 0.6176\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 117us/step - loss: 0.6598 - acc: 0.6250\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 131us/step - loss: 0.6512 - acc: 0.6324\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 102us/step - loss: 0.6431 - acc: 0.6471\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 136us/step - loss: 0.6350 - acc: 0.6691\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 128us/step - loss: 0.6275 - acc: 0.6912\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 108us/step - loss: 0.6195 - acc: 0.6985\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 136us/step - loss: 0.6120 - acc: 0.7353\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 129us/step - loss: 0.6043 - acc: 0.7500\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 149us/step - loss: 0.5967 - acc: 0.7574\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 243us/step - loss: 0.5896 - acc: 0.7574\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 111us/step - loss: 0.5822 - acc: 0.7574\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 90us/step - loss: 0.5756 - acc: 0.7574\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 92us/step - loss: 0.5684 - acc: 0.7647\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 118us/step - loss: 0.5615 - acc: 0.7647\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 114us/step - loss: 0.5552 - acc: 0.7721\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 100us/step - loss: 0.5492 - acc: 0.7721\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 103us/step - loss: 0.5427 - acc: 0.7794\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 112us/step - loss: 0.5371 - acc: 0.7868\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 109us/step - loss: 0.5315 - acc: 0.7868\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 108us/step - loss: 0.5258 - acc: 0.7868\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 138us/step - loss: 0.5207 - acc: 0.7868\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 107us/step - loss: 0.5157 - acc: 0.7868\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 123us/step - loss: 0.5107 - acc: 0.7868\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 109us/step - loss: 0.5060 - acc: 0.7941\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 89us/step - loss: 0.5015 - acc: 0.8088\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 141us/step - loss: 0.4973 - acc: 0.8088\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 110us/step - loss: 0.4930 - acc: 0.8088\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 100us/step - loss: 0.4889 - acc: 0.8162\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 96us/step - loss: 0.4848 - acc: 0.8162\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 97us/step - loss: 0.4810 - acc: 0.8162\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 97us/step - loss: 0.4770 - acc: 0.8235\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 0s 137us/step - loss: 0.4733 - acc: 0.8235\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 98us/step - loss: 0.4701 - acc: 0.8235\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 100us/step - loss: 0.4664 - acc: 0.8309\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 0s 90us/step - loss: 0.4630 - acc: 0.8382\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 0s 89us/step - loss: 0.4597 - acc: 0.8382\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 110us/step - loss: 0.4569 - acc: 0.8382\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 131us/step - loss: 0.4537 - acc: 0.8382\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 103us/step - loss: 0.4511 - acc: 0.8382\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 93us/step - loss: 0.4484 - acc: 0.8382\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 108us/step - loss: 0.4461 - acc: 0.8382\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 93us/step - loss: 0.4433 - acc: 0.8456\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 119us/step - loss: 0.4406 - acc: 0.8382\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 154us/step - loss: 0.4380 - acc: 0.8382\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 98us/step - loss: 0.4356 - acc: 0.8382\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 103us/step - loss: 0.4330 - acc: 0.8382\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 95us/step - loss: 0.4308 - acc: 0.8382\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 92us/step - loss: 0.4282 - acc: 0.8382\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 109us/step - loss: 0.4261 - acc: 0.8382\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 122us/step - loss: 0.4246 - acc: 0.8309\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 120us/step - loss: 0.4220 - acc: 0.8382\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 0s 135us/step - loss: 0.4197 - acc: 0.8382\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 0s 101us/step - loss: 0.4178 - acc: 0.8382\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 0s 103us/step - loss: 0.4160 - acc: 0.8382\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 0s 91us/step - loss: 0.4146 - acc: 0.8456\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - 0s 111us/step - loss: 0.4133 - acc: 0.8456\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 0s 96us/step - loss: 0.4115 - acc: 0.8456\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 0s 94us/step - loss: 0.4095 - acc: 0.8456\n",
      "Epoch 63/100\n",
      "136/136 [==============================] - 0s 104us/step - loss: 0.4076 - acc: 0.8382\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 0s 119us/step - loss: 0.4060 - acc: 0.8382\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 0s 117us/step - loss: 0.4045 - acc: 0.8529\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 0s 124us/step - loss: 0.4024 - acc: 0.8529\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 0s 101us/step - loss: 0.4010 - acc: 0.8529\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 0s 99us/step - loss: 0.3995 - acc: 0.8529\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 0s 103us/step - loss: 0.3983 - acc: 0.8529\n",
      "Epoch 70/100\n",
      "136/136 [==============================] - 0s 99us/step - loss: 0.3965 - acc: 0.8529\n",
      "Epoch 71/100\n",
      "136/136 [==============================] - 0s 112us/step - loss: 0.3950 - acc: 0.8456\n",
      "Epoch 72/100\n",
      "136/136 [==============================] - 0s 89us/step - loss: 0.3939 - acc: 0.8456\n",
      "Epoch 73/100\n",
      "136/136 [==============================] - 0s 104us/step - loss: 0.3924 - acc: 0.8456\n",
      "Epoch 74/100\n",
      "136/136 [==============================] - 0s 109us/step - loss: 0.3909 - acc: 0.8603\n",
      "Epoch 75/100\n",
      "136/136 [==============================] - 0s 107us/step - loss: 0.3895 - acc: 0.8529\n",
      "Epoch 76/100\n",
      "136/136 [==============================] - 0s 93us/step - loss: 0.3886 - acc: 0.8456\n",
      "Epoch 77/100\n",
      "136/136 [==============================] - 0s 90us/step - loss: 0.3872 - acc: 0.8456\n",
      "Epoch 78/100\n",
      "136/136 [==============================] - 0s 151us/step - loss: 0.3858 - acc: 0.8529\n",
      "Epoch 79/100\n",
      "136/136 [==============================] - 0s 94us/step - loss: 0.3844 - acc: 0.8456\n",
      "Epoch 80/100\n",
      "136/136 [==============================] - 0s 91us/step - loss: 0.3836 - acc: 0.8603\n",
      "Epoch 81/100\n",
      "136/136 [==============================] - 0s 104us/step - loss: 0.3819 - acc: 0.8603\n",
      "Epoch 82/100\n",
      "136/136 [==============================] - 0s 99us/step - loss: 0.3808 - acc: 0.8603\n",
      "Epoch 83/100\n",
      "136/136 [==============================] - 0s 95us/step - loss: 0.3797 - acc: 0.8603\n",
      "Epoch 84/100\n",
      "136/136 [==============================] - 0s 90us/step - loss: 0.3787 - acc: 0.8603\n",
      "Epoch 85/100\n",
      "136/136 [==============================] - 0s 128us/step - loss: 0.3776 - acc: 0.8603\n",
      "Epoch 86/100\n",
      "136/136 [==============================] - 0s 111us/step - loss: 0.3767 - acc: 0.8603\n",
      "Epoch 87/100\n",
      "136/136 [==============================] - 0s 102us/step - loss: 0.3759 - acc: 0.8676\n",
      "Epoch 88/100\n",
      "136/136 [==============================] - 0s 97us/step - loss: 0.3746 - acc: 0.8603\n",
      "Epoch 89/100\n",
      "136/136 [==============================] - 0s 115us/step - loss: 0.3739 - acc: 0.8676\n",
      "Epoch 90/100\n",
      "136/136 [==============================] - 0s 87us/step - loss: 0.3726 - acc: 0.8676\n",
      "Epoch 91/100\n",
      "136/136 [==============================] - 0s 99us/step - loss: 0.3718 - acc: 0.8676\n",
      "Epoch 92/100\n",
      "136/136 [==============================] - 0s 108us/step - loss: 0.3707 - acc: 0.8603\n",
      "Epoch 93/100\n",
      "136/136 [==============================] - 0s 122us/step - loss: 0.3698 - acc: 0.8529\n",
      "Epoch 94/100\n",
      "136/136 [==============================] - 0s 114us/step - loss: 0.3691 - acc: 0.8603\n",
      "Epoch 95/100\n",
      "136/136 [==============================] - 0s 102us/step - loss: 0.3682 - acc: 0.8603\n",
      "Epoch 96/100\n",
      "136/136 [==============================] - 0s 103us/step - loss: 0.3672 - acc: 0.8603\n",
      "Epoch 97/100\n",
      "136/136 [==============================] - 0s 100us/step - loss: 0.3664 - acc: 0.8603\n",
      "Epoch 98/100\n",
      "136/136 [==============================] - 0s 116us/step - loss: 0.3656 - acc: 0.8603\n",
      "Epoch 99/100\n",
      "136/136 [==============================] - 0s 116us/step - loss: 0.3648 - acc: 0.8603\n",
      "Epoch 100/100\n",
      "136/136 [==============================] - 0s 116us/step - loss: 0.3640 - acc: 0.8676\n",
      "67/67 [==============================] - 0s 6ms/step\n",
      "136/136 [==============================] - 0s 76us/step\n",
      "Epoch 1/20\n",
      "135/135 [==============================] - 1s 9ms/step - loss: 0.7596 - acc: 0.2963\n",
      "Epoch 2/20\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.7542 - acc: 0.3037\n",
      "Epoch 3/20\n",
      "135/135 [==============================] - 0s 98us/step - loss: 0.7484 - acc: 0.3185\n",
      "Epoch 4/20\n",
      "135/135 [==============================] - 0s 130us/step - loss: 0.7433 - acc: 0.3333\n",
      "Epoch 5/20\n",
      "135/135 [==============================] - 0s 97us/step - loss: 0.7383 - acc: 0.3704\n",
      "Epoch 6/20\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.7335 - acc: 0.3704\n",
      "Epoch 7/20\n",
      "135/135 [==============================] - 0s 98us/step - loss: 0.7288 - acc: 0.4074\n",
      "Epoch 8/20\n",
      "135/135 [==============================] - 0s 104us/step - loss: 0.7238 - acc: 0.4222\n",
      "Epoch 9/20\n",
      "135/135 [==============================] - 0s 138us/step - loss: 0.7192 - acc: 0.4296\n",
      "Epoch 10/20\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.7149 - acc: 0.4370\n",
      "Epoch 11/20\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.7104 - acc: 0.4444\n",
      "Epoch 12/20\n",
      "135/135 [==============================] - 0s 108us/step - loss: 0.7064 - acc: 0.4444\n",
      "Epoch 13/20\n",
      "135/135 [==============================] - 0s 96us/step - loss: 0.7023 - acc: 0.4963\n",
      "Epoch 14/20\n",
      "135/135 [==============================] - 0s 76us/step - loss: 0.6981 - acc: 0.5037\n",
      "Epoch 15/20\n",
      "135/135 [==============================] - 0s 86us/step - loss: 0.6939 - acc: 0.5259\n",
      "Epoch 16/20\n",
      "135/135 [==============================] - 0s 78us/step - loss: 0.6902 - acc: 0.5556\n",
      "Epoch 17/20\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.6861 - acc: 0.5630\n",
      "Epoch 18/20\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.6823 - acc: 0.5778\n",
      "Epoch 19/20\n",
      "135/135 [==============================] - 0s 96us/step - loss: 0.6784 - acc: 0.5926\n",
      "Epoch 20/20\n",
      "135/135 [==============================] - 0s 91us/step - loss: 0.6745 - acc: 0.6222\n",
      "68/68 [==============================] - 0s 6ms/step\n",
      "135/135 [==============================] - 0s 62us/step\n",
      "Epoch 1/20\n",
      "135/135 [==============================] - 1s 9ms/step - loss: 0.7292 - acc: 0.3037\n",
      "Epoch 2/20\n",
      "135/135 [==============================] - 0s 90us/step - loss: 0.7237 - acc: 0.3259\n",
      "Epoch 3/20\n",
      "135/135 [==============================] - 0s 91us/step - loss: 0.7182 - acc: 0.3481\n",
      "Epoch 4/20\n",
      "135/135 [==============================] - 0s 143us/step - loss: 0.7136 - acc: 0.3778\n",
      "Epoch 5/20\n",
      "135/135 [==============================] - 0s 97us/step - loss: 0.7083 - acc: 0.4074\n",
      "Epoch 6/20\n",
      "135/135 [==============================] - 0s 95us/step - loss: 0.7034 - acc: 0.4296\n",
      "Epoch 7/20\n",
      "135/135 [==============================] - 0s 144us/step - loss: 0.6985 - acc: 0.4815\n",
      "Epoch 8/20\n",
      "135/135 [==============================] - 0s 96us/step - loss: 0.6936 - acc: 0.5333\n",
      "Epoch 9/20\n",
      "135/135 [==============================] - 0s 99us/step - loss: 0.6886 - acc: 0.5333\n",
      "Epoch 10/20\n",
      "135/135 [==============================] - 0s 91us/step - loss: 0.6836 - acc: 0.6000\n",
      "Epoch 11/20\n",
      "135/135 [==============================] - 0s 138us/step - loss: 0.6787 - acc: 0.6444\n",
      "Epoch 12/20\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.6740 - acc: 0.6889\n",
      "Epoch 13/20\n",
      "135/135 [==============================] - 0s 89us/step - loss: 0.6690 - acc: 0.7185\n",
      "Epoch 14/20\n",
      "135/135 [==============================] - 0s 93us/step - loss: 0.6640 - acc: 0.7481\n",
      "Epoch 15/20\n",
      "135/135 [==============================] - 0s 154us/step - loss: 0.6592 - acc: 0.7778\n",
      "Epoch 16/20\n",
      "135/135 [==============================] - 0s 95us/step - loss: 0.6541 - acc: 0.7778\n",
      "Epoch 17/20\n",
      "135/135 [==============================] - 0s 99us/step - loss: 0.6489 - acc: 0.7852\n",
      "Epoch 18/20\n",
      "135/135 [==============================] - 0s 89us/step - loss: 0.6439 - acc: 0.7852\n",
      "Epoch 19/20\n",
      "135/135 [==============================] - 0s 120us/step - loss: 0.6390 - acc: 0.7852\n",
      "Epoch 20/20\n",
      "135/135 [==============================] - 0s 87us/step - loss: 0.6342 - acc: 0.7778\n",
      "68/68 [==============================] - 0s 7ms/step\n",
      "135/135 [==============================] - 0s 108us/step\n",
      "Epoch 1/20\n",
      "136/136 [==============================] - 1s 9ms/step - loss: 0.7849 - acc: 0.4485\n",
      "Epoch 2/20\n",
      "136/136 [==============================] - 0s 97us/step - loss: 0.7774 - acc: 0.4485\n",
      "Epoch 3/20\n",
      "136/136 [==============================] - 0s 93us/step - loss: 0.7701 - acc: 0.4485\n",
      "Epoch 4/20\n",
      "136/136 [==============================] - 0s 135us/step - loss: 0.7639 - acc: 0.4412\n",
      "Epoch 5/20\n",
      "136/136 [==============================] - 0s 99us/step - loss: 0.7576 - acc: 0.4338\n",
      "Epoch 6/20\n",
      "136/136 [==============================] - 0s 94us/step - loss: 0.7518 - acc: 0.4412\n",
      "Epoch 7/20\n",
      "136/136 [==============================] - 0s 115us/step - loss: 0.7465 - acc: 0.4485\n",
      "Epoch 8/20\n",
      "136/136 [==============================] - 0s 117us/step - loss: 0.7409 - acc: 0.4485\n",
      "Epoch 9/20\n",
      "136/136 [==============================] - 0s 98us/step - loss: 0.7357 - acc: 0.4412\n",
      "Epoch 10/20\n",
      "136/136 [==============================] - 0s 98us/step - loss: 0.7310 - acc: 0.4412\n",
      "Epoch 11/20\n",
      "136/136 [==============================] - 0s 95us/step - loss: 0.7276 - acc: 0.4485\n",
      "Epoch 12/20\n",
      "136/136 [==============================] - 0s 107us/step - loss: 0.7232 - acc: 0.4485\n",
      "Epoch 13/20\n",
      "136/136 [==============================] - 0s 90us/step - loss: 0.7193 - acc: 0.4485\n",
      "Epoch 14/20\n",
      "136/136 [==============================] - 0s 81us/step - loss: 0.7157 - acc: 0.4485\n",
      "Epoch 15/20\n",
      "136/136 [==============================] - 0s 88us/step - loss: 0.7118 - acc: 0.4485\n",
      "Epoch 16/20\n",
      "136/136 [==============================] - 0s 91us/step - loss: 0.7081 - acc: 0.4412\n",
      "Epoch 17/20\n",
      "136/136 [==============================] - 0s 91us/step - loss: 0.7043 - acc: 0.4412\n",
      "Epoch 18/20\n",
      "136/136 [==============================] - 0s 118us/step - loss: 0.7007 - acc: 0.4485\n",
      "Epoch 19/20\n",
      "136/136 [==============================] - 0s 111us/step - loss: 0.6971 - acc: 0.4632\n",
      "Epoch 20/20\n",
      "136/136 [==============================] - 0s 101us/step - loss: 0.6941 - acc: 0.5000\n",
      "67/67 [==============================] - 0s 6ms/step\n",
      "136/136 [==============================] - 0s 100us/step\n",
      "Epoch 1/50\n",
      "135/135 [==============================] - 1s 9ms/step - loss: 0.7026 - acc: 0.4815\n",
      "Epoch 2/50\n",
      "135/135 [==============================] - 0s 95us/step - loss: 0.6994 - acc: 0.4889\n",
      "Epoch 3/50\n",
      "135/135 [==============================] - 0s 89us/step - loss: 0.6966 - acc: 0.5037\n",
      "Epoch 4/50\n",
      "135/135 [==============================] - 0s 144us/step - loss: 0.6938 - acc: 0.5185\n",
      "Epoch 5/50\n",
      "135/135 [==============================] - 0s 96us/step - loss: 0.6912 - acc: 0.5185\n",
      "Epoch 6/50\n",
      "135/135 [==============================] - 0s 98us/step - loss: 0.6887 - acc: 0.5259\n",
      "Epoch 7/50\n",
      "135/135 [==============================] - 0s 152us/step - loss: 0.6863 - acc: 0.5481\n",
      "Epoch 8/50\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.6840 - acc: 0.5630\n",
      "Epoch 9/50\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.6814 - acc: 0.5778\n",
      "Epoch 10/50\n",
      "135/135 [==============================] - 0s 98us/step - loss: 0.6792 - acc: 0.6000\n",
      "Epoch 11/50\n",
      "135/135 [==============================] - 0s 127us/step - loss: 0.6769 - acc: 0.6148\n",
      "Epoch 12/50\n",
      "135/135 [==============================] - 0s 102us/step - loss: 0.6748 - acc: 0.6444\n",
      "Epoch 13/50\n",
      "135/135 [==============================] - 0s 99us/step - loss: 0.6725 - acc: 0.6593\n",
      "Epoch 14/50\n",
      "135/135 [==============================] - 0s 123us/step - loss: 0.6704 - acc: 0.6741\n",
      "Epoch 15/50\n",
      "135/135 [==============================] - 0s 86us/step - loss: 0.6681 - acc: 0.6741\n",
      "Epoch 16/50\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.6660 - acc: 0.6741\n",
      "Epoch 17/50\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.6638 - acc: 0.6889\n",
      "Epoch 18/50\n",
      "135/135 [==============================] - 0s 90us/step - loss: 0.6616 - acc: 0.6889\n",
      "Epoch 19/50\n",
      "135/135 [==============================] - 0s 77us/step - loss: 0.6597 - acc: 0.7111\n",
      "Epoch 20/50\n",
      "135/135 [==============================] - 0s 82us/step - loss: 0.6578 - acc: 0.7037\n",
      "Epoch 21/50\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.6556 - acc: 0.7185\n",
      "Epoch 22/50\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.6537 - acc: 0.7259\n",
      "Epoch 23/50\n",
      "135/135 [==============================] - 0s 99us/step - loss: 0.6516 - acc: 0.7333\n",
      "Epoch 24/50\n",
      "135/135 [==============================] - 0s 106us/step - loss: 0.6498 - acc: 0.7333\n",
      "Epoch 25/50\n",
      "135/135 [==============================] - 0s 122us/step - loss: 0.6475 - acc: 0.7259\n",
      "Epoch 26/50\n",
      "135/135 [==============================] - 0s 102us/step - loss: 0.6451 - acc: 0.7185\n",
      "Epoch 27/50\n",
      "135/135 [==============================] - 0s 74us/step - loss: 0.6431 - acc: 0.7333\n",
      "Epoch 28/50\n",
      "135/135 [==============================] - 0s 79us/step - loss: 0.6407 - acc: 0.7556\n",
      "Epoch 29/50\n",
      "135/135 [==============================] - 0s 87us/step - loss: 0.6382 - acc: 0.7556\n",
      "Epoch 30/50\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.6360 - acc: 0.7556\n",
      "Epoch 31/50\n",
      "135/135 [==============================] - 0s 95us/step - loss: 0.6337 - acc: 0.7556\n",
      "Epoch 32/50\n",
      "135/135 [==============================] - 0s 96us/step - loss: 0.6312 - acc: 0.7556\n",
      "Epoch 33/50\n",
      "135/135 [==============================] - 0s 130us/step - loss: 0.6286 - acc: 0.7630\n",
      "Epoch 34/50\n",
      "135/135 [==============================] - 0s 82us/step - loss: 0.6260 - acc: 0.7630\n",
      "Epoch 35/50\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.6228 - acc: 0.7778\n",
      "Epoch 36/50\n",
      "135/135 [==============================] - 0s 99us/step - loss: 0.6194 - acc: 0.7704\n",
      "Epoch 37/50\n",
      "135/135 [==============================] - 0s 105us/step - loss: 0.6163 - acc: 0.7704\n",
      "Epoch 38/50\n",
      "135/135 [==============================] - 0s 87us/step - loss: 0.6125 - acc: 0.7778\n",
      "Epoch 39/50\n",
      "135/135 [==============================] - 0s 88us/step - loss: 0.6091 - acc: 0.7630\n",
      "Epoch 40/50\n",
      "135/135 [==============================] - 0s 99us/step - loss: 0.6055 - acc: 0.7704\n",
      "Epoch 41/50\n",
      "135/135 [==============================] - 0s 107us/step - loss: 0.6019 - acc: 0.7778\n",
      "Epoch 42/50\n",
      "135/135 [==============================] - 0s 97us/step - loss: 0.5981 - acc: 0.7778\n",
      "Epoch 43/50\n",
      "135/135 [==============================] - 0s 121us/step - loss: 0.5943 - acc: 0.7778\n",
      "Epoch 44/50\n",
      "135/135 [==============================] - 0s 148us/step - loss: 0.5907 - acc: 0.7852\n",
      "Epoch 45/50\n",
      "135/135 [==============================] - 0s 114us/step - loss: 0.5872 - acc: 0.7778\n",
      "Epoch 46/50\n",
      "135/135 [==============================] - 0s 107us/step - loss: 0.5840 - acc: 0.7778\n",
      "Epoch 47/50\n",
      "135/135 [==============================] - 0s 124us/step - loss: 0.5802 - acc: 0.7778\n",
      "Epoch 48/50\n",
      "135/135 [==============================] - 0s 99us/step - loss: 0.5761 - acc: 0.7778\n",
      "Epoch 49/50\n",
      "135/135 [==============================] - 0s 99us/step - loss: 0.5721 - acc: 0.7852\n",
      "Epoch 50/50\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.5681 - acc: 0.7926\n",
      "68/68 [==============================] - 1s 7ms/step\n",
      "135/135 [==============================] - 0s 88us/step\n",
      "Epoch 1/50\n",
      "135/135 [==============================] - 1s 9ms/step - loss: 0.7402 - acc: 0.3704\n",
      "Epoch 2/50\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.7327 - acc: 0.3704\n",
      "Epoch 3/50\n",
      "135/135 [==============================] - 0s 115us/step - loss: 0.7258 - acc: 0.4000\n",
      "Epoch 4/50\n",
      "135/135 [==============================] - 0s 137us/step - loss: 0.7195 - acc: 0.4296\n",
      "Epoch 5/50\n",
      "135/135 [==============================] - 0s 101us/step - loss: 0.7135 - acc: 0.4148\n",
      "Epoch 6/50\n",
      "135/135 [==============================] - 0s 101us/step - loss: 0.7080 - acc: 0.4370\n",
      "Epoch 7/50\n",
      "135/135 [==============================] - 0s 140us/step - loss: 0.7025 - acc: 0.4370\n",
      "Epoch 8/50\n",
      "135/135 [==============================] - 0s 111us/step - loss: 0.6974 - acc: 0.4444\n",
      "Epoch 9/50\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.6930 - acc: 0.4444\n",
      "Epoch 10/50\n",
      "135/135 [==============================] - 0s 126us/step - loss: 0.6887 - acc: 0.4593\n",
      "Epoch 11/50\n",
      "135/135 [==============================] - 0s 113us/step - loss: 0.6846 - acc: 0.4667\n",
      "Epoch 12/50\n",
      "135/135 [==============================] - 0s 102us/step - loss: 0.6803 - acc: 0.4889\n",
      "Epoch 13/50\n",
      "135/135 [==============================] - 0s 110us/step - loss: 0.6764 - acc: 0.5111\n",
      "Epoch 14/50\n",
      "135/135 [==============================] - 0s 133us/step - loss: 0.6727 - acc: 0.5037\n",
      "Epoch 15/50\n",
      "135/135 [==============================] - 0s 127us/step - loss: 0.6692 - acc: 0.5407\n",
      "Epoch 16/50\n",
      "135/135 [==============================] - 0s 107us/step - loss: 0.6660 - acc: 0.5481\n",
      "Epoch 17/50\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.6630 - acc: 0.5556\n",
      "Epoch 18/50\n",
      "135/135 [==============================] - 0s 90us/step - loss: 0.6599 - acc: 0.5630\n",
      "Epoch 19/50\n",
      "135/135 [==============================] - 0s 157us/step - loss: 0.6571 - acc: 0.5852\n",
      "Epoch 20/50\n",
      "135/135 [==============================] - 0s 118us/step - loss: 0.6541 - acc: 0.6000\n",
      "Epoch 21/50\n",
      "135/135 [==============================] - 0s 99us/step - loss: 0.6514 - acc: 0.6148\n",
      "Epoch 22/50\n",
      "135/135 [==============================] - 0s 95us/step - loss: 0.6485 - acc: 0.6148\n",
      "Epoch 23/50\n",
      "135/135 [==============================] - 0s 133us/step - loss: 0.6456 - acc: 0.6148\n",
      "Epoch 24/50\n",
      "135/135 [==============================] - 0s 118us/step - loss: 0.6427 - acc: 0.6222\n",
      "Epoch 25/50\n",
      "135/135 [==============================] - 0s 87us/step - loss: 0.6395 - acc: 0.6370\n",
      "Epoch 26/50\n",
      "135/135 [==============================] - 0s 88us/step - loss: 0.6363 - acc: 0.6444\n",
      "Epoch 27/50\n",
      "135/135 [==============================] - 0s 81us/step - loss: 0.6332 - acc: 0.6741\n",
      "Epoch 28/50\n",
      "135/135 [==============================] - 0s 89us/step - loss: 0.6300 - acc: 0.6741\n",
      "Epoch 29/50\n",
      "135/135 [==============================] - 0s 90us/step - loss: 0.6272 - acc: 0.6815\n",
      "Epoch 30/50\n",
      "135/135 [==============================] - 0s 101us/step - loss: 0.6239 - acc: 0.7037\n",
      "Epoch 31/50\n",
      "135/135 [==============================] - 0s 127us/step - loss: 0.6212 - acc: 0.7037\n",
      "Epoch 32/50\n",
      "135/135 [==============================] - 0s 106us/step - loss: 0.6181 - acc: 0.7259\n",
      "Epoch 33/50\n",
      "135/135 [==============================] - 0s 91us/step - loss: 0.6151 - acc: 0.7407\n",
      "Epoch 34/50\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.6122 - acc: 0.7407\n",
      "Epoch 35/50\n",
      "135/135 [==============================] - 0s 106us/step - loss: 0.6093 - acc: 0.7481\n",
      "Epoch 36/50\n",
      "135/135 [==============================] - 0s 113us/step - loss: 0.6066 - acc: 0.7481\n",
      "Epoch 37/50\n",
      "135/135 [==============================] - 0s 114us/step - loss: 0.6037 - acc: 0.7556\n",
      "Epoch 38/50\n",
      "135/135 [==============================] - 0s 120us/step - loss: 0.6005 - acc: 0.7704\n",
      "Epoch 39/50\n",
      "135/135 [==============================] - 0s 99us/step - loss: 0.5972 - acc: 0.7778\n",
      "Epoch 40/50\n",
      "135/135 [==============================] - 0s 115us/step - loss: 0.5941 - acc: 0.7852\n",
      "Epoch 41/50\n",
      "135/135 [==============================] - 0s 109us/step - loss: 0.5905 - acc: 0.7852\n",
      "Epoch 42/50\n",
      "135/135 [==============================] - 0s 93us/step - loss: 0.5871 - acc: 0.7852\n",
      "Epoch 43/50\n",
      "135/135 [==============================] - 0s 83us/step - loss: 0.5836 - acc: 0.7852\n",
      "Epoch 44/50\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.5802 - acc: 0.7852\n",
      "Epoch 45/50\n",
      "135/135 [==============================] - 0s 101us/step - loss: 0.5767 - acc: 0.7852\n",
      "Epoch 46/50\n",
      "135/135 [==============================] - 0s 99us/step - loss: 0.5735 - acc: 0.7852\n",
      "Epoch 47/50\n",
      "135/135 [==============================] - 0s 84us/step - loss: 0.5699 - acc: 0.7852\n",
      "Epoch 48/50\n",
      "135/135 [==============================] - 0s 102us/step - loss: 0.5667 - acc: 0.7926\n",
      "Epoch 49/50\n",
      "135/135 [==============================] - 0s 64us/step - loss: 0.5633 - acc: 0.7926\n",
      "Epoch 50/50\n",
      "135/135 [==============================] - 0s 105us/step - loss: 0.5599 - acc: 0.8000\n",
      "68/68 [==============================] - 0s 7ms/step\n",
      "135/135 [==============================] - 0s 60us/step\n",
      "Epoch 1/50\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 0.8008 - acc: 0.4485\n",
      "Epoch 2/50\n",
      "136/136 [==============================] - 0s 101us/step - loss: 0.7916 - acc: 0.4485\n",
      "Epoch 3/50\n",
      "136/136 [==============================] - 0s 107us/step - loss: 0.7829 - acc: 0.4412\n",
      "Epoch 4/50\n",
      "136/136 [==============================] - 0s 145us/step - loss: 0.7749 - acc: 0.4412\n",
      "Epoch 5/50\n",
      "136/136 [==============================] - 0s 102us/step - loss: 0.7674 - acc: 0.4412\n",
      "Epoch 6/50\n",
      "136/136 [==============================] - 0s 99us/step - loss: 0.7599 - acc: 0.4412\n",
      "Epoch 7/50\n",
      "136/136 [==============================] - 0s 121us/step - loss: 0.7531 - acc: 0.4559\n",
      "Epoch 8/50\n",
      "136/136 [==============================] - 0s 136us/step - loss: 0.7464 - acc: 0.4559\n",
      "Epoch 9/50\n",
      "136/136 [==============================] - 0s 142us/step - loss: 0.7410 - acc: 0.4559\n",
      "Epoch 10/50\n",
      "136/136 [==============================] - 0s 94us/step - loss: 0.7354 - acc: 0.4559\n",
      "Epoch 11/50\n",
      "136/136 [==============================] - 0s 122us/step - loss: 0.7302 - acc: 0.4559\n",
      "Epoch 12/50\n",
      "136/136 [==============================] - 0s 129us/step - loss: 0.7254 - acc: 0.4632\n",
      "Epoch 13/50\n",
      "136/136 [==============================] - 0s 84us/step - loss: 0.7215 - acc: 0.4632\n",
      "Epoch 14/50\n",
      "136/136 [==============================] - 0s 90us/step - loss: 0.7172 - acc: 0.4485\n",
      "Epoch 15/50\n",
      "136/136 [==============================] - 0s 90us/step - loss: 0.7133 - acc: 0.4485\n",
      "Epoch 16/50\n",
      "136/136 [==============================] - 0s 84us/step - loss: 0.7094 - acc: 0.4485\n",
      "Epoch 17/50\n",
      "136/136 [==============================] - 0s 110us/step - loss: 0.7063 - acc: 0.4265\n",
      "Epoch 18/50\n",
      "136/136 [==============================] - 0s 86us/step - loss: 0.7026 - acc: 0.4412\n",
      "Epoch 19/50\n",
      "136/136 [==============================] - 0s 107us/step - loss: 0.6994 - acc: 0.4412\n",
      "Epoch 20/50\n",
      "136/136 [==============================] - 0s 101us/step - loss: 0.6962 - acc: 0.4412\n",
      "Epoch 21/50\n",
      "136/136 [==============================] - 0s 90us/step - loss: 0.6927 - acc: 0.4559\n",
      "Epoch 22/50\n",
      "136/136 [==============================] - 0s 96us/step - loss: 0.6897 - acc: 0.4706\n",
      "Epoch 23/50\n",
      "136/136 [==============================] - 0s 93us/step - loss: 0.6862 - acc: 0.4853\n",
      "Epoch 24/50\n",
      "136/136 [==============================] - 0s 108us/step - loss: 0.6831 - acc: 0.5074\n",
      "Epoch 25/50\n",
      "136/136 [==============================] - 0s 102us/step - loss: 0.6798 - acc: 0.5368\n",
      "Epoch 26/50\n",
      "136/136 [==============================] - 0s 102us/step - loss: 0.6765 - acc: 0.5368\n",
      "Epoch 27/50\n",
      "136/136 [==============================] - 0s 109us/step - loss: 0.6731 - acc: 0.5735\n",
      "Epoch 28/50\n",
      "136/136 [==============================] - 0s 107us/step - loss: 0.6698 - acc: 0.5809\n",
      "Epoch 29/50\n",
      "136/136 [==============================] - 0s 133us/step - loss: 0.6667 - acc: 0.5882\n",
      "Epoch 30/50\n",
      "136/136 [==============================] - 0s 117us/step - loss: 0.6634 - acc: 0.6029\n",
      "Epoch 31/50\n",
      "136/136 [==============================] - 0s 112us/step - loss: 0.6605 - acc: 0.6103\n",
      "Epoch 32/50\n",
      "136/136 [==============================] - 0s 108us/step - loss: 0.6571 - acc: 0.6176\n",
      "Epoch 33/50\n",
      "136/136 [==============================] - 0s 103us/step - loss: 0.6543 - acc: 0.6176\n",
      "Epoch 34/50\n",
      "136/136 [==============================] - 0s 112us/step - loss: 0.6515 - acc: 0.6324\n",
      "Epoch 35/50\n",
      "136/136 [==============================] - 0s 119us/step - loss: 0.6485 - acc: 0.6471\n",
      "Epoch 36/50\n",
      "136/136 [==============================] - 0s 108us/step - loss: 0.6457 - acc: 0.6618\n",
      "Epoch 37/50\n",
      "136/136 [==============================] - 0s 97us/step - loss: 0.6430 - acc: 0.6691\n",
      "Epoch 38/50\n",
      "136/136 [==============================] - 0s 129us/step - loss: 0.6402 - acc: 0.6912\n",
      "Epoch 39/50\n",
      "136/136 [==============================] - 0s 91us/step - loss: 0.6375 - acc: 0.6912\n",
      "Epoch 40/50\n",
      "136/136 [==============================] - 0s 102us/step - loss: 0.6346 - acc: 0.6912\n",
      "Epoch 41/50\n",
      "136/136 [==============================] - 0s 90us/step - loss: 0.6318 - acc: 0.6838\n",
      "Epoch 42/50\n",
      "136/136 [==============================] - 0s 94us/step - loss: 0.6290 - acc: 0.6912\n",
      "Epoch 43/50\n",
      "136/136 [==============================] - 0s 104us/step - loss: 0.6262 - acc: 0.6912\n",
      "Epoch 44/50\n",
      "136/136 [==============================] - 0s 91us/step - loss: 0.6233 - acc: 0.6985\n",
      "Epoch 45/50\n",
      "136/136 [==============================] - 0s 104us/step - loss: 0.6207 - acc: 0.6985\n",
      "Epoch 46/50\n",
      "136/136 [==============================] - 0s 100us/step - loss: 0.6179 - acc: 0.7059\n",
      "Epoch 47/50\n",
      "136/136 [==============================] - 0s 98us/step - loss: 0.6152 - acc: 0.7059\n",
      "Epoch 48/50\n",
      "136/136 [==============================] - 0s 81us/step - loss: 0.6127 - acc: 0.7059\n",
      "Epoch 49/50\n",
      "136/136 [==============================] - 0s 85us/step - loss: 0.6101 - acc: 0.7206\n",
      "Epoch 50/50\n",
      "136/136 [==============================] - 0s 99us/step - loss: 0.6078 - acc: 0.7206\n",
      "67/67 [==============================] - 1s 8ms/step\n",
      "136/136 [==============================] - 0s 104us/step\n",
      "Epoch 1/100\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 0.7383 - acc: 0.4444\n",
      "Epoch 2/100\n",
      "135/135 [==============================] - 0s 91us/step - loss: 0.7313 - acc: 0.4593\n",
      "Epoch 3/100\n",
      "135/135 [==============================] - 0s 111us/step - loss: 0.7250 - acc: 0.4519\n",
      "Epoch 4/100\n",
      "135/135 [==============================] - 0s 158us/step - loss: 0.7180 - acc: 0.4593\n",
      "Epoch 5/100\n",
      "135/135 [==============================] - 0s 106us/step - loss: 0.7124 - acc: 0.4593\n",
      "Epoch 6/100\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.7058 - acc: 0.4667\n",
      "Epoch 7/100\n",
      "135/135 [==============================] - 0s 115us/step - loss: 0.6994 - acc: 0.5111\n",
      "Epoch 8/100\n",
      "135/135 [==============================] - 0s 109us/step - loss: 0.6947 - acc: 0.5111\n",
      "Epoch 9/100\n",
      "135/135 [==============================] - 0s 96us/step - loss: 0.6892 - acc: 0.5481\n",
      "Epoch 10/100\n",
      "135/135 [==============================] - 0s 102us/step - loss: 0.6839 - acc: 0.5556\n",
      "Epoch 11/100\n",
      "135/135 [==============================] - 0s 102us/step - loss: 0.6793 - acc: 0.5481\n",
      "Epoch 12/100\n",
      "135/135 [==============================] - 0s 137us/step - loss: 0.6749 - acc: 0.5926\n",
      "Epoch 13/100\n",
      "135/135 [==============================] - 0s 128us/step - loss: 0.6708 - acc: 0.6074\n",
      "Epoch 14/100\n",
      "135/135 [==============================] - 0s 99us/step - loss: 0.6664 - acc: 0.6000\n",
      "Epoch 15/100\n",
      "135/135 [==============================] - 0s 102us/step - loss: 0.6626 - acc: 0.6148\n",
      "Epoch 16/100\n",
      "135/135 [==============================] - 0s 144us/step - loss: 0.6587 - acc: 0.6148\n",
      "Epoch 17/100\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.6547 - acc: 0.6222\n",
      "Epoch 18/100\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.6512 - acc: 0.6296\n",
      "Epoch 19/100\n",
      "135/135 [==============================] - 0s 196us/step - loss: 0.6474 - acc: 0.6370\n",
      "Epoch 20/100\n",
      "135/135 [==============================] - 0s 117us/step - loss: 0.6440 - acc: 0.6370\n",
      "Epoch 21/100\n",
      "135/135 [==============================] - 0s 123us/step - loss: 0.6405 - acc: 0.6519\n",
      "Epoch 22/100\n",
      "135/135 [==============================] - 0s 106us/step - loss: 0.6361 - acc: 0.6741\n",
      "Epoch 23/100\n",
      "135/135 [==============================] - 0s 102us/step - loss: 0.6325 - acc: 0.6741\n",
      "Epoch 24/100\n",
      "135/135 [==============================] - 0s 92us/step - loss: 0.6286 - acc: 0.6815\n",
      "Epoch 25/100\n",
      "135/135 [==============================] - 0s 87us/step - loss: 0.6246 - acc: 0.6815\n",
      "Epoch 26/100\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.6209 - acc: 0.6963\n",
      "Epoch 27/100\n",
      "135/135 [==============================] - 0s 158us/step - loss: 0.6170 - acc: 0.7111\n",
      "Epoch 28/100\n",
      "135/135 [==============================] - 0s 91us/step - loss: 0.6131 - acc: 0.7111\n",
      "Epoch 29/100\n",
      "135/135 [==============================] - 0s 98us/step - loss: 0.6095 - acc: 0.7185\n",
      "Epoch 30/100\n",
      "135/135 [==============================] - 0s 90us/step - loss: 0.6056 - acc: 0.7333\n",
      "Epoch 31/100\n",
      "135/135 [==============================] - 0s 95us/step - loss: 0.6014 - acc: 0.7407\n",
      "Epoch 32/100\n",
      "135/135 [==============================] - 0s 125us/step - loss: 0.5974 - acc: 0.7407\n",
      "Epoch 33/100\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.5936 - acc: 0.7407\n",
      "Epoch 34/100\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.5893 - acc: 0.7481\n",
      "Epoch 35/100\n",
      "135/135 [==============================] - 0s 160us/step - loss: 0.5855 - acc: 0.7556\n",
      "Epoch 36/100\n",
      "135/135 [==============================] - 0s 93us/step - loss: 0.5822 - acc: 0.7481\n",
      "Epoch 37/100\n",
      "135/135 [==============================] - 0s 82us/step - loss: 0.5779 - acc: 0.7481\n",
      "Epoch 38/100\n",
      "135/135 [==============================] - 0s 109us/step - loss: 0.5741 - acc: 0.7481\n",
      "Epoch 39/100\n",
      "135/135 [==============================] - 0s 68us/step - loss: 0.5705 - acc: 0.7481\n",
      "Epoch 40/100\n",
      "135/135 [==============================] - 0s 92us/step - loss: 0.5668 - acc: 0.7407\n",
      "Epoch 41/100\n",
      "135/135 [==============================] - 0s 108us/step - loss: 0.5634 - acc: 0.7556\n",
      "Epoch 42/100\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.5600 - acc: 0.7556\n",
      "Epoch 43/100\n",
      "135/135 [==============================] - 0s 101us/step - loss: 0.5565 - acc: 0.7556\n",
      "Epoch 44/100\n",
      "135/135 [==============================] - 0s 115us/step - loss: 0.5528 - acc: 0.7481\n",
      "Epoch 45/100\n",
      "135/135 [==============================] - 0s 99us/step - loss: 0.5495 - acc: 0.7556\n",
      "Epoch 46/100\n",
      "135/135 [==============================] - 0s 109us/step - loss: 0.5459 - acc: 0.7556\n",
      "Epoch 47/100\n",
      "135/135 [==============================] - 0s 93us/step - loss: 0.5423 - acc: 0.7630\n",
      "Epoch 48/100\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.5391 - acc: 0.7630\n",
      "Epoch 49/100\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.5356 - acc: 0.7630\n",
      "Epoch 50/100\n",
      "135/135 [==============================] - 0s 96us/step - loss: 0.5322 - acc: 0.7630\n",
      "Epoch 51/100\n",
      "135/135 [==============================] - 0s 151us/step - loss: 0.5288 - acc: 0.7704\n",
      "Epoch 52/100\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.5256 - acc: 0.7704\n",
      "Epoch 53/100\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.5225 - acc: 0.7778\n",
      "Epoch 54/100\n",
      "135/135 [==============================] - 0s 95us/step - loss: 0.5193 - acc: 0.7778\n",
      "Epoch 55/100\n",
      "135/135 [==============================] - 0s 124us/step - loss: 0.5161 - acc: 0.7778\n",
      "Epoch 56/100\n",
      "135/135 [==============================] - 0s 89us/step - loss: 0.5129 - acc: 0.7852\n",
      "Epoch 57/100\n",
      "135/135 [==============================] - 0s 107us/step - loss: 0.5097 - acc: 0.7778\n",
      "Epoch 58/100\n",
      "135/135 [==============================] - 0s 97us/step - loss: 0.5065 - acc: 0.7852\n",
      "Epoch 59/100\n",
      "135/135 [==============================] - 0s 96us/step - loss: 0.5030 - acc: 0.7852\n",
      "Epoch 60/100\n",
      "135/135 [==============================] - 0s 115us/step - loss: 0.5004 - acc: 0.7852\n",
      "Epoch 61/100\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.4968 - acc: 0.7852\n",
      "Epoch 62/100\n",
      "135/135 [==============================] - 0s 101us/step - loss: 0.4938 - acc: 0.7852\n",
      "Epoch 63/100\n",
      "135/135 [==============================] - 0s 107us/step - loss: 0.4911 - acc: 0.7852\n",
      "Epoch 64/100\n",
      "135/135 [==============================] - 0s 105us/step - loss: 0.4879 - acc: 0.7852\n",
      "Epoch 65/100\n",
      "135/135 [==============================] - 0s 116us/step - loss: 0.4853 - acc: 0.7852\n",
      "Epoch 66/100\n",
      "135/135 [==============================] - 0s 143us/step - loss: 0.4824 - acc: 0.7852\n",
      "Epoch 67/100\n",
      "135/135 [==============================] - 0s 107us/step - loss: 0.4797 - acc: 0.7852\n",
      "Epoch 68/100\n",
      "135/135 [==============================] - 0s 105us/step - loss: 0.4772 - acc: 0.7852\n",
      "Epoch 69/100\n",
      "135/135 [==============================] - 0s 101us/step - loss: 0.4746 - acc: 0.7926\n",
      "Epoch 70/100\n",
      "135/135 [==============================] - 0s 120us/step - loss: 0.4719 - acc: 0.7926\n",
      "Epoch 71/100\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.4695 - acc: 0.7926\n",
      "Epoch 72/100\n",
      "135/135 [==============================] - 0s 108us/step - loss: 0.4669 - acc: 0.7926\n",
      "Epoch 73/100\n",
      "135/135 [==============================] - 0s 118us/step - loss: 0.4644 - acc: 0.7926\n",
      "Epoch 74/100\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.4619 - acc: 0.8000\n",
      "Epoch 75/100\n",
      "135/135 [==============================] - 0s 105us/step - loss: 0.4598 - acc: 0.8000\n",
      "Epoch 76/100\n",
      "135/135 [==============================] - 0s 164us/step - loss: 0.4574 - acc: 0.8000\n",
      "Epoch 77/100\n",
      "135/135 [==============================] - 0s 108us/step - loss: 0.4551 - acc: 0.8000\n",
      "Epoch 78/100\n",
      "135/135 [==============================] - 0s 115us/step - loss: 0.4533 - acc: 0.8000\n",
      "Epoch 79/100\n",
      "135/135 [==============================] - 0s 110us/step - loss: 0.4508 - acc: 0.8000\n",
      "Epoch 80/100\n",
      "135/135 [==============================] - 0s 197us/step - loss: 0.4492 - acc: 0.7926\n",
      "Epoch 81/100\n",
      "135/135 [==============================] - 0s 266us/step - loss: 0.4472 - acc: 0.8000\n",
      "Epoch 82/100\n",
      "135/135 [==============================] - 0s 163us/step - loss: 0.4449 - acc: 0.8000\n",
      "Epoch 83/100\n",
      "135/135 [==============================] - 0s 90us/step - loss: 0.4430 - acc: 0.8000\n",
      "Epoch 84/100\n",
      "135/135 [==============================] - 0s 91us/step - loss: 0.4411 - acc: 0.8000\n",
      "Epoch 85/100\n",
      "135/135 [==============================] - 0s 102us/step - loss: 0.4390 - acc: 0.8000\n",
      "Epoch 86/100\n",
      "135/135 [==============================] - 0s 102us/step - loss: 0.4371 - acc: 0.8000\n",
      "Epoch 87/100\n",
      "135/135 [==============================] - 0s 87us/step - loss: 0.4355 - acc: 0.8000\n",
      "Epoch 88/100\n",
      "135/135 [==============================] - 0s 89us/step - loss: 0.4337 - acc: 0.8000\n",
      "Epoch 89/100\n",
      "135/135 [==============================] - 0s 107us/step - loss: 0.4318 - acc: 0.8074\n",
      "Epoch 90/100\n",
      "135/135 [==============================] - 0s 88us/step - loss: 0.4299 - acc: 0.8074\n",
      "Epoch 91/100\n",
      "135/135 [==============================] - 0s 84us/step - loss: 0.4287 - acc: 0.8074\n",
      "Epoch 92/100\n",
      "135/135 [==============================] - 0s 95us/step - loss: 0.4266 - acc: 0.8000\n",
      "Epoch 93/100\n",
      "135/135 [==============================] - 0s 128us/step - loss: 0.4249 - acc: 0.8074\n",
      "Epoch 94/100\n",
      "135/135 [==============================] - 0s 109us/step - loss: 0.4232 - acc: 0.8074\n",
      "Epoch 95/100\n",
      "135/135 [==============================] - 0s 92us/step - loss: 0.4216 - acc: 0.8074\n",
      "Epoch 96/100\n",
      "135/135 [==============================] - 0s 109us/step - loss: 0.4200 - acc: 0.8148\n",
      "Epoch 97/100\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.4184 - acc: 0.8148\n",
      "Epoch 98/100\n",
      "135/135 [==============================] - 0s 97us/step - loss: 0.4171 - acc: 0.8148\n",
      "Epoch 99/100\n",
      "135/135 [==============================] - 0s 97us/step - loss: 0.4160 - acc: 0.8148\n",
      "Epoch 100/100\n",
      "135/135 [==============================] - 0s 86us/step - loss: 0.4141 - acc: 0.8148\n",
      "68/68 [==============================] - 1s 7ms/step\n",
      "135/135 [==============================] - 0s 82us/step\n",
      "Epoch 1/100\n",
      "135/135 [==============================] - 2s 15ms/step - loss: 0.7274 - acc: 0.4222\n",
      "Epoch 2/100\n",
      "135/135 [==============================] - 0s 234us/step - loss: 0.7171 - acc: 0.4370\n",
      "Epoch 3/100\n",
      "135/135 [==============================] - 0s 204us/step - loss: 0.7090 - acc: 0.4519\n",
      "Epoch 4/100\n",
      "135/135 [==============================] - 0s 164us/step - loss: 0.6993 - acc: 0.4667\n",
      "Epoch 5/100\n",
      "135/135 [==============================] - 0s 142us/step - loss: 0.6905 - acc: 0.5037\n",
      "Epoch 6/100\n",
      "135/135 [==============================] - 0s 96us/step - loss: 0.6822 - acc: 0.5481\n",
      "Epoch 7/100\n",
      "135/135 [==============================] - 0s 153us/step - loss: 0.6750 - acc: 0.5704\n",
      "Epoch 8/100\n",
      "135/135 [==============================] - 0s 124us/step - loss: 0.6669 - acc: 0.5852\n",
      "Epoch 9/100\n",
      "135/135 [==============================] - 0s 88us/step - loss: 0.6598 - acc: 0.6074\n",
      "Epoch 10/100\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.6530 - acc: 0.6296\n",
      "Epoch 11/100\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.6456 - acc: 0.6296\n",
      "Epoch 12/100\n",
      "135/135 [==============================] - 0s 95us/step - loss: 0.6391 - acc: 0.6444\n",
      "Epoch 13/100\n",
      "135/135 [==============================] - 0s 105us/step - loss: 0.6325 - acc: 0.6444\n",
      "Epoch 14/100\n",
      "135/135 [==============================] - 0s 113us/step - loss: 0.6266 - acc: 0.6444\n",
      "Epoch 15/100\n",
      "135/135 [==============================] - 0s 122us/step - loss: 0.6209 - acc: 0.6667\n",
      "Epoch 16/100\n",
      "135/135 [==============================] - 0s 425us/step - loss: 0.6150 - acc: 0.6741\n",
      "Epoch 17/100\n",
      "135/135 [==============================] - 0s 140us/step - loss: 0.6100 - acc: 0.6741\n",
      "Epoch 18/100\n",
      "135/135 [==============================] - 0s 133us/step - loss: 0.6054 - acc: 0.6815\n",
      "Epoch 19/100\n",
      "135/135 [==============================] - 0s 120us/step - loss: 0.6002 - acc: 0.7037\n",
      "Epoch 20/100\n",
      "135/135 [==============================] - 0s 114us/step - loss: 0.5958 - acc: 0.7037\n",
      "Epoch 21/100\n",
      "135/135 [==============================] - 0s 258us/step - loss: 0.5914 - acc: 0.7185\n",
      "Epoch 22/100\n",
      "135/135 [==============================] - 0s 92us/step - loss: 0.5870 - acc: 0.7185\n",
      "Epoch 23/100\n",
      "135/135 [==============================] - 0s 152us/step - loss: 0.5830 - acc: 0.7185\n",
      "Epoch 24/100\n",
      "135/135 [==============================] - 0s 140us/step - loss: 0.5792 - acc: 0.7185\n",
      "Epoch 25/100\n",
      "135/135 [==============================] - 0s 78us/step - loss: 0.5753 - acc: 0.7111\n",
      "Epoch 26/100\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.5713 - acc: 0.7259\n",
      "Epoch 27/100\n",
      "135/135 [==============================] - 0s 340us/step - loss: 0.5672 - acc: 0.7333\n",
      "Epoch 28/100\n",
      "135/135 [==============================] - 0s 107us/step - loss: 0.5632 - acc: 0.7333\n",
      "Epoch 29/100\n",
      "135/135 [==============================] - 0s 160us/step - loss: 0.5591 - acc: 0.7333\n",
      "Epoch 30/100\n",
      "135/135 [==============================] - 0s 166us/step - loss: 0.5549 - acc: 0.7407\n",
      "Epoch 31/100\n",
      "135/135 [==============================] - 0s 102us/step - loss: 0.5505 - acc: 0.7556\n",
      "Epoch 32/100\n",
      "135/135 [==============================] - 0s 128us/step - loss: 0.5462 - acc: 0.7556\n",
      "Epoch 33/100\n",
      "135/135 [==============================] - 0s 109us/step - loss: 0.5419 - acc: 0.7630\n",
      "Epoch 34/100\n",
      "135/135 [==============================] - 0s 123us/step - loss: 0.5378 - acc: 0.7704\n",
      "Epoch 35/100\n",
      "135/135 [==============================] - 0s 117us/step - loss: 0.5335 - acc: 0.7704\n",
      "Epoch 36/100\n",
      "135/135 [==============================] - 0s 87us/step - loss: 0.5291 - acc: 0.7704\n",
      "Epoch 37/100\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.5247 - acc: 0.7704\n",
      "Epoch 38/100\n",
      "135/135 [==============================] - 0s 322us/step - loss: 0.5204 - acc: 0.7778\n",
      "Epoch 39/100\n",
      "135/135 [==============================] - 0s 846us/step - loss: 0.5161 - acc: 0.7778\n",
      "Epoch 40/100\n",
      "135/135 [==============================] - 0s 126us/step - loss: 0.5119 - acc: 0.7778\n",
      "Epoch 41/100\n",
      "135/135 [==============================] - 0s 185us/step - loss: 0.5074 - acc: 0.7778\n",
      "Epoch 42/100\n",
      "135/135 [==============================] - 0s 81us/step - loss: 0.5030 - acc: 0.7778\n",
      "Epoch 43/100\n",
      "135/135 [==============================] - 0s 113us/step - loss: 0.4988 - acc: 0.7852\n",
      "Epoch 44/100\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.4942 - acc: 0.8000\n",
      "Epoch 45/100\n",
      "135/135 [==============================] - 0s 91us/step - loss: 0.4901 - acc: 0.8000\n",
      "Epoch 46/100\n",
      "135/135 [==============================] - 0s 681us/step - loss: 0.4859 - acc: 0.8000\n",
      "Epoch 47/100\n",
      "135/135 [==============================] - 0s 107us/step - loss: 0.4820 - acc: 0.8222\n",
      "Epoch 48/100\n",
      "135/135 [==============================] - 0s 106us/step - loss: 0.4773 - acc: 0.8222\n",
      "Epoch 49/100\n",
      "135/135 [==============================] - 0s 120us/step - loss: 0.4732 - acc: 0.8148\n",
      "Epoch 50/100\n",
      "135/135 [==============================] - 0s 111us/step - loss: 0.4693 - acc: 0.8148\n",
      "Epoch 51/100\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.4656 - acc: 0.8148\n",
      "Epoch 52/100\n",
      "135/135 [==============================] - 0s 116us/step - loss: 0.4616 - acc: 0.8148\n",
      "Epoch 53/100\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.4576 - acc: 0.8222\n",
      "Epoch 54/100\n",
      "135/135 [==============================] - 0s 96us/step - loss: 0.4543 - acc: 0.8222\n",
      "Epoch 55/100\n",
      "135/135 [==============================] - 0s 152us/step - loss: 0.4508 - acc: 0.8222\n",
      "Epoch 56/100\n",
      "135/135 [==============================] - 0s 698us/step - loss: 0.4473 - acc: 0.8296\n",
      "Epoch 57/100\n",
      "135/135 [==============================] - 0s 158us/step - loss: 0.4441 - acc: 0.8296\n",
      "Epoch 58/100\n",
      "135/135 [==============================] - 0s 89us/step - loss: 0.4409 - acc: 0.8370\n",
      "Epoch 59/100\n",
      "135/135 [==============================] - 0s 109us/step - loss: 0.4378 - acc: 0.8296\n",
      "Epoch 60/100\n",
      "135/135 [==============================] - 0s 99us/step - loss: 0.4348 - acc: 0.8296\n",
      "Epoch 61/100\n",
      "135/135 [==============================] - 0s 86us/step - loss: 0.4318 - acc: 0.8296\n",
      "Epoch 62/100\n",
      "135/135 [==============================] - 0s 140us/step - loss: 0.4288 - acc: 0.8296\n",
      "Epoch 63/100\n",
      "135/135 [==============================] - 0s 130us/step - loss: 0.4258 - acc: 0.8296\n",
      "Epoch 64/100\n",
      "135/135 [==============================] - 0s 103us/step - loss: 0.4231 - acc: 0.8296\n",
      "Epoch 65/100\n",
      "135/135 [==============================] - 0s 134us/step - loss: 0.4201 - acc: 0.8296\n",
      "Epoch 66/100\n",
      "135/135 [==============================] - 0s 82us/step - loss: 0.4173 - acc: 0.8370\n",
      "Epoch 67/100\n",
      "135/135 [==============================] - 0s 78us/step - loss: 0.4148 - acc: 0.8370\n",
      "Epoch 68/100\n",
      "135/135 [==============================] - 0s 128us/step - loss: 0.4121 - acc: 0.8370\n",
      "Epoch 69/100\n",
      "135/135 [==============================] - 0s 97us/step - loss: 0.4094 - acc: 0.8370\n",
      "Epoch 70/100\n",
      "135/135 [==============================] - 0s 82us/step - loss: 0.4070 - acc: 0.8370\n",
      "Epoch 71/100\n",
      "135/135 [==============================] - 0s 108us/step - loss: 0.4050 - acc: 0.8296\n",
      "Epoch 72/100\n",
      "135/135 [==============================] - 0s 90us/step - loss: 0.4022 - acc: 0.8296\n",
      "Epoch 73/100\n",
      "135/135 [==============================] - 0s 127us/step - loss: 0.3999 - acc: 0.8370\n",
      "Epoch 74/100\n",
      "135/135 [==============================] - 0s 104us/step - loss: 0.3981 - acc: 0.8444\n",
      "Epoch 75/100\n",
      "135/135 [==============================] - 0s 108us/step - loss: 0.3958 - acc: 0.8444\n",
      "Epoch 76/100\n",
      "135/135 [==============================] - 0s 102us/step - loss: 0.3936 - acc: 0.8444\n",
      "Epoch 77/100\n",
      "135/135 [==============================] - 0s 115us/step - loss: 0.3918 - acc: 0.8519\n",
      "Epoch 78/100\n",
      "135/135 [==============================] - 0s 105us/step - loss: 0.3899 - acc: 0.8593\n",
      "Epoch 79/100\n",
      "135/135 [==============================] - 0s 116us/step - loss: 0.3879 - acc: 0.8593\n",
      "Epoch 80/100\n",
      "135/135 [==============================] - 0s 102us/step - loss: 0.3864 - acc: 0.8593\n",
      "Epoch 81/100\n",
      "135/135 [==============================] - 0s 354us/step - loss: 0.3840 - acc: 0.8593\n",
      "Epoch 82/100\n",
      "135/135 [==============================] - 0s 109us/step - loss: 0.3823 - acc: 0.8593\n",
      "Epoch 83/100\n",
      "135/135 [==============================] - 0s 101us/step - loss: 0.3803 - acc: 0.8593\n",
      "Epoch 84/100\n",
      "135/135 [==============================] - 0s 83us/step - loss: 0.3788 - acc: 0.8444\n",
      "Epoch 85/100\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.3768 - acc: 0.8444\n",
      "Epoch 86/100\n",
      "135/135 [==============================] - 0s 84us/step - loss: 0.3752 - acc: 0.8444\n",
      "Epoch 87/100\n",
      "135/135 [==============================] - 0s 93us/step - loss: 0.3735 - acc: 0.8444\n",
      "Epoch 88/100\n",
      "135/135 [==============================] - 0s 122us/step - loss: 0.3721 - acc: 0.8444\n",
      "Epoch 89/100\n",
      "135/135 [==============================] - 0s 101us/step - loss: 0.3705 - acc: 0.8444\n",
      "Epoch 90/100\n",
      "135/135 [==============================] - 0s 104us/step - loss: 0.3689 - acc: 0.8444\n",
      "Epoch 91/100\n",
      "135/135 [==============================] - 0s 90us/step - loss: 0.3674 - acc: 0.8444\n",
      "Epoch 92/100\n",
      "135/135 [==============================] - 0s 102us/step - loss: 0.3660 - acc: 0.8444\n",
      "Epoch 93/100\n",
      "135/135 [==============================] - 0s 94us/step - loss: 0.3647 - acc: 0.8444\n",
      "Epoch 94/100\n",
      "135/135 [==============================] - 0s 86us/step - loss: 0.3630 - acc: 0.8444\n",
      "Epoch 95/100\n",
      "135/135 [==============================] - 0s 87us/step - loss: 0.3619 - acc: 0.8444\n",
      "Epoch 96/100\n",
      "135/135 [==============================] - 0s 100us/step - loss: 0.3604 - acc: 0.8519\n",
      "Epoch 97/100\n",
      "135/135 [==============================] - 0s 104us/step - loss: 0.3594 - acc: 0.8519\n",
      "Epoch 98/100\n",
      "135/135 [==============================] - 0s 138us/step - loss: 0.3579 - acc: 0.8593\n",
      "Epoch 99/100\n",
      "135/135 [==============================] - 0s 101us/step - loss: 0.3567 - acc: 0.8593\n",
      "Epoch 100/100\n",
      "135/135 [==============================] - 0s 105us/step - loss: 0.3553 - acc: 0.8593\n",
      "68/68 [==============================] - 1s 8ms/step\n",
      "135/135 [==============================] - 0s 76us/step\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6776 - acc: 0.5588\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 108us/step - loss: 0.6732 - acc: 0.5662\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 109us/step - loss: 0.6692 - acc: 0.5735\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 112us/step - loss: 0.6652 - acc: 0.5809\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 131us/step - loss: 0.6610 - acc: 0.5735\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 103us/step - loss: 0.6568 - acc: 0.5735\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 98us/step - loss: 0.6524 - acc: 0.5809\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 92us/step - loss: 0.6481 - acc: 0.6029\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 169us/step - loss: 0.6436 - acc: 0.5956\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 100us/step - loss: 0.6394 - acc: 0.6250\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 89us/step - loss: 0.6351 - acc: 0.6397\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 98us/step - loss: 0.6302 - acc: 0.6838\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 97us/step - loss: 0.6257 - acc: 0.7059\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 97us/step - loss: 0.6207 - acc: 0.7279\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 102us/step - loss: 0.6158 - acc: 0.7426\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 141us/step - loss: 0.6111 - acc: 0.7500\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 101us/step - loss: 0.6061 - acc: 0.7574\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 85us/step - loss: 0.6016 - acc: 0.7574\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 101us/step - loss: 0.5970 - acc: 0.7500\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 98us/step - loss: 0.5922 - acc: 0.7721\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 99us/step - loss: 0.5878 - acc: 0.7794\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 94us/step - loss: 0.5834 - acc: 0.7868\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 92us/step - loss: 0.5788 - acc: 0.7941\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 119us/step - loss: 0.5745 - acc: 0.7941\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 121us/step - loss: 0.5703 - acc: 0.8015\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 122us/step - loss: 0.5660 - acc: 0.8088\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 99us/step - loss: 0.5624 - acc: 0.8088\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 96us/step - loss: 0.5585 - acc: 0.8088\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 94us/step - loss: 0.5545 - acc: 0.8088\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 95us/step - loss: 0.5506 - acc: 0.8162\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 90us/step - loss: 0.5468 - acc: 0.8162\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 85us/step - loss: 0.5432 - acc: 0.8162\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 96us/step - loss: 0.5392 - acc: 0.8162\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 126us/step - loss: 0.5354 - acc: 0.8162\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 114us/step - loss: 0.5322 - acc: 0.8162\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 0s 117us/step - loss: 0.5283 - acc: 0.8162\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 95us/step - loss: 0.5248 - acc: 0.8162\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 102us/step - loss: 0.5210 - acc: 0.8162\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 0s 101us/step - loss: 0.5172 - acc: 0.8162\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 0s 95us/step - loss: 0.5133 - acc: 0.8235\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 94us/step - loss: 0.5097 - acc: 0.8235\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 71us/step - loss: 0.5058 - acc: 0.8235\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 94us/step - loss: 0.5019 - acc: 0.8235\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 96us/step - loss: 0.4986 - acc: 0.8235\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 105us/step - loss: 0.4947 - acc: 0.8235\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 97us/step - loss: 0.4913 - acc: 0.8382\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 110us/step - loss: 0.4877 - acc: 0.8382\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 93us/step - loss: 0.4845 - acc: 0.8529\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 108us/step - loss: 0.4812 - acc: 0.8676\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 91us/step - loss: 0.4780 - acc: 0.8676\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 92us/step - loss: 0.4747 - acc: 0.8603\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 105us/step - loss: 0.4716 - acc: 0.8529\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 97us/step - loss: 0.4684 - acc: 0.8529\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 100us/step - loss: 0.4654 - acc: 0.8529\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 94us/step - loss: 0.4626 - acc: 0.8529\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 0s 95us/step - loss: 0.4600 - acc: 0.8529\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 0s 100us/step - loss: 0.4572 - acc: 0.8529\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 0s 85us/step - loss: 0.4547 - acc: 0.8529\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 0s 106us/step - loss: 0.4519 - acc: 0.8603\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - 0s 122us/step - loss: 0.4494 - acc: 0.8676\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 0s 94us/step - loss: 0.4469 - acc: 0.8676\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 0s 112us/step - loss: 0.4446 - acc: 0.8676\n",
      "Epoch 63/100\n",
      "136/136 [==============================] - 0s 99us/step - loss: 0.4420 - acc: 0.8676\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 0s 105us/step - loss: 0.4398 - acc: 0.8676\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 0s 96us/step - loss: 0.4374 - acc: 0.8676\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 0s 111us/step - loss: 0.4351 - acc: 0.8676\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 0s 87us/step - loss: 0.4330 - acc: 0.8676\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 0s 91us/step - loss: 0.4310 - acc: 0.8750\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 0s 104us/step - loss: 0.4293 - acc: 0.8750\n",
      "Epoch 70/100\n",
      "136/136 [==============================] - 0s 90us/step - loss: 0.4275 - acc: 0.8750\n",
      "Epoch 71/100\n",
      "136/136 [==============================] - 0s 100us/step - loss: 0.4253 - acc: 0.8750\n",
      "Epoch 72/100\n",
      "136/136 [==============================] - 0s 81us/step - loss: 0.4234 - acc: 0.8750\n",
      "Epoch 73/100\n",
      "136/136 [==============================] - 0s 90us/step - loss: 0.4211 - acc: 0.8750\n",
      "Epoch 74/100\n",
      "136/136 [==============================] - 0s 89us/step - loss: 0.4189 - acc: 0.8750\n",
      "Epoch 75/100\n",
      "136/136 [==============================] - 0s 92us/step - loss: 0.4172 - acc: 0.8750\n",
      "Epoch 76/100\n",
      "136/136 [==============================] - 0s 99us/step - loss: 0.4151 - acc: 0.8750\n",
      "Epoch 77/100\n",
      "136/136 [==============================] - 0s 102us/step - loss: 0.4131 - acc: 0.8750\n",
      "Epoch 78/100\n",
      "136/136 [==============================] - 0s 91us/step - loss: 0.4114 - acc: 0.8750\n",
      "Epoch 79/100\n",
      "136/136 [==============================] - 0s 94us/step - loss: 0.4098 - acc: 0.8750\n",
      "Epoch 80/100\n",
      "136/136 [==============================] - 0s 93us/step - loss: 0.4078 - acc: 0.8750\n",
      "Epoch 81/100\n",
      "136/136 [==============================] - 0s 74us/step - loss: 0.4063 - acc: 0.8676\n",
      "Epoch 82/100\n",
      "136/136 [==============================] - 0s 83us/step - loss: 0.4047 - acc: 0.8676\n",
      "Epoch 83/100\n",
      "136/136 [==============================] - 0s 122us/step - loss: 0.4034 - acc: 0.8750\n",
      "Epoch 84/100\n",
      "136/136 [==============================] - 0s 102us/step - loss: 0.4016 - acc: 0.8750\n",
      "Epoch 85/100\n",
      "136/136 [==============================] - 0s 119us/step - loss: 0.4004 - acc: 0.8750\n",
      "Epoch 86/100\n",
      "136/136 [==============================] - 0s 86us/step - loss: 0.3984 - acc: 0.8750\n",
      "Epoch 87/100\n",
      "136/136 [==============================] - 0s 93us/step - loss: 0.3973 - acc: 0.8676\n",
      "Epoch 88/100\n",
      "136/136 [==============================] - 0s 83us/step - loss: 0.3966 - acc: 0.8676\n",
      "Epoch 89/100\n",
      "136/136 [==============================] - 0s 96us/step - loss: 0.3954 - acc: 0.8603\n",
      "Epoch 90/100\n",
      "136/136 [==============================] - 0s 85us/step - loss: 0.3941 - acc: 0.8676\n",
      "Epoch 91/100\n",
      "136/136 [==============================] - 0s 101us/step - loss: 0.3925 - acc: 0.8750\n",
      "Epoch 92/100\n",
      "136/136 [==============================] - 0s 83us/step - loss: 0.3917 - acc: 0.8750\n",
      "Epoch 93/100\n",
      "136/136 [==============================] - 0s 103us/step - loss: 0.3899 - acc: 0.8750\n",
      "Epoch 94/100\n",
      "136/136 [==============================] - 0s 105us/step - loss: 0.3888 - acc: 0.8750\n",
      "Epoch 95/100\n",
      "136/136 [==============================] - 0s 95us/step - loss: 0.3881 - acc: 0.8750\n",
      "Epoch 96/100\n",
      "136/136 [==============================] - 0s 89us/step - loss: 0.3867 - acc: 0.8750\n",
      "Epoch 97/100\n",
      "136/136 [==============================] - 0s 91us/step - loss: 0.3855 - acc: 0.8750\n",
      "Epoch 98/100\n",
      "136/136 [==============================] - 0s 84us/step - loss: 0.3844 - acc: 0.8750\n",
      "Epoch 99/100\n",
      "136/136 [==============================] - 0s 90us/step - loss: 0.3833 - acc: 0.8750\n",
      "Epoch 100/100\n",
      "136/136 [==============================] - 0s 116us/step - loss: 0.3823 - acc: 0.8750\n",
      "67/67 [==============================] - 1s 8ms/step\n",
      "136/136 [==============================] - 0s 60us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "203/203 [==============================] - 2s 8ms/step - loss: 0.7043 - acc: 0.5468\n",
      "Epoch 2/100\n",
      "203/203 [==============================] - 0s 131us/step - loss: 0.6888 - acc: 0.5567\n",
      "Epoch 3/100\n",
      "203/203 [==============================] - 0s 145us/step - loss: 0.6749 - acc: 0.6108\n",
      "Epoch 4/100\n",
      "203/203 [==============================] - 0s 141us/step - loss: 0.6632 - acc: 0.6453\n",
      "Epoch 5/100\n",
      "203/203 [==============================] - 0s 144us/step - loss: 0.6533 - acc: 0.7044\n",
      "Epoch 6/100\n",
      "203/203 [==============================] - 0s 128us/step - loss: 0.6445 - acc: 0.7094\n",
      "Epoch 7/100\n",
      "203/203 [==============================] - 0s 137us/step - loss: 0.6363 - acc: 0.7143\n",
      "Epoch 8/100\n",
      "203/203 [==============================] - 0s 143us/step - loss: 0.6292 - acc: 0.7291\n",
      "Epoch 9/100\n",
      "203/203 [==============================] - 0s 138us/step - loss: 0.6219 - acc: 0.7291\n",
      "Epoch 10/100\n",
      "203/203 [==============================] - 0s 142us/step - loss: 0.6145 - acc: 0.7340\n",
      "Epoch 11/100\n",
      "203/203 [==============================] - 0s 145us/step - loss: 0.6072 - acc: 0.7340\n",
      "Epoch 12/100\n",
      "203/203 [==============================] - 0s 137us/step - loss: 0.6002 - acc: 0.7438\n",
      "Epoch 13/100\n",
      "203/203 [==============================] - 0s 144us/step - loss: 0.5933 - acc: 0.7488\n",
      "Epoch 14/100\n",
      "203/203 [==============================] - 0s 129us/step - loss: 0.5857 - acc: 0.7537\n",
      "Epoch 15/100\n",
      "203/203 [==============================] - 0s 154us/step - loss: 0.5784 - acc: 0.7537\n",
      "Epoch 16/100\n",
      "203/203 [==============================] - 0s 138us/step - loss: 0.5724 - acc: 0.7488\n",
      "Epoch 17/100\n",
      "203/203 [==============================] - 0s 147us/step - loss: 0.5651 - acc: 0.7635\n",
      "Epoch 18/100\n",
      "203/203 [==============================] - 0s 143us/step - loss: 0.5595 - acc: 0.7488\n",
      "Epoch 19/100\n",
      "203/203 [==============================] - 0s 127us/step - loss: 0.5528 - acc: 0.7438\n",
      "Epoch 20/100\n",
      "203/203 [==============================] - 0s 141us/step - loss: 0.5464 - acc: 0.7586\n",
      "Epoch 21/100\n",
      "203/203 [==============================] - 0s 143us/step - loss: 0.5397 - acc: 0.7635\n",
      "Epoch 22/100\n",
      "203/203 [==============================] - 0s 128us/step - loss: 0.5341 - acc: 0.7586\n",
      "Epoch 23/100\n",
      "203/203 [==============================] - 0s 206us/step - loss: 0.5280 - acc: 0.7635\n",
      "Epoch 24/100\n",
      "203/203 [==============================] - 0s 145us/step - loss: 0.5219 - acc: 0.7635\n",
      "Epoch 25/100\n",
      "203/203 [==============================] - 0s 133us/step - loss: 0.5166 - acc: 0.7635\n",
      "Epoch 26/100\n",
      "203/203 [==============================] - 0s 136us/step - loss: 0.5098 - acc: 0.7783\n",
      "Epoch 27/100\n",
      "203/203 [==============================] - 0s 138us/step - loss: 0.5058 - acc: 0.7685\n",
      "Epoch 28/100\n",
      "203/203 [==============================] - 0s 145us/step - loss: 0.4996 - acc: 0.7685\n",
      "Epoch 29/100\n",
      "203/203 [==============================] - 0s 129us/step - loss: 0.4948 - acc: 0.7931\n",
      "Epoch 30/100\n",
      "203/203 [==============================] - 0s 139us/step - loss: 0.4893 - acc: 0.7882\n",
      "Epoch 31/100\n",
      "203/203 [==============================] - 0s 142us/step - loss: 0.4835 - acc: 0.7833\n",
      "Epoch 32/100\n",
      "203/203 [==============================] - 0s 152us/step - loss: 0.4785 - acc: 0.7783\n",
      "Epoch 33/100\n",
      "203/203 [==============================] - 0s 141us/step - loss: 0.4735 - acc: 0.7783\n",
      "Epoch 34/100\n",
      "203/203 [==============================] - 0s 132us/step - loss: 0.4691 - acc: 0.7783\n",
      "Epoch 35/100\n",
      "203/203 [==============================] - 0s 139us/step - loss: 0.4663 - acc: 0.7882\n",
      "Epoch 36/100\n",
      "203/203 [==============================] - 0s 143us/step - loss: 0.4608 - acc: 0.7882\n",
      "Epoch 37/100\n",
      "203/203 [==============================] - 0s 144us/step - loss: 0.4563 - acc: 0.7882\n",
      "Epoch 38/100\n",
      "203/203 [==============================] - 0s 130us/step - loss: 0.4524 - acc: 0.7833\n",
      "Epoch 39/100\n",
      "203/203 [==============================] - 0s 140us/step - loss: 0.4484 - acc: 0.7833\n",
      "Epoch 40/100\n",
      "203/203 [==============================] - 0s 141us/step - loss: 0.4444 - acc: 0.7833\n",
      "Epoch 41/100\n",
      "203/203 [==============================] - 0s 145us/step - loss: 0.4406 - acc: 0.7833\n",
      "Epoch 42/100\n",
      "203/203 [==============================] - 0s 130us/step - loss: 0.4371 - acc: 0.7882\n",
      "Epoch 43/100\n",
      "203/203 [==============================] - 0s 142us/step - loss: 0.4338 - acc: 0.7980\n",
      "Epoch 44/100\n",
      "203/203 [==============================] - 0s 142us/step - loss: 0.4304 - acc: 0.7882\n",
      "Epoch 45/100\n",
      "203/203 [==============================] - 0s 139us/step - loss: 0.4271 - acc: 0.7882\n",
      "Epoch 46/100\n",
      "203/203 [==============================] - 0s 132us/step - loss: 0.4236 - acc: 0.7931\n",
      "Epoch 47/100\n",
      "203/203 [==============================] - 0s 139us/step - loss: 0.4202 - acc: 0.8030\n",
      "Epoch 48/100\n",
      "203/203 [==============================] - 0s 136us/step - loss: 0.4168 - acc: 0.8030\n",
      "Epoch 49/100\n",
      "203/203 [==============================] - 0s 135us/step - loss: 0.4139 - acc: 0.8030\n",
      "Epoch 50/100\n",
      "203/203 [==============================] - 0s 135us/step - loss: 0.4111 - acc: 0.8177\n",
      "Epoch 51/100\n",
      "203/203 [==============================] - 0s 144us/step - loss: 0.4106 - acc: 0.8177\n",
      "Epoch 52/100\n",
      "203/203 [==============================] - 0s 146us/step - loss: 0.4059 - acc: 0.8227\n",
      "Epoch 53/100\n",
      "203/203 [==============================] - 0s 142us/step - loss: 0.4019 - acc: 0.8227\n",
      "Epoch 54/100\n",
      "203/203 [==============================] - 0s 137us/step - loss: 0.3980 - acc: 0.8177\n",
      "Epoch 55/100\n",
      "203/203 [==============================] - 0s 140us/step - loss: 0.3946 - acc: 0.8079\n",
      "Epoch 56/100\n",
      "203/203 [==============================] - 0s 145us/step - loss: 0.3923 - acc: 0.8177\n",
      "Epoch 57/100\n",
      "203/203 [==============================] - 0s 148us/step - loss: 0.3889 - acc: 0.8177\n",
      "Epoch 58/100\n",
      "203/203 [==============================] - 0s 135us/step - loss: 0.3861 - acc: 0.8227\n",
      "Epoch 59/100\n",
      "203/203 [==============================] - 0s 152us/step - loss: 0.3841 - acc: 0.8325\n",
      "Epoch 60/100\n",
      "203/203 [==============================] - 0s 145us/step - loss: 0.3816 - acc: 0.8325\n",
      "Epoch 61/100\n",
      "203/203 [==============================] - 0s 144us/step - loss: 0.3798 - acc: 0.8325\n",
      "Epoch 62/100\n",
      "203/203 [==============================] - 0s 141us/step - loss: 0.3768 - acc: 0.8325\n",
      "Epoch 63/100\n",
      "203/203 [==============================] - 0s 134us/step - loss: 0.3743 - acc: 0.8325\n",
      "Epoch 64/100\n",
      "203/203 [==============================] - 0s 133us/step - loss: 0.3728 - acc: 0.8424\n",
      "Epoch 65/100\n",
      "203/203 [==============================] - 0s 139us/step - loss: 0.3700 - acc: 0.8424\n",
      "Epoch 66/100\n",
      "203/203 [==============================] - 0s 139us/step - loss: 0.3683 - acc: 0.8473\n",
      "Epoch 67/100\n",
      "203/203 [==============================] - 0s 130us/step - loss: 0.3663 - acc: 0.8473\n",
      "Epoch 68/100\n",
      "203/203 [==============================] - 0s 140us/step - loss: 0.3652 - acc: 0.8473\n",
      "Epoch 69/100\n",
      "203/203 [==============================] - 0s 152us/step - loss: 0.3651 - acc: 0.8473\n",
      "Epoch 70/100\n",
      "203/203 [==============================] - 0s 153us/step - loss: 0.3632 - acc: 0.8424\n",
      "Epoch 71/100\n",
      "203/203 [==============================] - 0s 186us/step - loss: 0.3617 - acc: 0.8522\n",
      "Epoch 72/100\n",
      "203/203 [==============================] - 0s 162us/step - loss: 0.3597 - acc: 0.8473\n",
      "Epoch 73/100\n",
      "203/203 [==============================] - 0s 247us/step - loss: 0.3578 - acc: 0.8522\n",
      "Epoch 74/100\n",
      "203/203 [==============================] - 0s 170us/step - loss: 0.3567 - acc: 0.8522\n",
      "Epoch 75/100\n",
      "203/203 [==============================] - 0s 160us/step - loss: 0.3556 - acc: 0.8522\n",
      "Epoch 76/100\n",
      "203/203 [==============================] - 0s 152us/step - loss: 0.3539 - acc: 0.8473\n",
      "Epoch 77/100\n",
      "203/203 [==============================] - 0s 152us/step - loss: 0.3530 - acc: 0.8473\n",
      "Epoch 78/100\n",
      "203/203 [==============================] - 0s 137us/step - loss: 0.3527 - acc: 0.8522\n",
      "Epoch 79/100\n",
      "203/203 [==============================] - 0s 145us/step - loss: 0.3512 - acc: 0.8522\n",
      "Epoch 80/100\n",
      "203/203 [==============================] - 0s 148us/step - loss: 0.3497 - acc: 0.8522\n",
      "Epoch 81/100\n",
      "203/203 [==============================] - 0s 146us/step - loss: 0.3483 - acc: 0.8571\n",
      "Epoch 82/100\n",
      "203/203 [==============================] - 0s 141us/step - loss: 0.3470 - acc: 0.8522\n",
      "Epoch 83/100\n",
      "203/203 [==============================] - 0s 134us/step - loss: 0.3458 - acc: 0.8522\n",
      "Epoch 84/100\n",
      "203/203 [==============================] - 0s 156us/step - loss: 0.3453 - acc: 0.8571\n",
      "Epoch 85/100\n",
      "203/203 [==============================] - 0s 147us/step - loss: 0.3450 - acc: 0.8670\n",
      "Epoch 86/100\n",
      "203/203 [==============================] - 0s 153us/step - loss: 0.3436 - acc: 0.8571\n",
      "Epoch 87/100\n",
      "203/203 [==============================] - 0s 146us/step - loss: 0.3428 - acc: 0.8571\n",
      "Epoch 88/100\n",
      "203/203 [==============================] - 0s 137us/step - loss: 0.3419 - acc: 0.8571\n",
      "Epoch 89/100\n",
      "203/203 [==============================] - 0s 143us/step - loss: 0.3408 - acc: 0.8571\n",
      "Epoch 90/100\n",
      "203/203 [==============================] - 0s 159us/step - loss: 0.3401 - acc: 0.8719\n",
      "Epoch 91/100\n",
      "203/203 [==============================] - 0s 150us/step - loss: 0.3404 - acc: 0.8768\n",
      "Epoch 92/100\n",
      "203/203 [==============================] - 0s 147us/step - loss: 0.3396 - acc: 0.8768\n",
      "Epoch 93/100\n",
      "203/203 [==============================] - 0s 148us/step - loss: 0.3382 - acc: 0.8621\n",
      "Epoch 94/100\n",
      "203/203 [==============================] - 0s 137us/step - loss: 0.3375 - acc: 0.8621\n",
      "Epoch 95/100\n",
      "203/203 [==============================] - 0s 140us/step - loss: 0.3388 - acc: 0.8670\n",
      "Epoch 96/100\n",
      "203/203 [==============================] - 0s 146us/step - loss: 0.3369 - acc: 0.8621\n",
      "Epoch 97/100\n",
      "203/203 [==============================] - 0s 148us/step - loss: 0.3357 - acc: 0.8768\n",
      "Epoch 98/100\n",
      "203/203 [==============================] - 0s 169us/step - loss: 0.3357 - acc: 0.8670\n",
      "Epoch 99/100\n",
      "203/203 [==============================] - 0s 156us/step - loss: 0.3343 - acc: 0.8719\n",
      "Epoch 100/100\n",
      "203/203 [==============================] - 0s 167us/step - loss: 0.3336 - acc: 0.8719\n",
      "Best: 0.8128078783967813 using {'batch_size': 20, 'epochs': 100}\n",
      "Means: 0.729064043813151, Stdev: 0.07589201347686614 with: {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.7832512234525727, Stdev: 0.0564642535454495 with: {'batch_size': 20, 'epochs': 50}\n",
      "Means: 0.8128078783967813, Stdev: 0.036553984997164765 with: {'batch_size': 20, 'epochs': 100}\n",
      "Means: 0.7241379405770983, Stdev: 0.04970161048304944 with: {'batch_size': 30, 'epochs': 20}\n",
      "Means: 0.7733990087591368, Stdev: 0.038090410445518405 with: {'batch_size': 30, 'epochs': 50}\n",
      "Means: 0.8029556666395347, Stdev: 0.03706564001393931 with: {'batch_size': 30, 'epochs': 100}\n",
      "Means: 0.6009852244642567, Stdev: 0.12906055879897765 with: {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.7733990121357547, Stdev: 0.017270858438419336 with: {'batch_size': 40, 'epochs': 50}\n",
      "Means: 0.778325124914423, Stdev: 0.04099357264792951 with: {'batch_size': 40, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [20, 30, 40],\n",
    "              'epochs': [20, 50, 100]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
