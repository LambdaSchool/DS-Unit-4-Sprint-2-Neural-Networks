{"cells":[{"cell_type":"markdown","metadata":{},"source":["<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n","<br></br>\n","<br></br>\n","\n","## *Data Science Unit 4 Sprint 2*\n","\n","# Sprint Challenge - Neural Network Foundations\n","\n","Table of Problems\n","\n","1. [Defining Neural Networks](#Q1)\n","2. [Chocolate Gummy Bears](#Q2)\n","    - Perceptron\n","    - Multilayer Perceptron\n","4. [Keras MMP](#Q3)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"Q1\"></a>\n","## 1. Define the following terms:\n","\n","- **Neuron:** A neuron is an individual node of a layer\n","- **Input Layer:** The input layer is where certain information is passed to the neural network\n","- **Hidden Layer:** A hidden layer is where information from the previous layer is transformed in some way and passed to the next layer. It is neither the input or output layer\n","- **Output Layer:** The output layer is where the results of the neural network end up to be used.\n","- **Activation:** The mapping of input(s) to output(s) for a given node\n","- **Backpropagation:** How weights are updated in the neural network\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n","\n","Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n","\n","Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n","\n","If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n","![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"]},{"cell_type":"code","execution_count":248,"metadata":{},"outputs":[],"source":["import pandas as pd\n","candy = pd.read_csv('chocolate_gummy_bears.csv')"]},{"cell_type":"code","execution_count":251,"metadata":{},"outputs":[{"data":{"text/plain":"0    1\n1    1\n2    1\n3    0\n4    0\nName: ate, dtype: int64"},"execution_count":251,"metadata":{},"output_type":"execute_result"}],"source":["y.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Perceptron\n","\n","To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n","\n","Once you've trained your model, report your accuracy. Explain why you could not achieve a higher accuracy with a *simple perceptron*. It's possible to achieve ~95% accuracy on this dataset."]},{"cell_type":"code","execution_count":250,"metadata":{},"outputs":[],"source":["# Start your candy perceptron here\n","\n","X = candy[['chocolate', 'gummy']]\n","y = candy['ate']"]},{"cell_type":"code","execution_count":252,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"cannot reshape array of size 30000 into shape (3,)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-252-3842a2ccf7be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-252-3842a2ccf7be>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Update the Weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0madjustments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sigmoid_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjustments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 30000 into shape (3,)"]}],"source":["# import numpy as np\n","\n","# class Perceptron(object):\n","    \n","#     def __init__(self, niter = 10):\n","#         self.niter = niter\n","    \n","#     def __sigmoid(self, x):\n","#         return 1 / (1 + np.exp(-x))\n","    \n","#     def __sigmoid_derivative(self, x):\n","#         sx = self.__sigmoid(x)\n","#         return sx * (1-sx)\n","\n","#     def fit(self, X, y):\n","#         \"\"\"Fit training data\n","#         X : Training vectors, X.shape : [#samples, #features]\n","#         y : Target values, y.shape : [#samples]\n","#         \"\"\"\n","#         # Randomly Initialize Weights\n","#         X['ones'] = np.ones(X.shape[0])\n","#         weights = np.zeros(X.shape[1])\n","\n","#         for i in range(self.niter):\n","#             # Weighted sum of inputs / weights\n","#             weighted = np.dot(X, weights)\n","            \n","#             # Activate!\n","#             pred = self.__sigmoid(weighted)\n","            \n","#             # Calc error\n","#             errors = y.values - pred.reshape(-1,1)\n","\n","#             # Update the Weights\n","#             adjustments = errors*self.__sigmoid_derivative(pred).reshape(-1,1)\n","#             weights += np.dot(X.T, adjustments).reshape(3)\n","\n","#         self.weights = weights \n","\n","#     def predict(self, X):\n","#         print(self.weights)\n","#         print(self.weights.shape, X.shape)\n","#         pred = np.dot(X,self.weights)\n","#         return np.where(pred >= 0.5, 1, 0)\n","    \n","# p = Perceptron(niter = 100)\n","# p.fit(X,y)\n","# p.predict(X.iloc[0])"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["y_pred = p.predict(X)\n","from sklearn.metrics import accuracy_score\n","print(accuracy_score(y, y_pred))\n","y_pred.min()\n"]},{"cell_type":"code","execution_count":254,"metadata":{},"outputs":[],"source":["class Perceptron(object):\n","  def __init__(self, rate = 0.01, niter = 10):\n","    self.rate = rate\n","    self.niter = niter\n","\n","  def fit(self, X, y):\n","    \"\"\"Fit training data\n","    X : Training vectors, X.shape : [#samples, #features]\n","    y : Target values, y.shape : [#samples]\n","    \"\"\"\n","\n","    # weights\n","    self.weight = np.zeros(1 + X.shape[1])\n","\n","    # Number of misclassifications\n","    self.errors = []  # Number of misclassifications\n","\n","    for i in range(self.niter):\n","      err = 0\n","      for xi, target in zip(X, y):\n","        delta_w = self.rate * (target - self.predict(xi))\n","        self.weight[1:] += delta_w * xi\n","        self.weight[0] += delta_w\n","        err += int(delta_w != 0.0)\n","      self.errors.append(err)\n","    return self\n","\n","  def net_input(self, X):\n","    \"\"\"Calculate net input\"\"\"\n","    return np.dot(X, self.weight[1:]) + self.weight[0]\n","\n","  def predict(self, X):\n","    \"\"\"Return class label after unit step\"\"\"\n","    return np.where(self.net_input(X) >= 0.5, 1, 0)"]},{"cell_type":"code","execution_count":255,"metadata":{},"outputs":[],"source":["p = Perceptron(niter = 100)\n","p.fit(X.to_numpy(),y.to_numpy())\n","y_pred = p.predict(X)"]},{"cell_type":"code","execution_count":256,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"0.5\n"},{"data":{"text/plain":"1"},"execution_count":256,"metadata":{},"output_type":"execute_result"}],"source":["y_pred = p.predict(X)\n","from sklearn.metrics import accuracy_score\n","print(accuracy_score(y, y_pred))\n","y_pred.min()\n"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"0.5\n"}],"source":["# class Perceptron(object):\n","#   def __init__(self, rate = 0.01, niter = 10):\n","#     self.rate = rate\n","#     self.niter = niter\n","\n","#   def fit(self, X, y):\n","#     \"\"\"Fit training data\n","#     X : Training vectors, X.shape : [#samples, #features]\n","#     y : Target values, y.shape : [#samples]\n","#     \"\"\"\n","\n","#     # weights\n","#     self.weight = np.zeros(1 + X.shape[1])\n","\n","#     # Number of misclassifications\n","#     self.errors = []  # Number of misclassifications\n","\n","#     for i in range(self.niter):\n","#       err = 0\n","#       for xi, target in zip(X, y):\n","#         delta_w = self.rate * (target - self.predict(xi))\n","#         self.weight[1:] += delta_w * xi\n","#         self.weight[0] += delta_w\n","#         err += int(delta_w != 0.0)\n","#       self.errors.append(err)\n","#     return self\n","\n","#   def net_input(self, X):\n","#     \"\"\"Calculate net input\"\"\"\n","#     return np.dot(X, self.weight[1:]) + self.weight[0]\n","\n","#   def predict(self, X):\n","#     \"\"\"Return class label after unit step\"\"\"\n","#     return np.where(self.net_input(X) >= 0.5, 1, 0)"]},{"cell_type":"code","execution_count":136,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"0.5\n"}],"source":["nn = NeuralNetwork()\n","\n","for i in range(1000):\n","    if i%100==0:\n","        print('Epoch: ', i)\n","        print('Accuracy: ', accuracy_score(y, nn.predict(X.to_numpy())))\n","        print('Weights1: ', nn.weights1)\n","        print('Weights2: ', nn.weights2)\n","    nn.train(X.to_numpy(),y)\n","\n","pred = nn.predict(X.to_numpy())"]},{"cell_type":"code","execution_count":275,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chocolate</th>\n      <th>gummy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   chocolate  gummy\n0          0      1\n1          1      0\n2          0      1\n3          0      0\n4          1      1"},"execution_count":275,"metadata":{},"output_type":"execute_result"}],"source":["X = candy[['chocolate', 'gummy']]\n","y = pd.Series(candy['ate'])\n","X.head()\n"]},{"cell_type":"code","execution_count":269,"metadata":{},"outputs":[],"source":["class NeuralNetwork: \n","    def __init__(self):\n","        # Set upArchietecture \n","        self.inputs = 2\n","        self.hiddenNodes = 3\n","        self.outputNodes = 1\n","        \n","        #Initial weights\n","        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes) #3x2\n","        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes) #3x1\n","    \n","    def sigmoid(self, s):\n","        return 1 / (1+np.exp(-s))\n","    \n","    def sigmoidPrime(self, s):\n","        return s * (1 - s)\n","    \n","    def feed_forward(self, X):\n","        \"\"\"\n","        Calculate the NN inference using feed forward.\n","        \"\"\"\n","        \n","        #Weighted sume of inputs and hidden layer\n","        # print('X.shape; ', X.shape)\n","        # print('self.weights1.shape: ', self.weights1.shape)\n","        self.hidden_sum = np.dot(X, self.weights1)\n","        \n","        #Acivations of weighted sum\n","        # print('hidden_sum.shape: ', self.hidden_sum.shape)\n","        self.activated_hidden = self.sigmoid(self.hidden_sum)\n","        # print('self.activaged_hidden.shape: ', self.activated_hidden.shape)\n","\n","        # Weight sum between hidden and output\n","        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n","        \n","        #Final activation of output\n","        self.activated_output = [] \n","        for s in self.output_sum:\n","            self.activated_output.append(self.sigmoid(s[0]))\n","        \n","        return np.array(self.activated_output)\n","    \n","    def backward(self, X, y, o):\n","        \"\"\"\n","        Backward propagate through the network\n","        \"\"\"\n","        # print('y shape: ', y.shape)\n","        # print('o shape: ', o.shape)\n","        # print('o reshaped shape: ', o.reshape(-1,1).shape)\n","        self.o_error = y - o #error in output\n","        # print('o_error shape: ', self.o_error.shape)\n","        # print('sigmoidPrime(o).shape: ', self.sigmoidPrime(o).shape)\n","        self.o_delta = np.array(self.o_error * self.sigmoidPrime(o)) # apply derivative of sigmoid to error\n","        # print('o_delta shape: ', self.o_delta.shape)\n","        \n","        # print('o_delta.shape: ', self.o_delta.shape)\n","        # print('weights2.shape: ', self.weights2.shape)\n","        self.z2_error = self.o_delta.reshape(-1,1).dot(self.weights2.T) # z2 error: how much our hidden layer weights were off\n","        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)\n","        \n","        self.weights1 += X.T.dot(self.z2_delta) #Adjust first set (input => hidden) weights\n","        # print('activated_hidden.T.shape: ', self.activated_hidden.T.shape)\n","        # print('o_delta.shape: ', self.o_delta.shape)\n","        self.weights2 += self.activated_hidden.T.dot(self.o_delta.reshape(-1,1)) #adjust second set (hidden => output) weights\n","        \n","    def train(self, X, y):\n","        o = self.feed_forward(X)\n","        self.backward(X, y, o)\n","\n","    def predict(self, X):\n","        return np.where(self.feed_forward(X) >= 0.5, 1, 0)\n"]},{"cell_type":"code","execution_count":270,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Epoch:  0\nAccuracy:  0.5\nWeights1:  [[-1.27041929 -0.79843408  1.01878276]\n [-0.14289034  1.54816237 -1.37344151]]\nWeights2:  [[ 0.50291634]\n [-0.62044255]\n [-1.29927435]]\nEpoch:  100\nAccuracy:  0.5\nWeights1:  [[ 17.12126716 -27.52335176 -42.73693875]\n [ 31.83900694 -11.4762905  -31.89629627]]\nWeights2:  [[127.69418203]\n [216.49169073]\n [180.58560817]]\nEpoch:  200\nAccuracy:  0.5\nWeights1:  [[ 17.12126716 -27.52335176 -42.73693875]\n [ 31.83900694 -11.4762905  -31.89629627]]\nWeights2:  [[127.69418203]\n [216.49169073]\n [180.58560817]]\nEpoch:  300\nAccuracy:  0.5\nWeights1:  [[ 17.12126716 -27.52335176 -42.73693875]\n [ 31.83900694 -11.4762905  -31.89629627]]\nWeights2:  [[127.69418203]\n [216.49169073]\n [180.58560817]]\nEpoch:  400\nAccuracy:  0.5\nWeights1:  [[ 17.12126716 -27.52335176 -42.73693875]\n [ 31.83900694 -11.4762905  -31.89629627]]\nWeights2:  [[127.69418203]\n [216.49169073]\n [180.58560817]]\nEpoch:  500\nAccuracy:  0.5\nWeights1:  [[ 17.12126716 -27.52335176 -42.73693875]\n [ 31.83900694 -11.4762905  -31.89629627]]\nWeights2:  [[127.69418203]\n [216.49169073]\n [180.58560817]]\nEpoch:  600\nAccuracy:  0.5\nWeights1:  [[ 17.12126716 -27.52335176 -42.73693875]\n [ 31.83900694 -11.4762905  -31.89629627]]\nWeights2:  [[127.69418203]\n [216.49169073]\n [180.58560817]]\nEpoch:  700\nAccuracy:  0.5\nWeights1:  [[ 17.12126716 -27.52335176 -42.73693875]\n [ 31.83900694 -11.4762905  -31.89629627]]\nWeights2:  [[127.69418203]\n [216.49169073]\n [180.58560817]]\nEpoch:  800\nAccuracy:  0.5\nWeights1:  [[ 17.12126716 -27.52335176 -42.73693875]\n [ 31.83900694 -11.4762905  -31.89629627]]\nWeights2:  [[127.69418203]\n [216.49169073]\n [180.58560817]]\nEpoch:  900\nAccuracy:  0.5\nWeights1:  [[ 17.12126716 -27.52335176 -42.73693875]\n [ 31.83900694 -11.4762905  -31.89629627]]\nWeights2:  [[127.69418203]\n [216.49169073]\n [180.58560817]]\n"}],"source":["nn = NeuralNetwork()\n","\n","for i in range(1000):\n","    if i%100==0:\n","        print('Epoch: ', i)\n","        print('Accuracy: ', accuracy_score(y, nn.predict(X.to_numpy())))\n","        print('Weights1: ', nn.weights1)\n","        print('Weights2: ', nn.weights2)\n","    nn.train(X.to_numpy(),y)\n","\n","pred = nn.predict(X.to_numpy())"]},{"cell_type":"code","execution_count":271,"metadata":{},"outputs":[],"source":["# pd.DataFrame(pred).describe()\n","\n","class Neural_Network(object):\n","    def __init__(self):        \n","        #Define Hyperparameters\n","        self.inputLayerSize = 2\n","        self.outputLayerSize = 1\n","        self.hiddenLayerSize = 3\n","        \n","        #Weights (parameters)\n","        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n","        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n","        \n","    def forward(self, X):\n","        #Propogate inputs though network\n","        self.z2 = np.dot(X, self.W1)\n","        self.a2 = self.sigmoid(self.z2)\n","        self.z3 = np.dot(self.a2, self.W2)\n","        yHat = self.sigmoid(self.z3) \n","        return yHat\n","        \n","    def sigmoid(self, z):\n","        #Apply sigmoid activation function to scalar, vector, or matrix\n","        return 1/(1+np.exp(-z))\n","    \n","    def sigmoidPrime(self,z):\n","        #Gradient of sigmoid\n","        return np.exp(-z)/((1+np.exp(-z))**2)\n","    \n","    def costFunction(self, X, y):\n","        #Compute cost for given X,y, use weights already stored in class.\n","        self.yHat = self.forward(X)\n","        J = 0.5*sum((y-self.yHat)**2)\n","        return J\n","        \n","    def costFunctionPrime(self, X, y):\n","        #Compute derivative with respect to W and W2 for a given X and y:\n","        self.yHat = self.forward(X)\n","        \n","        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n","        dJdW2 = np.dot(self.a2.T, delta3)\n","        \n","        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n","        dJdW1 = np.dot(X.T, delta2)  \n","        \n","        return dJdW1, dJdW2\n","    \n","    #Helper Functions for interacting with other classes:\n","    def getParams(self):\n","        #Get W1 and W2 unrolled into vector:\n","        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n","        return params\n","    \n","    def setParams(self, params):\n","        #Set W1 and W2 using single paramater vector.\n","        W1_start = 0\n","        W1_end = self.hiddenLayerSize * self.inputLayerSize\n","        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n","        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n","        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n","        \n","    def computeGradients(self, X, y):\n","        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n","        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"]},{"cell_type":"code","execution_count":276,"metadata":{},"outputs":[],"source":["# ff_pred = nn.feed_forward(X.to_numpy())\n","from scipy import optimize\n","class trainer(object):\n","    def __init__(self, N):\n","        #Make Local reference to network:\n","        self.N = N\n","        \n","    def callbackF(self, params):\n","        self.N.setParams(params)\n","        self.J.append(self.N.costFunction(self.X, self.y))   \n","        \n","    def costFunctionWrapper(self, params, X, y):\n","        self.N.setParams(params)\n","        cost = self.N.costFunction(X, y)\n","        grad = self.N.computeGradients(X,y)\n","        \n","        return cost, grad\n","        \n","    def train(self, X, y):\n","        #Make an internal variable for the callback function:\n","        self.X = X\n","        self.y = y\n","\n","        #Make empty list to store costs:\n","        self.J = []\n","        \n","        params0 = self.N.getParams()\n","\n","        options = {'maxiter': 200, 'disp' : True}\n","        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS',\n","                                 args=(X, y), options=options, callback=self.callbackF)\n","\n","        self.N.setParams(_res.x)\n","        self.optimizationResults = _res"]},{"cell_type":"code","execution_count":277,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"shapes (10000,10000) and (1,3) not aligned: 10000 (dim 1) != 1 (dim 0)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-277-06c0fbee0c21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted Output: \\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-276-75e51786b649>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'maxiter'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'disp'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS',\n\u001b[0;32m---> 31\u001b[0;31m                                  args=(X, y), options=options, callback=self.callbackF)\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m         \u001b[0mxkp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha_k\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mretall\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             \u001b[0mallvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxkp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0mftol\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mAbsolute\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxopt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mbetween\u001b[0m \u001b[0miterations\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0macceptable\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mconvergence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m     \u001b[0mmaxiter\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mMaximum\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0miterations\u001b[0m \u001b[0mto\u001b[0m \u001b[0mperform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mderivative\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-276-75e51786b649>\u001b[0m in \u001b[0;36mcostFunctionWrapper\u001b[0;34m(self, params, X, y)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcostFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeGradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-271-0711a832566f>\u001b[0m in \u001b[0;36mcomputeGradients\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcomputeGradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdJdW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdJdW2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcostFunctionPrime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdJdW1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdJdW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-271-0711a832566f>\u001b[0m in \u001b[0;36mcostFunctionPrime\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mdJdW2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mdelta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoidPrime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mdJdW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: shapes (10000,10000) and (1,3) not aligned: 10000 (dim 1) != 1 (dim 0)"]}],"source":["# pd.DataFrame(ff_pred).describe()\n","\n","NN = Neural_Network()\n","T = trainer(NN)\n","\n","T.train(X.to_numpy(),y.to_numpy())\n","\n","print(\"Predicted Output: \\n\" + str(NN.forward(X))) "]},{"cell_type":"code","execution_count":171,"metadata":{},"outputs":[{"data":{"text/plain":"count    10000.000000\nmean         0.500000\nstd          0.500025\nmin          0.000000\n25%          0.000000\n50%          0.500000\n75%          1.000000\nmax          1.000000\nName: ate, dtype: float64"},"execution_count":171,"metadata":{},"output_type":"execute_result"}],"source":["y.describe()"]},{"cell_type":"code","execution_count":172,"metadata":{},"outputs":[{"data":{"text/plain":"array([[0, 1],\n       [1, 0],\n       [0, 1],\n       ...,\n       [0, 1],\n       [0, 1],\n       [1, 0]])"},"execution_count":172,"metadata":{},"output_type":"execute_result"}],"source":["X.to_numpy()"]},{"cell_type":"markdown","metadata":{},"source":["P.S. Don't try candy gummy bears. They're disgusting. "]},{"cell_type":"markdown","metadata":{},"source":["## 3. Keras MMP <a id=\"Q3\"></a>\n","\n","Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n","Use the Heart Disease Dataset (binary classification)\n","Use an appropriate loss function for a binary classification task\n","Use an appropriate activation function on the final layer of your network.\n","Train your model using verbose output for ease of grading.\n","Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n","When hyperparameter tuning, show you work by adding code cells for each new experiment.\n","Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n","You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."]},{"cell_type":"code","execution_count":176,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"(303, 14)\n"},{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>cp</th>\n      <th>trestbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalach</th>\n      <th>exang</th>\n      <th>oldpeak</th>\n      <th>slope</th>\n      <th>ca</th>\n      <th>thal</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>97</th>\n      <td>52</td>\n      <td>1</td>\n      <td>0</td>\n      <td>108</td>\n      <td>233</td>\n      <td>1</td>\n      <td>1</td>\n      <td>147</td>\n      <td>0</td>\n      <td>0.1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>41</td>\n      <td>1</td>\n      <td>2</td>\n      <td>130</td>\n      <td>214</td>\n      <td>0</td>\n      <td>0</td>\n      <td>168</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>237</th>\n      <td>60</td>\n      <td>1</td>\n      <td>0</td>\n      <td>140</td>\n      <td>293</td>\n      <td>0</td>\n      <td>0</td>\n      <td>170</td>\n      <td>0</td>\n      <td>1.2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>254</th>\n      <td>59</td>\n      <td>1</td>\n      <td>3</td>\n      <td>160</td>\n      <td>273</td>\n      <td>0</td>\n      <td>0</td>\n      <td>125</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>202</th>\n      <td>58</td>\n      <td>1</td>\n      <td>0</td>\n      <td>150</td>\n      <td>270</td>\n      <td>0</td>\n      <td>0</td>\n      <td>111</td>\n      <td>1</td>\n      <td>0.8</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n97    52    1   0       108   233    1        1      147      0      0.1   \n116   41    1   2       130   214    0        0      168      0      2.0   \n237   60    1   0       140   293    0        0      170      0      1.2   \n254   59    1   3       160   273    0        0      125      0      0.0   \n202   58    1   0       150   270    0        0      111      1      0.8   \n\n     slope  ca  thal  target  \n97       2   3     3       1  \n116      1   0     2       1  \n237      1   2     3       0  \n254      2   0     2       0  \n202      2   0     3       0  "},"execution_count":176,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","\n","df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n","df = df.sample(frac=1)\n","print(df.shape)\n","df.head()"]},{"cell_type":"code","execution_count":182,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","\n","df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n","df = df.sample(frac=1)\n","print(df.shape)\n","df.head()"]},{"cell_type":"code","execution_count":192,"metadata":{},"outputs":[{"data":{"text/plain":"pandas.core.series.Series"},"execution_count":192,"metadata":{},"output_type":"execute_result"}],"source":["type(y)"]},{"cell_type":"code","execution_count":247,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Train on 203 samples, validate on 100 samples\nEpoch 1/20\n203/203 [==============================] - 1s 5ms/sample - loss: 3.3779 - accuracy: 0.4828 - val_loss: 2.7596 - val_accuracy: 0.4800\nEpoch 2/20\n203/203 [==============================] - 0s 421us/sample - loss: 3.2841 - accuracy: 0.5468 - val_loss: 2.5999 - val_accuracy: 0.5100\nEpoch 3/20\n203/203 [==============================] - 0s 392us/sample - loss: 3.1178 - accuracy: 0.5813 - val_loss: 2.4202 - val_accuracy: 0.5100\nEpoch 4/20\n203/203 [==============================] - 0s 497us/sample - loss: 2.6462 - accuracy: 0.6158 - val_loss: 2.1055 - val_accuracy: 0.5600\nEpoch 5/20\n203/203 [==============================] - 0s 408us/sample - loss: 2.2972 - accuracy: 0.6158 - val_loss: 1.8060 - val_accuracy: 0.6000\nEpoch 6/20\n203/203 [==============================] - 0s 406us/sample - loss: 2.0424 - accuracy: 0.6256 - val_loss: 1.6487 - val_accuracy: 0.6200\nEpoch 7/20\n203/203 [==============================] - 0s 444us/sample - loss: 1.8901 - accuracy: 0.6305 - val_loss: 1.4936 - val_accuracy: 0.6200\nEpoch 8/20\n203/203 [==============================] - 0s 445us/sample - loss: 1.6291 - accuracy: 0.6404 - val_loss: 1.3265 - val_accuracy: 0.6300\nEpoch 9/20\n203/203 [==============================] - 0s 479us/sample - loss: 1.5717 - accuracy: 0.6404 - val_loss: 1.3046 - val_accuracy: 0.6400\nEpoch 10/20\n203/203 [==============================] - 0s 503us/sample - loss: 1.5554 - accuracy: 0.6502 - val_loss: 1.2943 - val_accuracy: 0.6400\nEpoch 11/20\n203/203 [==============================] - 0s 499us/sample - loss: 1.5411 - accuracy: 0.6650 - val_loss: 1.2836 - val_accuracy: 0.6400\nEpoch 12/20\n203/203 [==============================] - 0s 639us/sample - loss: 1.5286 - accuracy: 0.6749 - val_loss: 1.2765 - val_accuracy: 0.6700\nEpoch 13/20\n203/203 [==============================] - 0s 452us/sample - loss: 1.5175 - accuracy: 0.6847 - val_loss: 1.2713 - val_accuracy: 0.6700\nEpoch 14/20\n203/203 [==============================] - 0s 519us/sample - loss: 1.5079 - accuracy: 0.6897 - val_loss: 1.1712 - val_accuracy: 0.6900\nEpoch 15/20\n203/203 [==============================] - 0s 438us/sample - loss: 1.4992 - accuracy: 0.6897 - val_loss: 1.1473 - val_accuracy: 0.7100\nEpoch 16/20\n203/203 [==============================] - 0s 470us/sample - loss: 1.4906 - accuracy: 0.6897 - val_loss: 1.1355 - val_accuracy: 0.7100\nEpoch 17/20\n203/203 [==============================] - 0s 429us/sample - loss: 1.3802 - accuracy: 0.6897 - val_loss: 1.1164 - val_accuracy: 0.7300\nEpoch 18/20\n203/203 [==============================] - 0s 402us/sample - loss: 1.3303 - accuracy: 0.7044 - val_loss: 1.1012 - val_accuracy: 0.7500\nEpoch 19/20\n203/203 [==============================] - 0s 442us/sample - loss: 1.2616 - accuracy: 0.7094 - val_loss: 1.2100 - val_accuracy: 0.7600\nEpoch 20/20\n203/203 [==============================] - 0s 402us/sample - loss: 1.2538 - accuracy: 0.7044 - val_loss: 1.2059 - val_accuracy: 0.7600\n"},{"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x1a2c053668>"},"execution_count":247,"metadata":{},"output_type":"execute_result"}],"source":["from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Important Hyperparameters\n","inputs = X.shape[1]\n","epochs = 20\n","batch_size = 10\n","\n","# Create Model\n","model = Sequential()\n","model.add(Dense(10, activation='relu', input_shape=(inputs,)))\n","model.add(Dense(5, activation='relu'))\n","model.add(Dense(1))\n","# Compile Model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Fit Model\n","\n","model.fit(X, y.to_numpy(), \n","          validation_split=0.33, \n","          epochs=epochs, \n","          batch_size=batch_size\n","         )\n","# Validation accuracy is .76"]},{"cell_type":"code","execution_count":241,"metadata":{},"outputs":[],"source":["def create_model(nodes = 50, optimizer='adam'):\n","    # Create Model\n","    model = Sequential()\n","    model.add(Dense(nodes, activation='relu', input_shape=(inputs,)))\n","    model.add(Dense(nodes/2, activation='relu'))\n","    model.add(Dense(1))\n","    # Compile Model\n","    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","    return model"]},{"cell_type":"code","execution_count":244,"metadata":{},"outputs":[],"source":["from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n","from sklearn.model_selection import GridSearchCV\n","\n","model = KerasClassifier(build_fn = create_model, verbose=0, epochs=20)\n","\n","optimizer = ['SGD', 'Adam']\n","nodes = [10,20,30,40]\n","param_grid = dict(optimizer=optimizer, nodes=nodes)\n","\n","grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n","grid_result = grid.fit(X, y)\n"]},{"cell_type":"code","execution_count":245,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Best: 0.7788778940836588 using {'nodes': 20, 'optimizer': 'SGD'}\nMeans: 0.7425742546717325, Stdev: 0.042777181169124885 with: {'nodes': 10, 'optimizer': 'SGD'}\nMeans: 0.5841584205627441, Stdev: 0.11404062515802182 with: {'nodes': 10, 'optimizer': 'Adam'}\nMeans: 0.7788778940836588, Stdev: 0.02034459460670113 with: {'nodes': 20, 'optimizer': 'SGD'}\nMeans: 0.6699670056502024, Stdev: 0.14056461615202767 with: {'nodes': 20, 'optimizer': 'Adam'}\nMeans: 0.7458746035893759, Stdev: 0.04667370101995412 with: {'nodes': 30, 'optimizer': 'SGD'}\nMeans: 0.7623762488365173, Stdev: 0.024252366463151962 with: {'nodes': 30, 'optimizer': 'Adam'}\nMeans: 0.5808580915133158, Stdev: 0.1469295693386048 with: {'nodes': 40, 'optimizer': 'SGD'}\nMeans: 0.7326732675234476, Stdev: 0.04277718116912489 with: {'nodes': 40, 'optimizer': 'Adam'}\n"}],"source":["# Report Results\n","print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n","means = grid_result.cv_results_['mean_test_score']\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# tuned accuracy is .778"]},{"cell_type":"code","execution_count":330,"metadata":{},"outputs":[],"source":["nn_architecture = [\n","    {\"input_dim\": 2, \"output_dim\": 4, \"activation\": \"relu\"},\n","    {\"input_dim\": 4, \"output_dim\": 6, \"activation\": \"relu\"},\n","    {\"input_dim\": 6, \"output_dim\": 6, \"activation\": \"relu\"},\n","    {\"input_dim\": 6, \"output_dim\": 4, \"activation\": \"relu\"},\n","    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n","]\n","\n","def init_layers(nn_architecture, seed = 99):\n","    np.random.seed(seed)\n","    number_of_layers = len(nn_architecture)\n","    params_values = {}\n","\n","    for idx, layer in enumerate(nn_architecture):\n","        layer_idx = idx\n","        layer_input_size = layer[\"input_dim\"]\n","        layer_output_size = layer[\"output_dim\"]\n","        \n","        params_values['W' + str(layer_idx)] = np.random.randn(\n","            layer_output_size, layer_input_size) * 0.1\n","        params_values['b' + str(layer_idx)] = np.random.randn(\n","            layer_output_size, 1) * 0.1\n","        \n","    print('init param_values')\n","    print(params_values)\n","    return params_values\n","\n","def sigmoid(Z):\n","    return 1/(1+np.exp(-Z))\n","\n","def relu(Z):\n","    return np.maximum(0,Z)\n","\n","def sigmoid_backward(dA, Z):\n","    sig = sigmoid(Z)\n","    return dA * sig * (1 - sig)\n","\n","def relu_backward(dA, Z):\n","    dZ = np.array(dA, copy = True)\n","    dZ[Z <= 0] = 0\n","    return dZ\n","\n","\n","def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n","    Z_curr = np.dot(W_curr, A_prev) + b_curr\n","    \n","    if activation is \"relu\":\n","        activation_func = relu\n","    elif activation is \"sigmoid\":\n","        activation_func = sigmoid\n","    else:\n","        raise Exception('Non-supported activation function')\n","        \n","    return activation_func(Z_curr), Z_curr\n","\n","\n","def full_forward_propagation(X, params_values, nn_architecture):\n","    memory = {}\n","    A_curr = X\n","    \n","    for idx, layer in enumerate(nn_architecture):\n","        layer_idx = idx + 1\n","        A_prev = A_curr\n","        \n","        activ_function_curr = layer[\"activation\"]\n","        W_curr = params_values[\"W\" + str(layer_idx)]\n","        b_curr = params_values[\"b\" + str(layer_idx)]\n","        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n","        \n","        memory[\"A\" + str(idx)] = A_prev\n","        memory[\"Z\" + str(layer_idx)] = Z_curr\n","    \n","    return A_curr, memory\n","\n","\n","def get_cost_value(Y_hat, Y):\n","    m = Y_hat.shape[1]\n","    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n","    return np.squeeze(cost)\n","\n","# an auxiliary function that converts probability into class\n","def convert_prob_into_class(probs):\n","    probs_ = np.copy(probs)\n","    probs_[probs_ > 0.5] = 1\n","    probs_[probs_ <= 0.5] = 0\n","    return probs_\n","\n","def get_accuracy_value(Y_hat, Y):\n","    Y_hat_ = convert_prob_into_class(Y_hat)\n","    return (Y_hat_ == Y).all(axis=0).mean()\n","\n","\n","def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n","    m = A_prev.shape[1]\n","    \n","    if activation is \"relu\":\n","        backward_activation_func = relu_backward\n","    elif activation is \"sigmoid\":\n","        backward_activation_func = sigmoid_backward\n","    else:\n","        raise Exception('Non-supported activation function')\n","    \n","    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n","    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n","    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n","    dA_prev = np.dot(W_curr.T, dZ_curr)\n","\n","    return dA_prev, dW_curr, db_curr\n","\n","def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n","    grads_values = {}\n","    m = Y.shape[1]\n","    Y = Y.reshape(Y_hat.shape)\n","\n","    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat))\n","    \n","    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n","        layer_idx_curr = layer_idx_prev + 1\n","        activ_function_curr = layer[\"activation\"]\n","        \n","        dA_curr = dA_prev\n","        \n","        A_prev = memory[\"A\" + str(layer_idx_prev)]\n","        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n","        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n","        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n","        \n","        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n","            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n","        \n","        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n","        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n","    \n","    return grads_values\n","\n","\n","def update(params_values, grads_values, nn_architecture, learning_rate):\n","    for layer_idx, layer in enumerate(nn_architecture):\n","        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n","        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n","\n","    return params_values\n","\n","def train(X, Y, nn_architecture, epochs, learning_rate):\n","    params_values = init_layers(nn_architecture, 2)\n","    cost_history = []\n","    accuracy_history = []\n","    \n","    for i in range(epochs):\n","        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n","        cost = get_cost_value(Y_hat, Y)\n","        cost_history.append(cost)\n","        accuracy = get_accuracy_value(Y_hat, Y)\n","        accuracy_history.append(accuracy)\n","        \n","        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n","        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n","        \n","    return params_values, cost_history, accuracy_history"]},{"cell_type":"code","execution_count":313,"metadata":{},"outputs":[],"source":["X.shape, y.shape\n","X = X.to_numpy()\n","y = y.to_numpy()"]},{"cell_type":"code","execution_count":331,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"<class 'list'>\ninit param_values\n{'W0': array([[-0.04167578, -0.00562668],\n       [-0.21361961,  0.16402708],\n       [-0.17934356, -0.08417474],\n       [ 0.05028814, -0.12452881]]), 'b0': array([[-0.10579522],\n       [-0.09090076],\n       [ 0.0551454 ],\n       [ 0.2292208 ]]), 'W1': array([[ 0.00415394, -0.11179254,  0.05390583, -0.05961597],\n       [-0.00191305,  0.11750012, -0.07478709,  0.00090253],\n       [-0.08781079, -0.01564342,  0.02565705, -0.0988779 ],\n       [-0.0338822 , -0.0236184 , -0.0637655 , -0.11876123],\n       [-0.14212172, -0.01534952, -0.0269057 ,  0.22313668],\n       [-0.24347676,  0.01127265,  0.03704445,  0.13596339]]), 'b1': array([[ 5.01857207e-02],\n       [-8.44213704e-02],\n       [ 9.76147160e-07],\n       [ 5.42352572e-02],\n       [-3.13508197e-02],\n       [ 7.71011738e-02]]), 'W2': array([[-0.18680907,  0.17311847,  0.1467678 , -0.03356773,  0.06113408,\n         0.00479706],\n       [-0.08291353,  0.00877102,  0.10003659, -0.03810925, -0.03756694,\n        -0.00744708],\n       [ 0.04334963,  0.12783792, -0.06346793,  0.05083962,  0.0216116 ,\n        -0.18586124],\n       [-0.04193165, -0.01323289, -0.00395702,  0.03260034, -0.2040323 ,\n         0.00462555],\n       [-0.06776756, -0.1439439 ,  0.05242964,  0.07352796, -0.06532503,\n         0.08424563],\n       [-0.03815165,  0.0066489 , -0.10987389,  0.15844871, -0.26594495,\n        -0.00914526]]), 'b2': array([[ 0.06951196],\n       [-0.20334665],\n       [-0.01894693],\n       [-0.00772187],\n       [ 0.0824703 ],\n       [ 0.12482129]]), 'W3': array([[-0.04038923, -0.13845187,  0.13672354,  0.12178856, -0.04620053,\n         0.03508885],\n       [ 0.03818662,  0.05662754,  0.0204208 ,  0.14066962, -0.17379595,\n         0.1040824 ],\n       [ 0.0380472 , -0.02171353,  0.11735315, -0.23436032,  0.11615215,\n         0.0386078 ],\n       [-0.11331333,  0.04330926, -0.03040864,  0.25852949,  0.18353327,\n         0.04406899]]), 'b3': array([[-0.07192538],\n       [-0.05834146],\n       [-0.03250496],\n       [-0.05602345]]), 'W4': array([[-0.09022461, -0.05909723, -0.02761795, -0.05168839]]), 'b4': array([[-0.06985899]])}\n"},{"ename":"ValueError","evalue":"shapes (6,4) and (2,10000) not aligned: 4 (dim 1) != 2 (dim 0)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-331-a823240e739d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_architecture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparams_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_architecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-330-4b0bcb0cbb3d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, Y, nn_architecture, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mY_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcashe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_forward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_architecture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cost_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mcost_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-330-4b0bcb0cbb3d>\u001b[0m in \u001b[0;36mfull_forward_propagation\u001b[0;34m(X, params_values, nn_architecture)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mW_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mb_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mA_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingle_layer_forward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactiv_function_curr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mmemory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"A\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-330-4b0bcb0cbb3d>\u001b[0m in \u001b[0;36msingle_layer_forward_propagation\u001b[0;34m(A_prev, W_curr, b_curr, activation)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msingle_layer_forward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mZ_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_curr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: shapes (6,4) and (2,10000) not aligned: 4 (dim 1) != 2 (dim 0)"]}],"source":["print(type(nn_architecture))\n","# Training\n","params_values = train(np.transpose(X), np.transpose(y.reshape((y.shape[0], 1))), nn_architecture, 10000, 0.01)[0]"]},{"cell_type":"code","execution_count":317,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'X_test' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-317-bcdf47e824f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mY_test_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_forward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_architecture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"]}],"source":["\n","# Prediction\n","Y_test_hat, _ = full_forward_propagation(np.transpose(X_test), params_values, nn_architecture)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}