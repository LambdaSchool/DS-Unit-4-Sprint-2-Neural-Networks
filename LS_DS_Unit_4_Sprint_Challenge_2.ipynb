{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nteract": {
      "version": "0.14.3"
    },
    "colab": {
      "name": "LS_DS_Unit_4_Sprint_Challenge_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickwinters1/DS-Unit-4-Sprint-2-Neural-Networks/blob/master/LS_DS_Unit_4_Sprint_Challenge_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv5rO4HME4Dw",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2*\n",
        "\n",
        "# Sprint Challenge - Neural Network Foundations\n",
        "\n",
        "Table of Problems\n",
        "\n",
        "1. [Defining Neural Networks](#Q1)\n",
        "2. [Perceptron on XOR Gates](#Q2)\n",
        "3. [Multilayer Perceptron](#Q3)\n",
        "4. [Keras MMP](#Q4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzM5NPBGE4Dx",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"Q1\"></a>\n",
        "## 1. Define the following terms:\n",
        "\n",
        "- **Neuron:** neurons receive inputs and pass on their signal to the next layer of nodes if a certain threshold is reached\n",
        "- **Input Layer:** The Input Layer is what receives input from our dataset. Sometimes it is called the visible layer because it's the only part that is exposed to our data and that our data interacts with directly.\n",
        "- **Hidden Layer:** Layers after the input layer are called Hidden Layers. This is because they cannot be accessed except through the input layer. They're inside of the network and they perform their functions, but we don't directly interact with them. \n",
        "- **Output Layer:** The final layer is called the Output Layer. The purpose of the output layer is to output a vector of values that is in a format that is suitable for the type of problem that we're trying to address\n",
        "- **Activation:** The activation function decides whether a cell \"fires\" or not. Sometimes it is said that the cell is \"activated\" or not. In Artificial Neural Networks activation functions decide how much signal to pass onto the next layer. \n",
        "- **Backpropagation:**  Back-propagation is the essence of neural net training. It is the practice of fine-tuning the weights of a neural net based on the error rate obtained in the previous epoch. Proper tuning of the weights ensures lower error rates, making the model reliable by increasing its generalization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_JvXkgFE4Dy",
        "colab_type": "text"
      },
      "source": [
        "## 2. Perceptron on XOR Gates <a id=\"Q3=2\"></a>\n",
        "\n",
        "Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
        "\n",
        "|x1\t|x2|x3|\ty|\n",
        "|---|---|---|---|\n",
        "1|\t1|\t1|\t1|\n",
        "1|\t0|\t1|\t0|\n",
        "0|\t1|\t1|\t0|\n",
        "0|\t0|\t1|\t0|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "cXpWAT9JE4Dz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "18c2012a-c3ef-4b1c-d209-21b98e4dca8d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD, Adam, Nadam\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIbRu0KrICKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PerceptronClassifier():\n",
        "    \"\"\"\n",
        "    Basic perceptron class for binary classification\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.1, n_iter=100, tolerance=0.000001):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iter = n_iter\n",
        "        self.tolerance = tolerance\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit perceptron to a set of training data using gradient descent\n",
        "        \"\"\"\n",
        "        # initialize weights and cost list\n",
        "        self.weights_ = np.random.uniform(-0.01, 0.01, X.shape[1] + 1)\n",
        "        self.costs_ = []\n",
        "        # iterate until fit is adequate\n",
        "        for i in range(self.n_iter):\n",
        "            preds = self.predict_proba(X)\n",
        "            errors = preds - y\n",
        "            cost = np.sum(errors ** 2)\n",
        "            self.costs_.append(cost)\n",
        "            gradient = np.dot(X.T, errors)\n",
        "            self.weights_[1:] -= self.learning_rate * gradient\n",
        "            self.weights_[0] -= np.mean(errors)\n",
        "            \n",
        "            # break the loop if we are close enough\n",
        "            if cost < self.tolerance:\n",
        "                break\n",
        "            \n",
        "        return self\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Computes sigmoid output value given X\n",
        "        \"\"\"\n",
        "        return 1. / (1. + np.exp(-(np.dot(X, self.weights_[1:]) + self.weights_[0])))\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts the binary class of X values\n",
        "        \"\"\"\n",
        "        return np.where(self.predict_proba(X)>=0.5, 1, 0)\n",
        "    \n",
        "    def show_loss(self):\n",
        "        \"\"\"\n",
        "        Shows loss along epochs\n",
        "        \"\"\"\n",
        "        try:\n",
        "            iters = range(len(self.costs_))\n",
        "            fig, ax = plt.subplots()\n",
        "            ax.plot(iters, self.costs_)\n",
        "            ax.set_xlabel('Number of Iterations')\n",
        "            ax.set_ylabel('Training Loss (SSE)')\n",
        "            ax.set_title('Training Loss')\n",
        "            plt.show()\n",
        "        except:\n",
        "            print ('Please train me first :)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGmQLsIEIYzF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69836680-0ddf-4c3a-e0d2-f08948a36628"
      },
      "source": [
        "X = np.array([[1,1,1],\n",
        "            [1,0,1],\n",
        "            [0,1,1],\n",
        "            [0,0,1]])\n",
        "y = np.array([1,0,0,0])\n",
        "\n",
        "ppn = PerceptronClassifier(learning_rate = 1.0, n_iter=100)\n",
        "ppn.fit(X, y)\n",
        "ppn.predict(X)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMb1N2EuE4D3",
        "colab_type": "text"
      },
      "source": [
        "## 3. Multilayer Perceptron <a id=\"Q3\"></a>\n",
        "\n",
        "Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights.\n",
        "Your network must have one hidden layer.\n",
        "You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
        "Train your model on the Heart Disease dataset from UCI:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "_1Uwb9zOE4D4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "69d3ce7c-68c6-47ad-e9a4-2fa463994c02"
      },
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
              "0   63    1   3       145   233    1  ...      0      2.3      0   0     1       1\n",
              "1   37    1   2       130   250    0  ...      0      3.5      0   0     2       1\n",
              "2   41    0   1       130   204    0  ...      0      1.4      2   0     2       1\n",
              "3   56    1   1       120   236    0  ...      0      0.8      2   0     2       1\n",
              "4   57    0   0       120   354    0  ...      1      0.6      2   0     2       1\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw4XbTmrIpXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNetMLP(object):\n",
        "    \"\"\"\n",
        "    Feed forward nueral network / multi-layer perceptron classifier\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    n_hidden : int (default: 30)\n",
        "        Number of hidden units\n",
        "    l2 : float (default 0.)\n",
        "        Lambda value for l2 normalization\n",
        "    epochs : int (default: 100)\n",
        "        Number of training epochs\n",
        "    eta : float (default = 0.001)\n",
        "        Learning rate\n",
        "    shuffle : bool (default: True)\n",
        "        Shuffles the training data every epoch if True\n",
        "    minibatch_size : int (default : 1)\n",
        "        Number of training samples per minibatch\n",
        "    seed : int (default: None)\n",
        "        Random seed for initalizing weights and shuffling\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    eval_ : dict\n",
        "        Dictionary collecting the cost, training accuracy,\n",
        "        and validation accuracy for each epoch during training\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_hidden=30, l2=0.,\n",
        "                epochs=100, eta=0.0001,\n",
        "                shuffle=True, minibatch_size=1, seed=None):\n",
        "        self.random = np.random.RandomState(seed)\n",
        "        self.n_hidden = n_hidden\n",
        "        self.l2 = l2\n",
        "        self.epochs = epochs\n",
        "        self.eta = eta\n",
        "        self.shuffle = shuffle\n",
        "        self.minibatch_size = minibatch_size\n",
        "        \n",
        "    def _onehot(self, y, n_classes):\n",
        "        \"\"\"\n",
        "        Encode labels into one hot representation\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        y : array, shape = [n_samples]\n",
        "            Target values\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        onehot : array, shape = (n_samples, n_labels)\n",
        "        \"\"\"\n",
        "        \n",
        "        onehot = np.zeros((n_classes, y.shape[0]))\n",
        "        for idx, val in enumerate(y.astype(int)):\n",
        "            onehot[val, idx] = 1.\n",
        "        return onehot.T\n",
        "          \n",
        "    def _sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Compute logistic function (sigmoid)\n",
        "        \"\"\"\n",
        "        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n",
        "    \n",
        "    def _forward(self, X):\n",
        "        \"\"\"\n",
        "        Compute forward propogation step\n",
        "        \"\"\"\n",
        "        # step 1: net input of hidden layer\n",
        "        z_h = np.dot(X, self.w_h) + self.b_h\n",
        "        \n",
        "        # step 2: activation of hidden layer\n",
        "        a_h = self._sigmoid(z_h)\n",
        "        \n",
        "        # step 3: net input of output layer\n",
        "        z_out = np.dot(a_h, self.w_out) + self.b_out\n",
        "        \n",
        "        # step 4: activation layer output\n",
        "        a_out = self._sigmoid(z_out)\n",
        "        \n",
        "        return z_h, a_h, z_out, a_out\n",
        "    \n",
        "    def _compute_cost(self, y_enc, output):\n",
        "        \"\"\"\n",
        "        Compute cost function\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        y_enc : array, shape = (n_samples, n_labels)\n",
        "            one-hot encoded class labels\n",
        "        output : array, shape = [n_samples, n_output_units]\n",
        "            Activation of the output layer (forward propoagation)\n",
        "        Returns\n",
        "        -------\n",
        "        cost : float\n",
        "            Regularized cost\n",
        "        \"\"\"\n",
        "        e = 0.000000001\n",
        "        L2_term = (self.l2 *\n",
        "                  (np.sum(self.w_h ** 2.) + \n",
        "                  np.sum(self.w_out ** 2.)))\n",
        "        term1 = -y_enc * (np.log(output + e))\n",
        "        term2 = (1. - y_enc) * np.log(1. - output + e)\n",
        "        cost = np.sum(term1 - term2) + L2_term\n",
        "        return cost\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels\n",
        "        \n",
        "        Paramters\n",
        "        ---------\n",
        "        X : array, shape = [n_samples, n_features]\n",
        "            Input layer with original features\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : array, shape = [n_samples]\n",
        "            Predicted class labels\n",
        "        \"\"\"\n",
        "        z_h, a_h, z_out, a_out = self._forward(X)\n",
        "        y_pred = np.argmax(z_out, axis=1)\n",
        "        return y_pred\n",
        "          \n",
        "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
        "        \"\"\"\n",
        "        Learn weights from training data\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X_train : array, shape = [n_samples, n_features]\n",
        "            Input layer with original features\n",
        "        y_train : array, shape = [n_samples]\n",
        "            Target class labels\n",
        "        X_test : array, shape = [n_samples, n_features]\n",
        "            Sample features for validation during training\n",
        "        y_test : array, shape = [n_samples]\n",
        "            Samples test class labels for validation during training\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        self\n",
        "        \"\"\"\n",
        "        n_output = np.unique(y_train).shape[0]\n",
        "        n_features = X_train.shape[1]\n",
        "        \n",
        "        #######################\n",
        "        # Weight Initialization\n",
        "        #######################\n",
        "        \n",
        "        # weights for input -> hidden\n",
        "        self.b_h = np.zeros(self.n_hidden)\n",
        "        self.w_h = self.random.normal(loc=0.0,\n",
        "                                     scale=0.1,\n",
        "                                     size=(n_features,\n",
        "                                          self.n_hidden))\n",
        "        \n",
        "        # weights for hidden -> output\n",
        "        self.b_out = np.zeros(n_output)\n",
        "        self.w_out = self.random.normal(loc=0.0,\n",
        "                                       scale=0.1,\n",
        "                                       size=(self.n_hidden,\n",
        "                                       n_output))\n",
        "        \n",
        "        epoch_strlen = len(str(self.epochs)) # for progr. format\n",
        "        self.eval_ = {'train_cost' : [],\n",
        "                      'val_cost' : [],\n",
        "                     'train_acc' : [],\n",
        "                     'valid_acc' : []}\n",
        "        \n",
        "        y_train_enc = self._onehot(y_train, n_output)\n",
        "        y_valid_enc = self._onehot(y_valid, n_output)\n",
        "        \n",
        "        # iterate over training epochs\n",
        "        for i in range(self.epochs):\n",
        "            \n",
        "            # iterate over mini batches\n",
        "            indices = np.arange(X_train.shape[0])\n",
        "            \n",
        "            if self.shuffle:\n",
        "                self.random.shuffle(indices)\n",
        "            \n",
        "            for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size):\n",
        "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
        "                                \n",
        "                # forward propagation\n",
        "                z_h, a_h, z_out, a_out = self._forward(X_train[batch_idx])\n",
        "                \n",
        "                #################\n",
        "                # Backpropagation\n",
        "                #################\n",
        "                \n",
        "                # [n_samples, n_classlabels]\n",
        "                sigma_out = a_out - y_train_enc[batch_idx]\n",
        "                \n",
        "                # [n_samples, n_hidden]\n",
        "                sigmoid_derivative_h = a_h * (1. - a_h)\n",
        "                \n",
        "                # [n_samples, n_classlabels] dot [n_classlabels, n_hidden]\n",
        "                # -> [n_samples, n_hidden]\n",
        "                sigma_h = (np.dot(sigma_out, self.w_out.T) * sigmoid_derivative_h)\n",
        "                \n",
        "                # [n_features, n_samples] dot [n_samples, n_hidden]\n",
        "                # -> [n_features, n_hidden]\n",
        "                grad_w_h = np.dot(X_train[batch_idx].T, sigma_h)\n",
        "                grad_b_h = np.sum(sigma_h, axis=0)\n",
        "                \n",
        "                # [n_hidden, n_samples] dot [n_samples, n_classlabels]\n",
        "                # -> [n_hidden, n_classlabels]\n",
        "                grad_w_out = np.dot(a_h.T, sigma_out)\n",
        "                grad_b_out = np.sum(sigma_out, axis=0)\n",
        "                \n",
        "                # Regularization and weight updates\n",
        "                delta_w_h = (grad_w_h + self.l2 * self.w_h)\n",
        "                delta_b_h = grad_b_h # bias is not regularized\n",
        "                self.w_h -= self.eta * delta_w_h\n",
        "                self.b_h -= self.eta * delta_b_h\n",
        "                \n",
        "                delta_w_out = grad_w_out + self.l2 * self.w_out\n",
        "                delta_b_out = grad_b_out # bias is not regularized\n",
        "                self.w_out -= self.eta * delta_w_out\n",
        "                self.b_out -= self.eta * delta_b_out\n",
        "                \n",
        "            ############\n",
        "            # Evaluation\n",
        "            ############\n",
        "\n",
        "            # evaluation after each epoch during training\n",
        "            z_h, a_h, z_out, a_out = self._forward(X_train)\n",
        "            z_h_val, a_h_val, z_out_val, a_out_val = self._forward(X_valid)\n",
        "            \n",
        "            cost = self._compute_cost(y_enc=y_train_enc, output=a_out) / a_out.shape[0]\n",
        "            cost_val = self._compute_cost(y_enc=y_valid_enc, output=a_out_val) / a_out_val.shape[0]\n",
        "            \n",
        "            y_train_pred = self.predict(X_train)\n",
        "            y_valid_pred = self.predict(X_valid)\n",
        "            \n",
        "            train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float) / X_train.shape[0])\n",
        "            valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(np.float) / X_valid.shape[0])\n",
        "            \n",
        "            sys.stderr.write('\\r%0*d/%d | Cost: %.2f | Train/Valid Acc: %.2f%%/%.2f%%'\n",
        "                            % (epoch_strlen, i+1, self.epochs, cost, train_acc*100, valid_acc*100))\n",
        "            sys.stderr.flush()\n",
        "            \n",
        "            self.eval_['train_cost'].append(cost)\n",
        "            self.eval_['val_cost'].append(cost_val)\n",
        "            self.eval_['train_acc'].append(train_acc)\n",
        "            self.eval_['valid_acc'].append(valid_acc)\n",
        "            \n",
        "        return self\n",
        "    \n",
        "    def plot_training(self):\n",
        "        \"\"\"\n",
        "        Plots training loss and accuracy\n",
        "        \"\"\"\n",
        "        fig, ax = plt.subplots(ncols=2, figsize=(12,7))\n",
        "        \n",
        "        # plotting loss\n",
        "        ax[0].plot(range(self.epochs), self.eval_['train_cost'], label='Train')\n",
        "        ax[0].plot(range(self.epochs), self.eval_['val_cost'], label='Validation')\n",
        "        ax[0].set_xlabel('Epochs')\n",
        "        ax[0].set_ylabel('Cost')\n",
        "        ax[0].set_title('Training Cost')\n",
        "        ax[0].legend(loc='best')\n",
        "        \n",
        "        ax[1].plot(range(self.epochs), self.eval_['train_acc'], label='Train')\n",
        "        ax[1].plot(range(self.epochs), self.eval_['valid_acc'], label='Validation')\n",
        "        ax[1].set_xlabel('Epochs')\n",
        "        ax[1].set_ylabel('Cost')\n",
        "        ax[1].set_title('Training Accuracy')\n",
        "        ax[0].legend(loc='best')\n",
        "        \n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obukB44VIpd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data.drop(columns='target')\n",
        "y = data.target\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, \n",
        "                                                  y,\n",
        "                                                 stratify=y)\n",
        "# scale data\n",
        "scale = StandardScaler()\n",
        "X_train = scale.fit_transform(X_train)\n",
        "X_val = scale.transform(X_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW_nUaL5Ipkh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c852345-e7ac-43f0-cacf-bafe6a371c1e"
      },
      "source": [
        "mlp = NeuralNetMLP(n_hidden=30,\n",
        "                  eta=0.001)\n",
        "mlp.fit(X_train=X_train, \n",
        "        y_train=y_train.values, \n",
        "        X_valid=X_val, \n",
        "        y_valid=y_val.values);"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100/100 | Cost: 0.78 | Train/Valid Acc: 85.02%/88.16%"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaJ2EAngIpq_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "594d763a-eae3-42d7-e735-045f805449a7"
      },
      "source": [
        "mlp.plot_training()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtQAAAG5CAYAAACur6PpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBscdn5IE4D6",
        "colab_type": "text"
      },
      "source": [
        "## 4. Keras MMP <a id=\"Q4\"></a>\n",
        "\n",
        "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
        "Use the Heart Disease Dataset (binary classification)\n",
        "Use an appropriate loss function for a binary classification task\n",
        "Use an appropriate activation function on the final layer of your network.\n",
        "Train your model using verbose output for ease of grading.\n",
        "Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
        "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
        "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
        "You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "NYyjCb8eE4D7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(dense_layers=2,\n",
        "                 dense_nodes=5,\n",
        "                 dropout=False,\n",
        "                 dropout_pct=0.0,\n",
        "                 activation='sigmoid',\n",
        "                 weight_initializer='glorot_uniform',\n",
        "                 optimizer=SGD,\n",
        "                 lr=0.0001,\n",
        "                 input_shape=(X_train.shape[1],)):\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # add input layer\n",
        "    model.add(Dense(dense_nodes, \n",
        "                    input_shape=input_shape,\n",
        "                    kernel_initializer=weight_initializer,\n",
        "                    activation=activation))\n",
        "    \n",
        "    # add dense layers and drop out\n",
        "    for _ in range(dense_layers):\n",
        "        # dense\n",
        "        model.add(Dense(dense_nodes,\n",
        "                        kernel_initializer=weight_initializer,\n",
        "                        activation=activation))\n",
        "        # dropout\n",
        "        if dropout:\n",
        "            model.add(Dropout(rate=dropout_pct))\n",
        "\n",
        "    # add final activation layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # optimizer\n",
        "    optimizer=optimizer(lr=lr)\n",
        "    # compile model\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
        "              \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flXB3vs1JfSD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "907337b5-3931-4ed1-c386-d446d62439e4"
      },
      "source": [
        "epochs = 20\n",
        "batch_size = 20\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, \n",
        "                               epochs=epochs,\n",
        "                               batch_size=batch_size,\n",
        "                               verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "param_grid = {}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=param_grid, \n",
        "                    cv=5,\n",
        "                    n_jobs=1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0719 17:00:04.877549 140022329350016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0719 17:00:04.919279 140022329350016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0719 17:00:04.926271 140022329350016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0719 17:00:05.013015 140022329350016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0719 17:00:05.023814 140022329350016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0719 17:00:05.031163 140022329350016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0719 17:00:05.272962 140022329350016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.4581497765847765 using {}\n",
            "Means: 0.4581497765847765, Stdev: 0.07513302108562053 with: {}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjvh9g0lJfXg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "92aa0665-6267-4793-85e2-8cf63a2fc25a"
      },
      "source": [
        "# define the grid search parameters\n",
        "param_grid = {'batch_size': [10, 20, 40, 60, 80],\n",
        "              'epochs': [30, 60, 90]}\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, \n",
        "                               epochs=epochs,\n",
        "                               batch_size=batch_size,\n",
        "                               verbose=0)\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=param_grid, \n",
        "                    cv=5,\n",
        "                    n_jobs=1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.5462555178986772 using {'batch_size': 80, 'epochs': 60}\n",
            "Means: 0.5462555144851954, Stdev: 0.07250406530604263 with: {'batch_size': 10, 'epochs': 30}\n",
            "Means: 0.5462555144851954, Stdev: 0.07250406530604263 with: {'batch_size': 10, 'epochs': 60}\n",
            "Means: 0.48458150462432054, Stdev: 0.08460899147660379 with: {'batch_size': 10, 'epochs': 90}\n",
            "Means: 0.4537444929982072, Stdev: 0.07250406738596081 with: {'batch_size': 20, 'epochs': 30}\n",
            "Means: 0.5110132158590308, Stdev: 0.056245179467437435 with: {'batch_size': 20, 'epochs': 60}\n",
            "Means: 0.5462555005030485, Stdev: 0.07250406719969738 with: {'batch_size': 20, 'epochs': 90}\n",
            "Means: 0.5462555083803143, Stdev: 0.07250406544585167 with: {'batch_size': 40, 'epochs': 30}\n",
            "Means: 0.4845815006856876, Stdev: 0.08460899015167125 with: {'batch_size': 40, 'epochs': 60}\n",
            "Means: 0.5154184978045031, Stdev: 0.08460898633204307 with: {'batch_size': 40, 'epochs': 90}\n",
            "Means: 0.4581497844620423, Stdev: 0.07513302803961282 with: {'batch_size': 60, 'epochs': 30}\n",
            "Means: 0.4581497844620423, Stdev: 0.07513302803961282 with: {'batch_size': 60, 'epochs': 60}\n",
            "Means: 0.5066079334540514, Stdev: 0.08630477800911489 with: {'batch_size': 60, 'epochs': 90}\n",
            "Means: 0.5418502272225687, Stdev: 0.07513302764702459 with: {'batch_size': 80, 'epochs': 30}\n",
            "Means: 0.5462555178986772, Stdev: 0.07250408355848231 with: {'batch_size': 80, 'epochs': 60}\n",
            "Means: 0.4537444937859338, Stdev: 0.07250406113531985 with: {'batch_size': 80, 'epochs': 90}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsgACewRJfdE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "50b2720a-7062-4962-8946-0ed91ec0a957"
      },
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, \n",
        "                               epochs=30,\n",
        "                               batch_size=10,\n",
        "                               verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "param_grid = {'optimizer': [SGD, Adam, Nadam],\n",
        "              'lr': [.01, .001, .0001, .00001]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.806167389852885 using {'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.5462555112030012, Stdev: 0.061423006305868166 with: {'lr': 0.01, 'optimizer': <class 'keras.optimizers.SGD'>}\n",
            "Means: 0.806167389852885, Stdev: 0.04336120800360705 with: {'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.7797356780930238, Stdev: 0.054054760898376186 with: {'lr': 0.01, 'optimizer': <class 'keras.optimizers.Nadam'>}\n",
            "Means: 0.42731278274815515, Stdev: 0.025078386373467973 with: {'lr': 0.001, 'optimizer': <class 'keras.optimizers.SGD'>}\n",
            "Means: 0.6123348034032116, Stdev: 0.1404456012497063 with: {'lr': 0.001, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.6563876715656944, Stdev: 0.13901254631686935 with: {'lr': 0.001, 'optimizer': <class 'keras.optimizers.Nadam'>}\n",
            "Means: 0.5726872278861538, Stdev: 0.02507838355185056 with: {'lr': 0.0001, 'optimizer': <class 'keras.optimizers.SGD'>}\n",
            "Means: 0.4801762157205968, Stdev: 0.07429249629909093 with: {'lr': 0.0001, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.5726872278861538, Stdev: 0.02507838355185056 with: {'lr': 0.0001, 'optimizer': <class 'keras.optimizers.Nadam'>}\n",
            "Means: 0.5462555112030012, Stdev: 0.061423006305868166 with: {'lr': 1e-05, 'optimizer': <class 'keras.optimizers.SGD'>}\n",
            "Means: 0.5726872278861538, Stdev: 0.02507838355185056 with: {'lr': 1e-05, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.5066079324037494, Stdev: 0.07660739558227989 with: {'lr': 1e-05, 'optimizer': <class 'keras.optimizers.Nadam'>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I99hiMxlJfik",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "78c5dbd6-fab4-4f76-aad6-008df0edf5bf"
      },
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, \n",
        "                               epochs=30,\n",
        "                               batch_size=10,\n",
        "                               verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "param_grid = {'activation': ['sigmoid', 'tanh', 'relu'],\n",
        "              'optimizer': [Adam],\n",
        "              'lr': [.01]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.8061673945792446 using {'activation': 'sigmoid', 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.8061673945792446, Stdev: 0.027263929006820124 with: {'activation': 'sigmoid', 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.7709251159088202, Stdev: 0.03772249056440599 with: {'activation': 'tanh', 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
            "Means: 0.7533039642325582, Stdev: 0.033239984292422864 with: {'activation': 'relu', 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avd7sOc0Jfn0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "dd692729-b0a3-4259-862c-1a6eaaf6a408"
      },
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, \n",
        "                               epochs=30,\n",
        "                               batch_size=10,\n",
        "                               verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "param_grid = {'weight_initializer': ['glorot_uniform', \n",
        "                                     'random_uniform', \n",
        "                                     'random_normal'],\n",
        "              'activation' : ['relu'],\n",
        "             'optimizer' : [Adam],\n",
        "             'lr' : [0.01]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.7797356833445344 using {'activation': 'relu', 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'glorot_uniform'}\n",
            "Means: 0.7797356833445344, Stdev: 0.04334236567606776 with: {'activation': 'relu', 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'glorot_uniform'}\n",
            "Means: 0.6916299577851652, Stdev: 0.08086188901394809 with: {'activation': 'relu', 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.5462555112030012, Stdev: 0.061423006305868166 with: {'activation': 'relu', 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_normal'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_IEK_GKJfsz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "b4f19386-370f-42e0-d8fc-cc742d032b90"
      },
      "source": [
        "model = KerasClassifier(build_fn=create_model, \n",
        "                               epochs=30,\n",
        "                               batch_size=10,\n",
        "                               verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "param_grid = {'dropout' : [True, False],\n",
        "             'dropout_pct' : [0.1, 0.2, 0.3],\n",
        "              'weight_initializer': ['random_uniform'],\n",
        "              'activation' : ['relu'],\n",
        "             'optimizer' : [Adam],\n",
        "             'lr' : [0.01]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n",
            "W0719 17:45:11.134628 140022329350016 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.8017621129619917 using {'activation': 'relu', 'dropout': True, 'dropout_pct': 0.3, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7356828189237528, Stdev: 0.10458801355945829 with: {'activation': 'relu', 'dropout': True, 'dropout_pct': 0.1, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7885462547188814, Stdev: 0.04750726381637613 with: {'activation': 'relu', 'dropout': True, 'dropout_pct': 0.2, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.8017621129619917, Stdev: 0.061078373459293886 with: {'activation': 'relu', 'dropout': True, 'dropout_pct': 0.3, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7753303906991094, Stdev: 0.02899168905333891 with: {'activation': 'relu', 'dropout': False, 'dropout_pct': 0.1, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7180616745996055, Stdev: 0.08899963486378117 with: {'activation': 'relu', 'dropout': False, 'dropout_pct': 0.2, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.6916299530588058, Stdev: 0.08576002212465583 with: {'activation': 'relu', 'dropout': False, 'dropout_pct': 0.3, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chuF_OR4J17e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "18b75443-1dd8-4ff3-993d-67943312b639"
      },
      "source": [
        "model = KerasClassifier(build_fn=create_model, \n",
        "                               epochs=30,\n",
        "                               batch_size=10,\n",
        "                               verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "param_grid = {'dense_layers' : [1,2],\n",
        "              'dense_nodes' : [5, 10, 15, 20, 25],\n",
        "              'weight_initializer': ['random_uniform'],\n",
        "              'activation' : ['relu'],\n",
        "             'optimizer' : [Adam],\n",
        "             'lr' : [0.01]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.7973568203165667 using {'activation': 'relu', 'dense_layers': 2, 'dense_nodes': 10, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7797356786181748, Stdev: 0.05405476520288825 with: {'activation': 'relu', 'dense_layers': 1, 'dense_nodes': 5, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7488986768386438, Stdev: 0.07041868238947577 with: {'activation': 'relu', 'dense_layers': 1, 'dense_nodes': 10, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7753303933248646, Stdev: 0.05531874173361575 with: {'activation': 'relu', 'dense_layers': 1, 'dense_nodes': 15, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7400881059894477, Stdev: 0.054033032819223316 with: {'activation': 'relu', 'dense_layers': 1, 'dense_nodes': 20, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7577092490007173, Stdev: 0.044444423301022676 with: {'activation': 'relu', 'dense_layers': 1, 'dense_nodes': 25, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7753303933248646, Stdev: 0.0399352770509068 with: {'activation': 'relu', 'dense_layers': 2, 'dense_nodes': 5, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7973568203165667, Stdev: 0.04129034238650365 with: {'activation': 'relu', 'dense_layers': 2, 'dense_nodes': 10, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7533039642325582, Stdev: 0.059401819115089904 with: {'activation': 'relu', 'dense_layers': 2, 'dense_nodes': 15, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7533039616068029, Stdev: 0.06872147489727856 with: {'activation': 'relu', 'dense_layers': 2, 'dense_nodes': 20, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n",
            "Means: 0.7797356807187791, Stdev: 0.022077536669685673 with: {'activation': 'relu', 'dense_layers': 2, 'dense_nodes': 25, 'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>, 'weight_initializer': 'random_uniform'}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}