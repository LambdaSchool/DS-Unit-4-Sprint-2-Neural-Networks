{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.15.0"
    },
    "colab": {
      "name": "LS_DS_Unit_4_Sprint_Challenge_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJCjZZbnVLLi",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2*\n",
        "\n",
        "# Sprint Challenge - Neural Network Foundations\n",
        "\n",
        "Table of Problems\n",
        "\n",
        "1. [Defining Neural Networks](#Q1)\n",
        "2. [Chocolate Gummy Bears](#Q2)\n",
        "    - Perceptron\n",
        "    - Multilayer Perceptron\n",
        "4. [Keras MMP](#Q3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSTnfiIIVLLp",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"Q1\"></a>\n",
        "## 1. Define the following terms:\n",
        "\n",
        "- **Neuron:**\n",
        "-- a perceptron that takes a weighted sum of inputs, adds bias, as passes through an activation function to determine whether it passes info to the next stage; where output = activation_function(WX + b)\n",
        "- **Input Layer:**\n",
        "-- the layer that takes the input neurons/ brings the data into the system\n",
        "- **Hidden Layer:**\n",
        "-- the \"black box\" of the NN, in between the input and output layers, doesn't look outside\n",
        "- **Output Layer:**\n",
        "-- the final layer that outputs the numbers/makes inferences\n",
        "- **Activation:**\n",
        "-- transforms a matrix into a desirable shape for output. (sigmoid, tanh, step, relu)\n",
        "- **Backpropagation:**\n",
        "-- goal is to adjust each weight in the network proportional to how much it contributes to the overall error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M78UOOvLVLLr",
        "colab_type": "text"
      },
      "source": [
        "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
        "\n",
        "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
        "\n",
        "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
        "\n",
        "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
        "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh1vDjUkXONp",
        "colab_type": "code",
        "outputId": "3f76ff05-35cf-4984-8487-a951fe75a42c",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c7b4974b-7b3b-452f-8dcb-ede01a080e52\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c7b4974b-7b3b-452f-8dcb-ede01a080e52\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving chocolate_gummy_bears.csv to chocolate_gummy_bears (2).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "FeeCtyM3VLLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "candy = pd.read_csv('chocolate_gummy_bears.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "KEFGu_GuVLL2",
        "colab_type": "code",
        "outputId": "931266f9-91f7-4778-85a8-b8a1720b5d66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "print(candy.shape)\n",
        "candy.head()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chocolate</th>\n",
              "      <th>gummy</th>\n",
              "      <th>ate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   chocolate  gummy  ate\n",
              "0          0      1    1\n",
              "1          1      0    1\n",
              "2          0      1    1\n",
              "3          0      0    0\n",
              "4          1      1    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_TypWygVLMA",
        "colab_type": "text"
      },
      "source": [
        "### Perceptron\n",
        "\n",
        "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
        "\n",
        "Once you've trained your model, report your accuracy. Explain why you could not achieve a higher accuracy with a *simple perceptron*. It's possible to achieve ~95% accuracy on this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc9bvFMJXvdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "%matplotlib inline\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "hXUKwWREVLMD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d151cd1-ea66-49a9-e0aa-607b134d28c4"
      },
      "source": [
        "# Start your candy perceptron here\n",
        "\n",
        "# train, test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(candy.iloc[:, :-1].values, \n",
        "                                                    candy.iloc[:, -1].values, \n",
        "                                                    test_size=0.2)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8000, 2), (2000, 2), (8000,), (2000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsvqfKU_XhBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Perceptron():\n",
        "    def __init__(self, inputs=2, output=1, rate=0.01, n_iter=1000):\n",
        "        self.input = inputs\n",
        "        self.output = output\n",
        "        self.rate = rate\n",
        "        self.n_iter = n_iter\n",
        "        \n",
        "        # Initialize random weights\n",
        "        self.weight = np.random.randn(inputs, output)\n",
        "        \n",
        "        # Initialize bias\n",
        "        self.bias = np.zeros(output)\n",
        "        \n",
        "        # Initialize loss function (empty list for empty perceptron)\n",
        "        self.loss = []\n",
        "        pass\n",
        "    \n",
        "    # Define sigmoid function\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"\n",
        "        Sigmoid function\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "    \n",
        "    # Sigmoid derivative function\n",
        "    def sigmoid_derivative(self, x):\n",
        "        \"\"\"\n",
        "        Sigmoid derivative\n",
        "        \"\"\"\n",
        "        sx = self.sigmoid(x)\n",
        "        return sx * (1-sx)\n",
        "    \n",
        "    # Define neural network affine function\n",
        "    def affine_func(self, x, w, b):\n",
        "        '''\n",
        "        y = Wx + b\n",
        "        '''\n",
        "        scores = x.dot(w) + b\n",
        "        cache = (x, w, b)\n",
        "        return scores, cache\n",
        "    \n",
        "    # Define back-propagation\n",
        "    def back_prop(self, d_out, cache):\n",
        "        '''\n",
        "        dY\n",
        "        '''\n",
        "        x, w, b = cache\n",
        "        dx = d_out.dot(w.T)\n",
        "        dw = x.reshape(-1, 1).dot(d_out.reshape(-1, 1))\n",
        "        db = np.sum(d_out, axis=0)\n",
        "        \n",
        "        return dx, dw, db\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        '''\n",
        "        Fit the perceptron\n",
        "        '''\n",
        "        for i in range(self.n_iter):            \n",
        "            for j in range(x.shape[0]):\n",
        "                # forward prop\n",
        "                scores, cache = self.affine_func(x[j], self.weight, self.bias)\n",
        "                \n",
        "                # scoring\n",
        "                out = self.sigmoid(scores)\n",
        "                loss = y[j] - out.reshape(-1,)\n",
        "                self.loss.append(loss)\n",
        "                \n",
        "                # back prop\n",
        "                dout = loss * self.sigmoid_derivative(out)\n",
        "                _, dw, db = self.back_prop(dout.reshape(-1,), cache)\n",
        "        \n",
        "                # gradient update\n",
        "                self.weight += dw\n",
        "                self.bias += db\n",
        "        pass\n",
        "        \n",
        "    def predict(self, x):\n",
        "        '''\n",
        "        Make predictions\n",
        "        '''\n",
        "        return self.sigmoid(self.affine_func(x, self.weight, self.bias)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duLGCYVxXhIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "choco_gum = Perceptron(inputs=2, output=1)\n",
        "choco_gum.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0v6zi0zme8d",
        "colab_type": "code",
        "outputId": "2a60545d-f1c9-47f9-dd0d-1a8a76f4aed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred = choco_gum.predict(X_test)\n",
        "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
        "print(f'accuracy is {accuracy_score(y_test, y_pred.reshape(-1,))}')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 0.7295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLRMbfPyXhGz",
        "colab_type": "code",
        "outputId": "616e3409-f478-4b39-fc09-dcd7df19986c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "There is only so much we can do with a simple perceptron because of it's\n",
        "simplicity. There are no hidden layers to help identify different patterns\n",
        "within the dataset\n",
        "'''"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nThere is only so much we can do with a simple perceptron because of it's\\nsimplicity. There are no hidden layers to help identify different patterns\\nwithin the dataset\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYfw92tdVLML",
        "colab_type": "text"
      },
      "source": [
        "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
        "\n",
        "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
        "Your network must have one hidden layer.\n",
        "\n",
        "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fLexVexc7tu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWFlHQxja_WO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import optimize\n",
        "\n",
        "\n",
        "class Neural_Network:\n",
        "    def __init__(self, inputLayerSize=3, hiddenLayerSize=4, outputLayerSize=1, seed=42):\n",
        "        \"\"\"\n",
        "        Initialize Single Hidden Layer Neural Network with input size, hidden layer size, output layer size, learning rate, momemtum, iterations, and random seed\n",
        "        \"\"\"       \n",
        "        # Setup architecture of neural network\n",
        "        self.inputLayerSize = inputLayerSize\n",
        "        self.hiddenLayerSize = hiddenLayerSize\n",
        "        self.outputLayerSize = outputLayerSize\n",
        "        \n",
        "        # Set random seed fixed\n",
        "        self.__set_seed(seed=seed)\n",
        "\n",
        "        #Weights (parameters)\n",
        "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
        "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
        "  \n",
        "    def __set_seed(self, seed):\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        #Propogate inputs though network\n",
        "        self.z2 = np.dot(X, self.W1)\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "        self.z3 = np.dot(self.a2, self.W2)\n",
        "        yHat = self.sigmoid(self.z3) \n",
        "        return yHat\n",
        "        \n",
        "    def sigmoid(self, z):\n",
        "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
        "        return 1/(1+np.exp(-z))\n",
        "    \n",
        "    def sigmoidPrime(self,z):\n",
        "        #Gradient of sigmoid\n",
        "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
        "    \n",
        "    def costFunction(self, X, y):\n",
        "        #Compute cost for given X,y, use weights already stored in class.\n",
        "        self.yHat = self.forward(X)\n",
        "        J = 0.5*sum((y-self.yHat)**2)\n",
        "        return J\n",
        "        \n",
        "    def costFunctionPrime(self, X, y):\n",
        "        #Compute derivative with respect to W and W2 for a given X and y:\n",
        "        self.yHat = self.forward(X)\n",
        "        \n",
        "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
        "        dJdW2 = np.dot(self.a2.T, delta3)\n",
        "        delta2 = np.dot(delta3, self.W2.T) * self.sigmoidPrime(self.z2)\n",
        "        dJdW1 = np.dot(X.T, delta2)  \n",
        "        \n",
        "        return dJdW1, dJdW2\n",
        "    \n",
        "    #Helper Functions for interacting with other classes:\n",
        "    def getParams(self):\n",
        "        #Get W1 and W2 unrolled into vector:\n",
        "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
        "        return params\n",
        "    \n",
        "    def setParams(self, params):\n",
        "        #Set W1 and W2 using single paramater vector.\n",
        "        W1_start = 0\n",
        "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
        "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
        "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
        "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
        "        \n",
        "    def computeGradients(self, X, y):\n",
        "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
        "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        y_preds = nn.forward(X)\n",
        "        return (y_preds > threshold).astype('int32')\n",
        "\n",
        "class Optimizer:\n",
        "    def __init__(self, N):\n",
        "        #Make Local reference to network:\n",
        "        self.N = N\n",
        "        \n",
        "    def callbackF(self, params):\n",
        "        self.N.setParams(params)\n",
        "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
        "        \n",
        "    def costFunctionWrapper(self, params, X, y):\n",
        "        self.N.setParams(params)\n",
        "        cost = self.N.costFunction(X, y)\n",
        "        grad = self.N.computeGradients(X,y)\n",
        "        \n",
        "        return cost, grad\n",
        "        \n",
        "    def train(self, X, y):\n",
        "        #Make an internal variable for the callback function:\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        #Make empty list to store costs:\n",
        "        self.J = []\n",
        "        \n",
        "        params0 = self.N.getParams()\n",
        "\n",
        "        options = {'maxiter': 200, 'disp' : True}\n",
        "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
        "                                 args=(X, y), options=options, callback=self.callbackF)\n",
        "\n",
        "        self.N.setParams(_res.x)\n",
        "        self.optimizationResults = _res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG3x4-1ra_Ta",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "cae88c28-4b0c-4e48-9194-2e30f41141bd"
      },
      "source": [
        "nn = Neural_Network(inputLayerSize=2, hiddenLayerSize=4, outputLayerSize=1, seed=42)\n",
        "opt = Optimizer(nn)\n",
        "\n",
        "opt.train(X_train, y_train)\n",
        "y_preds = nn.forward(X_train)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: overflow encountered in square\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: overflow encountered in square\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Warning: Desired error not necessarily achieved due to precision loss.\n",
            "         Current function value: 318.444613\n",
            "         Iterations: 17\n",
            "         Function evaluations: 78\n",
            "         Gradient evaluations: 66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcrxTcQ5a_Q3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17fc8226-9f78-463a-c639-8a57a75b72f6"
      },
      "source": [
        "print(f\"Loss: {np.mean(np.square(y_train - y_preds))}\") "
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.07961115330340161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zayBni2Va_NZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7e74eea0-f67f-40ed-a0ed-7f1ae75ce596"
      },
      "source": [
        "y_preds = nn.forward(X_test)\n",
        "y_preds = (y_preds > .5).astype(int)\n",
        "accuracy_score(y_test, y_preds)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.946"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75rd8gpId3ZX",
        "colab_type": "code",
        "outputId": "05f4d050-e815-442d-ff76-2b67cbd5f268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''\n",
        "Multilayer are able to identify non-linear relationships, and hidden patterns\n",
        "in the data, which makes them much more durable than a singular perceptron.\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nMultilayer are able to identify non-linear relationships, which makes them\\nmuch more durable than a singular perceptron.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzk3JUwrVLMT",
        "colab_type": "text"
      },
      "source": [
        "P.S. Don't try candy gummy bears. They're disgusting. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLEHMhOIVLMV",
        "colab_type": "text"
      },
      "source": [
        "## 3. Keras MMP <a id=\"Q3\"></a>\n",
        "\n",
        "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
        "Use the Heart Disease Dataset (binary classification)\n",
        "Use an appropriate loss function for a binary classification task\n",
        "Use an appropriate activation function on the final layer of your network.\n",
        "Train your model using verbose output for ease of grading.\n",
        "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
        "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
        "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
        "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "fTpY2fTdVLMa",
        "colab_type": "code",
        "outputId": "220ab4b4-061e-4d45-9c28-86a58541190e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
        "df = df.sample(frac=1)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(303, 14)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>67</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>299</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>125</td>\n",
              "      <td>1</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>44</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>263</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>173</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>192</td>\n",
              "      <td>283</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>195</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>65</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>160</td>\n",
              "      <td>360</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>151</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>67</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>125</td>\n",
              "      <td>254</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
              "253   67    1   0       100   299    0  ...      1      0.9      1   2     2       0\n",
              "7     44    1   1       120   263    0  ...      0      0.0      2   0     3       1\n",
              "248   54    1   1       192   283    0  ...      0      0.0      2   1     3       0\n",
              "39    65    0   2       160   360    0  ...      0      0.8      2   0     2       1\n",
              "197   67    1   0       125   254    1  ...      0      0.2      1   2     3       0\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1LF8yYom4PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.drop(columns='target')\n",
        "y = df['target']\n",
        "\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3yDfLMGSY-T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "feefc189-c4c9-4f2e-a265-a7482e73c064"
      },
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((227, 13), (76, 13), (227,), (76,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBrmdoQgm4UU",
        "colab_type": "code",
        "outputId": "ca9e7685-ac80-4c4c-e8c7-6689b9143671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "# Instantiate the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers to the model (1 hidden layer as base model)\n",
        "\n",
        "# Input -> Hidden\n",
        "model.add(Dense(64, input_dim=13, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Hidden\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Hidden -> Output\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "history = model.fit(X_train, y_train, validation_split=0.2,\n",
        "                    epochs=100, batch_size=50)\n",
        "\n",
        "# Inspect architecture\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 181 samples, validate on 46 samples\n",
            "Epoch 1/100\n",
            "181/181 [==============================] - 1s 3ms/sample - loss: 0.6247 - acc: 0.6464 - val_loss: 0.6143 - val_acc: 0.6087\n",
            "Epoch 2/100\n",
            "181/181 [==============================] - 0s 92us/sample - loss: 0.4786 - acc: 0.7735 - val_loss: 0.5772 - val_acc: 0.7174\n",
            "Epoch 3/100\n",
            "181/181 [==============================] - 0s 67us/sample - loss: 0.4047 - acc: 0.7735 - val_loss: 0.5454 - val_acc: 0.7391\n",
            "Epoch 4/100\n",
            "181/181 [==============================] - 0s 78us/sample - loss: 0.3471 - acc: 0.8398 - val_loss: 0.5212 - val_acc: 0.7609\n",
            "Epoch 5/100\n",
            "181/181 [==============================] - 0s 94us/sample - loss: 0.3154 - acc: 0.8674 - val_loss: 0.5025 - val_acc: 0.7826\n",
            "Epoch 6/100\n",
            "181/181 [==============================] - 0s 81us/sample - loss: 0.2910 - acc: 0.8785 - val_loss: 0.4856 - val_acc: 0.8261\n",
            "Epoch 7/100\n",
            "181/181 [==============================] - 0s 75us/sample - loss: 0.2780 - acc: 0.8729 - val_loss: 0.4768 - val_acc: 0.8261\n",
            "Epoch 8/100\n",
            "181/181 [==============================] - 0s 75us/sample - loss: 0.2525 - acc: 0.9116 - val_loss: 0.4673 - val_acc: 0.8261\n",
            "Epoch 9/100\n",
            "181/181 [==============================] - 0s 65us/sample - loss: 0.2508 - acc: 0.8950 - val_loss: 0.4576 - val_acc: 0.8696\n",
            "Epoch 10/100\n",
            "181/181 [==============================] - 0s 87us/sample - loss: 0.2239 - acc: 0.9116 - val_loss: 0.4502 - val_acc: 0.8913\n",
            "Epoch 11/100\n",
            "181/181 [==============================] - 0s 69us/sample - loss: 0.2260 - acc: 0.9171 - val_loss: 0.4441 - val_acc: 0.8913\n",
            "Epoch 12/100\n",
            "181/181 [==============================] - 0s 68us/sample - loss: 0.2164 - acc: 0.9337 - val_loss: 0.4372 - val_acc: 0.8913\n",
            "Epoch 13/100\n",
            "181/181 [==============================] - 0s 72us/sample - loss: 0.1947 - acc: 0.9337 - val_loss: 0.4309 - val_acc: 0.8913\n",
            "Epoch 14/100\n",
            "181/181 [==============================] - 0s 76us/sample - loss: 0.1782 - acc: 0.9448 - val_loss: 0.4252 - val_acc: 0.8913\n",
            "Epoch 15/100\n",
            "181/181 [==============================] - 0s 76us/sample - loss: 0.1768 - acc: 0.9503 - val_loss: 0.4185 - val_acc: 0.9130\n",
            "Epoch 16/100\n",
            "181/181 [==============================] - 0s 76us/sample - loss: 0.1736 - acc: 0.9448 - val_loss: 0.4135 - val_acc: 0.9130\n",
            "Epoch 17/100\n",
            "181/181 [==============================] - 0s 74us/sample - loss: 0.1709 - acc: 0.9669 - val_loss: 0.4082 - val_acc: 0.9130\n",
            "Epoch 18/100\n",
            "181/181 [==============================] - 0s 73us/sample - loss: 0.1622 - acc: 0.9503 - val_loss: 0.4020 - val_acc: 0.9130\n",
            "Epoch 19/100\n",
            "181/181 [==============================] - 0s 77us/sample - loss: 0.1424 - acc: 0.9779 - val_loss: 0.3961 - val_acc: 0.9130\n",
            "Epoch 20/100\n",
            "181/181 [==============================] - 0s 89us/sample - loss: 0.1358 - acc: 0.9834 - val_loss: 0.3886 - val_acc: 0.9130\n",
            "Epoch 21/100\n",
            "181/181 [==============================] - 0s 67us/sample - loss: 0.1322 - acc: 0.9779 - val_loss: 0.3835 - val_acc: 0.9130\n",
            "Epoch 22/100\n",
            "181/181 [==============================] - 0s 86us/sample - loss: 0.1155 - acc: 0.9779 - val_loss: 0.3771 - val_acc: 0.9130\n",
            "Epoch 23/100\n",
            "181/181 [==============================] - 0s 82us/sample - loss: 0.1194 - acc: 0.9834 - val_loss: 0.3699 - val_acc: 0.9130\n",
            "Epoch 24/100\n",
            "181/181 [==============================] - 0s 65us/sample - loss: 0.1061 - acc: 0.9834 - val_loss: 0.3633 - val_acc: 0.9130\n",
            "Epoch 25/100\n",
            "181/181 [==============================] - 0s 66us/sample - loss: 0.1220 - acc: 0.9724 - val_loss: 0.3588 - val_acc: 0.9130\n",
            "Epoch 26/100\n",
            "181/181 [==============================] - 0s 78us/sample - loss: 0.0976 - acc: 0.9779 - val_loss: 0.3544 - val_acc: 0.9130\n",
            "Epoch 27/100\n",
            "181/181 [==============================] - 0s 80us/sample - loss: 0.1038 - acc: 0.9890 - val_loss: 0.3508 - val_acc: 0.9130\n",
            "Epoch 28/100\n",
            "181/181 [==============================] - 0s 79us/sample - loss: 0.1145 - acc: 0.9779 - val_loss: 0.3500 - val_acc: 0.9348\n",
            "Epoch 29/100\n",
            "181/181 [==============================] - 0s 81us/sample - loss: 0.1069 - acc: 0.9834 - val_loss: 0.3483 - val_acc: 0.9348\n",
            "Epoch 30/100\n",
            "181/181 [==============================] - 0s 74us/sample - loss: 0.0895 - acc: 0.9834 - val_loss: 0.3436 - val_acc: 0.9348\n",
            "Epoch 31/100\n",
            "181/181 [==============================] - 0s 75us/sample - loss: 0.0868 - acc: 0.9945 - val_loss: 0.3378 - val_acc: 0.9348\n",
            "Epoch 32/100\n",
            "181/181 [==============================] - 0s 64us/sample - loss: 0.0735 - acc: 0.9945 - val_loss: 0.3322 - val_acc: 0.9348\n",
            "Epoch 33/100\n",
            "181/181 [==============================] - 0s 66us/sample - loss: 0.0708 - acc: 1.0000 - val_loss: 0.3274 - val_acc: 0.9348\n",
            "Epoch 34/100\n",
            "181/181 [==============================] - 0s 71us/sample - loss: 0.0665 - acc: 0.9945 - val_loss: 0.3228 - val_acc: 0.9348\n",
            "Epoch 35/100\n",
            "181/181 [==============================] - 0s 68us/sample - loss: 0.0673 - acc: 0.9945 - val_loss: 0.3215 - val_acc: 0.9348\n",
            "Epoch 36/100\n",
            "181/181 [==============================] - 0s 71us/sample - loss: 0.0610 - acc: 0.9945 - val_loss: 0.3197 - val_acc: 0.9348\n",
            "Epoch 37/100\n",
            "181/181 [==============================] - 0s 73us/sample - loss: 0.0531 - acc: 1.0000 - val_loss: 0.3169 - val_acc: 0.9348\n",
            "Epoch 38/100\n",
            "181/181 [==============================] - 0s 72us/sample - loss: 0.0639 - acc: 0.9945 - val_loss: 0.3151 - val_acc: 0.9348\n",
            "Epoch 39/100\n",
            "181/181 [==============================] - 0s 89us/sample - loss: 0.0549 - acc: 0.9945 - val_loss: 0.3121 - val_acc: 0.9348\n",
            "Epoch 40/100\n",
            "181/181 [==============================] - 0s 76us/sample - loss: 0.0475 - acc: 1.0000 - val_loss: 0.3079 - val_acc: 0.9348\n",
            "Epoch 41/100\n",
            "181/181 [==============================] - 0s 94us/sample - loss: 0.0473 - acc: 1.0000 - val_loss: 0.3023 - val_acc: 0.9348\n",
            "Epoch 42/100\n",
            "181/181 [==============================] - 0s 73us/sample - loss: 0.0629 - acc: 0.9945 - val_loss: 0.2990 - val_acc: 0.9348\n",
            "Epoch 43/100\n",
            "181/181 [==============================] - 0s 71us/sample - loss: 0.0452 - acc: 0.9945 - val_loss: 0.2963 - val_acc: 0.9348\n",
            "Epoch 44/100\n",
            "181/181 [==============================] - 0s 70us/sample - loss: 0.0390 - acc: 1.0000 - val_loss: 0.2938 - val_acc: 0.9348\n",
            "Epoch 45/100\n",
            "181/181 [==============================] - 0s 69us/sample - loss: 0.0575 - acc: 0.9945 - val_loss: 0.2875 - val_acc: 0.9348\n",
            "Epoch 46/100\n",
            "181/181 [==============================] - 0s 136us/sample - loss: 0.0425 - acc: 1.0000 - val_loss: 0.2819 - val_acc: 0.9348\n",
            "Epoch 47/100\n",
            "181/181 [==============================] - 0s 87us/sample - loss: 0.0385 - acc: 0.9945 - val_loss: 0.2798 - val_acc: 0.9130\n",
            "Epoch 48/100\n",
            "181/181 [==============================] - 0s 69us/sample - loss: 0.0435 - acc: 1.0000 - val_loss: 0.2840 - val_acc: 0.9130\n",
            "Epoch 49/100\n",
            "181/181 [==============================] - 0s 72us/sample - loss: 0.0339 - acc: 1.0000 - val_loss: 0.2889 - val_acc: 0.9130\n",
            "Epoch 50/100\n",
            "181/181 [==============================] - 0s 78us/sample - loss: 0.0308 - acc: 1.0000 - val_loss: 0.2945 - val_acc: 0.9130\n",
            "Epoch 51/100\n",
            "181/181 [==============================] - 0s 76us/sample - loss: 0.0360 - acc: 1.0000 - val_loss: 0.2953 - val_acc: 0.9130\n",
            "Epoch 52/100\n",
            "181/181 [==============================] - 0s 73us/sample - loss: 0.0323 - acc: 1.0000 - val_loss: 0.2932 - val_acc: 0.9130\n",
            "Epoch 53/100\n",
            "181/181 [==============================] - 0s 70us/sample - loss: 0.0255 - acc: 1.0000 - val_loss: 0.2889 - val_acc: 0.9130\n",
            "Epoch 54/100\n",
            "181/181 [==============================] - 0s 70us/sample - loss: 0.0239 - acc: 1.0000 - val_loss: 0.2847 - val_acc: 0.9130\n",
            "Epoch 55/100\n",
            "181/181 [==============================] - 0s 71us/sample - loss: 0.0248 - acc: 1.0000 - val_loss: 0.2811 - val_acc: 0.9130\n",
            "Epoch 56/100\n",
            "181/181 [==============================] - 0s 75us/sample - loss: 0.0267 - acc: 1.0000 - val_loss: 0.2785 - val_acc: 0.9130\n",
            "Epoch 57/100\n",
            "181/181 [==============================] - 0s 111us/sample - loss: 0.0308 - acc: 1.0000 - val_loss: 0.2783 - val_acc: 0.9130\n",
            "Epoch 58/100\n",
            "181/181 [==============================] - 0s 80us/sample - loss: 0.0261 - acc: 1.0000 - val_loss: 0.2796 - val_acc: 0.9130\n",
            "Epoch 59/100\n",
            "181/181 [==============================] - 0s 79us/sample - loss: 0.0198 - acc: 1.0000 - val_loss: 0.2815 - val_acc: 0.9130\n",
            "Epoch 60/100\n",
            "181/181 [==============================] - 0s 94us/sample - loss: 0.0394 - acc: 0.9945 - val_loss: 0.2838 - val_acc: 0.9130\n",
            "Epoch 61/100\n",
            "181/181 [==============================] - 0s 70us/sample - loss: 0.0220 - acc: 1.0000 - val_loss: 0.2890 - val_acc: 0.9130\n",
            "Epoch 62/100\n",
            "181/181 [==============================] - 0s 67us/sample - loss: 0.0181 - acc: 1.0000 - val_loss: 0.2909 - val_acc: 0.9130\n",
            "Epoch 63/100\n",
            "181/181 [==============================] - 0s 69us/sample - loss: 0.0200 - acc: 1.0000 - val_loss: 0.2910 - val_acc: 0.9130\n",
            "Epoch 64/100\n",
            "181/181 [==============================] - 0s 64us/sample - loss: 0.0278 - acc: 1.0000 - val_loss: 0.2895 - val_acc: 0.9130\n",
            "Epoch 65/100\n",
            "181/181 [==============================] - 0s 73us/sample - loss: 0.0197 - acc: 1.0000 - val_loss: 0.2860 - val_acc: 0.9130\n",
            "Epoch 66/100\n",
            "181/181 [==============================] - 0s 73us/sample - loss: 0.0193 - acc: 1.0000 - val_loss: 0.2853 - val_acc: 0.9130\n",
            "Epoch 67/100\n",
            "181/181 [==============================] - 0s 69us/sample - loss: 0.0201 - acc: 1.0000 - val_loss: 0.2843 - val_acc: 0.9130\n",
            "Epoch 68/100\n",
            "181/181 [==============================] - 0s 91us/sample - loss: 0.0182 - acc: 1.0000 - val_loss: 0.2858 - val_acc: 0.9130\n",
            "Epoch 69/100\n",
            "181/181 [==============================] - 0s 66us/sample - loss: 0.0140 - acc: 1.0000 - val_loss: 0.2884 - val_acc: 0.9130\n",
            "Epoch 70/100\n",
            "181/181 [==============================] - 0s 79us/sample - loss: 0.0154 - acc: 1.0000 - val_loss: 0.2916 - val_acc: 0.9130\n",
            "Epoch 71/100\n",
            "181/181 [==============================] - 0s 89us/sample - loss: 0.0151 - acc: 1.0000 - val_loss: 0.2941 - val_acc: 0.9130\n",
            "Epoch 72/100\n",
            "181/181 [==============================] - 0s 69us/sample - loss: 0.0207 - acc: 1.0000 - val_loss: 0.2975 - val_acc: 0.9130\n",
            "Epoch 73/100\n",
            "181/181 [==============================] - 0s 68us/sample - loss: 0.0156 - acc: 1.0000 - val_loss: 0.2983 - val_acc: 0.9130\n",
            "Epoch 74/100\n",
            "181/181 [==============================] - 0s 76us/sample - loss: 0.0133 - acc: 1.0000 - val_loss: 0.2983 - val_acc: 0.9130\n",
            "Epoch 75/100\n",
            "181/181 [==============================] - 0s 93us/sample - loss: 0.0160 - acc: 1.0000 - val_loss: 0.3003 - val_acc: 0.9130\n",
            "Epoch 76/100\n",
            "181/181 [==============================] - 0s 95us/sample - loss: 0.0148 - acc: 1.0000 - val_loss: 0.2985 - val_acc: 0.9130\n",
            "Epoch 77/100\n",
            "181/181 [==============================] - 0s 95us/sample - loss: 0.0212 - acc: 1.0000 - val_loss: 0.2972 - val_acc: 0.9130\n",
            "Epoch 78/100\n",
            "181/181 [==============================] - 0s 92us/sample - loss: 0.0177 - acc: 1.0000 - val_loss: 0.2979 - val_acc: 0.9130\n",
            "Epoch 79/100\n",
            "181/181 [==============================] - 0s 78us/sample - loss: 0.0133 - acc: 1.0000 - val_loss: 0.3028 - val_acc: 0.9130\n",
            "Epoch 80/100\n",
            "181/181 [==============================] - 0s 89us/sample - loss: 0.0110 - acc: 1.0000 - val_loss: 0.3064 - val_acc: 0.9130\n",
            "Epoch 81/100\n",
            "181/181 [==============================] - 0s 78us/sample - loss: 0.0153 - acc: 1.0000 - val_loss: 0.3095 - val_acc: 0.9130\n",
            "Epoch 82/100\n",
            "181/181 [==============================] - 0s 70us/sample - loss: 0.0115 - acc: 1.0000 - val_loss: 0.3109 - val_acc: 0.9130\n",
            "Epoch 83/100\n",
            "181/181 [==============================] - 0s 74us/sample - loss: 0.0161 - acc: 1.0000 - val_loss: 0.3123 - val_acc: 0.9130\n",
            "Epoch 84/100\n",
            "181/181 [==============================] - 0s 75us/sample - loss: 0.0109 - acc: 1.0000 - val_loss: 0.3187 - val_acc: 0.8913\n",
            "Epoch 85/100\n",
            "181/181 [==============================] - 0s 79us/sample - loss: 0.0109 - acc: 1.0000 - val_loss: 0.3276 - val_acc: 0.8913\n",
            "Epoch 86/100\n",
            "181/181 [==============================] - 0s 75us/sample - loss: 0.0101 - acc: 1.0000 - val_loss: 0.3321 - val_acc: 0.8913\n",
            "Epoch 87/100\n",
            "181/181 [==============================] - 0s 71us/sample - loss: 0.0142 - acc: 1.0000 - val_loss: 0.3372 - val_acc: 0.8696\n",
            "Epoch 88/100\n",
            "181/181 [==============================] - 0s 99us/sample - loss: 0.0101 - acc: 1.0000 - val_loss: 0.3387 - val_acc: 0.8696\n",
            "Epoch 89/100\n",
            "181/181 [==============================] - 0s 71us/sample - loss: 0.0074 - acc: 1.0000 - val_loss: 0.3395 - val_acc: 0.8696\n",
            "Epoch 90/100\n",
            "181/181 [==============================] - 0s 83us/sample - loss: 0.0085 - acc: 1.0000 - val_loss: 0.3392 - val_acc: 0.8696\n",
            "Epoch 91/100\n",
            "181/181 [==============================] - 0s 77us/sample - loss: 0.0148 - acc: 1.0000 - val_loss: 0.3389 - val_acc: 0.8696\n",
            "Epoch 92/100\n",
            "181/181 [==============================] - 0s 70us/sample - loss: 0.0082 - acc: 1.0000 - val_loss: 0.3382 - val_acc: 0.8696\n",
            "Epoch 93/100\n",
            "181/181 [==============================] - 0s 80us/sample - loss: 0.0130 - acc: 1.0000 - val_loss: 0.3451 - val_acc: 0.8696\n",
            "Epoch 94/100\n",
            "181/181 [==============================] - 0s 78us/sample - loss: 0.0094 - acc: 1.0000 - val_loss: 0.3492 - val_acc: 0.8913\n",
            "Epoch 95/100\n",
            "181/181 [==============================] - 0s 76us/sample - loss: 0.0182 - acc: 1.0000 - val_loss: 0.3542 - val_acc: 0.8696\n",
            "Epoch 96/100\n",
            "181/181 [==============================] - 0s 70us/sample - loss: 0.0134 - acc: 1.0000 - val_loss: 0.3573 - val_acc: 0.8696\n",
            "Epoch 97/100\n",
            "181/181 [==============================] - 0s 82us/sample - loss: 0.0072 - acc: 1.0000 - val_loss: 0.3606 - val_acc: 0.8696\n",
            "Epoch 98/100\n",
            "181/181 [==============================] - 0s 69us/sample - loss: 0.0088 - acc: 1.0000 - val_loss: 0.3593 - val_acc: 0.8913\n",
            "Epoch 99/100\n",
            "181/181 [==============================] - 0s 71us/sample - loss: 0.0063 - acc: 1.0000 - val_loss: 0.3585 - val_acc: 0.8913\n",
            "Epoch 100/100\n",
            "181/181 [==============================] - 0s 68us/sample - loss: 0.0075 - acc: 1.0000 - val_loss: 0.3597 - val_acc: 0.8913\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                896       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 5,633\n",
            "Trainable params: 5,377\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxmyxE40m4Yh",
        "colab_type": "code",
        "outputId": "57aef29f-3c58-4df1-95cc-6279a231015b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "plt.show();"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbMwifqUm4Ry",
        "colab_type": "code",
        "outputId": "55e36483-446c-4755-8152-ec94d87e9c30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test accuracy is {scores[1]}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy is 0.7105262875556946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN50ToGxm4Mr",
        "colab_type": "code",
        "outputId": "93965275-11c8-4ca4-bdb6-f92666f5426c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "# GridSearch for accuracy\n",
        "inputs = X_train.shape[1]\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_shape=(inputs,)))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "param_grid = {'batch_size': [16, 32, 64],\n",
        "              'epochs': [20, 80]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.8590308427810669 using {'batch_size': 16, 'epochs': 20}\n",
            "Means: 0.8590308427810669, Stdev: 0.02185862701482951 with: {'batch_size': 16, 'epochs': 20}\n",
            "Means: 0.806167393266367, Stdev: 0.006910155139040082 with: {'batch_size': 16, 'epochs': 80}\n",
            "Means: 0.8458149890017405, Stdev: 0.005281195485131463 with: {'batch_size': 32, 'epochs': 20}\n",
            "Means: 0.8237885341770323, Stdev: 0.030119293470170134 with: {'batch_size': 32, 'epochs': 80}\n",
            "Means: 0.841409692155107, Stdev: 0.01774149856074191 with: {'batch_size': 64, 'epochs': 20}\n",
            "Means: 0.8061673985178776, Stdev: 0.021637560564110855 with: {'batch_size': 64, 'epochs': 80}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpTfJLwXszFj",
        "colab_type": "code",
        "outputId": "54606749-b173-4a47-87e6-09e78668aac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f'{grid_result.best_params_} {grid_result.best_score_}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_size': 16, 'epochs': 20} 0.8590308427810669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSkJMASwtchL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74516ebe-2ec1-4d9c-d450-1b4cb483c943"
      },
      "source": [
        "g_pred = grid.predict(X_test)\n",
        "accuracy_score(y_test, g_pred)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7894736842105263"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WC8NdWhTChg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}