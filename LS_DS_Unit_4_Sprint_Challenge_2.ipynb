{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Perceptron on XOR Gates](#Q2)\n",
    "3. [Multilayer Perceptron](#Q3)\n",
    "4. [Keras MMP](#Q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:** \n",
    "A neuron, or node, is the indivisible object that forms a neural network, akin to atoms within molecules (assuming atoms cannot be further divisible into sub-atomic particles). A neuron contains information pertaining to one specific feature in our dataset. For example, if we are dealing with pictures, a picture can be composed of 28x28 pixels (greyscale); were we to encode these pictures within our neural networks model, we would have to initialize our model with 784 neurons, since each neuron would hold information (for example: intensity, brightness or luminosity) necessary for our machines to understand what a picture means. \n",
    "\n",
    "\n",
    "- **Input Layer:**\n",
    "Input layer corresponds to the amalgamation of individual neurons that form our initial neural network layer.\n",
    "\n",
    "\n",
    "- **Hidden Layer:**\n",
    "The hidden layer is the layer, or layers, that lie between our input layer and our output layer. The word 'deep' from 'Deep Learning' stems from the idea that neural networks can contain multiple hidden layers. Neural networks are sometimes called black box models since it can become very difficult to interpret neurons individually, let alone neurons within neurons within multiple hidden layers; we know hidden layers can help us achieve higher accuracy by allowing us to further tweak our weights to activate certain neurons in the direction of our choosing. \n",
    "\n",
    "\n",
    "- **Output Layer:**\n",
    "Output layer is our final layer, depending on our machine learning problem at hand, the output layer is adjusted accordingly to reflect our final output; for example, regarding a regression problem our output would be a nx1 matrix.\n",
    "\n",
    "\n",
    "- **Activation:**\n",
    "Activation is what enables us to give meaning to our neurons; it not sufficient to solely pass on signals from our neurons to other neurons from subsequent layers just by calculating the sum of the product between our weights and out neuron inputs. For example, using the sigmoid function applied in logistic regression problems, we are able to squishify our results (X would be in the range of -1 and 1) and consequently pass on values in a range that can be interpretable, thus 'activating' our neurons. \n",
    "\n",
    "\n",
    "- **Backpropagation:**\n",
    "As the name implies, backpropagation consists of adjusting our errors in reverse; the idea here is that while we may not know exactly what each individual neuron within our many hidden layers may be doing, we do know that by identifying which neurons are more sensible to activation, and knowing which direction we'd like our neurons to activate towards, we are able to adjust our weights from output layer -> hidden layer  | hidden layer -> input layer, by using the derivative of the activation function (rate of change) and multiplying it by our error and applying it to our weights to see how far off our weight terms were. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perceptron on XOR Gates <a id=\"Q3=2\"></a>\n",
    "\n",
    "Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "|x1\t|x2|x3|\ty|\n",
    "|---|---|---|---|\n",
    "1|\t1|\t1|\t1|\n",
    "1|\t0|\t1|\t0|\n",
    "0|\t1|\t1|\t0|\n",
    "0|\t0|\t1|\t0|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [1, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 1],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "    [1], \n",
    "    [0], \n",
    "    [0], \n",
    "    [0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.7917965 ],\n",
       "       [ 0.11374332],\n",
       "       [-0.98208266]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = 2 * np.random.random((3,1)) - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights after training\n",
      "[[ 18.14159145]\n",
      " [ 18.14159145]\n",
      " [-27.50073592]]\n",
      "output after training\n",
      "[[9.99846621e-01]\n",
      " [8.61667372e-05]\n",
      " [8.61667372e-05]\n",
      " [1.13916794e-12]]\n"
     ]
    }
   ],
   "source": [
    "#iterating multiple times enables us adjust our errors, our weights, and our final activation output \n",
    "\n",
    "for i in range(1000):\n",
    "    weighted_sum = np.dot(X, weights)\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    error = y - activated_output\n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "    weights += np.dot(X.T, adjustments)\n",
    "    \n",
    "print('weights after training')\n",
    "print(weights)\n",
    "\n",
    "print('output after training')\n",
    "print(activated_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights.\n",
    "Your network must have one hidden layer.\n",
    "You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "Train your model on the Heart Disease dataset from UCI:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('target', axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[63.,  1.,  3., ...,  0.,  0.,  1.],\n",
       "       [37.,  1.,  2., ...,  0.,  0.,  2.],\n",
       "       [41.,  0.,  1., ...,  2.,  0.,  2.],\n",
       "       ...,\n",
       "       [68.,  1.,  0., ...,  1.,  2.,  3.],\n",
       "       [57.,  1.,  0., ...,  1.,  1.,  3.],\n",
       "       [57.,  0.,  1., ...,  1.,  1.,  2.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((303, 13), (303,))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.resize(303, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's scale our data by transforming mean=0 and variance=1\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network class borrowed from DS3 class notes\n",
    "#neural network class that implements from beginning (forward feed) to end (backward prop)\n",
    "\n",
    "class NeuralNetwork: \n",
    "    def __init__(self):\n",
    "        # Set up Architecture \n",
    "        self.inputs = 13\n",
    "        self.hiddenNodes = 2\n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        #Initial weights\n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes) #13x1\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes) #1x1\n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        \"\"\"\n",
    "        \n",
    "        #Weighted sume of inputs and hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        #Acivations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        #Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        #error in output\n",
    "        self.o_error = y - o \n",
    "        \n",
    "        # apply derivative of sigmoid to error\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o) \n",
    "        \n",
    "        # z2 error: how much our hidden layer weights were off\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        #Adjust first set (input => hidden) of weights\n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        \n",
    "        #adjust second set (hidden => output) of weights\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------EPOCH 1---------\n",
      "Loss: \n",
      " 0.2955039833554743\n",
      "---------EPOCH 5---------\n",
      "Loss: \n",
      " 0.1445848880286784\n",
      "---------EPOCH 20---------\n",
      "Loss: \n",
      " 0.09179568155170025\n",
      "---------EPOCH 50---------\n",
      "Loss: \n",
      " 0.08341488251011715\n",
      "---------EPOCH 100---------\n",
      "Loss: \n",
      " 0.08128590694678048\n",
      "---------EPOCH 500---------\n",
      "Loss: \n",
      " 0.0739358134425191\n",
      "---------EPOCH 1000---------\n",
      "Loss: \n",
      " 0.0731647939835647\n",
      "---------EPOCH 2000---------\n",
      "Loss: \n",
      " 0.06868466060760119\n",
      "---------EPOCH 4000---------\n",
      "Loss: \n",
      " 0.06832010212977495\n",
      "---------EPOCH 6000---------\n",
      "Loss: \n",
      " 0.0681943249704681\n",
      "---------EPOCH 8000---------\n",
      "Loss: \n",
      " 0.06812024466014227\n",
      "---------EPOCH 10000---------\n",
      "Loss: \n",
      " 0.06806938010662296\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "\n",
    "#intervals increase exponentially to make it easier to visualize decreases in loss \n",
    "#Our loss function below is simply our mean squared error\n",
    "\n",
    "for i in range(10000):\n",
    "    if (i+1 in [1, 5, 20, 50, 100, 500, 1000]) or ((i+1) % 2000 ==0):\n",
    "        print('---' * 3 + f'EPOCH {i+1}' + '---'*3)\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Keras MMP <a id=\"Q4\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 98.02%\n",
      "acc: 99.01%\n",
      "acc: 99.01%\n",
      "98.68% +/- 0.47%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "#k-fold in order to improve accuracy\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    \n",
    "  # create model\n",
    "  def create_model():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(13, input_dim=13, activation='relu'))\n",
    "        model.add(Dense(13, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  # compile model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model \n",
    "    \n",
    "  # in order to hyperparameter tune we need to pass in KerasClassifier\n",
    "  model_class = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "    \n",
    "\n",
    "  # fit the model\n",
    "  model.fit(X[train], y[train], epochs=150, batch_size=10, verbose=0)\n",
    "    \n",
    "  # evaluate the model\n",
    "  scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "  print(f'{model.metrics_names[1]}: {(scores[1]*100):.2f}%') \n",
    "  cvscores.append(scores[1]*100)\n",
    "print(f'{np.mean(cvscores):.2f}% +/- {np.std(cvscores):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.6831683119138082 using {'batch_size': 50, 'epochs': 10}\n",
      "Means: 0.6237623790899912, Stdev: 0.21555981664777982 with: {'batch_size': 20, 'epochs': 1}\n",
      "Means: 0.4257425864537557, Stdev: 0.3055970272720392 with: {'batch_size': 20, 'epochs': 10}\n",
      "Means: 0.6633663376172384, Stdev: 0.10321118913257638 with: {'batch_size': 20, 'epochs': 100}\n",
      "Means: 0.49834983547528583, Stdev: 0.22010999215576987 with: {'batch_size': 50, 'epochs': 1}\n",
      "Means: 0.6831683119138082, Stdev: 0.11461223986680608 with: {'batch_size': 50, 'epochs': 10}\n",
      "Means: 0.6369637052218119, Stdev: 0.09369682329296686 with: {'batch_size': 50, 'epochs': 100}\n",
      "Means: 0.5742574234803518, Stdev: 0.16228766996652716 with: {'batch_size': 100, 'epochs': 1}\n",
      "Means: 0.5577557782332102, Stdev: 0.1995540139622511 with: {'batch_size': 100, 'epochs': 10}\n",
      "Means: 0.6435643633206686, Stdev: 0.1244535055230195 with: {'batch_size': 100, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "#hyperparam tuning with gridsearch \n",
    "\n",
    "param_grid = {'batch_size': [20, 50, 100],\n",
    "              'epochs': [1, 10, 100]}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=model_class, param_grid=param_grid, n_jobs=1, verbose=0)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
