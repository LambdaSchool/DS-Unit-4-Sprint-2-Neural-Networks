{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Perceptron on XOR Gates](#Q2)\n",
    "3. [Multilayer Perceptron](#Q3)\n",
    "4. [Keras MMP](#Q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:**\n",
    "  * Neurons are the building blocks when talking about a neural network.  They can take on different \"shapes\" or forms depending on their purpose.  They are called neurons because a neural network got is loosely based on a brain. The idea is that the information that is fed into the neurons and weights associated with them will either make them fire or not to determine the final output of our classification or regression problem.\n",
    "- **Input Layer:**\n",
    "  * The input layer of a neural network is the initial layer. Where everything begins. It essentially is made up of the nodes that receive the data that will go through all the layers.  The amount of nodes or neurons that make up the input layer is largely determined by the amount of features you have in your data.\n",
    "- **Hidden Layer:**\n",
    "  * The hidden layers in a neural network is where a lot of the magic happens.  Taking the weights associated with the inputs, it usually is also coupled with some activation function that determines if the neuron fires or not. There can be a lot of hidden layers and a lot of neurons within each layer.  It depends on the problem you are trying to solve and the information you have to determine the amount. A neural network with many hidden layers is what people refer to when they talk about 'deep learning'.\n",
    "- **Output Layer:**\n",
    "  * The output layer is always the final layer of a neural network.  For binary or regression problem, there is usually only one neuron in the output layer.  If you are trying to solve a multiple classification problem, you may want multiple neurons in this layer.  Once the inputs have gone through the network, the last layer uses the weights and activation fucntion to determine the 'answer' of what you are trying to find.  For example, in a binary problem, it will output either a 1 or a 0, depending on what the network believes the input is according to the training.\n",
    "- **Activation:**\n",
    "  * An activation function is what determines if a neuron fires or not.  In the output layer, it determines whether an input is something or not something.  There are different types of activation functions, which can be more useful or not, depending on the problem at hand.  What an activation function does, is it normalizes the combination of the weights inputs and bias, and given a certain threshold, determines if a neuron will fire or not.  In a way, activation functions are the gate keepers of a neural network.\n",
    "- **Backpropagation:**\n",
    "  * Backpropagation is a technique that can be used to make a neural network learn and improve upon itself.  Given specific inputs and targets for those inputs. Once inputs go through a neural network, it calculates the amount of error, according to the true target output, and feeds that information back into the network to adjust the weights accordingly.  Using some optimization algorithm, most popularly Gradient Descent, it will adjust the weights until it finds some optimum result to minimize the final error. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perceptron on XOR Gates <a id=\"Q3=2\"></a>\n",
    "\n",
    "Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "|x1\t|x2|x3|\ty|\n",
    "|---|---|---|---|\n",
    "1|\t1|\t1|\t1|\n",
    "1|\t0|\t1|\t0|\n",
    "0|\t1|\t1|\t0|\n",
    "0|\t0|\t1|\t0|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([\n",
    "    [1,1,1],\n",
    "    [1,0,1],\n",
    "    [0,1,1],\n",
    "    [0,0,1]\n",
    "])\n",
    "\n",
    "y = [\n",
    "    [1],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0]\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(X.shape[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    def __init__(self, niter=1000):\n",
    "        self.niter = niter\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # weights\n",
    "        self.weight = 2 * np.random.random((3,1)) - 1\n",
    "    \n",
    "        for iteration in range(self.niter):\n",
    "            \n",
    "            self.weighted_sum = np.dot(X, self.weight)\n",
    "\n",
    "            self.activated_outputs = sigmoid(self.weighted_sum)\n",
    "\n",
    "            self.error = y - self.activated_outputs\n",
    "\n",
    "            self.adjustments = self.error * sigmoid_derivative(self.activated_outputs)\n",
    "\n",
    "            self.weight += np.dot(X.T, self.adjustments)\n",
    "\n",
    "        print(\"Weights after training: \", self.weight)\n",
    "        print(\"Output after training: \", self.activated_outputs)\n",
    "        return self\n",
    "    \n",
    "   \n",
    "    def sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    def sigmoid_derivative(self, X):\n",
    "        sx = sigmoid(X)\n",
    "        return sx * (1-sx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training:  [[  7.22463508]\n",
      " [  7.22463535]\n",
      " [-11.12512232]]\n",
      "Output after training:  [[9.65214673e-01]\n",
      " [1.98501452e-02]\n",
      " [1.98501504e-02]\n",
      " [1.47811523e-05]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Perceptron at 0x1a45232b70>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = Perceptron(1000)\n",
    "\n",
    "nn.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights.\n",
    "Your network must have one hidden layer.\n",
    "You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "Train your model on the Heart Disease dataset from UCI:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
       "       'exang', 'oldpeak', 'slope', 'ca', 'thal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['target']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.481132</td>\n",
       "      <td>0.244292</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.283105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.885496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.564516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.562500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.251142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.816794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.702290</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  sex        cp  trestbps      chol  fbs  restecg   thalach  exang  \\\n",
       "0  0.708333  1.0  1.000000  0.481132  0.244292  1.0      0.0  0.603053    0.0   \n",
       "1  0.166667  1.0  0.666667  0.339623  0.283105  0.0      0.5  0.885496    0.0   \n",
       "2  0.250000  0.0  0.333333  0.339623  0.178082  0.0      0.0  0.770992    0.0   \n",
       "3  0.562500  1.0  0.333333  0.245283  0.251142  0.0      0.5  0.816794    0.0   \n",
       "4  0.583333  0.0  0.000000  0.245283  0.520548  0.0      0.5  0.702290    1.0   \n",
       "\n",
       "    oldpeak  slope   ca      thal  target  \n",
       "0  0.370968    0.0  0.0  0.333333       1  \n",
       "1  0.564516    0.0  0.0  0.666667       1  \n",
       "2  0.225806    1.0  0.0  0.666667       1  \n",
       "3  0.129032    1.0  0.0  0.666667       1  \n",
       "4  0.096774    1.0  0.0  0.666667       1  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert numeric cols to floats and normalize:\n",
    "num_cols = df.drop(columns=['target']).columns\n",
    "scaler = MinMaxScaler()\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols].values)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      "age         303 non-null float32\n",
      "sex         303 non-null float32\n",
      "cp          303 non-null float32\n",
      "trestbps    303 non-null float32\n",
      "chol        303 non-null float32\n",
      "fbs         303 non-null float32\n",
      "restecg     303 non-null float32\n",
      "thalach     303 non-null float32\n",
      "exang       303 non-null float32\n",
      "oldpeak     303 non-null float32\n",
      "slope       303 non-null float32\n",
      "ca          303 non-null float32\n",
      "thal        303 non-null float32\n",
      "target      303 non-null float32\n",
      "dtypes: float32(14)\n",
      "memory usage: 16.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((203, 13), (203,), (100, 13), (100,))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.values\n",
    "x_test = x_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 13\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #Make empty list to store costs:\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 4.069501\n",
      "         Iterations: 200\n",
      "         Function evaluations: 233\n",
      "         Gradient evaluations: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in square\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "T = trainer(NN)\n",
    "\n",
    "T.train(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[0.39732664]\n",
      " [0.42027121]\n",
      " [0.35282868]\n",
      " [0.48522254]\n",
      " [0.38224776]\n",
      " [0.43770891]\n",
      " [0.43618112]\n",
      " [0.43337599]\n",
      " [0.3874501 ]\n",
      " [0.42466675]]\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[0.53368136]\n",
      " [0.52111132]\n",
      " [0.69570645]\n",
      " [0.51923808]\n",
      " [0.58391362]\n",
      " [0.54330918]\n",
      " [0.5243472 ]\n",
      " [0.53105981]\n",
      " [0.57295674]\n",
      " [0.63077824]]\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[0.50244927]\n",
      " [0.50804801]\n",
      " [0.52069316]\n",
      " [0.50098091]\n",
      " [0.50407521]\n",
      " [0.51685921]\n",
      " [0.50802254]\n",
      " [0.50782181]\n",
      " [0.50365904]\n",
      " [0.50196903]]\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[0.50850623]\n",
      " [0.52372654]\n",
      " [0.55173323]\n",
      " [0.50191452]\n",
      " [0.511918  ]\n",
      " [0.535251  ]\n",
      " [0.52060237]\n",
      " [0.51922974]\n",
      " [0.51074074]\n",
      " [0.50414982]]\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[0.62437467]\n",
      " [0.70750824]\n",
      " [0.77969338]\n",
      " [0.50879006]\n",
      " [0.62039962]\n",
      " [0.66096142]\n",
      " [0.64845764]\n",
      " [0.6294364 ]\n",
      " [0.61086454]\n",
      " [0.52257392]]\n",
      "+---------EPOCH 100---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[0.54122152]\n",
      " [0.99040842]\n",
      " [0.59978746]\n",
      " [0.50000131]\n",
      " [0.50070039]\n",
      " [0.95470823]\n",
      " [0.98449475]\n",
      " [0.97167317]\n",
      " [0.50587628]\n",
      " [0.5000136 ]]\n",
      "+---------EPOCH 200---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[0.59515195]\n",
      " [0.99243061]\n",
      " [0.87335153]\n",
      " [0.5000005 ]\n",
      " [0.50148579]\n",
      " [0.98467881]\n",
      " [0.99189119]\n",
      " [0.99155663]\n",
      " [0.53306421]\n",
      " [0.50000272]]\n",
      "+---------EPOCH 300---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[0.612552  ]\n",
      " [0.99743109]\n",
      " [0.94128321]\n",
      " [0.50000028]\n",
      " [0.50121699]\n",
      " [0.99576494]\n",
      " [0.99735127]\n",
      " [0.99729002]\n",
      " [0.53556602]\n",
      " [0.5000005 ]]\n",
      "+---------EPOCH 400---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[0.62022699]\n",
      " [0.998857  ]\n",
      " [0.96869742]\n",
      " [0.50000015]\n",
      " [0.50085957]\n",
      " [0.99830726]\n",
      " [0.9988391 ]\n",
      " [0.99881947]\n",
      " [0.53075144]\n",
      " [0.50000012]]\n",
      "+---------EPOCH 500---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[0.63954069]\n",
      " [0.99942412]\n",
      " [0.98466918]\n",
      " [0.50000009]\n",
      " [0.50068642]\n",
      " [0.99920403]\n",
      " [0.99941887]\n",
      " [0.99941163]\n",
      " [0.52981825]\n",
      " [0.50000004]]\n",
      "+---------EPOCH 600---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[0.66144645]\n",
      " [0.99968468]\n",
      " [0.99250913]\n",
      " [0.50000006]\n",
      " [0.50058714]\n",
      " [0.99958369]\n",
      " [0.99968281]\n",
      " [0.99967985]\n",
      " [0.53048867]\n",
      " [0.50000002]]\n",
      "+---------EPOCH 700---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[0.65854976]\n",
      " [0.99981134]\n",
      " [0.99508533]\n",
      " [0.50000004]\n",
      " [0.50044495]\n",
      " [0.99975471]\n",
      " [0.99981048]\n",
      " [0.999809  ]\n",
      " [0.52715863]\n",
      " [0.50000001]]\n",
      "+---------EPOCH 800---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[0.64958505]\n",
      " [0.99987928]\n",
      " [0.99643001]\n",
      " [0.50000003]\n",
      " [0.50033273]\n",
      " [0.99984445]\n",
      " [0.99987883]\n",
      " [0.99987801]\n",
      " [0.52349368]\n",
      " [0.5       ]]\n",
      "+---------EPOCH 900---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[0.64007807]\n",
      " [0.9999189 ]\n",
      " [0.99730716]\n",
      " [0.50000002]\n",
      " [0.50025229]\n",
      " [0.99989633]\n",
      " [0.99991866]\n",
      " [0.99991818]\n",
      " [0.52034782]\n",
      " [0.5       ]]\n",
      "+---------EPOCH 1000---------+\n",
      "Input: \n",
      " [[0.5208333  1.         0.6666667  ... 1.         0.         1.        ]\n",
      " [0.6041667  0.         0.6666667  ... 1.         0.         0.6666667 ]\n",
      " [0.375      1.         0.         ... 1.         0.         0.6666667 ]\n",
      " ...\n",
      " [0.8333333  1.         1.         ... 0.5        0.25       0.6666667 ]\n",
      " [0.35416666 1.         0.         ... 1.         0.         1.        ]\n",
      " [0.7083333  0.         0.33333334 ... 1.         0.5        0.6666667 ]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      " [[0.63015551]\n",
      " [0.99994347]\n",
      " [0.99790388]\n",
      " [0.50000001]\n",
      " [0.50019322]\n",
      " [0.99992822]\n",
      " [0.99994333]\n",
      " [0.99994304]\n",
      " [0.51762011]\n",
      " [0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    if (epoch+1 in [1,2,3,4,5]) or ((epoch+1) % 100 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {epoch+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', x_train)\n",
    "        print('Actual Output: \\n', y_train[:10])\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(x_train[:10])))\n",
    "    nn.train(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Keras MMP <a id=\"Q4\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203 samples, validate on 100 samples\n",
      "Epoch 1/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 5.9496 - acc: 0.4975 - val_loss: 5.1151 - val_acc: 0.5500\n",
      "Epoch 2/50\n",
      "203/203 [==============================] - 0s 179us/step - loss: 5.7530 - acc: 0.5074 - val_loss: 4.9585 - val_acc: 0.5800\n",
      "Epoch 3/50\n",
      "203/203 [==============================] - 0s 202us/step - loss: 5.4459 - acc: 0.4975 - val_loss: 4.7412 - val_acc: 0.4600\n",
      "Epoch 4/50\n",
      "203/203 [==============================] - 0s 170us/step - loss: 5.1208 - acc: 0.4483 - val_loss: 4.1662 - val_acc: 0.4700\n",
      "Epoch 5/50\n",
      "203/203 [==============================] - 0s 173us/step - loss: 4.5065 - acc: 0.5025 - val_loss: 3.4658 - val_acc: 0.5400\n",
      "Epoch 6/50\n",
      "203/203 [==============================] - 0s 166us/step - loss: 3.6545 - acc: 0.4828 - val_loss: 2.3150 - val_acc: 0.5100\n",
      "Epoch 7/50\n",
      "203/203 [==============================] - 0s 151us/step - loss: 2.1516 - acc: 0.5271 - val_loss: 1.0339 - val_acc: 0.7500\n",
      "Epoch 8/50\n",
      "203/203 [==============================] - 0s 151us/step - loss: 1.4604 - acc: 0.6059 - val_loss: 0.8315 - val_acc: 0.7200\n",
      "Epoch 9/50\n",
      "203/203 [==============================] - 0s 155us/step - loss: 1.3614 - acc: 0.6158 - val_loss: 0.7393 - val_acc: 0.7500\n",
      "Epoch 10/50\n",
      "203/203 [==============================] - 0s 147us/step - loss: 1.1408 - acc: 0.6158 - val_loss: 0.7504 - val_acc: 0.7300\n",
      "Epoch 11/50\n",
      "203/203 [==============================] - 0s 181us/step - loss: 1.1193 - acc: 0.5567 - val_loss: 0.6825 - val_acc: 0.7300\n",
      "Epoch 12/50\n",
      "203/203 [==============================] - 0s 188us/step - loss: 0.9393 - acc: 0.5961 - val_loss: 0.8335 - val_acc: 0.6500\n",
      "Epoch 13/50\n",
      "203/203 [==============================] - 0s 175us/step - loss: 0.9075 - acc: 0.5961 - val_loss: 0.6613 - val_acc: 0.7300\n",
      "Epoch 14/50\n",
      "203/203 [==============================] - 0s 167us/step - loss: 0.9337 - acc: 0.5764 - val_loss: 0.6704 - val_acc: 0.7200\n",
      "Epoch 15/50\n",
      "203/203 [==============================] - 0s 166us/step - loss: 0.9443 - acc: 0.6059 - val_loss: 0.7147 - val_acc: 0.6300\n",
      "Epoch 16/50\n",
      "203/203 [==============================] - 0s 169us/step - loss: 0.8902 - acc: 0.5665 - val_loss: 0.6937 - val_acc: 0.6500\n",
      "Epoch 17/50\n",
      "203/203 [==============================] - 0s 163us/step - loss: 0.8559 - acc: 0.5862 - val_loss: 0.6632 - val_acc: 0.6700\n",
      "Epoch 18/50\n",
      "203/203 [==============================] - 0s 159us/step - loss: 0.8501 - acc: 0.5813 - val_loss: 0.7298 - val_acc: 0.6200\n",
      "Epoch 19/50\n",
      "203/203 [==============================] - 0s 164us/step - loss: 0.8788 - acc: 0.6256 - val_loss: 0.6845 - val_acc: 0.7300\n",
      "Epoch 20/50\n",
      "203/203 [==============================] - 0s 155us/step - loss: 0.8554 - acc: 0.6305 - val_loss: 0.6940 - val_acc: 0.6400\n",
      "Epoch 21/50\n",
      "203/203 [==============================] - 0s 161us/step - loss: 0.8410 - acc: 0.6453 - val_loss: 0.7042 - val_acc: 0.6500\n",
      "Epoch 22/50\n",
      "203/203 [==============================] - 0s 150us/step - loss: 0.8459 - acc: 0.6059 - val_loss: 0.6191 - val_acc: 0.7100\n",
      "Epoch 23/50\n",
      "203/203 [==============================] - 0s 190us/step - loss: 0.8349 - acc: 0.5862 - val_loss: 0.6056 - val_acc: 0.7300\n",
      "Epoch 24/50\n",
      "203/203 [==============================] - 0s 163us/step - loss: 0.7736 - acc: 0.6158 - val_loss: 0.6134 - val_acc: 0.7500\n",
      "Epoch 25/50\n",
      "203/203 [==============================] - 0s 165us/step - loss: 0.7776 - acc: 0.6404 - val_loss: 0.6420 - val_acc: 0.6400\n",
      "Epoch 26/50\n",
      "203/203 [==============================] - 0s 166us/step - loss: 0.8145 - acc: 0.5961 - val_loss: 0.6093 - val_acc: 0.7400\n",
      "Epoch 27/50\n",
      "203/203 [==============================] - 0s 159us/step - loss: 0.7524 - acc: 0.6502 - val_loss: 0.5947 - val_acc: 0.7000\n",
      "Epoch 28/50\n",
      "203/203 [==============================] - 0s 165us/step - loss: 0.7687 - acc: 0.6256 - val_loss: 0.6110 - val_acc: 0.6400\n",
      "Epoch 29/50\n",
      "203/203 [==============================] - 0s 160us/step - loss: 0.7375 - acc: 0.6256 - val_loss: 0.6122 - val_acc: 0.6800\n",
      "Epoch 30/50\n",
      "203/203 [==============================] - 0s 156us/step - loss: 0.7329 - acc: 0.6404 - val_loss: 0.5892 - val_acc: 0.7500\n",
      "Epoch 31/50\n",
      "203/203 [==============================] - 0s 166us/step - loss: 0.7397 - acc: 0.6404 - val_loss: 0.6287 - val_acc: 0.6800\n",
      "Epoch 32/50\n",
      "203/203 [==============================] - 0s 156us/step - loss: 0.7269 - acc: 0.6601 - val_loss: 0.6987 - val_acc: 0.6100\n",
      "Epoch 33/50\n",
      "203/203 [==============================] - 0s 147us/step - loss: 0.7670 - acc: 0.6404 - val_loss: 0.6057 - val_acc: 0.6800\n",
      "Epoch 34/50\n",
      "203/203 [==============================] - 0s 156us/step - loss: 0.7472 - acc: 0.6355 - val_loss: 0.5894 - val_acc: 0.7500\n",
      "Epoch 35/50\n",
      "203/203 [==============================] - 0s 182us/step - loss: 0.6993 - acc: 0.6847 - val_loss: 0.5932 - val_acc: 0.7700\n",
      "Epoch 36/50\n",
      "203/203 [==============================] - 0s 187us/step - loss: 0.6981 - acc: 0.6650 - val_loss: 0.5794 - val_acc: 0.7200\n",
      "Epoch 37/50\n",
      "203/203 [==============================] - 0s 273us/step - loss: 0.6776 - acc: 0.6946 - val_loss: 0.6112 - val_acc: 0.6300\n",
      "Epoch 38/50\n",
      "203/203 [==============================] - 0s 312us/step - loss: 0.6729 - acc: 0.6749 - val_loss: 0.5980 - val_acc: 0.7300\n",
      "Epoch 39/50\n",
      "203/203 [==============================] - 0s 311us/step - loss: 0.6574 - acc: 0.6897 - val_loss: 0.6481 - val_acc: 0.6300\n",
      "Epoch 40/50\n",
      "203/203 [==============================] - 0s 268us/step - loss: 0.6497 - acc: 0.6700 - val_loss: 0.5902 - val_acc: 0.7200\n",
      "Epoch 41/50\n",
      "203/203 [==============================] - 0s 240us/step - loss: 0.6418 - acc: 0.6847 - val_loss: 0.5896 - val_acc: 0.7100\n",
      "Epoch 42/50\n",
      "203/203 [==============================] - 0s 253us/step - loss: 0.6652 - acc: 0.6700 - val_loss: 0.5822 - val_acc: 0.7200\n",
      "Epoch 43/50\n",
      "203/203 [==============================] - 0s 228us/step - loss: 0.6504 - acc: 0.7044 - val_loss: 0.6068 - val_acc: 0.6400\n",
      "Epoch 44/50\n",
      "203/203 [==============================] - 0s 286us/step - loss: 0.6852 - acc: 0.6404 - val_loss: 0.8700 - val_acc: 0.5900\n",
      "Epoch 45/50\n",
      "203/203 [==============================] - 0s 269us/step - loss: 0.6774 - acc: 0.6897 - val_loss: 0.5698 - val_acc: 0.7500\n",
      "Epoch 46/50\n",
      "203/203 [==============================] - 0s 262us/step - loss: 0.6291 - acc: 0.6847 - val_loss: 0.5671 - val_acc: 0.7100\n",
      "Epoch 47/50\n",
      "203/203 [==============================] - 0s 272us/step - loss: 0.6225 - acc: 0.7192 - val_loss: 0.6220 - val_acc: 0.6500\n",
      "Epoch 48/50\n",
      "203/203 [==============================] - 0s 248us/step - loss: 0.6229 - acc: 0.6847 - val_loss: 0.5577 - val_acc: 0.7600\n",
      "Epoch 49/50\n",
      "203/203 [==============================] - 0s 290us/step - loss: 0.6130 - acc: 0.7143 - val_loss: 0.5723 - val_acc: 0.7000\n",
      "Epoch 50/50\n",
      "203/203 [==============================] - 0s 251us/step - loss: 0.6185 - acc: 0.7192 - val_loss: 0.5604 - val_acc: 0.7100\n"
     ]
    }
   ],
   "source": [
    "# Keras imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create Model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape=(13,)))\n",
    "model.add(Dense(13, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Fit Model\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=50, batch_size=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypertune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.7635467933316537 using {'batch_size': 30, 'epochs': 100}\n",
      "Means: 0.47783251804084026, Stdev: 0.0365794108889656 with: {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.6206896606043641, Stdev: 0.09484876453405078 with: {'batch_size': 20, 'epochs': 50}\n",
      "Means: 0.6453202001273338, Stdev: 0.10790992719751503 with: {'batch_size': 20, 'epochs': 100}\n",
      "Means: 0.5073891675530984, Stdev: 0.06973419785597788 with: {'batch_size': 30, 'epochs': 20}\n",
      "Means: 0.5862068964049146, Stdev: 0.07660003001572495 with: {'batch_size': 30, 'epochs': 50}\n",
      "Means: 0.7635467933316537, Stdev: 0.11379559263822903 with: {'batch_size': 30, 'epochs': 100}\n",
      "Means: 0.41871921020775593, Stdev: 0.07020009313355102 with: {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.5221674938507268, Stdev: 0.03657940635525028 with: {'batch_size': 40, 'epochs': 50}\n",
      "Means: 0.47783250849822473, Stdev: 0.03657940777873878 with: {'batch_size': 40, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [20, 30, 40],\n",
    "              'epochs': [20, 50, 100]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
