{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:** A neuron is the fundamental building block of a neural network. It consists of a set of weights (used to generate a weighted sum of inputs) and an activation, or transfer, function (run on the aforementioned weighted sum to produce an output).\n",
    "\n",
    "- **Input Layer:** Neurons in the first, or input, layer of a neural network contain a number weights equal to the number of external inputs, bias term included.\n",
    "\n",
    "- **Hidden Layer:** Every neural network does not contain a hidden layer, although most do, and this is necessary for complete classification of problems that are not linearly separable. Nodes in a hidden layer contain a number of weights equal to the number of nodes in the previous layer, plus an optional bias term. ReLU activation functions are very commonly used in hidden layers.\n",
    "\n",
    "- **Output Layer:** Neurons in the output layer of a neural network contain a number of weights equal to the number of neurons in the preceding layer, plus an optional bias term. Many networks have only a single output neuron, although some may have more. \n",
    "\n",
    "- **Activation:** The activation function of a neuron transforms the weighted sum it calculates into an output (generally with some given set of properties). Common activation functions include the sigmoid and tanh functions, as well as the ReLU family of functions. The identity function can also be considered an activation function!\n",
    "\n",
    "- **Backpropagation:** Just as a neural network propagates information about its inputs *forward* through its layers, so it propagates information about the error as compared with a target output *backward* through those same layers. This allows the weights in those layers to be updated little by little, gradually reducing the error produced by the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility.\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Load data.\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data shape.\n",
    "candy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>gummy</th>\n",
       "      <th>ate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chocolate  gummy  ate\n",
       "0          0      1    1\n",
       "1          1      0    1\n",
       "2          0      1    1\n",
       "3          0      0    0\n",
       "4          1      1    0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview data.\n",
    "candy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2377\n",
       "1     141\n",
       "Name: ate, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Candies that are neither chocolate nor gummy.\n",
    "# Mostly not eaten (94.4%).\n",
    "candy[candy['chocolate'] + candy['gummy'] == 0]['ate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2360\n",
       "0     131\n",
       "Name: ate, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Candies that are gummy but not chocolate.\n",
    "# Mostly eaten (94.8%).\n",
    "candy[(candy['chocolate'] == 0) & (candy['gummy'] == 1)]['ate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2359\n",
       "0     130\n",
       "Name: ate, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Candies that are chocolate but not gummy.\n",
    "# Mostly eaten (94.8%).\n",
    "candy[(candy['chocolate'] == 1) & (candy['gummy'] == 0)]['ate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2362\n",
       "1     140\n",
       "Name: ate, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Candies that are both chocolate and gummy.\n",
    "# Mostly not eaten (94.4%).\n",
    "candy[candy['chocolate'] + candy['gummy'] == 2]['ate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     9458\n",
       "False     542\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not do any feature engineering (xor function).\n",
    "xor_candy = (candy['chocolate'] + candy['gummy']) % 2\n",
    "(xor_candy == candy['ate']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     0.9458\n",
       "False    0.0542\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not normalize any feature engineering (it's just moving\n",
    "# the decimal point anyway, since we have an even 10k examples.\n",
    "#\n",
    "# (Exclusive or accuracy 94.6%.)\n",
    "(xor_candy == candy['ate']).value_counts(normalize='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. You will not be able to achieve more than ~50% with the simple perceptron. Explain why you could not achieve a higher accuracy with the *simple perceptron* architecture, because it's possible to achieve ~95% accuracy on this dataset. Provide your answer in markdown (and *optional* data analysis code) after your perceptron implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Define input matrix and target output.\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy['ate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "  \"\"\"\n",
    "  The perceptron class from our module one assignment.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               activation='sigmoid',\n",
    "               learning_rate=0.01,\n",
    "               max_epochs=1000):\n",
    "    self.activation = activation\n",
    "    self.learning_rate = learning_rate\n",
    "    self.max_epochs = max_epochs\n",
    "    \n",
    "  def sigmoid(self, x):\n",
    "    \"\"\"\n",
    "    The sigmoid function.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "  def d_sigmoid(self, x):\n",
    "    \"\"\"\n",
    "    The derivative of the sigmoid function. For gradient descent.\n",
    "    \"\"\"\n",
    "    return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "\n",
    "  def predict(self, inputs):\n",
    "    \"\"\"\n",
    "    Add bias term; generate predictions.\n",
    "    \"\"\"\n",
    "    inputs = np.c_[np.ones(len(inputs)), inputs]\n",
    "    return self.sigmoid(np.dot(inputs, self.weights))\n",
    "\n",
    "  def predict_bias(self, inputs_with_bias):\n",
    "    \"\"\"\n",
    "    Generate predictions - used when the inputs already include a bias.\n",
    "    \"\"\"\n",
    "    return self.sigmoid(np.dot(inputs_with_bias, self.weights))\n",
    "\n",
    "  def train(self, inputs, target):\n",
    "    \"\"\"\n",
    "    Fit the perceptron to a given input dataset and target output.\n",
    "    \"\"\"\n",
    "    # Add bias term.\n",
    "    input = np.c_[np.ones(len(inputs)), inputs]\n",
    "\n",
    "    # Adjust dimensionality for single-column target - we don't want to type a\n",
    "    # bunch of nested brackets.\n",
    "    target = np.expand_dims(target, axis=1)\n",
    "\n",
    "    # Initialize random weights in the range (-1, 1).\n",
    "    self.weights = 2 * np.random.random((len(input[0]), 1)) - 1\n",
    "\n",
    "    for epoch in range(self.max_epochs):\n",
    "        prediction = self.predict_bias(input)\n",
    "        error = prediction - target\n",
    "        gradient = self.d_sigmoid(np.dot(input, self.weights))\n",
    "        adjustments = self.learning_rate * error * gradient\n",
    "        self.weights = self.weights - np.dot(input.T, adjustments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate perceptron.\n",
    "p = Perceptron(max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train perceptron model.\n",
    "p.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output predictions from the trained model.\n",
    "perceptron_predictions = np.rint(p.predict(X)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2518, 7482], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarize predictions (2518 not eaten, 7482 eaten).\n",
    "np.bincount(np.squeeze(perceptron_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Perceptron Accuracy\n",
    "\n",
    "Once you've trained your model, report your accuracy. You will not be able to achieve more than ~50% with the simple perceptron. Explain why you could not achieve a higher accuracy with the simple perceptron architecture, because it's possible to achieve ~95% accuracy on this dataset. Provide your answer in markdown (and optional data analysis code) after your perceptron implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7236"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Report simple perceptron accuracy.\n",
    "accuracy_score(y, perceptron_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Perceptron Accuracy (Explanation)\n",
    "\n",
    "A simple perceptron can only completely classify a linearly separable function. The exclusive or (xor) function, which this data approximates (94.6% accuracy), is not linearly separable.\n",
    "\n",
    "100% accuracy will not be achievable, however, even with a multilayer perceptron, because the data is not perfectly clean (it only _approximates_ the xor function - identical inputs do not always produce identical output).\n",
    "\n",
    "I have no idea what I've done wrong (or non-simple?) to get about 72% accuracy here, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    A simple multilayer perceptron, or feed-forward neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs=2, hiddenNodes=[3], outputNodes=1):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = inputs\n",
    "        self.hiddenNodes = hiddenNodes\n",
    "        self.outputNodes = outputNodes\n",
    "\n",
    "        # Initial Weights\n",
    "        if len(hiddenNodes) == 0:\n",
    "            self.weights = [np.random.rand(self.inputs, self.outputNodes)]\n",
    "        else:\n",
    "            self.weights = [np.random.rand(self.inputs, self.hiddenNodes[0])]\n",
    "            for i in range(1, len(hiddenNodes)):\n",
    "                self.weights.append(np.random.rand((hiddenNodes[i-1]), hiddenNodes[i]))\n",
    "            self.weights.append(np.random.rand(hiddenNodes[-1], outputNodes))\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        \"\"\"\n",
    "        The sigmoid function.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        \"\"\"\n",
    "        The derivative of the sigmoid function. For gradient descent.\n",
    "        \"\"\"\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"      \n",
    "        # Weighted sum of inputs => hidden layer\n",
    "        weighted_sum = np.dot(X, self.weights[0])\n",
    "        self.activated_outputs = [self.sigmoid(weighted_sum)]\n",
    "        for i in range(1, len(self.weights)):\n",
    "            weighted_sum = np.dot(self.activated_outputs[i-1], self.weights[i])\n",
    "            self.activated_outputs.append(self.sigmoid(weighted_sum))\n",
    "                \n",
    "        return self.activated_outputs[-1]\n",
    "        \n",
    "    def backward(self, X, y, o):\n",
    "        \"\"\"\n",
    "        Backward propagate error & updates through the network.\n",
    "        \"\"\"\n",
    "        # Error in Output\n",
    "        self.o_error = y - o\n",
    "        \n",
    "        # Apply Derivative of Sigmoid to error\n",
    "        # How far off are we in relation to the Sigmoid f(x) of the output\n",
    "        # ^- aka hidden => output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        z2_error = self.o_delta.dot(self.weights[-1].T)\n",
    "\n",
    "        # Adjustment to final set of weights (hidden => output)\n",
    "        if len(self.weights) == 1:\n",
    "            self.weights[0] += X.T.dot(self.o_delta)\n",
    "            return\n",
    "        \n",
    "        self.weights[-1] += self.activated_outputs[-2].T.dot(self.o_delta)\n",
    "        \n",
    "        # Hidden layer error\n",
    "        for i in reversed(range(2, len(self.weights))):\n",
    "            z2_delta = z2_error * self.sigmoidPrime(self.activated_outputs[i-1])\n",
    "            z2_error = z2_delta.dot(self.weights[i-1].T)\n",
    "        \n",
    "            # Adjustment to interior set of weights (hidden => hidden)\n",
    "            self.weights[i-1] += self.activated_outputs[i-2].T.dot(z2_delta)\n",
    "        \n",
    "        # Input layer error (if not same layer as output)\n",
    "        z2_delta = z2_error * self.sigmoidPrime(self.activated_outputs[0])\n",
    "        self.weights[0] += X.T.dot(z2_delta)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Run a single training epoch, one pass in each direction.\n",
    "        \"\"\"\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)\n",
    "\n",
    "    def fit(self, X, y,  max_epochs=100, print_epochs=[]):\n",
    "        \"\"\"\n",
    "        Fit the MLP to the given training input and target output.\n",
    "        \"\"\"\n",
    "        for i in range(max_epochs):\n",
    "            if (i+ 1 in print_epochs): \n",
    "                print('+' + '---' * 3 + f'EPOCH {i + 1}' + '---' * 3 + '+')\n",
    "                print('Input: \\n', X)\n",
    "                print('Actual Output: \\n', y)\n",
    "                print('Predicted Output: \\n', str(self.feed_forward(X)))\n",
    "                print(\"Loss: \\n\", str(np.mean(np.square(y - self.feed_forward(X)))))\n",
    "            self.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(2, [2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 100---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Loss: \n",
      " 0.2011500000031213\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(X, np.expand_dims(y, axis=1), 100, [100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Accuracy\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Report multilayer perceptron accuracy.\n",
    "accuracy_score(y, np.rint(mlp.feed_forward(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Accuracy (Explanation)\n",
    "\n",
    "The performance we're getting here is abysmal, worse than that of the simple perceptron. Perhaps we're getting stuck at a local minimum? Or perhaps the lack of a tunable learning rate is tripping us up? We're not using the ideal loss function for a binary classification problem either, but I doubt that alone is the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>306</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>134</td>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>261</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "134   41    0   1       126   306    0        1      163      0      0.0   \n",
       "250   51    1   0       140   298    0        1      122      1      4.2   \n",
       "55    52    1   1       134   201    0        1      158      0      0.8   \n",
       "255   45    1   0       142   309    0        0      147      1      0.0   \n",
       "277   57    1   1       124   261    0        1      141      0      0.3   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "134      2   0     2       1  \n",
       "250      1   3     3       0  \n",
       "55       2   1     2       1  \n",
       "255      1   3     3       0  \n",
       "277      2   0     3       0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
