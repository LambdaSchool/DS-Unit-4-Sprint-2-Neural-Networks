{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nteract": {
      "version": "0.14.3"
    },
    "colab": {
      "name": "LS_DS_Unit_4_Sprint_Challenge_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tortas/DS-Unit-4-Sprint-2-Neural-Networks/blob/master/LS_DS_Unit_4_Sprint_Challenge_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExHdmkjcC9RS",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2*\n",
        "\n",
        "# Sprint Challenge - Neural Network Foundations\n",
        "\n",
        "Table of Problems\n",
        "\n",
        "1. [Defining Neural Networks](#Q1)\n",
        "2. [Perceptron on XOR Gates](#Q2)\n",
        "3. [Multilayer Perceptron](#Q3)\n",
        "4. [Keras MMP](#Q4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsmHTtdrC9RX",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"Q1\"></a>\n",
        "## 1. Define the following terms:\n",
        "\n",
        "- **Neuron:**\n",
        "\n",
        "A single node in the network. \n",
        "\n",
        "- **Input Layer:**\n",
        "\n",
        "The features being fed into the neural network.\n",
        "\n",
        "- **Hidden Layer:**\n",
        "\n",
        "Intermediary nodes adjusted by weights.\n",
        "\n",
        "- **Output Layer:**\n",
        "\n",
        "Output is the prediction given from network.\n",
        "\n",
        "- **Activation:**\n",
        "\n",
        "Function that takes in a weighted sum and any biases and decides whether or not the neuron in question should be activated.\n",
        "\n",
        "- **Backpropagation:**\n",
        "\n",
        "Backpropagation algorithms are a family of methods used to efficiently train artificial neural networks (ANNs) following a gradient descent approach that exploits the chain rule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY-rH-czC9RZ",
        "colab_type": "text"
      },
      "source": [
        "## 2. Perceptron on XOR Gates <a id=\"Q3=2\"></a>\n",
        "\n",
        "Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
        "\n",
        "|x1\t|x2|x3|\ty|\n",
        "|---|---|---|---|\n",
        "1|\t1|\t1|\t1|\n",
        "1|\t0|\t1|\t0|\n",
        "0|\t1|\t1|\t0|\n",
        "0|\t0|\t1|\t0|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "uGJSo0kPC9Rd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Perceptron(object):\n",
        "  def __init__(self, rate = 0.01, niter = 10):\n",
        "    self.rate = rate\n",
        "    self.niter = niter\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    \"\"\"Fit training data\n",
        "    X : Training vectors, X.shape : [#samples, #features]\n",
        "    y : Target values, y.shape : [#samples]\n",
        "    \"\"\"\n",
        "\n",
        "    # weights\n",
        "    self.weight = np.zeros(1 + X.shape[1])\n",
        "\n",
        "    # Number of misclassifications\n",
        "    self.errors = []  # Number of misclassifications\n",
        "\n",
        "    for i in range(self.niter):\n",
        "      err = 0\n",
        "      for xi, target in zip(X, y):\n",
        "        delta_w = self.rate * (target - self.predict(xi))\n",
        "        self.weight[1:] += delta_w * xi\n",
        "        self.weight[0] += delta_w\n",
        "        err += int(delta_w != 0.0)\n",
        "      self.errors.append(err)\n",
        "    return self\n",
        "\n",
        "  def net_input(self, X):\n",
        "    \"\"\"Calculate net input\"\"\"\n",
        "    return np.dot(X, self.weight[1:]) + self.weight[0]\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"Return class label after unit step\"\"\"\n",
        "    return np.where(self.net_input(X) >= 0.0, 1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRGumdDeC9Ro",
        "colab_type": "text"
      },
      "source": [
        "## 3. Multilayer Perceptron <a id=\"Q3\"></a>\n",
        "\n",
        "Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights.\n",
        "Your network must have one hidden layer.\n",
        "You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
        "Train your model on the Heart Disease dataset from UCI:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "4acw0-oxC9Rr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self):\n",
        "    # Set up architecture\n",
        "    self.input = 3\n",
        "    self.hiddenNodes = 4\n",
        "    self.outputNodes = 1\n",
        "    \n",
        "    # Initial weights\n",
        "    # 3x4 matrix array for first layer\n",
        "    self.weights1 = np.random.randn(self.input, self.hiddenNodes)\n",
        "    # 4x1 matrix for hidden to output\n",
        "    self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
        "    \n",
        "  def sigmoid(self, s):\n",
        "    return 1 / (1+np.exp(-s))\n",
        "  \n",
        "  def sigmoidPrime(self, s):\n",
        "    return s * (1-s)\n",
        "  \n",
        "  def feed_forward(self,X):\n",
        "    \"\"\"\n",
        "    Calculate the NN inference using feed forward\n",
        "    \"\"\"\n",
        "    \n",
        "    # Weighted sum of inputs & hidden\n",
        "    self.hidden_sum = np.dot(X, self.weights1)\n",
        "    \n",
        "    # Activations of weighted sum\n",
        "    self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "    \n",
        "    # Weighted sum between hidden and output\n",
        "    self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
        "    \n",
        "    # Final activation of output\n",
        "    self.activated_output = self.sigmoid(self.output_sum)\n",
        "    \n",
        "    return self.activated_output\n",
        "  \n",
        "  def backward(self, X, y, o):\n",
        "    \"\"\"\n",
        "    Backward propogate through the network\n",
        "    \"\"\"\n",
        "    self.o_error = y - o #error in output\n",
        "    self.o_delta = self.o_error * self.sigmoidPrime(o) #apply derivative of sigmoid\n",
        "    \n",
        "    self.z2_error = self.o_delta.dot(self.weights2.T)\n",
        "    self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)\n",
        "    \n",
        "    self.weights1 += X.T.dot(self.z2_delta)\n",
        "    self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
        "    \n",
        "  def train(self, X, y):\n",
        "    o = self.feed_forward(X)\n",
        "    self.backward(X, y, o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbw3tnF9C9Rx",
        "colab_type": "text"
      },
      "source": [
        "## 4. Keras MMP <a id=\"Q4\"></a>\n",
        "\n",
        "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
        "Use the Heart Disease Dataset (binary classification)\n",
        "Use an appropriate loss function for a binary classification task\n",
        "Use an appropriate activation function on the final layer of your network.\n",
        "Train your model using verbose output for ease of grading.\n",
        "Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
        "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
        "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
        "You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "272ivMltC9Ry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}