{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_433_Keras_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DimaKav/DS-Unit-4-Sprint-3-Neural-Networks/blob/master/module3-Intro-to-Keras/LS_DS_433_Keras_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBQsZEJmubLs",
        "colab_type": "text"
      },
      "source": [
        "## Use the Keras Library to build a Multi-Layer Perceptron Model on the Boston Housing dataset\n",
        "\n",
        "- The Boston Housing dataset comes with the Keras library so use Keras to import it into your notebook. \n",
        "- Normalize the data (all features should have roughly the same scale)\n",
        "- Import the type of model and layers that you will need from Keras.\n",
        "- Instantiate a model object and use `model.add()` to add layers to your model\n",
        "- Since this is a regression model you will have a single output node in the final layer.\n",
        "- Use activation functions that are appropriate for this task\n",
        "- Compile your model\n",
        "- Fit your model and report its accuracy in terms of Mean Squared Error\n",
        "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
        "- Run this same data through a linear regression model. Which achieves higher accuracy?\n",
        "- Do a little bit of feature engineering and see how that affects your neural network model. (you will need to change your model to accept more inputs)\n",
        "- After feature engineering, which model sees a greater accuracy boost due to the new features?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri35azDdmR_j",
        "colab_type": "text"
      },
      "source": [
        "# Scope out the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NLTAR87uYJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.datasets import boston_housing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL2pf6STkxMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train),(X_test,y_test)=boston_housing.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZh0F6KfkzMK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bca5cbef-2654-4af2-d66d-079c25ef41c0"
      },
      "source": [
        "X_train[1], y_train[1]"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2.1770e-02, 8.2500e+01, 2.0300e+00, 0.0000e+00, 4.1500e-01,\n",
              "        7.6100e+00, 1.5700e+01, 6.2700e+00, 2.0000e+00, 3.4800e+02,\n",
              "        1.4700e+01, 3.9538e+02, 3.1100e+00]), 42.3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK7nw9Aofcrq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0071685f-682a-4ae4-c972-81f636acea78"
      },
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((404, 13), (404,), (102, 13), (102,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmkwDpr7iUzV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ae741eb-6c74-420f-da61-e523cabc8299"
      },
      "source": [
        "X_train.dtype"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSxqancnk-kt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "column_names=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX',\n",
        "              'PTRATIO','B','LSTAT']\n",
        "              \n",
        "key= ['Per capita crime rate.',\n",
        "    'The proportion of residential land zoned for lots over 25,000\\\n",
        "     square feet.',\n",
        "    'The proportion of non-retail business acres per town.',\n",
        "    'Charles River dummy variable (=1 if tract bounds river; 0\\\n",
        "     otherwise).',\n",
        "    'Nitric oxides concentration (parts per 10 million).',\n",
        "    'The average number of rooms per dwelling.',\n",
        "    'The porportion of owner-occupied units built before 1940.',\n",
        "    'Weighted distances to five Boston employment centers.',\n",
        "    'Index of accessibility to radial highways.',\n",
        "    'Full-value property tax rate per $10,000.',\n",
        "    'Pupil-Teacher ratio by town.',\n",
        "    '1000*(Bk-0.63)**2 where Bk is the proportion of Black people by\\\n",
        "     town.',\n",
        "    'Percentage lower status of the population.']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm1XQqhRk-iJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2b064a97-b17b-427d-d21a-44be7ef139d0"
      },
      "source": [
        "import pandas as pd\n",
        "df= pd.DataFrame(X_train, columns=column_names)\n",
        "df.head()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.23247</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.142</td>\n",
              "      <td>91.7</td>\n",
              "      <td>3.9769</td>\n",
              "      <td>4.0</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>18.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02177</td>\n",
              "      <td>82.5</td>\n",
              "      <td>2.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.415</td>\n",
              "      <td>7.610</td>\n",
              "      <td>15.7</td>\n",
              "      <td>6.2700</td>\n",
              "      <td>2.0</td>\n",
              "      <td>348.0</td>\n",
              "      <td>14.7</td>\n",
              "      <td>395.38</td>\n",
              "      <td>3.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.89822</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.631</td>\n",
              "      <td>4.970</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.3325</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>375.52</td>\n",
              "      <td>3.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.19</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.515</td>\n",
              "      <td>6.037</td>\n",
              "      <td>34.5</td>\n",
              "      <td>5.9853</td>\n",
              "      <td>5.0</td>\n",
              "      <td>224.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>8.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.69311</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.713</td>\n",
              "      <td>6.376</td>\n",
              "      <td>88.4</td>\n",
              "      <td>2.5671</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>391.43</td>\n",
              "      <td>14.65</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS   RAD    TAX  \\\n",
              "0  1.23247   0.0   8.14   0.0  0.538  6.142   91.7  3.9769   4.0  307.0   \n",
              "1  0.02177  82.5   2.03   0.0  0.415  7.610   15.7  6.2700   2.0  348.0   \n",
              "2  4.89822   0.0  18.10   0.0  0.631  4.970  100.0  1.3325  24.0  666.0   \n",
              "3  0.03961   0.0   5.19   0.0  0.515  6.037   34.5  5.9853   5.0  224.0   \n",
              "4  3.69311   0.0  18.10   0.0  0.713  6.376   88.4  2.5671  24.0  666.0   \n",
              "\n",
              "   PTRATIO       B  LSTAT  \n",
              "0     21.0  396.90  18.72  \n",
              "1     14.7  395.38   3.11  \n",
              "2     20.2  375.52   3.26  \n",
              "3     20.2  396.90   8.01  \n",
              "4     20.2  391.43  14.65  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7-sckPbD9NV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUCDxHBUk-dJ",
        "colab_type": "text"
      },
      "source": [
        "# Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88GZwe9p6uQn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fbeccbc8-42ba-4720-b150-88cd62a6de0e"
      },
      "source": [
        "mean = X_train.mean(axis=0)\n",
        "std = X_train.std(axis=0)\n",
        "X_train = (X_train - mean)/std\n",
        "X_test = (X_test - mean)/std\n",
        "print(x_train[0])"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.27224633 -0.48361547 -0.43576161 -0.25683275 -0.1652266  -0.1764426\n",
            "  0.81306188  0.1166983  -0.62624905 -0.59517003  1.14850044  0.44807713\n",
            "  0.8252202 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCVfmolfEBXe",
        "colab_type": "text"
      },
      "source": [
        "#### Linear regression first..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EuU_ZJsEEhc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b444fd9c-d7d1-4c7d-dff4-7628975b830b"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred = lr.predict(X_test)\n",
        "mean_squared_error(y_test, y_pred)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23.195599256423"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY2MmDJnE9v3",
        "colab_type": "text"
      },
      "source": [
        "Should not be hard to beast this score..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V36h0_9L6uNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "model= Sequential()\n",
        "model.add(Dense(26, activation='relu',input_shape=(13,)))\n",
        "model.add(Dense(26, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLvvSZXe6uDW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "f5285817-df57-4604-c354-ace595164541"
      },
      "source": [
        "from keras import optimizers\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "              loss='mse', metrics=['mae'])\n",
        "model.summary()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 26)                364       \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 26)                702       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 12)                324       \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 13        \n",
            "=================================================================\n",
            "Total params: 1,403\n",
            "Trainable params: 1,403\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgupgzN67u_S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3471
        },
        "outputId": "e2b088e9-ed57-4fc7-9117-95c742ec2cb0"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=100, validation_split=.1)\n",
        "scores = model.evaluate(X_test, y_test)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 363 samples, validate on 41 samples\n",
            "Epoch 1/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 559.3300 - mean_absolute_error: 21.7249 - val_loss: 454.1082 - val_mean_absolute_error: 20.2988\n",
            "Epoch 2/100\n",
            "363/363 [==============================] - 0s 57us/step - loss: 519.3410 - mean_absolute_error: 20.8331 - val_loss: 413.6596 - val_mean_absolute_error: 19.2965\n",
            "Epoch 3/100\n",
            "363/363 [==============================] - 0s 52us/step - loss: 466.1051 - mean_absolute_error: 19.6154 - val_loss: 356.1211 - val_mean_absolute_error: 17.7789\n",
            "Epoch 4/100\n",
            "363/363 [==============================] - 0s 52us/step - loss: 400.0661 - mean_absolute_error: 17.9490 - val_loss: 291.6543 - val_mean_absolute_error: 15.9165\n",
            "Epoch 5/100\n",
            "363/363 [==============================] - 0s 50us/step - loss: 321.6712 - mean_absolute_error: 15.8134 - val_loss: 216.1832 - val_mean_absolute_error: 13.5057\n",
            "Epoch 6/100\n",
            "363/363 [==============================] - 0s 51us/step - loss: 236.7825 - mean_absolute_error: 13.1963 - val_loss: 141.1629 - val_mean_absolute_error: 10.4782\n",
            "Epoch 7/100\n",
            "363/363 [==============================] - 0s 52us/step - loss: 159.0119 - mean_absolute_error: 10.2426 - val_loss: 84.5858 - val_mean_absolute_error: 7.6460\n",
            "Epoch 8/100\n",
            "363/363 [==============================] - 0s 53us/step - loss: 105.4772 - mean_absolute_error: 7.8777 - val_loss: 54.9437 - val_mean_absolute_error: 6.3282\n",
            "Epoch 9/100\n",
            "363/363 [==============================] - 0s 50us/step - loss: 75.8853 - mean_absolute_error: 6.5791 - val_loss: 42.7651 - val_mean_absolute_error: 5.4379\n",
            "Epoch 10/100\n",
            "363/363 [==============================] - 0s 50us/step - loss: 60.0169 - mean_absolute_error: 5.8535 - val_loss: 35.5638 - val_mean_absolute_error: 4.9462\n",
            "Epoch 11/100\n",
            "363/363 [==============================] - 0s 52us/step - loss: 48.4345 - mean_absolute_error: 5.1956 - val_loss: 29.1292 - val_mean_absolute_error: 4.4002\n",
            "Epoch 12/100\n",
            "363/363 [==============================] - 0s 54us/step - loss: 39.4884 - mean_absolute_error: 4.6428 - val_loss: 28.5413 - val_mean_absolute_error: 4.3630\n",
            "Epoch 13/100\n",
            "363/363 [==============================] - 0s 51us/step - loss: 34.2196 - mean_absolute_error: 4.3182 - val_loss: 24.0032 - val_mean_absolute_error: 3.9532\n",
            "Epoch 14/100\n",
            "363/363 [==============================] - 0s 54us/step - loss: 30.0213 - mean_absolute_error: 4.0005 - val_loss: 22.9491 - val_mean_absolute_error: 3.8409\n",
            "Epoch 15/100\n",
            "363/363 [==============================] - 0s 50us/step - loss: 26.8744 - mean_absolute_error: 3.7571 - val_loss: 21.3446 - val_mean_absolute_error: 3.6092\n",
            "Epoch 16/100\n",
            "363/363 [==============================] - 0s 49us/step - loss: 24.6404 - mean_absolute_error: 3.5705 - val_loss: 20.7033 - val_mean_absolute_error: 3.4974\n",
            "Epoch 17/100\n",
            "363/363 [==============================] - 0s 53us/step - loss: 22.9767 - mean_absolute_error: 3.3885 - val_loss: 20.2614 - val_mean_absolute_error: 3.3309\n",
            "Epoch 18/100\n",
            "363/363 [==============================] - 0s 47us/step - loss: 21.3664 - mean_absolute_error: 3.3157 - val_loss: 20.6718 - val_mean_absolute_error: 3.3761\n",
            "Epoch 19/100\n",
            "363/363 [==============================] - 0s 52us/step - loss: 20.1784 - mean_absolute_error: 3.1956 - val_loss: 21.9927 - val_mean_absolute_error: 3.5053\n",
            "Epoch 20/100\n",
            "363/363 [==============================] - 0s 56us/step - loss: 19.0970 - mean_absolute_error: 3.1518 - val_loss: 19.9210 - val_mean_absolute_error: 3.2853\n",
            "Epoch 21/100\n",
            "363/363 [==============================] - 0s 49us/step - loss: 18.5160 - mean_absolute_error: 3.0935 - val_loss: 18.2241 - val_mean_absolute_error: 3.0658\n",
            "Epoch 22/100\n",
            "363/363 [==============================] - 0s 60us/step - loss: 17.6715 - mean_absolute_error: 2.9669 - val_loss: 18.8198 - val_mean_absolute_error: 3.1453\n",
            "Epoch 23/100\n",
            "363/363 [==============================] - 0s 49us/step - loss: 17.0187 - mean_absolute_error: 2.9077 - val_loss: 19.9994 - val_mean_absolute_error: 3.2248\n",
            "Epoch 24/100\n",
            "363/363 [==============================] - 0s 51us/step - loss: 16.3496 - mean_absolute_error: 2.8642 - val_loss: 18.6297 - val_mean_absolute_error: 3.1301\n",
            "Epoch 25/100\n",
            "363/363 [==============================] - 0s 52us/step - loss: 15.6793 - mean_absolute_error: 2.8089 - val_loss: 16.8216 - val_mean_absolute_error: 2.8702\n",
            "Epoch 26/100\n",
            "363/363 [==============================] - 0s 47us/step - loss: 15.2415 - mean_absolute_error: 2.7331 - val_loss: 17.0363 - val_mean_absolute_error: 2.9742\n",
            "Epoch 27/100\n",
            "363/363 [==============================] - 0s 49us/step - loss: 14.4486 - mean_absolute_error: 2.6205 - val_loss: 17.8882 - val_mean_absolute_error: 3.1778\n",
            "Epoch 28/100\n",
            "363/363 [==============================] - 0s 48us/step - loss: 14.3853 - mean_absolute_error: 2.6710 - val_loss: 15.9371 - val_mean_absolute_error: 2.9095\n",
            "Epoch 29/100\n",
            "363/363 [==============================] - 0s 53us/step - loss: 13.9485 - mean_absolute_error: 2.5907 - val_loss: 16.5060 - val_mean_absolute_error: 2.9749\n",
            "Epoch 30/100\n",
            "363/363 [==============================] - 0s 49us/step - loss: 13.4550 - mean_absolute_error: 2.5670 - val_loss: 17.4108 - val_mean_absolute_error: 3.1507\n",
            "Epoch 31/100\n",
            "363/363 [==============================] - 0s 52us/step - loss: 13.1709 - mean_absolute_error: 2.5472 - val_loss: 15.0406 - val_mean_absolute_error: 2.7805\n",
            "Epoch 32/100\n",
            "363/363 [==============================] - 0s 52us/step - loss: 12.8655 - mean_absolute_error: 2.5281 - val_loss: 14.1125 - val_mean_absolute_error: 2.6725\n",
            "Epoch 33/100\n",
            "363/363 [==============================] - 0s 63us/step - loss: 12.3957 - mean_absolute_error: 2.4976 - val_loss: 13.7744 - val_mean_absolute_error: 2.7055\n",
            "Epoch 34/100\n",
            "363/363 [==============================] - 0s 48us/step - loss: 12.4211 - mean_absolute_error: 2.4475 - val_loss: 12.7977 - val_mean_absolute_error: 2.5712\n",
            "Epoch 35/100\n",
            "363/363 [==============================] - 0s 66us/step - loss: 12.2156 - mean_absolute_error: 2.4461 - val_loss: 13.0654 - val_mean_absolute_error: 2.6207\n",
            "Epoch 36/100\n",
            "363/363 [==============================] - 0s 54us/step - loss: 11.8008 - mean_absolute_error: 2.4207 - val_loss: 14.3213 - val_mean_absolute_error: 2.9668\n",
            "Epoch 37/100\n",
            "363/363 [==============================] - 0s 50us/step - loss: 11.5121 - mean_absolute_error: 2.4019 - val_loss: 12.5618 - val_mean_absolute_error: 2.7086\n",
            "Epoch 38/100\n",
            "363/363 [==============================] - 0s 50us/step - loss: 11.3870 - mean_absolute_error: 2.3493 - val_loss: 11.8195 - val_mean_absolute_error: 2.4832\n",
            "Epoch 39/100\n",
            "363/363 [==============================] - 0s 53us/step - loss: 11.3414 - mean_absolute_error: 2.3436 - val_loss: 12.7374 - val_mean_absolute_error: 2.7341\n",
            "Epoch 40/100\n",
            "363/363 [==============================] - 0s 52us/step - loss: 10.9328 - mean_absolute_error: 2.3096 - val_loss: 11.6532 - val_mean_absolute_error: 2.6055\n",
            "Epoch 41/100\n",
            "363/363 [==============================] - 0s 49us/step - loss: 10.7688 - mean_absolute_error: 2.2782 - val_loss: 11.6563 - val_mean_absolute_error: 2.6351\n",
            "Epoch 42/100\n",
            "363/363 [==============================] - 0s 49us/step - loss: 10.5804 - mean_absolute_error: 2.2240 - val_loss: 11.2701 - val_mean_absolute_error: 2.4380\n",
            "Epoch 43/100\n",
            "363/363 [==============================] - 0s 51us/step - loss: 10.5257 - mean_absolute_error: 2.3217 - val_loss: 11.2682 - val_mean_absolute_error: 2.6790\n",
            "Epoch 44/100\n",
            "363/363 [==============================] - 0s 50us/step - loss: 10.4689 - mean_absolute_error: 2.2463 - val_loss: 11.1197 - val_mean_absolute_error: 2.5929\n",
            "Epoch 45/100\n",
            "363/363 [==============================] - 0s 58us/step - loss: 10.0725 - mean_absolute_error: 2.1868 - val_loss: 10.6383 - val_mean_absolute_error: 2.4955\n",
            "Epoch 46/100\n",
            "363/363 [==============================] - 0s 57us/step - loss: 10.1640 - mean_absolute_error: 2.2117 - val_loss: 10.5307 - val_mean_absolute_error: 2.5339\n",
            "Epoch 47/100\n",
            "363/363 [==============================] - 0s 60us/step - loss: 10.1295 - mean_absolute_error: 2.1784 - val_loss: 10.6724 - val_mean_absolute_error: 2.4769\n",
            "Epoch 48/100\n",
            "363/363 [==============================] - 0s 56us/step - loss: 9.8739 - mean_absolute_error: 2.1657 - val_loss: 12.6948 - val_mean_absolute_error: 2.7156\n",
            "Epoch 49/100\n",
            "363/363 [==============================] - 0s 52us/step - loss: 9.6826 - mean_absolute_error: 2.2061 - val_loss: 10.3159 - val_mean_absolute_error: 2.5108\n",
            "Epoch 50/100\n",
            "363/363 [==============================] - 0s 48us/step - loss: 9.6897 - mean_absolute_error: 2.1153 - val_loss: 10.4239 - val_mean_absolute_error: 2.5853\n",
            "Epoch 51/100\n",
            "363/363 [==============================] - 0s 54us/step - loss: 9.7065 - mean_absolute_error: 2.1483 - val_loss: 10.4149 - val_mean_absolute_error: 2.5646\n",
            "Epoch 52/100\n",
            "363/363 [==============================] - 0s 53us/step - loss: 9.5704 - mean_absolute_error: 2.1312 - val_loss: 10.7122 - val_mean_absolute_error: 2.5321\n",
            "Epoch 53/100\n",
            "363/363 [==============================] - 0s 58us/step - loss: 9.4760 - mean_absolute_error: 2.1157 - val_loss: 12.1548 - val_mean_absolute_error: 2.7375\n",
            "Epoch 54/100\n",
            "363/363 [==============================] - 0s 57us/step - loss: 9.2602 - mean_absolute_error: 2.0950 - val_loss: 10.5926 - val_mean_absolute_error: 2.6142\n",
            "Epoch 55/100\n",
            "363/363 [==============================] - 0s 61us/step - loss: 9.0919 - mean_absolute_error: 2.0723 - val_loss: 11.4057 - val_mean_absolute_error: 2.5245\n",
            "Epoch 56/100\n",
            "363/363 [==============================] - 0s 56us/step - loss: 9.2976 - mean_absolute_error: 2.0926 - val_loss: 10.1564 - val_mean_absolute_error: 2.3554\n",
            "Epoch 57/100\n",
            "363/363 [==============================] - 0s 58us/step - loss: 9.0057 - mean_absolute_error: 2.0718 - val_loss: 10.4116 - val_mean_absolute_error: 2.5322\n",
            "Epoch 58/100\n",
            "363/363 [==============================] - 0s 67us/step - loss: 8.9983 - mean_absolute_error: 2.0579 - val_loss: 9.9917 - val_mean_absolute_error: 2.5221\n",
            "Epoch 59/100\n",
            "363/363 [==============================] - 0s 57us/step - loss: 8.8223 - mean_absolute_error: 2.0506 - val_loss: 10.2014 - val_mean_absolute_error: 2.4155\n",
            "Epoch 60/100\n",
            "363/363 [==============================] - 0s 51us/step - loss: 9.1519 - mean_absolute_error: 2.0771 - val_loss: 9.9831 - val_mean_absolute_error: 2.3574\n",
            "Epoch 61/100\n",
            "363/363 [==============================] - 0s 52us/step - loss: 9.0196 - mean_absolute_error: 2.0472 - val_loss: 10.0234 - val_mean_absolute_error: 2.4296\n",
            "Epoch 62/100\n",
            "363/363 [==============================] - 0s 60us/step - loss: 8.7807 - mean_absolute_error: 2.0184 - val_loss: 10.1186 - val_mean_absolute_error: 2.5921\n",
            "Epoch 63/100\n",
            "363/363 [==============================] - 0s 60us/step - loss: 8.7039 - mean_absolute_error: 2.0318 - val_loss: 10.6826 - val_mean_absolute_error: 2.6993\n",
            "Epoch 64/100\n",
            "363/363 [==============================] - 0s 53us/step - loss: 8.5988 - mean_absolute_error: 2.0219 - val_loss: 9.4539 - val_mean_absolute_error: 2.4197\n",
            "Epoch 65/100\n",
            "363/363 [==============================] - 0s 58us/step - loss: 8.5883 - mean_absolute_error: 2.0211 - val_loss: 10.9604 - val_mean_absolute_error: 2.6935\n",
            "Epoch 66/100\n",
            "363/363 [==============================] - 0s 51us/step - loss: 8.4621 - mean_absolute_error: 2.0078 - val_loss: 9.6095 - val_mean_absolute_error: 2.5386\n",
            "Epoch 67/100\n",
            "363/363 [==============================] - 0s 54us/step - loss: 8.4086 - mean_absolute_error: 1.9915 - val_loss: 9.3641 - val_mean_absolute_error: 2.4150\n",
            "Epoch 68/100\n",
            "363/363 [==============================] - 0s 55us/step - loss: 8.5450 - mean_absolute_error: 1.9895 - val_loss: 8.9214 - val_mean_absolute_error: 2.2882\n",
            "Epoch 69/100\n",
            "363/363 [==============================] - 0s 52us/step - loss: 8.3411 - mean_absolute_error: 1.9972 - val_loss: 10.9906 - val_mean_absolute_error: 2.6691\n",
            "Epoch 70/100\n",
            "363/363 [==============================] - 0s 49us/step - loss: 8.2844 - mean_absolute_error: 1.9641 - val_loss: 10.2101 - val_mean_absolute_error: 2.6059\n",
            "Epoch 71/100\n",
            "363/363 [==============================] - 0s 53us/step - loss: 8.1972 - mean_absolute_error: 1.9991 - val_loss: 9.1590 - val_mean_absolute_error: 2.3738\n",
            "Epoch 72/100\n",
            "363/363 [==============================] - 0s 49us/step - loss: 8.3406 - mean_absolute_error: 1.9675 - val_loss: 9.0334 - val_mean_absolute_error: 2.2606\n",
            "Epoch 73/100\n",
            "363/363 [==============================] - 0s 50us/step - loss: 8.0036 - mean_absolute_error: 1.9532 - val_loss: 8.9770 - val_mean_absolute_error: 2.2471\n",
            "Epoch 74/100\n",
            "363/363 [==============================] - 0s 49us/step - loss: 8.0009 - mean_absolute_error: 1.9398 - val_loss: 9.9109 - val_mean_absolute_error: 2.6085\n",
            "Epoch 75/100\n",
            "363/363 [==============================] - 0s 46us/step - loss: 8.2528 - mean_absolute_error: 1.9750 - val_loss: 9.1595 - val_mean_absolute_error: 2.4312\n",
            "Epoch 76/100\n",
            "363/363 [==============================] - 0s 42us/step - loss: 8.0182 - mean_absolute_error: 1.9413 - val_loss: 9.1994 - val_mean_absolute_error: 2.2553\n",
            "Epoch 77/100\n",
            "363/363 [==============================] - 0s 74us/step - loss: 7.9587 - mean_absolute_error: 1.9322 - val_loss: 9.8680 - val_mean_absolute_error: 2.4096\n",
            "Epoch 78/100\n",
            "363/363 [==============================] - 0s 48us/step - loss: 7.8736 - mean_absolute_error: 1.9529 - val_loss: 9.1070 - val_mean_absolute_error: 2.2403\n",
            "Epoch 79/100\n",
            "363/363 [==============================] - 0s 48us/step - loss: 7.9030 - mean_absolute_error: 1.9322 - val_loss: 9.0020 - val_mean_absolute_error: 2.3526\n",
            "Epoch 80/100\n",
            "363/363 [==============================] - 0s 62us/step - loss: 7.9273 - mean_absolute_error: 1.9368 - val_loss: 8.7947 - val_mean_absolute_error: 2.3290\n",
            "Epoch 81/100\n",
            "363/363 [==============================] - 0s 54us/step - loss: 7.8158 - mean_absolute_error: 1.9193 - val_loss: 8.9786 - val_mean_absolute_error: 2.3651\n",
            "Epoch 82/100\n",
            "363/363 [==============================] - 0s 55us/step - loss: 7.8014 - mean_absolute_error: 1.9354 - val_loss: 9.2943 - val_mean_absolute_error: 2.3993\n",
            "Epoch 83/100\n",
            "363/363 [==============================] - 0s 52us/step - loss: 7.8569 - mean_absolute_error: 1.9094 - val_loss: 8.9322 - val_mean_absolute_error: 2.3006\n",
            "Epoch 84/100\n",
            "363/363 [==============================] - 0s 57us/step - loss: 7.9560 - mean_absolute_error: 1.9654 - val_loss: 8.6553 - val_mean_absolute_error: 2.2841\n",
            "Epoch 85/100\n",
            "363/363 [==============================] - 0s 51us/step - loss: 7.6835 - mean_absolute_error: 1.9226 - val_loss: 8.7265 - val_mean_absolute_error: 2.3126\n",
            "Epoch 86/100\n",
            "363/363 [==============================] - 0s 52us/step - loss: 7.8331 - mean_absolute_error: 1.9501 - val_loss: 8.5391 - val_mean_absolute_error: 2.3300\n",
            "Epoch 87/100\n",
            "363/363 [==============================] - 0s 49us/step - loss: 7.5398 - mean_absolute_error: 1.8653 - val_loss: 8.4162 - val_mean_absolute_error: 2.2435\n",
            "Epoch 88/100\n",
            "363/363 [==============================] - 0s 51us/step - loss: 7.4855 - mean_absolute_error: 1.8793 - val_loss: 9.8332 - val_mean_absolute_error: 2.6914\n",
            "Epoch 89/100\n",
            "363/363 [==============================] - 0s 48us/step - loss: 7.4872 - mean_absolute_error: 1.8544 - val_loss: 8.4518 - val_mean_absolute_error: 2.2586\n",
            "Epoch 90/100\n",
            "363/363 [==============================] - 0s 49us/step - loss: 7.5140 - mean_absolute_error: 1.8726 - val_loss: 8.5997 - val_mean_absolute_error: 2.3894\n",
            "Epoch 91/100\n",
            "363/363 [==============================] - 0s 61us/step - loss: 7.4844 - mean_absolute_error: 1.8963 - val_loss: 8.8673 - val_mean_absolute_error: 2.4875\n",
            "Epoch 92/100\n",
            "363/363 [==============================] - 0s 51us/step - loss: 7.4097 - mean_absolute_error: 1.8536 - val_loss: 8.5726 - val_mean_absolute_error: 2.3230\n",
            "Epoch 93/100\n",
            "363/363 [==============================] - 0s 50us/step - loss: 7.4400 - mean_absolute_error: 1.8711 - val_loss: 8.3063 - val_mean_absolute_error: 2.3275\n",
            "Epoch 94/100\n",
            "363/363 [==============================] - 0s 47us/step - loss: 7.3312 - mean_absolute_error: 1.8634 - val_loss: 8.2465 - val_mean_absolute_error: 2.2898\n",
            "Epoch 95/100\n",
            "363/363 [==============================] - 0s 56us/step - loss: 7.2888 - mean_absolute_error: 1.8615 - val_loss: 8.5377 - val_mean_absolute_error: 2.3062\n",
            "Epoch 96/100\n",
            "363/363 [==============================] - 0s 51us/step - loss: 7.1667 - mean_absolute_error: 1.8787 - val_loss: 8.7052 - val_mean_absolute_error: 2.4484\n",
            "Epoch 97/100\n",
            "363/363 [==============================] - 0s 61us/step - loss: 7.3192 - mean_absolute_error: 1.8513 - val_loss: 8.3704 - val_mean_absolute_error: 2.3614\n",
            "Epoch 98/100\n",
            "363/363 [==============================] - 0s 59us/step - loss: 7.1321 - mean_absolute_error: 1.8225 - val_loss: 8.6300 - val_mean_absolute_error: 2.3981\n",
            "Epoch 99/100\n",
            "363/363 [==============================] - 0s 56us/step - loss: 7.1402 - mean_absolute_error: 1.8238 - val_loss: 9.2111 - val_mean_absolute_error: 2.5002\n",
            "Epoch 100/100\n",
            "363/363 [==============================] - 0s 59us/step - loss: 7.1359 - mean_absolute_error: 1.8541 - val_loss: 8.9321 - val_mean_absolute_error: 2.5085\n",
            "102/102 [==============================] - 0s 49us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO8z4HXy7xPm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "1078da66-e796-4f78-b1c6-3ee89b240f8e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.plot(history.epoch, np.array(history.history['loss']))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f3cbe1b7e80>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nR8OCAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iglJBIn67xT_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "839716a1-7bff-440f-b465-4c4de14788ea"
      },
      "source": [
        "# Predict some housing prices using data from the test set\n",
        "\n",
        "test_predictions = model.predict(X_test).flatten()\n",
        "\n",
        "plt.scatter(y_test, test_predictions)\n",
        "plt.xlabel('True values')\n",
        "plt.ylabel('Predictions')\n",
        "plt.axis('equal')\n",
        "plt.xlim(plt.xlim())\n",
        "plt.ylim(plt.ylim())\n",
        "_ = plt.plot([-100,100],[-100,100])"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBFpcUoEIiIt7v8DevT9BF3CzZoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STPqN9767xX3",
        "colab_type": "text"
      },
      "source": [
        "The model seems to capture the general trend in the data. Let's plot a histogram that shows the distribution of our prediction errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vZqbGuNBGCF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "250aa4bf-618e-4a48-8422-9c6bdb1679af"
      },
      "source": [
        "error = test_predictions - y_test\n",
        "plt.hist(error, bins=50)\n",
        "plt.xlabel(\"Prediction Error\")\n",
        "_ = plt.ylabel('Count')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSY0zCCSpcQaBJDXOIJCkxhkEktS4/weBoaDE8p7RmAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GH-Ajds4TK8",
        "colab_type": "text"
      },
      "source": [
        "# Cross validate the model\n",
        "There isn't very much data so repeated sub-sampling via cross validation over all observations (train+test) needs to happen. Here, we train the model on four random split and test it on the fifth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo0fUU1muNrS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41fb6d55-63fe-49d9-f471-fc493b62074e"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from keras.datasets import boston_housing\n",
        "\n",
        "(X_train,y_train),(X_test,y_test) = boston_housing.load_data()\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((404, 13), (102, 13))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4JPLUpSw6SY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dde8802b-8b8c-4263-cb89-1ef08044a9e8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X_train = np.concatenate((X_train,X_test), axis=0)\n",
        "y_train = np.concatenate((y_train,y_test), axis=0)\n",
        "\n",
        "X_train.shape, y_train.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((506, 13), (506,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR--T5iHw6Pk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseline_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(13, input_dim=13, kernel_initializer='normal', \n",
        "              activation='relu'))\n",
        "    model.add(Dense(1, kernel_initializer='normal'))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2Tdqq5ow6M7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "1d945561-c4af-4ec3-b676-7551b9f62ba8"
      },
      "source": [
        "# Initialize random number generator with a constant random seed\n",
        "# to ensure consistency in initializing model weights and the model\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Add scaler and Keras regressor with the model\n",
        "\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model,   \n",
        "                    epochs=100, batch_size=5, verbose=0)))\n",
        "    \n",
        "# Add estimator list to a Sklearn pipeline\n",
        "\n",
        "pipeline = Pipeline(estimators)\n",
        " \n",
        "# Initialize instance of k-fold validation from sklearn api\n",
        "\n",
        "kfold = KFold(n_splits=5, random_state=42)\n",
        "\n",
        "# Pass pipeline instance, training data and labels, and k-fold crossvalidator instance to evaluate score\n",
        "\n",
        "results = cross_val_score(pipeline, X_train, y_train, cv=kfold)\n",
        "print(\"Average MSE of all 5 runs: %.2f, with standard dev: (%.2f)\" %   \n",
        "      (-1*(results.mean()), results.std()))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Average MSE of all 5 runs: 14.96, with standard dev: (2.56)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfcFnOONyuNm",
        "colab_type": "text"
      },
      "source": [
        "## Use the Keras Library to build an image recognition network using the Fashion-MNIST dataset (also comes with keras)\n",
        "\n",
        "- Load and preprocess the image data similar to how we preprocessed the MNIST data in class.\n",
        "- Make sure to one-hot encode your category labels\n",
        "- Make sure to have your final layer have as many nodes as the number of classes that you want to predict.\n",
        "- Try different hyperparameters. What is the highest accuracy that you are able to achieve.\n",
        "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
        "- Remember that neural networks fall prey to randomness so you may need to run your model multiple times (or use Cross Validation) in order to tell if a change to a hyperparameter is truly producing better results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSAuKYAjuMtZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06a641d4-6d07-49df-ade7-7c2d517f4470"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout \n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szi6-IpuzaH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train),(X_test,y_test)=fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PgImNU8FsAN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "605b166e-ce12-44f6-b350-40111f9ab5e1"
      },
      "source": [
        "X_train.shape, X_test.shape, y_train.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (10000, 28, 28), (60000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oCIbm1UHNm_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9c91f21-3968-4adf-9316-991323479e91"
      },
      "source": [
        "len(np.unique(y_train))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxpF8CZkro7a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0e3cd6d-fa85-4f24-8206-84c1e2343ce5"
      },
      "source": [
        "X_train.dtype"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('uint8')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUnf1tiwrDpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape the 3d arrays and cast as float\n",
        "\n",
        "X_train = X_train.reshape(60000, 784).astype('float32') # 28 * 28 = 784\n",
        "X_test = X_test.reshape(10000, 784).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-5A21Wyr4K2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2cc8a2c8-9165-4514-b30f-4a84e2b3bd48"
      },
      "source": [
        "pd.DataFrame(y_train)[0].value_counts()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9    6000\n",
              "8    6000\n",
              "7    6000\n",
              "6    6000\n",
              "5    6000\n",
              "4    6000\n",
              "3    6000\n",
              "2    6000\n",
              "1    6000\n",
              "0    6000\n",
              "Name: 0, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-xJh0bqtvmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob8Kmeukr4IM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "07fef4cd-d545-4af3-b44d-afc9b8422670"
      },
      "source": [
        "# One hot encode the category labels to a binary class matrix\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "y_train[:10]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8AKu5uNr4Fj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scale the data\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "X_train = MinMaxScaler().fit_transform(X_train)\n",
        "X_test = MinMaxScaler().fit_transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQhueZ0nveYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "26f4f729-d5c8-4e79-ec01-ba77785a2d74"
      },
      "source": [
        "pd.DataFrame(X_train).sample(5)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>34420</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027559</td>\n",
              "      <td>0.011765</td>\n",
              "      <td>...</td>\n",
              "      <td>0.149020</td>\n",
              "      <td>0.125490</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.043137</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23303</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013043</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.074510</td>\n",
              "      <td>0.043137</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003922</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38419</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006098</td>\n",
              "      <td>0.004464</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.074510</td>\n",
              "      <td>...</td>\n",
              "      <td>0.407843</td>\n",
              "      <td>0.435294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007843</td>\n",
              "      <td>0.003922</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49670</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007874</td>\n",
              "      <td>0.007843</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>0.752941</td>\n",
              "      <td>0.392157</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43133</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004525</td>\n",
              "      <td>0.003937</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0    1    2         3         4         5    6         7         8    \\\n",
              "34420  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.000000  0.027559   \n",
              "23303  0.0  0.0  0.0  0.000000  0.000000  0.013043  0.0  0.000000  0.000000   \n",
              "38419  0.0  0.0  0.0  0.006098  0.004464  0.000000  0.0  0.000000  0.000000   \n",
              "49670  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.000000  0.007874   \n",
              "43133  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.004525  0.003937   \n",
              "\n",
              "            9    ...       774       775       776       777       778  \\\n",
              "34420  0.011765  ...  0.149020  0.125490  0.133333  0.043137  0.000000   \n",
              "23303  0.000000  ...  0.074510  0.043137  0.000000  0.000000  0.000000   \n",
              "38419  0.074510  ...  0.407843  0.435294  0.000000  0.000000  0.007843   \n",
              "49670  0.007843  ...  0.000000  0.000000  0.000000  0.705882  0.752941   \n",
              "43133  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "\n",
              "            779  780  781  782  783  \n",
              "34420  0.000000  0.0  0.0  0.0  0.0  \n",
              "23303  0.003922  0.0  0.0  0.0  0.0  \n",
              "38419  0.003922  0.0  0.0  0.0  0.0  \n",
              "49670  0.392157  0.0  0.0  0.0  0.0  \n",
              "43133  0.000000  0.0  0.0  0.0  0.0  \n",
              "\n",
              "[5 rows x 784 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZfPZ9GSvjqM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "24e5edf8-1844-417a-a491-2d1602232d8f"
      },
      "source": [
        "mnist_model = Sequential()\n",
        "mnist_model.add(Dense(16, input_shape=(784,), activation='relu'))  # input\n",
        "\n",
        "mnist_model.add(Dropout(0.1))\n",
        "\n",
        "mnist_model.add(Dense(16, activation='sigmoid'))  # hidden\n",
        "\n",
        "mnist_model.add(Dropout(0.1))\n",
        "\n",
        "mnist_model.add(Dense(num_classes, activation='softmax'))  # output\n",
        "mnist_model.compile(loss='binary_crossentropy', optimizer='adadelta',\n",
        "                    metrics=['accuracy'])\n",
        "mnist_model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_11 (Dense)             (None, 16)                12560     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 10)                170       \n",
            "=================================================================\n",
            "Total params: 13,002\n",
            "Trainable params: 13,002\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6MrMSI0vjn6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "3a8756d9-cad1-4f80-ff52-562fef334dec"
      },
      "source": [
        "history = mnist_model.fit(X_train, y_train, epochs=epochs, validation_split=.1)\n",
        "scores = mnist_model.evaluate(X_test, y_test)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54000 samples, validate on 6000 samples\n",
            "Epoch 1/20\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.1748 - acc: 0.9323 - val_loss: 0.1035 - val_acc: 0.9609\n",
            "Epoch 2/20\n",
            "54000/54000 [==============================] - 3s 56us/step - loss: 0.1111 - acc: 0.9569 - val_loss: 0.0880 - val_acc: 0.9663\n",
            "Epoch 3/20\n",
            "54000/54000 [==============================] - 3s 56us/step - loss: 0.0995 - acc: 0.9613 - val_loss: 0.0807 - val_acc: 0.9677\n",
            "Epoch 4/20\n",
            "54000/54000 [==============================] - 3s 56us/step - loss: 0.0941 - acc: 0.9634 - val_loss: 0.0779 - val_acc: 0.9692\n",
            "Epoch 5/20\n",
            "54000/54000 [==============================] - 3s 57us/step - loss: 0.0916 - acc: 0.9645 - val_loss: 0.0758 - val_acc: 0.9703\n",
            "Epoch 6/20\n",
            "54000/54000 [==============================] - 3s 58us/step - loss: 0.0887 - acc: 0.9657 - val_loss: 0.0751 - val_acc: 0.9708\n",
            "Epoch 7/20\n",
            "54000/54000 [==============================] - 3s 56us/step - loss: 0.0873 - acc: 0.9662 - val_loss: 0.0740 - val_acc: 0.9708\n",
            "Epoch 8/20\n",
            "54000/54000 [==============================] - 3s 56us/step - loss: 0.0857 - acc: 0.9667 - val_loss: 0.0728 - val_acc: 0.9716\n",
            "Epoch 9/20\n",
            "54000/54000 [==============================] - 3s 57us/step - loss: 0.0844 - acc: 0.9674 - val_loss: 0.0733 - val_acc: 0.9711\n",
            "Epoch 10/20\n",
            "54000/54000 [==============================] - 3s 57us/step - loss: 0.0833 - acc: 0.9676 - val_loss: 0.0733 - val_acc: 0.9707\n",
            "Epoch 11/20\n",
            "54000/54000 [==============================] - 3s 57us/step - loss: 0.0821 - acc: 0.9682 - val_loss: 0.0760 - val_acc: 0.9700\n",
            "Epoch 12/20\n",
            "54000/54000 [==============================] - 3s 57us/step - loss: 0.0815 - acc: 0.9681 - val_loss: 0.0713 - val_acc: 0.9721\n",
            "Epoch 13/20\n",
            "54000/54000 [==============================] - 3s 58us/step - loss: 0.0804 - acc: 0.9689 - val_loss: 0.0738 - val_acc: 0.9710\n",
            "Epoch 14/20\n",
            "54000/54000 [==============================] - 3s 56us/step - loss: 0.0800 - acc: 0.9692 - val_loss: 0.0703 - val_acc: 0.9725\n",
            "Epoch 15/20\n",
            "54000/54000 [==============================] - 3s 56us/step - loss: 0.0792 - acc: 0.9691 - val_loss: 0.0746 - val_acc: 0.9702\n",
            "Epoch 16/20\n",
            "54000/54000 [==============================] - 3s 56us/step - loss: 0.0787 - acc: 0.9696 - val_loss: 0.0731 - val_acc: 0.9710\n",
            "Epoch 17/20\n",
            "54000/54000 [==============================] - 3s 56us/step - loss: 0.0785 - acc: 0.9695 - val_loss: 0.0722 - val_acc: 0.9717\n",
            "Epoch 18/20\n",
            "54000/54000 [==============================] - 3s 57us/step - loss: 0.0780 - acc: 0.9699 - val_loss: 0.0715 - val_acc: 0.9718\n",
            "Epoch 19/20\n",
            "54000/54000 [==============================] - 3s 57us/step - loss: 0.0771 - acc: 0.9701 - val_loss: 0.0709 - val_acc: 0.9722\n",
            "Epoch 20/20\n",
            "54000/54000 [==============================] - 3s 57us/step - loss: 0.0774 - acc: 0.9698 - val_loss: 0.0709 - val_acc: 0.9723\n",
            "10000/10000 [==============================] - 0s 24us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E1Bmqtm0Km5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "be3460a9-d3df-4374-f155-01ed7390abfd"
      },
      "source": [
        "scores"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07445742788612843, 0.9714200055122375]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7ymDWeP0oO5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "b49020f0-03e2-4aa4-e1b3-629687b69494"
      },
      "source": [
        "history.history['acc']"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9323425837622749,\n",
              " 0.9568629653718737,\n",
              " 0.9613462982530947,\n",
              " 0.9633814826541477,\n",
              " 0.9644962986840142,\n",
              " 0.9657388912836711,\n",
              " 0.9662203733656142,\n",
              " 0.9666574109042132,\n",
              " 0.9673722242779202,\n",
              " 0.9676296323317068,\n",
              " 0.9682425952134309,\n",
              " 0.9680611139756662,\n",
              " 0.968944447905929,\n",
              " 0.9691722262700398,\n",
              " 0.9691462997860378,\n",
              " 0.9696333367206432,\n",
              " 0.9695388920042249,\n",
              " 0.9698648182904279,\n",
              " 0.9700555587168093,\n",
              " 0.9698333376072071]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PLr0S7b0mb7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "0fc2b4f0-47bc-46e4-a834-7f08826a47a0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.epoch, np.array(history.history['acc']))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f85953f2b00>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzMD4P8D1yefggkUd7YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv_3xNMjzdLI",
        "colab_type": "text"
      },
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- Use Hyperparameter Tuning to make the accuracy of your models as high as possible. (error as low as possible)\n",
        "- Use Cross Validation techniques to get more consistent results with your model.\n",
        "- Use GridSearchCV to try different combinations of hyperparameters. \n",
        "- Start looking into other types of Keras layers for CNNs and RNNs maybe try and build a CNN model for fashion-MNIST to see how the results compare."
      ]
    }
  ]
}