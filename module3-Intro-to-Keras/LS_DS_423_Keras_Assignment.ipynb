{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBQsZEJmubLs"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Neural Network Framework (Keras)\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignmnet 3*\n",
    "\n",
    "## Use the Keras Library to build a Multi-Layer Perceptron Model on the Boston Housing dataset\n",
    "\n",
    "- The Boston Housing dataset comes with the Keras library so use Keras to import it into your notebook. \n",
    "- Normalize the data (all features should have roughly the same scale)\n",
    "- Import the type of model and layers that you will need from Keras.\n",
    "- Instantiate a model object and use `model.add()` to add layers to your model\n",
    "- Since this is a regression model you will have a single output node in the final layer.\n",
    "- Use activation functions that are appropriate for this task\n",
    "- Compile your model\n",
    "- Fit your model and report its accuracy in terms of Mean Squared Error\n",
    "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
    "- Run this same data through a linear regression model. Which achieves higher accuracy?\n",
    "- Do a little bit of feature engineering and see how that affects your neural network model. (you will need to change your model to accept more inputs)\n",
    "- After feature engineering, which model sees a greater accuracy boost due to the new features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NLTAR87uYJ-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import normalize\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.datasets import boston_housing, mnist\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (404,), (102, 13), (102,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size = 0.8, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.fit_transform(x_val)\n",
    "x_test = scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\supas\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(128, activation= 'sigmoid', input_shape=(13,)),\n",
    "    Dense(64, activation= 'relu'),\n",
    "    Dense(32, activation= 'relu'),\n",
    "    Dense(16, activation= 'relu'),\n",
    "    Dense(1, activation= 'relu')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\supas\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 323 samples, validate on 81 samples\n",
      "Epoch 1/100\n",
      "323/323 [==============================] - 1s 2ms/step - loss: 519.6991 - val_loss: 517.7893\n",
      "Epoch 2/100\n",
      "323/323 [==============================] - 0s 71us/step - loss: 396.2005 - val_loss: 346.1421\n",
      "Epoch 3/100\n",
      "323/323 [==============================] - 0s 77us/step - loss: 222.4178 - val_loss: 169.8095\n",
      "Epoch 4/100\n",
      "323/323 [==============================] - 0s 49us/step - loss: 102.5136 - val_loss: 107.0310\n",
      "Epoch 5/100\n",
      "323/323 [==============================] - 0s 133us/step - loss: 88.0625 - val_loss: 106.7570\n",
      "Epoch 6/100\n",
      "323/323 [==============================] - 0s 62us/step - loss: 83.2114 - val_loss: 102.9403\n",
      "Epoch 7/100\n",
      "323/323 [==============================] - 0s 77us/step - loss: 78.1975 - val_loss: 103.7383\n",
      "Epoch 8/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 77.5906 - val_loss: 100.6697\n",
      "Epoch 9/100\n",
      "323/323 [==============================] - 0s 77us/step - loss: 75.3574 - val_loss: 97.0951\n",
      "Epoch 10/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 74.2698 - val_loss: 94.4293\n",
      "Epoch 11/100\n",
      "323/323 [==============================] - 0s 43us/step - loss: 72.5869 - val_loss: 92.5895\n",
      "Epoch 12/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 71.4270 - val_loss: 91.3097\n",
      "Epoch 13/100\n",
      "323/323 [==============================] - 0s 77us/step - loss: 69.8696 - val_loss: 88.1485\n",
      "Epoch 14/100\n",
      "323/323 [==============================] - 0s 62us/step - loss: 68.8805 - val_loss: 86.3084\n",
      "Epoch 15/100\n",
      "323/323 [==============================] - 0s 65us/step - loss: 67.5035 - val_loss: 86.1612\n",
      "Epoch 16/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 65.2336 - val_loss: 82.1060\n",
      "Epoch 17/100\n",
      "323/323 [==============================] - 0s 65us/step - loss: 64.9713 - val_loss: 80.9849\n",
      "Epoch 18/100\n",
      "323/323 [==============================] - 0s 68us/step - loss: 63.4661 - val_loss: 80.6849\n",
      "Epoch 19/100\n",
      "323/323 [==============================] - 0s 77us/step - loss: 62.9271 - val_loss: 77.3170\n",
      "Epoch 20/100\n",
      "323/323 [==============================] - 0s 90us/step - loss: 61.0904 - val_loss: 77.4405\n",
      "Epoch 21/100\n",
      "323/323 [==============================] - 0s 71us/step - loss: 61.5710 - val_loss: 76.3626\n",
      "Epoch 22/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 60.9167 - val_loss: 72.7629\n",
      "Epoch 23/100\n",
      "323/323 [==============================] - 0s 62us/step - loss: 58.4482 - val_loss: 75.3835\n",
      "Epoch 24/100\n",
      "323/323 [==============================] - 0s 71us/step - loss: 58.9358 - val_loss: 72.5877\n",
      "Epoch 25/100\n",
      "323/323 [==============================] - 0s 71us/step - loss: 57.1378 - val_loss: 69.9385\n",
      "Epoch 26/100\n",
      "323/323 [==============================] - 0s 68us/step - loss: 56.2447 - val_loss: 69.3093\n",
      "Epoch 27/100\n",
      "323/323 [==============================] - 0s 83us/step - loss: 55.5012 - val_loss: 66.9818\n",
      "Epoch 28/100\n",
      "323/323 [==============================] - 0s 71us/step - loss: 55.0469 - val_loss: 65.9251\n",
      "Epoch 29/100\n",
      "323/323 [==============================] - 0s 77us/step - loss: 53.8802 - val_loss: 66.4841\n",
      "Epoch 30/100\n",
      "323/323 [==============================] - 0s 77us/step - loss: 53.5261 - val_loss: 63.9280\n",
      "Epoch 31/100\n",
      "323/323 [==============================] - 0s 77us/step - loss: 52.9155 - val_loss: 63.3506\n",
      "Epoch 32/100\n",
      "323/323 [==============================] - 0s 77us/step - loss: 52.0755 - val_loss: 63.7152\n",
      "Epoch 33/100\n",
      "323/323 [==============================] - 0s 83us/step - loss: 51.6684 - val_loss: 61.1275\n",
      "Epoch 34/100\n",
      "323/323 [==============================] - 0s 71us/step - loss: 51.4653 - val_loss: 60.0277\n",
      "Epoch 35/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 50.8703 - val_loss: 60.8399\n",
      "Epoch 36/100\n",
      "323/323 [==============================] - 0s 71us/step - loss: 49.9974 - val_loss: 59.3442\n",
      "Epoch 37/100\n",
      "323/323 [==============================] - 0s 86us/step - loss: 49.6262 - val_loss: 58.3716\n",
      "Epoch 38/100\n",
      "323/323 [==============================] - 0s 62us/step - loss: 49.0139 - val_loss: 58.9780\n",
      "Epoch 39/100\n",
      "323/323 [==============================] - 0s 59us/step - loss: 48.8472 - val_loss: 56.8194\n",
      "Epoch 40/100\n",
      "323/323 [==============================] - 0s 65us/step - loss: 47.8962 - val_loss: 56.1067\n",
      "Epoch 41/100\n",
      "323/323 [==============================] - 0s 68us/step - loss: 47.5307 - val_loss: 55.0817\n",
      "Epoch 42/100\n",
      "323/323 [==============================] - 0s 68us/step - loss: 47.4750 - val_loss: 53.4916\n",
      "Epoch 43/100\n",
      "323/323 [==============================] - 0s 68us/step - loss: 46.5031 - val_loss: 54.6827\n",
      "Epoch 44/100\n",
      "323/323 [==============================] - 0s 65us/step - loss: 46.3412 - val_loss: 53.2182\n",
      "Epoch 45/100\n",
      "323/323 [==============================] - 0s 80us/step - loss: 45.5478 - val_loss: 53.2426\n",
      "Epoch 46/100\n",
      "323/323 [==============================] - 0s 117us/step - loss: 45.5837 - val_loss: 52.1272\n",
      "Epoch 47/100\n",
      "323/323 [==============================] - 0s 86us/step - loss: 45.7369 - val_loss: 50.0489\n",
      "Epoch 48/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 45.5666 - val_loss: 49.7906\n",
      "Epoch 49/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 43.7221 - val_loss: 51.0265\n",
      "Epoch 50/100\n",
      "323/323 [==============================] - 0s 80us/step - loss: 44.0534 - val_loss: 48.0295\n",
      "Epoch 51/100\n",
      "323/323 [==============================] - 0s 83us/step - loss: 43.0731 - val_loss: 49.9509\n",
      "Epoch 52/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 42.5334 - val_loss: 46.9637\n",
      "Epoch 53/100\n",
      "323/323 [==============================] - 0s 80us/step - loss: 44.2329 - val_loss: 45.5328\n",
      "Epoch 54/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 41.8047 - val_loss: 46.5972\n",
      "Epoch 55/100\n",
      "323/323 [==============================] - 0s 83us/step - loss: 40.9907 - val_loss: 47.5072\n",
      "Epoch 56/100\n",
      "323/323 [==============================] - 0s 83us/step - loss: 40.8794 - val_loss: 44.5987\n",
      "Epoch 57/100\n",
      "323/323 [==============================] - 0s 68us/step - loss: 40.0548 - val_loss: 45.8858\n",
      "Epoch 58/100\n",
      "323/323 [==============================] - 0s 80us/step - loss: 40.1768 - val_loss: 44.1008\n",
      "Epoch 59/100\n",
      "323/323 [==============================] - 0s 77us/step - loss: 39.6149 - val_loss: 43.1360\n",
      "Epoch 60/100\n",
      "323/323 [==============================] - 0s 83us/step - loss: 39.3507 - val_loss: 44.1520\n",
      "Epoch 61/100\n",
      "323/323 [==============================] - 0s 86us/step - loss: 39.0418 - val_loss: 41.3818\n",
      "Epoch 62/100\n",
      "323/323 [==============================] - 0s 77us/step - loss: 38.2699 - val_loss: 41.8579\n",
      "Epoch 63/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 39.6708 - val_loss: 39.6015\n",
      "Epoch 64/100\n",
      "323/323 [==============================] - 0s 83us/step - loss: 37.7077 - val_loss: 42.8706\n",
      "Epoch 65/100\n",
      "323/323 [==============================] - 0s 102us/step - loss: 36.9104 - val_loss: 39.0702\n",
      "Epoch 66/100\n",
      "323/323 [==============================] - 0s 71us/step - loss: 36.3779 - val_loss: 39.5549\n",
      "Epoch 67/100\n",
      "323/323 [==============================] - 0s 77us/step - loss: 36.5308 - val_loss: 38.1157\n",
      "Epoch 68/100\n",
      "323/323 [==============================] - 0s 96us/step - loss: 35.4839 - val_loss: 37.0635\n",
      "Epoch 69/100\n",
      "323/323 [==============================] - 0s 77us/step - loss: 35.0334 - val_loss: 37.8779\n",
      "Epoch 70/100\n",
      "323/323 [==============================] - 0s 80us/step - loss: 34.7463 - val_loss: 38.5764\n",
      "Epoch 71/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 34.6755 - val_loss: 34.8942\n",
      "Epoch 72/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 33.3597 - val_loss: 36.4972\n",
      "Epoch 73/100\n",
      "323/323 [==============================] - 0s 71us/step - loss: 33.2736 - val_loss: 33.4286\n",
      "Epoch 74/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 36.4486 - val_loss: 34.1330\n",
      "Epoch 75/100\n",
      "323/323 [==============================] - 0s 77us/step - loss: 33.0883 - val_loss: 36.6615\n",
      "Epoch 76/100\n",
      "323/323 [==============================] - 0s 83us/step - loss: 32.7330 - val_loss: 31.3788\n",
      "Epoch 77/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 31.8220 - val_loss: 33.7368\n",
      "Epoch 78/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 30.7592 - val_loss: 30.0207\n",
      "Epoch 79/100\n",
      "323/323 [==============================] - 0s 65us/step - loss: 32.5055 - val_loss: 30.4335\n",
      "Epoch 80/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 29.9428 - val_loss: 30.6845\n",
      "Epoch 81/100\n",
      "323/323 [==============================] - 0s 77us/step - loss: 30.3782 - val_loss: 29.5486\n",
      "Epoch 82/100\n",
      "323/323 [==============================] - 0s 77us/step - loss: 29.2742 - val_loss: 29.4009\n",
      "Epoch 83/100\n",
      "323/323 [==============================] - 0s 235us/step - loss: 28.9134 - val_loss: 27.2574\n",
      "Epoch 84/100\n",
      "323/323 [==============================] - 0s 90us/step - loss: 28.4389 - val_loss: 27.8103\n",
      "Epoch 85/100\n",
      "323/323 [==============================] - 0s 96us/step - loss: 28.0982 - val_loss: 27.0042\n",
      "Epoch 86/100\n",
      "323/323 [==============================] - 0s 71us/step - loss: 27.7572 - val_loss: 26.4042\n",
      "Epoch 87/100\n",
      "323/323 [==============================] - 0s 65us/step - loss: 27.5364 - val_loss: 26.7133\n",
      "Epoch 88/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 27.2478 - val_loss: 26.0199\n",
      "Epoch 89/100\n",
      "323/323 [==============================] - 0s 59us/step - loss: 27.5913 - val_loss: 24.1164\n",
      "Epoch 90/100\n",
      "323/323 [==============================] - 0s 68us/step - loss: 26.6150 - val_loss: 25.3174\n",
      "Epoch 91/100\n",
      "323/323 [==============================] - 0s 83us/step - loss: 25.7616 - val_loss: 23.5762\n",
      "Epoch 92/100\n",
      "323/323 [==============================] - 0s 71us/step - loss: 25.0472 - val_loss: 27.1287\n",
      "Epoch 93/100\n",
      "323/323 [==============================] - 0s 62us/step - loss: 26.1932 - val_loss: 22.7308\n",
      "Epoch 94/100\n",
      "323/323 [==============================] - 0s 68us/step - loss: 26.4859 - val_loss: 25.2563\n",
      "Epoch 95/100\n",
      "323/323 [==============================] - 0s 71us/step - loss: 25.6399 - val_loss: 23.4206\n",
      "Epoch 96/100\n",
      "323/323 [==============================] - 0s 105us/step - loss: 25.1195 - val_loss: 23.8768\n",
      "Epoch 97/100\n",
      "323/323 [==============================] - 0s 151us/step - loss: 24.1894 - val_loss: 23.6364\n",
      "Epoch 98/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 23.3937 - val_loss: 22.2172\n",
      "Epoch 99/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 23.3907 - val_loss: 21.6927\n",
      "Epoch 100/100\n",
      "323/323 [==============================] - 0s 74us/step - loss: 22.9760 - val_loss: 22.0817\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train,\n",
    "          batch_size=32, epochs=100,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred= model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.352629671318685"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(val_pred, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SfcFnOONyuNm"
   },
   "source": [
    "## Use the Keras Library to build an image recognition network using the Fashion-MNIST dataset (also comes with keras)\n",
    "\n",
    "- Load and preprocess the image data similar to how we preprocessed the MNIST data in class.\n",
    "- Make sure to one-hot encode your category labels\n",
    "- Make sure to have your final layer have as many nodes as the number of classes that you want to predict.\n",
    "- Try different hyperparameters. What is the highest accuracy that you are able to achieve.\n",
    "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
    "- Remember that neural networks fall prey to randomness so you may need to run your model multiple times (or use Cross Validation) in order to tell if a change to a hyperparameter is truly producing better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szi6-IpuzaH1"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test)= mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[2], cmap= plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train= normalize(x_train, axis= 1)\n",
    "x_test= normalize(x_test, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(),\n",
    "    Dense(128, activation= 'relu'),\n",
    "    Dense(10, activation= 'softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= 'adam',\n",
    "             loss= 'sparse_categorical_crossentropy',\n",
    "              metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.0440 - acc: 0.9873\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.0343 - acc: 0.9902\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.0280 - acc: 0.9918\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.0222 - acc: 0.9938\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0178 - acc: 0.9954\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.0138 - acc: 0.9964\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.0114 - acc: 0.9973\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0086 - acc: 0.9984\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0080 - acc: 0.9980\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.0062 - acc: 0.9987\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zv_3xNMjzdLI"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Use Hyperparameter Tuning to make the accuracy of your models as high as possible. (error as low as possible)\n",
    "- Use Cross Validation techniques to get more consistent results with your model.\n",
    "- Use GridSearchCV to try different combinations of hyperparameters. \n",
    "- Start looking into other types of Keras layers for CNNs and RNNs maybe try and build a CNN model for fashion-MNIST to see how the results compare."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_433_Keras_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
