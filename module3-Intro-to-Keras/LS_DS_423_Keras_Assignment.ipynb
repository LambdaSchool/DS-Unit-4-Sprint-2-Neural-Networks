{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBQsZEJmubLs"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Neural Network Framework (Keras)\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignmnet 3*\n",
    "\n",
    "## Use the Keras Library to build a Multi-Layer Perceptron Model on the Boston Housing dataset\n",
    "\n",
    "- The Boston Housing dataset comes with the Keras library so use Keras to import it into your notebook. \n",
    "- Normalize the data (all features should have roughly the same scale)\n",
    "- Import the type of model and layers that you will need from Keras.\n",
    "- Instantiate a model object and use `model.add()` to add layers to your model\n",
    "- Since this is a regression model you will have a single output node in the final layer.\n",
    "- Use activation functions that are appropriate for this task\n",
    "- Compile your model\n",
    "- Fit your model and report its accuracy in terms of Mean Squared Error\n",
    "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
    "- Run this same data through a linear regression model. Which achieves higher accuracy?\n",
    "- Do a little bit of feature engineering and see how that affects your neural network model. (you will need to change your model to accept more inputs)\n",
    "- After feature engineering, which model sees a greater accuracy boost due to the new features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NLTAR87uYJ-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./amesHousePrice.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities LotConfig  ... PoolArea PoolQC Fence MiscFeature  \\\n",
       "0         Lvl    AllPub    Inside  ...        0    NaN   NaN         NaN   \n",
       "1         Lvl    AllPub       FR2  ...        0    NaN   NaN         NaN   \n",
       "2         Lvl    AllPub    Inside  ...        0    NaN   NaN         NaN   \n",
       "3         Lvl    AllPub    Corner  ...        0    NaN   NaN         NaN   \n",
       "4         Lvl    AllPub       FR2  ...        0    NaN   NaN         NaN   \n",
       "\n",
       "  MiscVal MoSold  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0       0      2    2008        WD         Normal     208500  \n",
       "1       0      5    2007        WD         Normal     181500  \n",
       "2       0      9    2008        WD         Normal     223500  \n",
       "3       0      2    2006        WD        Abnorml     140000  \n",
       "4       0     12    2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('Id', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoolQC           1453\n",
       "MiscFeature      1406\n",
       "Alley            1369\n",
       "Fence            1179\n",
       "FireplaceQu       690\n",
       "LotFrontage       259\n",
       "GarageType         81\n",
       "GarageCond         81\n",
       "GarageFinish       81\n",
       "GarageQual         81\n",
       "GarageYrBlt        81\n",
       "BsmtFinType2       38\n",
       "BsmtExposure       38\n",
       "BsmtQual           37\n",
       "BsmtCond           37\n",
       "BsmtFinType1       37\n",
       "MasVnrArea          8\n",
       "MasVnrType          8\n",
       "Electrical          1\n",
       "RoofMatl            0\n",
       "Exterior1st         0\n",
       "RoofStyle           0\n",
       "ExterQual           0\n",
       "Exterior2nd         0\n",
       "YearBuilt           0\n",
       "ExterCond           0\n",
       "Foundation          0\n",
       "YearRemodAdd        0\n",
       "SalePrice           0\n",
       "OverallCond         0\n",
       "                 ... \n",
       "GarageArea          0\n",
       "PavedDrive          0\n",
       "WoodDeckSF          0\n",
       "OpenPorchSF         0\n",
       "3SsnPorch           0\n",
       "BsmtUnfSF           0\n",
       "ScreenPorch         0\n",
       "PoolArea            0\n",
       "MiscVal             0\n",
       "MoSold              0\n",
       "YrSold              0\n",
       "SaleType            0\n",
       "Functional          0\n",
       "TotRmsAbvGrd        0\n",
       "KitchenQual         0\n",
       "KitchenAbvGr        0\n",
       "BedroomAbvGr        0\n",
       "HalfBath            0\n",
       "FullBath            0\n",
       "BsmtHalfBath        0\n",
       "BsmtFullBath        0\n",
       "GrLivArea           0\n",
       "LowQualFinSF        0\n",
       "2ndFlrSF            0\n",
       "1stFlrSF            0\n",
       "CentralAir          0\n",
       "SaleCondition       0\n",
       "Heating             0\n",
       "TotalBsmtSF         0\n",
       "MSSubClass          0\n",
       "Length: 80, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gd    3\n",
       "Fa    2\n",
       "Ex    2\n",
       "Name: PoolQC, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['PoolQC'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Shed    49\n",
       "Othr     2\n",
       "Gar2     2\n",
       "TenC     1\n",
       "Name: MiscFeature, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['MiscFeature'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grvl    50\n",
       "Pave    41\n",
       "Name: Alley, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Alley'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gd    380\n",
       "TA    313\n",
       "Fa     33\n",
       "Ex     24\n",
       "Po     20\n",
       "Name: FireplaceQu, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['FireplaceQu'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['FireplaceQu'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SalePrice        0\n",
       "SaleCondition    0\n",
       "RoofMatl         0\n",
       "Exterior1st      0\n",
       "Exterior2nd      0\n",
       "MasVnrType       0\n",
       "MasVnrArea       0\n",
       "ExterQual        0\n",
       "ExterCond        0\n",
       "Foundation       0\n",
       "BsmtQual         0\n",
       "BsmtCond         0\n",
       "BsmtExposure     0\n",
       "BsmtFinType1     0\n",
       "BsmtFinSF1       0\n",
       "BsmtFinType2     0\n",
       "BsmtFinSF2       0\n",
       "BsmtUnfSF        0\n",
       "TotalBsmtSF      0\n",
       "RoofStyle        0\n",
       "YearRemodAdd     0\n",
       "YearBuilt        0\n",
       "Utilities        0\n",
       "MSZoning         0\n",
       "LotFrontage      0\n",
       "LotArea          0\n",
       "Street           0\n",
       "Alley            0\n",
       "LotShape         0\n",
       "LandContour      0\n",
       "                ..\n",
       "PoolArea         0\n",
       "GarageCars       0\n",
       "PoolQC           0\n",
       "Fence            0\n",
       "MiscFeature      0\n",
       "MiscVal          0\n",
       "MoSold           0\n",
       "YrSold           0\n",
       "SaleType         0\n",
       "GarageArea       0\n",
       "GarageFinish     0\n",
       "Electrical       0\n",
       "HalfBath         0\n",
       "1stFlrSF         0\n",
       "2ndFlrSF         0\n",
       "LowQualFinSF     0\n",
       "GrLivArea        0\n",
       "BsmtFullBath     0\n",
       "BsmtHalfBath     0\n",
       "FullBath         0\n",
       "BedroomAbvGr     0\n",
       "GarageYrBlt      0\n",
       "KitchenAbvGr     0\n",
       "KitchenQual      0\n",
       "TotRmsAbvGrd     0\n",
       "Functional       0\n",
       "Fireplaces       0\n",
       "FireplaceQu      0\n",
       "GarageType       0\n",
       "MSSubClass       0\n",
       "Length: 80, dtype: int64"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.copy()\n",
    "\n",
    "for column in df2.columns:\n",
    "    if df2[column].dtype == 'O':\n",
    "        df2[column] = df2[column].fillna(value='Missing')\n",
    "    else:\n",
    "        df2[column] = df2[column].fillna(value=0)\n",
    "        \n",
    "df2.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>90</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.0</td>\n",
       "      <td>11841</td>\n",
       "      <td>Grvl</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9200</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>50</td>\n",
       "      <td>RM</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8520</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Grvl</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>GdWo</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>CWD</td>\n",
       "      <td>Family</td>\n",
       "      <td>136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>45</td>\n",
       "      <td>RM</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>70.0</td>\n",
       "      <td>8400</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>213000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass MSZoning  LotFrontage  LotArea Street    Alley LotShape  \\\n",
       "582           90       RL         81.0    11841   Grvl  Missing      Reg   \n",
       "423           60       RL         80.0     9200   Pave  Missing      Reg   \n",
       "1387          50       RM         60.0     8520   Pave     Grvl      Reg   \n",
       "437           45       RM         50.0     6000   Pave  Missing      Reg   \n",
       "704           20       RL         70.0     8400   Pave  Missing      Reg   \n",
       "\n",
       "     LandContour Utilities LotConfig  ... PoolArea   PoolQC    Fence  \\\n",
       "582          Lvl    AllPub    Inside  ...        0  Missing  Missing   \n",
       "423          Lvl    AllPub    Inside  ...        0  Missing  Missing   \n",
       "1387         Lvl    AllPub    Inside  ...        0  Missing     GdWo   \n",
       "437          Lvl    AllPub    Inside  ...        0  Missing  Missing   \n",
       "704          Lvl    AllPub    Inside  ...        0  Missing  Missing   \n",
       "\n",
       "     MiscFeature MiscVal MoSold  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "582      Missing       0      5    2007        WD         Normal     118500  \n",
       "423      Missing       0      6    2008        WD         Normal     315000  \n",
       "1387     Missing       0      8    2007       CWD         Family     136000  \n",
       "437      Missing       0      1    2009        WD         Normal     119000  \n",
       "704      Missing       0      5    2010        WD         Normal     213000  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df2)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>96.0</td>\n",
       "      <td>12444</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2008</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>50</td>\n",
       "      <td>RM</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6120</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>10041</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Missing</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>30</td>\n",
       "      <td>RM</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass MSZoning  LotFrontage  LotArea Street    Alley LotShape  \\\n",
       "1437          20       RL         96.0    12444   Pave  Missing      Reg   \n",
       "584           50       RM         51.0     6120   Pave  Missing      Reg   \n",
       "1261          20       RL         80.0     9600   Pave  Missing      Reg   \n",
       "602           60       RL         80.0    10041   Pave  Missing      IR1   \n",
       "1063          30       RM         50.0     6000   Pave  Missing      Reg   \n",
       "\n",
       "     LandContour Utilities LotConfig  ... ScreenPorch PoolArea   PoolQC  \\\n",
       "1437         Lvl    AllPub       FR2  ...           0        0  Missing   \n",
       "584          Lvl    AllPub    Inside  ...         120        0  Missing   \n",
       "1261         Lvl    AllPub    Inside  ...           0        0  Missing   \n",
       "602          Lvl    AllPub    Inside  ...           0        0  Missing   \n",
       "1063         Lvl    AllPub    Inside  ...           0        0  Missing   \n",
       "\n",
       "        Fence MiscFeature MiscVal  MoSold  YrSold  SaleType  SaleCondition  \n",
       "1437  Missing     Missing       0      11    2008       New        Partial  \n",
       "584   Missing     Missing       0       7    2009        WD         Normal  \n",
       "1261  Missing     Missing       0       6    2009        WD         Normal  \n",
       "602   Missing     Missing       0       2    2006        WD        Abnorml  \n",
       "1063    MnPrv     Missing       0       7    2006        WD         Normal  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train['SalePrice']\n",
    "X_train = train.drop('SalePrice', axis=1)\n",
    "y_test = test['SalePrice']\n",
    "X_test = test.drop('SalePrice', axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category_encoders\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/52/c54191ad3782de633ea3d6ee3bb2837bda0cf3bc97644bb6375cf14150a0/category_encoders-2.1.0-py2.py3-none-any.whl (100kB)\n",
      "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 20.9MB/s a 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from category_encoders) (1.14.3)\n",
      "Requirement already satisfied: statsmodels>=0.6.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from category_encoders) (0.9.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from category_encoders) (0.20.3)\n",
      "Requirement already satisfied: patsy>=0.4.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from category_encoders) (0.5.0)\n",
      "Requirement already satisfied: pandas>=0.21.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from category_encoders) (0.24.2)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from category_encoders) (1.1.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from patsy>=0.4.1->category_encoders) (1.11.0)\n",
      "Requirement already satisfied: pytz>=2011k in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from pandas>=0.21.1->category_encoders) (2018.4)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from pandas>=0.21.1->category_encoders) (2.7.3)\n",
      "Installing collected packages: category-encoders\n",
      "Successfully installed category-encoders-2.1.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6240</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>190</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164660</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>700</td>\n",
       "      <td>8</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>66.0</td>\n",
       "      <td>7399</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass  MSZoning  LotFrontage  LotArea  Street  Alley  LotShape  \\\n",
       "294           20         1         80.0     9600       1      1         1   \n",
       "1145          50         2         52.0     6240       1      1         1   \n",
       "335          190         1          0.0   164660       2      1         2   \n",
       "297           60         3         66.0     7399       1      2         2   \n",
       "1457          70         1         66.0     9042       1      1         1   \n",
       "\n",
       "      LandContour  Utilities  LotConfig  ...  ScreenPorch  PoolArea  PoolQC  \\\n",
       "294             1          1          1  ...            0         0       1   \n",
       "1145            1          1          1  ...            0         0       1   \n",
       "335             2          1          2  ...            0         0       1   \n",
       "297             1          1          1  ...          198         0       1   \n",
       "1457            1          1          1  ...            0         0       1   \n",
       "\n",
       "      Fence  MiscFeature  MiscVal  MoSold  YrSold  SaleType  SaleCondition  \n",
       "294       1            1        0      10    2009         1              1  \n",
       "1145      1            1        0       8    2006         1              2  \n",
       "335       1            2      700       8    2008         1              1  \n",
       "297       1            1        0       6    2007         1              1  \n",
       "1457      2            2     2500       5    2010         1              1  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "encoder = ce.OrdinalEncoder()\n",
    "X_train_encoded = encoder.fit_transform(X_train)\n",
    "X_test_encoded = encoder.transform(X_test)\n",
    "X_train_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.84925227e-03, 9.24626136e-05, 7.39700909e-03, 8.87641091e-01,\n",
       "        9.24626136e-05, 9.24626136e-05, 9.24626136e-05, 9.24626136e-05,\n",
       "        9.24626136e-05, 9.24626136e-05, 9.24626136e-05, 9.24626136e-05,\n",
       "        9.24626136e-05, 9.24626136e-05, 9.24626136e-05, 9.24626136e-05,\n",
       "        5.54775682e-04, 4.62313068e-04, 1.80579484e-01, 1.80579484e-01,\n",
       "        9.24626136e-05, 9.24626136e-05, 9.24626136e-05, 9.24626136e-05,\n",
       "        9.24626136e-05, 2.20061020e-02, 9.24626136e-05, 9.24626136e-05,\n",
       "        9.24626136e-05, 9.24626136e-05, 9.24626136e-05, 9.24626136e-05,\n",
       "        9.24626136e-05, 1.18814458e-01, 9.24626136e-05, 0.00000000e+00,\n",
       "        1.21126024e-02, 1.30927061e-01, 9.24626136e-05, 9.24626136e-05,\n",
       "        9.24626136e-05, 9.24626136e-05, 1.52008537e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.52008537e-01, 9.24626136e-05, 0.00000000e+00,\n",
       "        9.24626136e-05, 0.00000000e+00, 2.77387841e-04, 9.24626136e-05,\n",
       "        9.24626136e-05, 6.47238295e-04, 9.24626136e-05, 1.84925227e-04,\n",
       "        9.24626136e-05, 9.24626136e-05, 1.80579484e-01, 9.24626136e-05,\n",
       "        1.84925227e-04, 3.86493725e-02, 9.24626136e-05, 9.24626136e-05,\n",
       "        9.24626136e-05, 1.01708875e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.24626136e-05,\n",
       "        9.24626136e-05, 9.24626136e-05, 0.00000000e+00, 9.24626136e-04,\n",
       "        1.85757391e-01, 9.24626136e-05, 9.24626136e-05],\n",
       "       [6.43991823e-03, 2.57596729e-04, 6.69751496e-03, 8.03701795e-01,\n",
       "        1.28798365e-04, 1.28798365e-04, 1.28798365e-04, 1.28798365e-04,\n",
       "        1.28798365e-04, 1.28798365e-04, 1.28798365e-04, 2.57596729e-04,\n",
       "        1.28798365e-04, 1.28798365e-04, 1.28798365e-04, 2.57596729e-04,\n",
       "        6.43991823e-04, 7.72790188e-04, 2.48323247e-01, 2.51156811e-01,\n",
       "        2.57596729e-04, 1.28798365e-04, 2.57596729e-04, 2.57596729e-04,\n",
       "        2.57596729e-04, 0.00000000e+00, 1.28798365e-04, 1.28798365e-04,\n",
       "        2.57596729e-04, 1.28798365e-04, 1.28798365e-04, 1.28798365e-04,\n",
       "        2.57596729e-04, 0.00000000e+00, 1.28798365e-04, 0.00000000e+00,\n",
       "        1.34207896e-01, 1.34207896e-01, 1.28798365e-04, 2.57596729e-04,\n",
       "        1.28798365e-04, 1.28798365e-04, 1.34207896e-01, 6.87783267e-02,\n",
       "        0.00000000e+00, 2.02986223e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.28798365e-04, 0.00000000e+00, 3.86395094e-04, 1.28798365e-04,\n",
       "        1.28798365e-04, 1.03038692e-03, 1.28798365e-04, 1.28798365e-04,\n",
       "        1.28798365e-04, 2.57596729e-04, 2.48323247e-01, 2.57596729e-04,\n",
       "        1.28798365e-04, 2.89796320e-02, 1.28798365e-04, 1.28798365e-04,\n",
       "        1.28798365e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.28798365e-04,\n",
       "        1.28798365e-04, 1.28798365e-04, 0.00000000e+00, 1.03038692e-03,\n",
       "        2.58369519e-01, 1.28798365e-04, 2.57596729e-04],\n",
       "       [1.15332834e-03, 6.07014915e-06, 0.00000000e+00, 9.99510759e-01,\n",
       "        1.21402983e-05, 6.07014915e-06, 1.21402983e-05, 1.21402983e-05,\n",
       "        6.07014915e-06, 1.21402983e-05, 1.21402983e-05, 1.82104475e-05,\n",
       "        6.07014915e-06, 6.07014915e-06, 1.21402983e-05, 1.21402983e-05,\n",
       "        3.03507458e-05, 3.64208949e-05, 1.19278431e-02, 1.19278431e-02,\n",
       "        1.21402983e-05, 6.07014915e-06, 1.82104475e-05, 1.82104475e-05,\n",
       "        1.21402983e-05, 0.00000000e+00, 6.07014915e-06, 6.07014915e-06,\n",
       "        6.07014915e-06, 6.07014915e-06, 6.07014915e-06, 1.21402983e-05,\n",
       "        1.82104475e-05, 7.58161629e-03, 1.21402983e-05, 8.92311925e-04,\n",
       "        6.25225363e-04, 9.09915358e-03, 6.07014915e-06, 1.21402983e-05,\n",
       "        6.07014915e-06, 6.07014915e-06, 9.82757148e-03, 1.01371491e-03,\n",
       "        0.00000000e+00, 1.08412864e-02, 1.21402983e-05, 0.00000000e+00,\n",
       "        1.21402983e-05, 0.00000000e+00, 1.82104475e-05, 6.07014915e-06,\n",
       "        6.07014915e-06, 4.24910441e-05, 6.07014915e-06, 1.21402983e-05,\n",
       "        6.07014915e-06, 6.07014915e-06, 1.19278431e-02, 6.07014915e-06,\n",
       "        1.21402983e-05, 3.21110890e-03, 6.07014915e-06, 6.07014915e-06,\n",
       "        6.07014915e-06, 4.06699993e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.07014915e-06,\n",
       "        6.07014915e-06, 1.21402983e-05, 4.24910441e-03, 4.85611932e-05,\n",
       "        1.21888595e-02, 6.07014915e-06, 6.07014915e-06],\n",
       "       [6.67233207e-03, 3.33616603e-04, 7.33956528e-03, 8.22809750e-01,\n",
       "        1.11205534e-04, 2.22411069e-04, 2.22411069e-04, 1.11205534e-04,\n",
       "        1.11205534e-04, 1.11205534e-04, 1.11205534e-04, 4.44822138e-04,\n",
       "        1.11205534e-04, 1.11205534e-04, 1.11205534e-04, 3.33616603e-04,\n",
       "        7.78438741e-04, 5.56027672e-04, 2.22077452e-01, 2.22188658e-01,\n",
       "        1.11205534e-04, 1.11205534e-04, 4.44822138e-04, 4.44822138e-04,\n",
       "        3.33616603e-04, 1.77928855e-01, 2.22411069e-04, 1.11205534e-04,\n",
       "        3.33616603e-04, 2.22411069e-04, 1.11205534e-04, 1.11205534e-04,\n",
       "        4.44822138e-04, 7.21723919e-02, 1.11205534e-04, 0.00000000e+00,\n",
       "        3.62530042e-02, 1.08425396e-01, 1.11205534e-04, 2.22411069e-04,\n",
       "        1.11205534e-04, 1.11205534e-04, 1.08425396e-01, 1.08425396e-01,\n",
       "        0.00000000e+00, 2.16850792e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.22411069e-04, 1.11205534e-04, 3.33616603e-04, 1.11205534e-04,\n",
       "        2.22411069e-04, 7.78438741e-04, 1.11205534e-04, 1.11205534e-04,\n",
       "        2.22411069e-04, 2.22411069e-04, 2.22077452e-01, 3.33616603e-04,\n",
       "        2.22411069e-04, 6.40543879e-02, 1.11205534e-04, 1.11205534e-04,\n",
       "        1.11205534e-04, 0.00000000e+00, 1.11205534e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.20186958e-02, 0.00000000e+00, 1.11205534e-04,\n",
       "        1.11205534e-04, 1.11205534e-04, 0.00000000e+00, 6.67233207e-04,\n",
       "        2.23189508e-01, 1.11205534e-04, 1.11205534e-04],\n",
       "       [6.55410118e-03, 9.36300169e-05, 6.17958111e-03, 8.46602612e-01,\n",
       "        9.36300169e-05, 9.36300169e-05, 9.36300169e-05, 9.36300169e-05,\n",
       "        9.36300169e-05, 9.36300169e-05, 9.36300169e-05, 4.68150084e-04,\n",
       "        9.36300169e-05, 9.36300169e-05, 9.36300169e-05, 2.80890051e-04,\n",
       "        6.55410118e-04, 8.42670152e-04, 1.81735863e-01, 1.87821814e-01,\n",
       "        1.87260034e-04, 9.36300169e-05, 4.68150084e-04, 4.68150084e-04,\n",
       "        1.87260034e-04, 0.00000000e+00, 2.80890051e-04, 1.87260034e-04,\n",
       "        3.74520067e-04, 9.36300169e-05, 1.87260034e-04, 9.36300169e-05,\n",
       "        9.36300169e-05, 2.57482546e-02, 9.36300169e-05, 0.00000000e+00,\n",
       "        8.21135248e-02, 1.07861779e-01, 9.36300169e-05, 1.87260034e-04,\n",
       "        9.36300169e-05, 9.36300169e-05, 1.11232460e-01, 1.07861779e-01,\n",
       "        0.00000000e+00, 2.19094239e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.87260034e-04, 0.00000000e+00, 3.74520067e-04, 9.36300169e-05,\n",
       "        1.87260034e-04, 8.42670152e-04, 9.36300169e-05, 1.87260034e-04,\n",
       "        9.36300169e-05, 9.36300169e-05, 1.81735863e-01, 2.80890051e-04,\n",
       "        9.36300169e-05, 2.35947642e-02, 9.36300169e-05, 9.36300169e-05,\n",
       "        9.36300169e-05, 0.00000000e+00, 5.61780101e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.36300169e-05,\n",
       "        1.87260034e-04, 1.87260034e-04, 2.34075042e-01, 4.68150084e-04,\n",
       "        1.88196334e-01, 9.36300169e-05, 9.36300169e-05]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "norm = Normalizer()\n",
    "\n",
    "X_train_normalized = norm.fit_transform(X_train_encoded)\n",
    "X_test_normalized = norm.transform(X_test_encoded)\n",
    "\n",
    "X_train_normalized[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 40)                3200      \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,241\n",
      "Trainable params: 4,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential(name=\"nn1\")\n",
    "model.add(Dense(40, input_dim=79, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 985 samples, validate on 110 samples\n",
      "Epoch 1/250\n",
      "985/985 [==============================] - 0s 472us/sample - loss: 39057246754.8264 - mean_squared_error: 39057248256.0000 - val_loss: 36152318585.0182 - val_mean_squared_error: 36152315904.0000\n",
      "Epoch 2/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 39056471440.2437 - mean_squared_error: 39056470016.0000 - val_loss: 36150999747.4909 - val_mean_squared_error: 36150996992.0000\n",
      "Epoch 3/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 39053988072.8690 - mean_squared_error: 39053983744.0000 - val_loss: 36146962729.8909 - val_mean_squared_error: 36146966528.0000\n",
      "Epoch 4/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 39047273913.8274 - mean_squared_error: 39047270400.0000 - val_loss: 36137057335.8545 - val_mean_squared_error: 36137054208.0000\n",
      "Epoch 5/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 39032133072.6985 - mean_squared_error: 39032131584.0000 - val_loss: 36116200838.9818 - val_mean_squared_error: 36116201472.0000\n",
      "Epoch 6/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 39002487937.9492 - mean_squared_error: 39002488832.0000 - val_loss: 36077480420.0727 - val_mean_squared_error: 36077481984.0000\n",
      "Epoch 7/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 38950151183.5939 - mean_squared_error: 38950158336.0000 - val_loss: 36012737033.3091 - val_mean_squared_error: 36012736512.0000\n",
      "Epoch 8/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 38866119115.5005 - mean_squared_error: 38866116608.0000 - val_loss: 35911651179.0545 - val_mean_squared_error: 35911651328.0000\n",
      "Epoch 9/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 38739056696.1381 - mean_squared_error: 38739050496.0000 - val_loss: 35761904844.8000 - val_mean_squared_error: 35761905664.0000\n",
      "Epoch 10/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 38555337978.5421 - mean_squared_error: 38555336704.0000 - val_loss: 35551172496.2909 - val_mean_squared_error: 35551174656.0000\n",
      "Epoch 11/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 38300739374.0020 - mean_squared_error: 38300737536.0000 - val_loss: 35265635234.9091 - val_mean_squared_error: 35265638400.0000\n",
      "Epoch 12/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 37962059085.7096 - mean_squared_error: 37962067968.0000 - val_loss: 34888912225.7455 - val_mean_squared_error: 34888912896.0000\n",
      "Epoch 13/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 37525609346.2091 - mean_squared_error: 37525610496.0000 - val_loss: 34406453024.5818 - val_mean_squared_error: 34406453248.0000\n",
      "Epoch 14/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 36972431349.6041 - mean_squared_error: 36972433408.0000 - val_loss: 33812673107.7818 - val_mean_squared_error: 33812672512.0000\n",
      "Epoch 15/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 36290529591.8782 - mean_squared_error: 36290531328.0000 - val_loss: 33087694177.7455 - val_mean_squared_error: 33087694848.0000\n",
      "Epoch 16/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 35470263789.8071 - mean_squared_error: 35470262272.0000 - val_loss: 32216545391.7091 - val_mean_squared_error: 32216545280.0000\n",
      "Epoch 17/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 34501966587.0619 - mean_squared_error: 34501963776.0000 - val_loss: 31193138566.9818 - val_mean_squared_error: 31193139200.0000\n",
      "Epoch 18/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 33373670935.3909 - mean_squared_error: 33373669376.0000 - val_loss: 30027832822.6909 - val_mean_squared_error: 30027835392.0000\n",
      "Epoch 19/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 32086031340.2477 - mean_squared_error: 32086032384.0000 - val_loss: 28719004914.0364 - val_mean_squared_error: 28719005696.0000\n",
      "Epoch 20/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 30651133434.2822 - mean_squared_error: 30651133952.0000 - val_loss: 27246136413.0909 - val_mean_squared_error: 27246137344.0000\n",
      "Epoch 21/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 29067890989.4822 - mean_squared_error: 29067890688.0000 - val_loss: 25644974042.7636 - val_mean_squared_error: 25644974080.0000\n",
      "Epoch 22/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 27355459265.8843 - mean_squared_error: 27355455488.0000 - val_loss: 23928455987.2000 - val_mean_squared_error: 23928457216.0000\n",
      "Epoch 23/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 25533312177.7706 - mean_squared_error: 25533315072.0000 - val_loss: 22132545051.9273 - val_mean_squared_error: 22132545536.0000\n",
      "Epoch 24/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 23631931011.5086 - mean_squared_error: 23631931392.0000 - val_loss: 20277273804.8000 - val_mean_squared_error: 20277274624.0000\n",
      "Epoch 25/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 21690146533.2305 - mean_squared_error: 21690146816.0000 - val_loss: 18395782832.8727 - val_mean_squared_error: 18395783168.0000\n",
      "Epoch 26/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 19749469863.8944 - mean_squared_error: 19749468160.0000 - val_loss: 16525781252.6545 - val_mean_squared_error: 16525782016.0000\n",
      "Epoch 27/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 17836112985.4051 - mean_squared_error: 17836113920.0000 - val_loss: 14738742085.8182 - val_mean_squared_error: 14738742272.0000\n",
      "Epoch 28/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 16013185789.1411 - mean_squared_error: 16013188096.0000 - val_loss: 13056096944.8727 - val_mean_squared_error: 13056096256.0000\n",
      "Epoch 29/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 14321342225.9330 - mean_squared_error: 14321342464.0000 - val_loss: 11498935686.9818 - val_mean_squared_error: 11498936320.0000\n",
      "Epoch 30/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 12787384517.5228 - mean_squared_error: 12787384320.0000 - val_loss: 10126235415.2727 - val_mean_squared_error: 10126235648.0000\n",
      "Epoch 31/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 11441913074.2254 - mean_squared_error: 11441911808.0000 - val_loss: 8943496247.8545 - val_mean_squared_error: 8943496192.0000\n",
      "Epoch 32/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 10302056299.8579 - mean_squared_error: 10302053376.0000 - val_loss: 7972585285.8182 - val_mean_squared_error: 7972585472.0000\n",
      "Epoch 33/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 9379986449.1533 - mean_squared_error: 9379987456.0000 - val_loss: 7192198116.0727 - val_mean_squared_error: 7192198144.0000\n",
      "Epoch 34/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 8646303041.2345 - mean_squared_error: 8646301696.0000 - val_loss: 6606909812.3636 - val_mean_squared_error: 6606909440.0000\n",
      "Epoch 35/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 8100688576.8447 - mean_squared_error: 8100688896.0000 - val_loss: 6169864601.6000 - val_mean_squared_error: 6169865216.0000\n",
      "Epoch 36/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 7694741685.9289 - mean_squared_error: 7694742528.0000 - val_loss: 5887038780.5091 - val_mean_squared_error: 5887039488.0000\n",
      "Epoch 37/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 7426919402.6883 - mean_squared_error: 7426919424.0000 - val_loss: 5666767341.3818 - val_mean_squared_error: 5666767360.0000\n",
      "Epoch 38/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 7235071014.4650 - mean_squared_error: 7235069440.0000 - val_loss: 5540583591.5636 - val_mean_squared_error: 5540583936.0000\n",
      "Epoch 39/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 7113426915.9310 - mean_squared_error: 7113426944.0000 - val_loss: 5461418859.0545 - val_mean_squared_error: 5461418496.0000\n",
      "Epoch 40/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 7037001895.8944 - mean_squared_error: 7037003264.0000 - val_loss: 5409020537.0182 - val_mean_squared_error: 5409020416.0000\n",
      "Epoch 41/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6982984034.7614 - mean_squared_error: 6982983680.0000 - val_loss: 5380425942.1091 - val_mean_squared_error: 5380425728.0000\n",
      "Epoch 42/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 6948485094.5299 - mean_squared_error: 6948485632.0000 - val_loss: 5361960145.4545 - val_mean_squared_error: 5361959936.0000\n",
      "Epoch 43/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 6924486062.3919 - mean_squared_error: 6924485632.0000 - val_loss: 5346231049.3091 - val_mean_squared_error: 5346231296.0000\n",
      "Epoch 44/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 6904960289.5269 - mean_squared_error: 6904960000.0000 - val_loss: 5334806965.5273 - val_mean_squared_error: 5334806528.0000\n",
      "Epoch 45/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6889125170.1604 - mean_squared_error: 6889123840.0000 - val_loss: 5325020564.9455 - val_mean_squared_error: 5325020672.0000\n",
      "Epoch 46/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6873663508.7919 - mean_squared_error: 6873663488.0000 - val_loss: 5315442720.5818 - val_mean_squared_error: 5315442688.0000\n",
      "Epoch 47/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 6861743024.4711 - mean_squared_error: 6861744128.0000 - val_loss: 5306618535.5636 - val_mean_squared_error: 5306618368.0000\n",
      "Epoch 48/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6845996235.5005 - mean_squared_error: 6845996544.0000 - val_loss: 5296795778.3273 - val_mean_squared_error: 5296795648.0000\n",
      "Epoch 49/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 6833142189.8721 - mean_squared_error: 6833141760.0000 - val_loss: 5287647688.1455 - val_mean_squared_error: 5287647744.0000\n",
      "Epoch 50/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6821106275.2812 - mean_squared_error: 6821106176.0000 - val_loss: 5277108614.9818 - val_mean_squared_error: 5277108736.0000\n",
      "Epoch 51/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 6807385258.4934 - mean_squared_error: 6807385600.0000 - val_loss: 5268643933.0909 - val_mean_squared_error: 5268643840.0000\n",
      "Epoch 52/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 6794878758.7249 - mean_squared_error: 6794879488.0000 - val_loss: 5258491196.5091 - val_mean_squared_error: 5258491392.0000\n",
      "Epoch 53/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 6781347828.5645 - mean_squared_error: 6781347328.0000 - val_loss: 5248047276.2182 - val_mean_squared_error: 5248047104.0000\n",
      "Epoch 54/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 6766809375.9675 - mean_squared_error: 6766810112.0000 - val_loss: 5236605952.0000 - val_mean_squared_error: 5236605952.0000\n",
      "Epoch 55/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 6753242995.1350 - mean_squared_error: 6753243648.0000 - val_loss: 5227837737.8909 - val_mean_squared_error: 5227837952.0000\n",
      "Epoch 56/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 6739849695.7726 - mean_squared_error: 6739848704.0000 - val_loss: 5217135457.7455 - val_mean_squared_error: 5217135616.0000\n",
      "Epoch 57/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 6726186573.4497 - mean_squared_error: 6726185984.0000 - val_loss: 5206559106.3273 - val_mean_squared_error: 5206558720.0000\n",
      "Epoch 58/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6715350898.0954 - mean_squared_error: 6715350528.0000 - val_loss: 5196612226.3273 - val_mean_squared_error: 5196612096.0000\n",
      "Epoch 59/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6699159513.5350 - mean_squared_error: 6699159552.0000 - val_loss: 5186526989.9636 - val_mean_squared_error: 5186526720.0000\n",
      "Epoch 60/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6685713955.3462 - mean_squared_error: 6685713408.0000 - val_loss: 5174881233.4545 - val_mean_squared_error: 5174881280.0000\n",
      "Epoch 61/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 6671071853.6772 - mean_squared_error: 6671071232.0000 - val_loss: 5164612584.7273 - val_mean_squared_error: 5164612608.0000\n",
      "Epoch 62/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 6657277753.1777 - mean_squared_error: 6657277952.0000 - val_loss: 5153576922.7636 - val_mean_squared_error: 5153576960.0000\n",
      "Epoch 63/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6643551959.7157 - mean_squared_error: 6643551744.0000 - val_loss: 5143093820.5091 - val_mean_squared_error: 5143093760.0000\n",
      "Epoch 64/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 6631620581.4904 - mean_squared_error: 6631621120.0000 - val_loss: 5134818667.0545 - val_mean_squared_error: 5134818816.0000\n",
      "Epoch 65/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 6615354838.9360 - mean_squared_error: 6615354368.0000 - val_loss: 5122533701.8182 - val_mean_squared_error: 5122533888.0000\n",
      "Epoch 66/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 6600611403.3706 - mean_squared_error: 6600612352.0000 - val_loss: 5110274173.6727 - val_mean_squared_error: 5110274560.0000\n",
      "Epoch 67/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 6592624164.3858 - mean_squared_error: 6592624640.0000 - val_loss: 5102115961.0182 - val_mean_squared_error: 5102115840.0000\n",
      "Epoch 68/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6574521993.7462 - mean_squared_error: 6574521344.0000 - val_loss: 5087592001.1636 - val_mean_squared_error: 5087591936.0000\n",
      "Epoch 69/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 6559876849.7056 - mean_squared_error: 6559876608.0000 - val_loss: 5077463272.7273 - val_mean_squared_error: 5077463552.0000\n",
      "Epoch 70/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 6544211923.0376 - mean_squared_error: 6544211968.0000 - val_loss: 5067628869.8182 - val_mean_squared_error: 5067629056.0000\n",
      "Epoch 71/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 6530650998.2538 - mean_squared_error: 6530651648.0000 - val_loss: 5055503034.1818 - val_mean_squared_error: 5055502848.0000\n",
      "Epoch 72/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 6516147050.2985 - mean_squared_error: 6516147200.0000 - val_loss: 5044144751.7091 - val_mean_squared_error: 5044145152.0000\n",
      "Epoch 73/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 6501046370.7614 - mean_squared_error: 6501046784.0000 - val_loss: 5035826027.0545 - val_mean_squared_error: 5035826176.0000\n",
      "Epoch 74/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 6489650971.8091 - mean_squared_error: 6489650688.0000 - val_loss: 5025382260.3636 - val_mean_squared_error: 5025381888.0000\n",
      "Epoch 75/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 6474378693.2629 - mean_squared_error: 6474378752.0000 - val_loss: 5010853766.9818 - val_mean_squared_error: 5010853888.0000\n",
      "Epoch 76/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 6456850786.5015 - mean_squared_error: 6456851456.0000 - val_loss: 5000999563.6364 - val_mean_squared_error: 5000999936.0000\n",
      "Epoch 77/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 6442099704.7228 - mean_squared_error: 6442100224.0000 - val_loss: 4991893666.9091 - val_mean_squared_error: 4991893504.0000\n",
      "Epoch 78/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6428679100.9462 - mean_squared_error: 6428679168.0000 - val_loss: 4979707145.3091 - val_mean_squared_error: 4979706880.0000\n",
      "Epoch 79/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 6414076935.0173 - mean_squared_error: 6414076928.0000 - val_loss: 4966524509.0909 - val_mean_squared_error: 4966524416.0000\n",
      "Epoch 80/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 6398975569.6081 - mean_squared_error: 6398976000.0000 - val_loss: 4956906542.5455 - val_mean_squared_error: 4956906496.0000\n",
      "Epoch 81/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6383361389.9371 - mean_squared_error: 6383361536.0000 - val_loss: 4944493940.3636 - val_mean_squared_error: 4944494080.0000\n",
      "Epoch 82/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 6369600585.8112 - mean_squared_error: 6369600512.0000 - val_loss: 4935765289.8909 - val_mean_squared_error: 4935765504.0000\n",
      "Epoch 83/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6354272195.9635 - mean_squared_error: 6354272256.0000 - val_loss: 4922946141.0909 - val_mean_squared_error: 4922946048.0000\n",
      "Epoch 84/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 6338671456.4223 - mean_squared_error: 6338671104.0000 - val_loss: 4908956967.5636 - val_mean_squared_error: 4908956672.0000\n",
      "Epoch 85/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 6324577801.8761 - mean_squared_error: 6324578304.0000 - val_loss: 4898444641.7455 - val_mean_squared_error: 4898444288.0000\n",
      "Epoch 86/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6309804082.9401 - mean_squared_error: 6309804544.0000 - val_loss: 4886696922.7636 - val_mean_squared_error: 4886696960.0000\n",
      "Epoch 87/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 6294604988.6863 - mean_squared_error: 6294605824.0000 - val_loss: 4874907499.0545 - val_mean_squared_error: 4874907648.0000\n",
      "Epoch 88/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 6279543528.3492 - mean_squared_error: 6279542784.0000 - val_loss: 4864603983.1273 - val_mean_squared_error: 4864604160.0000\n",
      "Epoch 89/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 6264284823.2609 - mean_squared_error: 6264283648.0000 - val_loss: 4853399030.6909 - val_mean_squared_error: 4853399040.0000\n",
      "Epoch 90/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 6249406618.8995 - mean_squared_error: 6249408000.0000 - val_loss: 4840947916.8000 - val_mean_squared_error: 4840948224.0000\n",
      "Epoch 91/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6236170820.8731 - mean_squared_error: 6236171776.0000 - val_loss: 4833060296.1455 - val_mean_squared_error: 4833060352.0000\n",
      "Epoch 92/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 6218487473.2508 - mean_squared_error: 6218488320.0000 - val_loss: 4818930222.5455 - val_mean_squared_error: 4818930176.0000\n",
      "Epoch 93/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6204808267.3706 - mean_squared_error: 6204808192.0000 - val_loss: 4807326571.0545 - val_mean_squared_error: 4807326720.0000\n",
      "Epoch 94/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 6192932112.3736 - mean_squared_error: 6192932352.0000 - val_loss: 4794083453.6727 - val_mean_squared_error: 4794083328.0000\n",
      "Epoch 95/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6173238459.1269 - mean_squared_error: 6173238784.0000 - val_loss: 4782890049.1636 - val_mean_squared_error: 4782889984.0000\n",
      "Epoch 96/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6159991026.7452 - mean_squared_error: 6159991296.0000 - val_loss: 4774181883.3455 - val_mean_squared_error: 4774181888.0000\n",
      "Epoch 97/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 6143112403.0376 - mean_squared_error: 6143113216.0000 - val_loss: 4760072536.4364 - val_mean_squared_error: 4760072704.0000\n",
      "Epoch 98/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 6127658595.8010 - mean_squared_error: 6127659520.0000 - val_loss: 4748884684.8000 - val_mean_squared_error: 4748884992.0000\n",
      "Epoch 99/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 6111943440.3736 - mean_squared_error: 6111943168.0000 - val_loss: 4737707594.4727 - val_mean_squared_error: 4737707520.0000\n",
      "Epoch 100/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 6097247571.1675 - mean_squared_error: 6097248256.0000 - val_loss: 4724994576.2909 - val_mean_squared_error: 4724994048.0000\n",
      "Epoch 101/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6085199918.2619 - mean_squared_error: 6085199360.0000 - val_loss: 4711124840.7273 - val_mean_squared_error: 4711124992.0000\n",
      "Epoch 102/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6067803832.0081 - mean_squared_error: 6067804160.0000 - val_loss: 4701445138.6182 - val_mean_squared_error: 4701444608.0000\n",
      "Epoch 103/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 6051638922.7858 - mean_squared_error: 6051639808.0000 - val_loss: 4689996837.2364 - val_mean_squared_error: 4689996800.0000\n",
      "Epoch 104/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 6039479675.9716 - mean_squared_error: 6039480320.0000 - val_loss: 4678034534.4000 - val_mean_squared_error: 4678034432.0000\n",
      "Epoch 105/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 6020213354.5584 - mean_squared_error: 6020212736.0000 - val_loss: 4663930477.3818 - val_mean_squared_error: 4663930368.0000\n",
      "Epoch 106/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 6007060732.8812 - mean_squared_error: 6007060992.0000 - val_loss: 4652565504.0000 - val_mean_squared_error: 4652565504.0000\n",
      "Epoch 107/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 5989468239.0091 - mean_squared_error: 5989468672.0000 - val_loss: 4642628859.3455 - val_mean_squared_error: 4642628608.0000\n",
      "Epoch 108/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 5977665858.0142 - mean_squared_error: 5977666560.0000 - val_loss: 4628417703.5636 - val_mean_squared_error: 4628417536.0000\n",
      "Epoch 109/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 5961008839.0822 - mean_squared_error: 5961008640.0000 - val_loss: 4623057445.2364 - val_mean_squared_error: 4623057408.0000\n",
      "Epoch 110/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 5944500843.5980 - mean_squared_error: 5944500224.0000 - val_loss: 4606134216.1455 - val_mean_squared_error: 4606134272.0000\n",
      "Epoch 111/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 5928657333.1492 - mean_squared_error: 5928657920.0000 - val_loss: 4595388972.2182 - val_mean_squared_error: 4595388928.0000\n",
      "Epoch 112/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 5912321065.5838 - mean_squared_error: 5912320512.0000 - val_loss: 4581718365.0909 - val_mean_squared_error: 4581718528.0000\n",
      "Epoch 113/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 5898335736.2030 - mean_squared_error: 5898335744.0000 - val_loss: 4570122630.9818 - val_mean_squared_error: 4570122752.0000\n",
      "Epoch 114/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 5881303277.0274 - mean_squared_error: 5881303552.0000 - val_loss: 4559481893.2364 - val_mean_squared_error: 4559481856.0000\n",
      "Epoch 115/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 5865631041.4944 - mean_squared_error: 5865630720.0000 - val_loss: 4549044559.1273 - val_mean_squared_error: 4549044224.0000\n",
      "Epoch 116/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 5850555368.0893 - mean_squared_error: 5850554880.0000 - val_loss: 4532386918.4000 - val_mean_squared_error: 4532386816.0000\n",
      "Epoch 117/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 5838981111.1635 - mean_squared_error: 5838980096.0000 - val_loss: 4527166878.2545 - val_mean_squared_error: 4527166976.0000\n",
      "Epoch 118/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 5818064025.3401 - mean_squared_error: 5818064896.0000 - val_loss: 4507682806.6909 - val_mean_squared_error: 4507682816.0000\n",
      "Epoch 119/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 5803168516.4183 - mean_squared_error: 5803168256.0000 - val_loss: 4496785994.4727 - val_mean_squared_error: 4496785920.0000\n",
      "Epoch 120/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 5788317325.3848 - mean_squared_error: 5788317696.0000 - val_loss: 4488831232.0000 - val_mean_squared_error: 4488830976.0000\n",
      "Epoch 121/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 5770997387.3056 - mean_squared_error: 5770997248.0000 - val_loss: 4474796320.5818 - val_mean_squared_error: 4474796544.0000\n",
      "Epoch 122/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 5761223937.8193 - mean_squared_error: 5761223680.0000 - val_loss: 4458271706.7636 - val_mean_squared_error: 4458271744.0000\n",
      "Epoch 123/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 5740826088.6091 - mean_squared_error: 5740826624.0000 - val_loss: 4453342631.5636 - val_mean_squared_error: 4453342720.0000\n",
      "Epoch 124/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 5724342987.2406 - mean_squared_error: 5724342272.0000 - val_loss: 4437923574.6909 - val_mean_squared_error: 4437923840.0000\n",
      "Epoch 125/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 5710255604.0447 - mean_squared_error: 5710255104.0000 - val_loss: 4421413082.7636 - val_mean_squared_error: 4421413376.0000\n",
      "Epoch 126/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 5692277724.9137 - mean_squared_error: 5692277760.0000 - val_loss: 4413031814.9818 - val_mean_squared_error: 4413031424.0000\n",
      "Epoch 127/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 5677023349.4741 - mean_squared_error: 5677022720.0000 - val_loss: 4403077757.6727 - val_mean_squared_error: 4403077632.0000\n",
      "Epoch 128/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 5660633590.1239 - mean_squared_error: 5660633088.0000 - val_loss: 4389945088.0000 - val_mean_squared_error: 4389944832.0000\n",
      "Epoch 129/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 5646810603.4680 - mean_squared_error: 5646810112.0000 - val_loss: 4374956455.5636 - val_mean_squared_error: 4374956544.0000\n",
      "Epoch 130/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 5630250867.9147 - mean_squared_error: 5630251520.0000 - val_loss: 4363576832.0000 - val_mean_squared_error: 4363576832.0000\n",
      "Epoch 131/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 5612846765.0924 - mean_squared_error: 5612847616.0000 - val_loss: 4351201764.0727 - val_mean_squared_error: 4351201792.0000\n",
      "Epoch 132/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 5597541164.4426 - mean_squared_error: 5597540352.0000 - val_loss: 4340790811.9273 - val_mean_squared_error: 4340791296.0000\n",
      "Epoch 133/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 5582992630.3838 - mean_squared_error: 5582992384.0000 - val_loss: 4327190146.3273 - val_mean_squared_error: 4327190528.0000\n",
      "Epoch 134/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 5564644965.3604 - mean_squared_error: 5564644864.0000 - val_loss: 4310939824.8727 - val_mean_squared_error: 4310939648.0000\n",
      "Epoch 135/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 5550306213.5553 - mean_squared_error: 5550306816.0000 - val_loss: 4298959750.9818 - val_mean_squared_error: 4298959872.0000\n",
      "Epoch 136/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 5534174704.4061 - mean_squared_error: 5534173696.0000 - val_loss: 4286173686.6909 - val_mean_squared_error: 4286173440.0000\n",
      "Epoch 137/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 5516806122.1685 - mean_squared_error: 5516806144.0000 - val_loss: 4275590451.2000 - val_mean_squared_error: 4275590656.0000\n",
      "Epoch 138/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 5501837374.8954 - mean_squared_error: 5501837824.0000 - val_loss: 4266321780.3636 - val_mean_squared_error: 4266321920.0000\n",
      "Epoch 139/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 5484981912.3005 - mean_squared_error: 5484980736.0000 - val_loss: 4254721042.6182 - val_mean_squared_error: 4254721024.0000\n",
      "Epoch 140/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 5466494608.5036 - mean_squared_error: 5466493952.0000 - val_loss: 4238734452.3636 - val_mean_squared_error: 4238734336.0000\n",
      "Epoch 141/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 5452183390.8629 - mean_squared_error: 5452182528.0000 - val_loss: 4222704202.4727 - val_mean_squared_error: 4222704384.0000\n",
      "Epoch 142/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 5434782883.7360 - mean_squared_error: 5434783232.0000 - val_loss: 4212375840.5818 - val_mean_squared_error: 4212375808.0000\n",
      "Epoch 143/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 5418992899.8985 - mean_squared_error: 5418993152.0000 - val_loss: 4197466386.6182 - val_mean_squared_error: 4197466368.0000\n",
      "Epoch 144/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 5401667970.2091 - mean_squared_error: 5401668096.0000 - val_loss: 4186875410.6182 - val_mean_squared_error: 4186875392.0000\n",
      "Epoch 145/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 5385858159.7563 - mean_squared_error: 5385858048.0000 - val_loss: 4176112612.0727 - val_mean_squared_error: 4176112640.0000\n",
      "Epoch 146/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 5368703575.3259 - mean_squared_error: 5368704000.0000 - val_loss: 4159706726.4000 - val_mean_squared_error: 4159706624.0000\n",
      "Epoch 147/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 5352054962.8102 - mean_squared_error: 5352055296.0000 - val_loss: 4145635933.0909 - val_mean_squared_error: 4145635840.0000\n",
      "Epoch 148/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 5335811891.7198 - mean_squared_error: 5335811584.0000 - val_loss: 4133828496.2909 - val_mean_squared_error: 4133828608.0000\n",
      "Epoch 149/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 5319714969.6000 - mean_squared_error: 5319714304.0000 - val_loss: 4122108434.6182 - val_mean_squared_error: 4122108416.0000\n",
      "Epoch 150/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 5302504608.6173 - mean_squared_error: 5302504448.0000 - val_loss: 4111136400.2909 - val_mean_squared_error: 4111136256.0000\n",
      "Epoch 151/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 5287677375.5452 - mean_squared_error: 5287677440.0000 - val_loss: 4097230987.6364 - val_mean_squared_error: 4097230848.0000\n",
      "Epoch 152/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 5269041123.9310 - mean_squared_error: 5269041152.0000 - val_loss: 4082309971.7818 - val_mean_squared_error: 4082309632.0000\n",
      "Epoch 153/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 5251412078.7168 - mean_squared_error: 5251411456.0000 - val_loss: 4068225205.5273 - val_mean_squared_error: 4068225024.0000\n",
      "Epoch 154/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 5237009028.5482 - mean_squared_error: 5237008384.0000 - val_loss: 4051493133.9636 - val_mean_squared_error: 4051493376.0000\n",
      "Epoch 155/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 5217918392.2680 - mean_squared_error: 5217918464.0000 - val_loss: 4040498641.4545 - val_mean_squared_error: 4040498688.0000\n",
      "Epoch 156/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 5201285108.8244 - mean_squared_error: 5201285120.0000 - val_loss: 4032998595.4909 - val_mean_squared_error: 4032998656.0000\n",
      "Epoch 157/250\n",
      "985/985 [==============================] - 0s 44us/sample - loss: 5190508774.2701 - mean_squared_error: 5190509056.0000 - val_loss: 4012606738.6182 - val_mean_squared_error: 4012606976.0000\n",
      "Epoch 158/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 5169508992.3898 - mean_squared_error: 5169508864.0000 - val_loss: 4005642212.0727 - val_mean_squared_error: 4005642240.0000\n",
      "Epoch 159/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 5150390591.0254 - mean_squared_error: 5150390272.0000 - val_loss: 3990548293.8182 - val_mean_squared_error: 3990548480.0000\n",
      "Epoch 160/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 5133004050.1929 - mean_squared_error: 5133004288.0000 - val_loss: 3975231534.5455 - val_mean_squared_error: 3975231488.0000\n",
      "Epoch 161/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 5115515518.3107 - mean_squared_error: 5115514880.0000 - val_loss: 3960509607.5636 - val_mean_squared_error: 3960509440.0000\n",
      "Epoch 162/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 5099500920.0731 - mean_squared_error: 5099500032.0000 - val_loss: 3948322625.1636 - val_mean_squared_error: 3948322560.0000\n",
      "Epoch 163/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 5081667392.7147 - mean_squared_error: 5081667584.0000 - val_loss: 3933166866.6182 - val_mean_squared_error: 3933166592.0000\n",
      "Epoch 164/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 5064404995.7685 - mean_squared_error: 5064404992.0000 - val_loss: 3920342732.8000 - val_mean_squared_error: 3920342784.0000\n",
      "Epoch 165/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 5046924833.7868 - mean_squared_error: 5046924800.0000 - val_loss: 3907011667.7818 - val_mean_squared_error: 3907011328.0000\n",
      "Epoch 166/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 5032223003.2893 - mean_squared_error: 5032223232.0000 - val_loss: 3898552166.4000 - val_mean_squared_error: 3898552320.0000\n",
      "Epoch 167/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 5012749725.8883 - mean_squared_error: 5012749312.0000 - val_loss: 3880516542.8364 - val_mean_squared_error: 3880516352.0000\n",
      "Epoch 168/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 4998438494.6030 - mean_squared_error: 4998438912.0000 - val_loss: 3861111908.0727 - val_mean_squared_error: 3861112064.0000\n",
      "Epoch 169/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 4978882655.1228 - mean_squared_error: 4978882560.0000 - val_loss: 3854341413.2364 - val_mean_squared_error: 3854341376.0000\n",
      "Epoch 170/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 4960296977.4132 - mean_squared_error: 4960296960.0000 - val_loss: 3839571921.4545 - val_mean_squared_error: 3839571968.0000\n",
      "Epoch 171/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 4942817447.1147 - mean_squared_error: 4942816768.0000 - val_loss: 3825198996.9455 - val_mean_squared_error: 3825199104.0000\n",
      "Epoch 172/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 4926046808.8853 - mean_squared_error: 4926046720.0000 - val_loss: 3811938406.4000 - val_mean_squared_error: 3811938304.0000\n",
      "Epoch 173/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 4909682423.1635 - mean_squared_error: 4909682176.0000 - val_loss: 3799012033.1636 - val_mean_squared_error: 3799012096.0000\n",
      "Epoch 174/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 4895535191.0660 - mean_squared_error: 4895535104.0000 - val_loss: 3777106827.6364 - val_mean_squared_error: 3777106944.0000\n",
      "Epoch 175/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 4872711573.7015 - mean_squared_error: 4872711168.0000 - val_loss: 3769308597.5273 - val_mean_squared_error: 3769308672.0000\n",
      "Epoch 176/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 4858429801.7787 - mean_squared_error: 4858429440.0000 - val_loss: 3756687797.5273 - val_mean_squared_error: 3756687616.0000\n",
      "Epoch 177/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 4846731528.3168 - mean_squared_error: 4846731776.0000 - val_loss: 3751243852.8000 - val_mean_squared_error: 3751244032.0000\n",
      "Epoch 178/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 4825839220.9543 - mean_squared_error: 4825839104.0000 - val_loss: 3721335086.5455 - val_mean_squared_error: 3721335040.0000\n",
      "Epoch 179/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 4803211473.9980 - mean_squared_error: 4803211776.0000 - val_loss: 3713730536.7273 - val_mean_squared_error: 3713730304.0000\n",
      "Epoch 180/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 4787314790.4000 - mean_squared_error: 4787314688.0000 - val_loss: 3698814594.3273 - val_mean_squared_error: 3698814208.0000\n",
      "Epoch 181/250\n",
      "985/985 [==============================] - 0s 44us/sample - loss: 4768712878.9117 - mean_squared_error: 4768713216.0000 - val_loss: 3688376008.1455 - val_mean_squared_error: 3688375808.0000\n",
      "Epoch 182/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 4750040604.0690 - mean_squared_error: 4750040576.0000 - val_loss: 3671915431.5636 - val_mean_squared_error: 3671915264.0000\n",
      "Epoch 183/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 4740044053.5716 - mean_squared_error: 4740044288.0000 - val_loss: 3651194121.3091 - val_mean_squared_error: 3651194368.0000\n",
      "Epoch 184/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 4721872526.9442 - mean_squared_error: 4721872384.0000 - val_loss: 3655057398.6909 - val_mean_squared_error: 3655057408.0000\n",
      "Epoch 185/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 4698695547.1919 - mean_squared_error: 4698695168.0000 - val_loss: 3629219048.7273 - val_mean_squared_error: 3629219072.0000\n",
      "Epoch 186/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 4679620348.6213 - mean_squared_error: 4679620096.0000 - val_loss: 3612143541.5273 - val_mean_squared_error: 3612143616.0000\n",
      "Epoch 187/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 4661307021.9046 - mean_squared_error: 4661306880.0000 - val_loss: 3600512719.1273 - val_mean_squared_error: 3600512768.0000\n",
      "Epoch 188/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 4643303858.5503 - mean_squared_error: 4643303936.0000 - val_loss: 3588863027.2000 - val_mean_squared_error: 3588862976.0000\n",
      "Epoch 189/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 4626280812.1178 - mean_squared_error: 4626280960.0000 - val_loss: 3573436965.2364 - val_mean_squared_error: 3573436672.0000\n",
      "Epoch 190/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 4607462218.0711 - mean_squared_error: 4607461888.0000 - val_loss: 3553968858.7636 - val_mean_squared_error: 3553968640.0000\n",
      "Epoch 191/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 4589891726.4244 - mean_squared_error: 4589891072.0000 - val_loss: 3539397757.6727 - val_mean_squared_error: 3539397632.0000\n",
      "Epoch 192/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 4572579791.6589 - mean_squared_error: 4572579840.0000 - val_loss: 3532629168.8727 - val_mean_squared_error: 3532629248.0000\n",
      "Epoch 193/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 4554377582.4569 - mean_squared_error: 4554377728.0000 - val_loss: 3514309815.8545 - val_mean_squared_error: 3514309632.0000\n",
      "Epoch 194/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 4536983774.4731 - mean_squared_error: 4536984064.0000 - val_loss: 3500165306.1818 - val_mean_squared_error: 3500165120.0000\n",
      "Epoch 195/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 4518727319.7807 - mean_squared_error: 4518727680.0000 - val_loss: 3488408617.8909 - val_mean_squared_error: 3488408576.0000\n",
      "Epoch 196/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 4501157025.6569 - mean_squared_error: 4501157376.0000 - val_loss: 3467357779.7818 - val_mean_squared_error: 3467357440.0000\n",
      "Epoch 197/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 4483207918.8467 - mean_squared_error: 4483208704.0000 - val_loss: 3457997786.7636 - val_mean_squared_error: 3457997824.0000\n",
      "Epoch 198/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 4464047885.7746 - mean_squared_error: 4464047616.0000 - val_loss: 3443744393.3091 - val_mean_squared_error: 3443744256.0000\n",
      "Epoch 199/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 4445590769.7056 - mean_squared_error: 4445590016.0000 - val_loss: 3427569398.6909 - val_mean_squared_error: 3427569408.0000\n",
      "Epoch 200/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 4429049955.0213 - mean_squared_error: 4429049856.0000 - val_loss: 3410314412.2182 - val_mean_squared_error: 3410314752.0000\n",
      "Epoch 201/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 4410222856.5766 - mean_squared_error: 4410222592.0000 - val_loss: 3403137852.5091 - val_mean_squared_error: 3403137792.0000\n",
      "Epoch 202/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 4393005538.3716 - mean_squared_error: 4393005568.0000 - val_loss: 3387950205.6727 - val_mean_squared_error: 3387950336.0000\n",
      "Epoch 203/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 4375962758.6274 - mean_squared_error: 4375962112.0000 - val_loss: 3367101239.8545 - val_mean_squared_error: 3367101440.0000\n",
      "Epoch 204/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 4354652885.8964 - mean_squared_error: 4354653184.0000 - val_loss: 3358815953.4545 - val_mean_squared_error: 3358816000.0000\n",
      "Epoch 205/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 4337643275.9553 - mean_squared_error: 4337643520.0000 - val_loss: 3344298733.3818 - val_mean_squared_error: 3344298752.0000\n",
      "Epoch 206/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 4319394620.2964 - mean_squared_error: 4319394816.0000 - val_loss: 3328773608.7273 - val_mean_squared_error: 3328773632.0000\n",
      "Epoch 207/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 4304797587.3624 - mean_squared_error: 4304798208.0000 - val_loss: 3311189773.9636 - val_mean_squared_error: 3311189760.0000\n",
      "Epoch 208/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 4282823410.7452 - mean_squared_error: 4282823680.0000 - val_loss: 3295370528.5818 - val_mean_squared_error: 3295370496.0000\n",
      "Epoch 209/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 4266638619.5492 - mean_squared_error: 4266638592.0000 - val_loss: 3282141435.3455 - val_mean_squared_error: 3282141440.0000\n",
      "Epoch 210/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 4246906339.4112 - mean_squared_error: 4246906624.0000 - val_loss: 3267578368.0000 - val_mean_squared_error: 3267578112.0000\n",
      "Epoch 211/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 4228943729.3157 - mean_squared_error: 4228944384.0000 - val_loss: 3256065468.5091 - val_mean_squared_error: 3256065536.0000\n",
      "Epoch 212/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 4210181686.3188 - mean_squared_error: 4210181888.0000 - val_loss: 3238835325.6727 - val_mean_squared_error: 3238835200.0000\n",
      "Epoch 213/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 4192707547.6142 - mean_squared_error: 4192706816.0000 - val_loss: 3223983010.9091 - val_mean_squared_error: 3223982848.0000\n",
      "Epoch 214/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 4173800205.2548 - mean_squared_error: 4173800192.0000 - val_loss: 3213088237.3818 - val_mean_squared_error: 3213088000.0000\n",
      "Epoch 215/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 4157727833.1452 - mean_squared_error: 4157727232.0000 - val_loss: 3196660016.8727 - val_mean_squared_error: 3196659968.0000\n",
      "Epoch 216/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 4140129241.0152 - mean_squared_error: 4140129536.0000 - val_loss: 3189469095.5636 - val_mean_squared_error: 3189469184.0000\n",
      "Epoch 217/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 4119472497.8355 - mean_squared_error: 4119472896.0000 - val_loss: 3166471856.8727 - val_mean_squared_error: 3166471936.0000\n",
      "Epoch 218/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 4103934491.0294 - mean_squared_error: 4103934208.0000 - val_loss: 3148484875.6364 - val_mean_squared_error: 3148484864.0000\n",
      "Epoch 219/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 4083824023.9756 - mean_squared_error: 4083824384.0000 - val_loss: 3141336636.5091 - val_mean_squared_error: 3141336576.0000\n",
      "Epoch 220/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 4067671281.7056 - mean_squared_error: 4067670784.0000 - val_loss: 3124686373.2364 - val_mean_squared_error: 3124686592.0000\n",
      "Epoch 221/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 4049593868.9949 - mean_squared_error: 4049593856.0000 - val_loss: 3105595243.0545 - val_mean_squared_error: 3105594880.0000\n",
      "Epoch 222/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 4031364614.2376 - mean_squared_error: 4031364352.0000 - val_loss: 3100014277.8182 - val_mean_squared_error: 3100014336.0000\n",
      "Epoch 223/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 4014774493.4335 - mean_squared_error: 4014774016.0000 - val_loss: 3076142922.4727 - val_mean_squared_error: 3076142848.0000\n",
      "Epoch 224/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 3997193048.3655 - mean_squared_error: 3997192704.0000 - val_loss: 3072233613.9636 - val_mean_squared_error: 3072233472.0000\n",
      "Epoch 225/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 3978084955.7442 - mean_squared_error: 3978084864.0000 - val_loss: 3052395580.5091 - val_mean_squared_error: 3052395520.0000\n",
      "Epoch 226/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 3959502866.1929 - mean_squared_error: 3959503360.0000 - val_loss: 3038614388.3636 - val_mean_squared_error: 3038614528.0000\n",
      "Epoch 227/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 3942879043.5736 - mean_squared_error: 3942879232.0000 - val_loss: 3023473394.0364 - val_mean_squared_error: 3023473152.0000\n",
      "Epoch 228/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 3925390469.0680 - mean_squared_error: 3925390592.0000 - val_loss: 3009593525.5273 - val_mean_squared_error: 3009593600.0000\n",
      "Epoch 229/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 3907866161.7706 - mean_squared_error: 3907865856.0000 - val_loss: 2999319444.9455 - val_mean_squared_error: 2999319296.0000\n",
      "Epoch 230/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 3890259749.6853 - mean_squared_error: 3890259456.0000 - val_loss: 2979113067.0545 - val_mean_squared_error: 2979113216.0000\n",
      "Epoch 231/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 3873960003.5736 - mean_squared_error: 3873959936.0000 - val_loss: 2971666366.8364 - val_mean_squared_error: 2971666432.0000\n",
      "Epoch 232/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 3855470450.4853 - mean_squared_error: 3855470336.0000 - val_loss: 2954543834.7636 - val_mean_squared_error: 2954543872.0000\n",
      "Epoch 233/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 3837679682.0142 - mean_squared_error: 3837679872.0000 - val_loss: 2937837665.7455 - val_mean_squared_error: 2937837568.0000\n",
      "Epoch 234/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 3821384118.9685 - mean_squared_error: 3821384448.0000 - val_loss: 2927194167.8545 - val_mean_squared_error: 2927194368.0000\n",
      "Epoch 235/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 3804124880.6985 - mean_squared_error: 3804125184.0000 - val_loss: 2912350792.1455 - val_mean_squared_error: 2912350720.0000\n",
      "Epoch 236/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 3787272741.9452 - mean_squared_error: 3787272960.0000 - val_loss: 2899166654.8364 - val_mean_squared_error: 2899166720.0000\n",
      "Epoch 237/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 3770135877.9127 - mean_squared_error: 3770136064.0000 - val_loss: 2886813663.4182 - val_mean_squared_error: 2886813696.0000\n",
      "Epoch 238/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 3753047000.4954 - mean_squared_error: 3753046784.0000 - val_loss: 2870269118.8364 - val_mean_squared_error: 2870269184.0000\n",
      "Epoch 239/250\n",
      "985/985 [==============================] - 0s 47us/sample - loss: 3736696286.9929 - mean_squared_error: 3736696320.0000 - val_loss: 2859268142.5455 - val_mean_squared_error: 2859268352.0000\n",
      "Epoch 240/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 3720767663.8213 - mean_squared_error: 3720767488.0000 - val_loss: 2846465615.1273 - val_mean_squared_error: 2846465536.0000\n",
      "Epoch 241/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 3703416257.6244 - mean_squared_error: 3703416064.0000 - val_loss: 2830925370.1818 - val_mean_squared_error: 2830925568.0000\n",
      "Epoch 242/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 3687427629.0924 - mean_squared_error: 3687427584.0000 - val_loss: 2820957733.2364 - val_mean_squared_error: 2820957696.0000\n",
      "Epoch 243/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 3671584957.7259 - mean_squared_error: 3671585024.0000 - val_loss: 2806822888.7273 - val_mean_squared_error: 2806822912.0000\n",
      "Epoch 244/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 3655091528.5117 - mean_squared_error: 3655091712.0000 - val_loss: 2791040339.7818 - val_mean_squared_error: 2791040256.0000\n",
      "Epoch 245/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 3637702032.7635 - mean_squared_error: 3637702400.0000 - val_loss: 2780732420.6545 - val_mean_squared_error: 2780732416.0000\n",
      "Epoch 246/250\n",
      "985/985 [==============================] - 0s 48us/sample - loss: 3622132446.2132 - mean_squared_error: 3622131968.0000 - val_loss: 2768282540.2182 - val_mean_squared_error: 2768282368.0000\n",
      "Epoch 247/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 3605907602.5827 - mean_squared_error: 3605907712.0000 - val_loss: 2754647161.0182 - val_mean_squared_error: 2754647296.0000\n",
      "Epoch 248/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 3590709297.9005 - mean_squared_error: 3590708992.0000 - val_loss: 2740719476.3636 - val_mean_squared_error: 2740719360.0000\n",
      "Epoch 249/250\n",
      "985/985 [==============================] - 0s 46us/sample - loss: 3576394258.8426 - mean_squared_error: 3576394496.0000 - val_loss: 2730197894.9818 - val_mean_squared_error: 2730197760.0000\n",
      "Epoch 250/250\n",
      "985/985 [==============================] - 0s 45us/sample - loss: 3559393633.2020 - mean_squared_error: 3559393536.0000 - val_loss: 2717955891.2000 - val_mean_squared_error: 2717956096.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_normalized, y_train, epochs=250, validation_split=.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 25us/sample - loss: 4242867097.6000 - mean_squared_error: 4242867200.0000\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test_normalized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_squared_error: 4242867200.0\n"
     ]
    }
   ],
   "source": [
    "print(f'{model.metrics_names[1]}: {scores[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4714fe7b00>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "history dict: {'loss': [4560759408.276142, 4540851892.369543, 4529272637.3360405, 4513658013.498477, 4500250274.696446, 4486020668.556345, 4473076856.073096, 4458707273.551269, 4443677051.4517765, 4428913408.259898, 4415976157.9532995, 4401264911.33401, 4388170873.372589, 4372585793.494416, 4360320322.014214, 4345352176.925888, 4330433189.035533, 4318301761.494416, 4302362395.029442, 4288045200.2436547, 4274532154.737056, 4261010745.4375634, 4246572693.181726, 4234059698.290355, 4217359680.5847716, 4206845762.2741117, 4193093195.630457, 4175729569.6568527, 4161689471.870051, 4146699443.849746, 4133225866.26599, 4118670358.871066, 4104297097.746193, 4091405434.152284, 4077132827.9390864, 4062746316.8, 4050568597.181726, 4033786075.8741117, 4021295889.932995, 4008557967.204061, 3992977925.457868, 3979828203.9878173, 3965257973.8639593, 3953145980.4913707, 3940636222.3756347, 3924746054.9522843, 3912986511.723858, 3896661251.1187816, 3883617418.7857866, 3872367490.8588834, 3856827592.6416245, 3843887181.969543, 3829935001.6, 3816004088.203046, 3802939429.55533, 3789066141.238579, 3777252344.203046, 3764653645.969543, 3750524793.502538, 3741216741.230457, 3722404484.028426, 3712063990.123858, 3699931457.754315, 3686072667.7441626, 3672647337.323858, 3663225361.6730967, 3647346161.4456854, 3635805867.857868, 3622194651.094416, 3612448302.781726, 3597843906.1441627, 3585324656.536041, 3572903350.7086296, 3561947359.252792, 3549617611.11066, 3538330655.577665, 3525975373.449746, 3513989358.067005, 3502803142.5624366, 3492149320.7715735, 3478730109.920812, 3469151979.4680204, 3455738643.362437, 3444838350.8791876, 3434767767.001015, 3422690686.050761, 3410965154.696447, 3399300537.827411, 3389228756.0771575, 3377325019.8741117, 3365554052.8081217, 3354814348.3451777, 3346227579.4517765, 3332990132.369543, 3325363501.4822335, 3312306631.73198, 3301478616.1055837, 3290066435.37868, 3280257560.950254, 3270213090.3715734, 3259294255.3015227, 3248433080.7878175, 3238277021.7583756, 3227173626.542132, 3215908102.2375636, 3206188721.2507615, 3198414535.0822334, 3185569186.1766496, 3175422703.7563453, 3167531407.9837565, 3154217402.6071067, 3145074303.4152284, 3136975549.7258883, 3127604967.5695434, 3115918084.418274, 3106215947.435533, 3097201589.1492386, 3086396167.667005, 3078084921.3076143, 3067192230.594924, 3060362340.320812, 3048441424.5685277, 3041286945.5269036, 3030755232.3573604, 3021129445.7502537, 3012190384.7309647, 3003350532.8081217, 2993257438.4730964, 2986278191.4314723, 2977356712.1543145, 2971192526.3593907, 2959274515.752284, 2950105390.0020304, 2942018705.543147, 2934383078.1401014, 2926045605.6852794, 2918049069.4822335, 2909376335.528934, 2901309496.9177666, 2894777774.521827, 2886575213.4172587, 2878455336.0243654, 2870611219.362437, 2862890872.852792, 2855003110.2700505, 2848558020.22335, 2840186187.11066, 2834155865.924873, 2825524076.1177664, 2821587318.253807, 2811618152.2192893, 2804496146.7126904, 2797732848.9258885, 2791985020.4913707, 2788285097.7137055, 2777707132.231472, 2771582417.608122, 2766241270.383756, 2758383304.9015226, 2753659521.4294415, 2747432970.915736, 2739792140.3451777, 2733474154.298477, 2729566098.062944, 2720821233.7055836, 2714743384.2355328, 2710040369.6406093, 2704581637.7177663, 2697348382.408122, 2694914641.348223, 2684846172.523858, 2680741038.521827, 2674299517.790863, 2669699649.754315, 2662552434.550254, 2658146296.5928936, 2652110690.501523, 2647504818.810152, 2640686390.318782, 2636985440.1624365, 2631432995.9959393, 2624237271.4558377, 2622381082.639594, 2612959820.9299493, 2612079999.6101522, 2603444071.3096447, 2600746229.0192895, 2592226959.9837565, 2587652213.4741116, 2582337766.789848, 2577587197.271066, 2574774917.1979694, 2567321444.190863, 2563291822.651777, 2557457310.7979693, 2552791570.1928935, 2548701897.551269, 2543657322.0385785, 2540181118.5705585, 2534726808.3005075, 2531067831.2284265, 2525256432.66599, 2523419339.760406, 2517126995.297462, 2513281736.1218276, 2509528416.422335, 2506894673.738071, 2503620429.189848, 2501083424.4873095, 2495569939.232487, 2492034760.1218276, 2491002594.2416244, 2485880121.1776648, 2480144494.456853, 2477923228.97868, 2474042309.782741, 2472053808.2111673, 2468816746.298477, 2464585705.9086294, 2461714446.034518, 2458578648.4954314, 2456917916.4588833, 2452619925.701523, 2449928363.5329947, 2446425827.9309645, 2443685678.0020304, 2440630176.877157, 2438268254.0832486, 2435178493.6609135, 2431857607.0822334, 2430462310.6598983, 2429471059.0375633, 2424928272.1137056, 2422667614.3431473, 2418644777.323858, 2416300634.44467, 2415052983.2284265, 2411607262.4730964, 2409483890.7451777, 2406433167.9837565, 2406957227.013198, 2400990787.833503, 2400627532.6700506, 2397865007.5614214, 2396983112.381726, 2391831542.9035535, 2388920198.8873096, 2386475895.943147, 2383362865.5106597, 2381891791.0416245], 'mean_squared_error': [4560759300.0, 4540852000.0, 4529272300.0, 4513658400.0, 4500250600.0, 4486020600.0, 4473077000.0, 4458708000.0, 4443677000.0, 4428913000.0, 4415976400.0, 4401264600.0, 4388171000.0, 4372586000.0, 4360321000.0, 4345352000.0, 4330433000.0, 4318301700.0, 4302362600.0, 4288044800.0, 4274532000.0, 4261010200.0, 4246573000.0, 4234060000.0, 4217359000.0, 4206845700.0, 4193093000.0, 4175729700.0, 4161689000.0, 4146700000.0, 4133226200.0, 4118669800.0, 4104297000.0, 4091405300.0, 4077132300.0, 4062746600.0, 4050568000.0, 4033786400.0, 4021295900.0, 4008557600.0, 3992978000.0, 3979828000.0, 3965257700.0, 3953145900.0, 3940636200.0, 3924746000.0, 3912987000.0, 3896661200.0, 3883618000.0, 3872367000.0, 3856827000.0, 3843887400.0, 3829935400.0, 3816003600.0, 3802940200.0, 3789066800.0, 3777252400.0, 3764653800.0, 3750525000.0, 3741216800.0, 3722404600.0, 3712064300.0, 3699931100.0, 3686072600.0, 3672647000.0, 3663225300.0, 3647345700.0, 3635805200.0, 3622194200.0, 3612448000.0, 3597843700.0, 3585324500.0, 3572903000.0, 3561947100.0, 3549617700.0, 3538331100.0, 3525976000.0, 3513989000.0, 3502803000.0, 3492149200.0, 3478730200.0, 3469152000.0, 3455738600.0, 3444838400.0, 3434768000.0, 3422690600.0, 3410965200.0, 3399300900.0, 3389228300.0, 3377324800.0, 3365554000.0, 3354814000.0, 3346228200.0, 3332990200.0, 3325363500.0, 3312306400.0, 3301478100.0, 3290066400.0, 3280257300.0, 3270213000.0, 3259294500.0, 3248433700.0, 3238277000.0, 3227173600.0, 3215908000.0, 3206188300.0, 3198414600.0, 3185569300.0, 3175422500.0, 3167531300.0, 3154217200.0, 3145074200.0, 3136975600.0, 3127605000.0, 3115917800.0, 3106216000.0, 3097202000.0, 3086396700.0, 3078085000.0, 3067192000.0, 3060362000.0, 3048441600.0, 3041287200.0, 3030755000.0, 3021129500.0, 3012190200.0, 3003350800.0, 2993257200.0, 2986278000.0, 2977356300.0, 2971193000.0, 2959275000.0, 2950105300.0, 2942018300.0, 2934383400.0, 2926045200.0, 2918048800.0, 2909376300.0, 2901309400.0, 2894777600.0, 2886575000.0, 2878455300.0, 2870611000.0, 2862890200.0, 2855003400.0, 2848557600.0, 2840186600.0, 2834155800.0, 2825523700.0, 2821587200.0, 2811618300.0, 2804496400.0, 2797732900.0, 2791985200.0, 2788284700.0, 2777706800.0, 2771582000.0, 2766240800.0, 2758383600.0, 2753660000.0, 2747433000.0, 2739792600.0, 2733474300.0, 2729566500.0, 2720821000.0, 2714743300.0, 2710040300.0, 2704581600.0, 2697348600.0, 2694915000.0, 2684846300.0, 2680741000.0, 2674299400.0, 2669699800.0, 2662552600.0, 2658146300.0, 2652110600.0, 2647505200.0, 2640686300.0, 2636985300.0, 2631433000.0, 2624237300.0, 2622380800.0, 2612959500.0, 2612079900.0, 2603444000.0, 2600746000.0, 2592226800.0, 2587652400.0, 2582338000.0, 2577587200.0, 2574774800.0, 2567321600.0, 2563292000.0, 2557457400.0, 2552791300.0, 2548702500.0, 2543657200.0, 2540181500.0, 2534726700.0, 2531068000.0, 2525256400.0, 2523419400.0, 2517127400.0, 2513281800.0, 2509528600.0, 2506894800.0, 2503620600.0, 2501083100.0, 2495569700.0, 2492034800.0, 2491002400.0, 2485880300.0, 2480144600.0, 2477923800.0, 2474042400.0, 2472053500.0, 2468817000.0, 2464585700.0, 2461714400.0, 2458578700.0, 2456917500.0, 2452619800.0, 2449928200.0, 2446426000.0, 2443686000.0, 2440630300.0, 2438268400.0, 2435178500.0, 2431857700.0, 2430462500.0, 2429471000.0, 2424928300.0, 2422667500.0, 2418645000.0, 2416300500.0, 2415052800.0, 2411607000.0, 2409484000.0, 2406433500.0, 2406957600.0, 2400991000.0, 2400627500.0, 2397865000.0, 2396983300.0, 2391831300.0, 2388920000.0, 2386476000.0, 2383362600.0, 2381891300.0], 'val_loss': [3512184350.2545457, 3508052577.7454543, 3498504313.018182, 3484236311.2727275, 3469899515.3454547, 3464370394.7636366, 3454100638.2545457, 3439698131.7818184, 3429033397.5272727, 3414870639.7090907, 3402843962.181818, 3392344487.5636363, 3382645285.2363634, 3373086152.1454544, 3361120484.072727, 3348722208.581818, 3338221972.9454546, 3324999735.8545456, 3311869498.181818, 3301302830.5454545, 3295666536.7272725, 3279099496.7272725, 3267171346.6181817, 3264991818.4727273, 3244429302.690909, 3230511113.309091, 3227881904.8727274, 3208892774.4, 3203138266.7636366, 3189631052.8, 3179228257.7454543, 3168645115.3454547, 3155348898.909091, 3142611579.3454547, 3131638325.5272727, 3121729899.0545454, 3117018391.2727275, 3098883328.0, 3086120752.8727274, 3077748964.072727, 3064921274.181818, 3057097285.818182, 3043432780.8, 3030336833.163636, 3019844817.4545455, 3012671222.690909, 3003047121.4545455, 2989210689.163636, 2974801149.672727, 2970745211.3454547, 2954842088.7272725, 2946707707.3454547, 2931832729.6, 2924213666.909091, 2914654454.690909, 2902751008.581818, 2889222090.4727273, 2883741712.2909093, 2870337024.0, 2863246196.3636365, 2846445451.6363635, 2833223093.5272727, 2825640359.5636363, 2820717023.418182, 2807380940.8, 2794246800.2909093, 2788930317.9636364, 2783284577.7454543, 2768821566.836364, 2757497285.818182, 2746682186.4727273, 2735008847.1272726, 2727352801.7454543, 2721529213.672727, 2713093748.3636365, 2695543787.0545454, 2691067233.7454543, 2679756576.581818, 2667800275.7818184, 2665158195.2, 2650412274.0363636, 2644134860.8, 2633317885.672727, 2625526388.3636365, 2614723970.327273, 2609832501.5272727, 2592990694.4, 2589821705.309091, 2585214403.490909, 2570046077.672727, 2561824134.981818, 2557451461.818182, 2548674899.7818184, 2535757707.6363635, 2529036641.7454543, 2522742067.2, 2508316204.2181816, 2504534974.836364, 2498207083.0545454, 2490688856.4363637, 2483852609.163636, 2474878422.109091, 2458554409.890909, 2458399097.018182, 2447273439.418182, 2444225670.981818, 2430049154.327273, 2428585988.6545453, 2418245338.7636366, 2413747346.6181817, 2399706205.090909, 2392449887.418182, 2391857093.818182, 2376240276.9454546, 2372210259.7818184, 2369545574.4, 2358371004.509091, 2351873084.509091, 2348383888.2909093, 2334206440.7272725, 2328294495.418182, 2322975934.836364, 2315406501.2363634, 2303874120.1454544, 2298749817.018182, 2293590914.327273, 2282887168.0, 2282667331.490909, 2270661124.6545453, 2272739588.6545453, 2257539634.0363636, 2256426647.2727275, 2247929404.509091, 2240658054.981818, 2234460923.3454547, 2227515496.7272725, 2225276928.0, 2214436836.072727, 2210683331.490909, 2201816740.072727, 2198729958.4, 2197650085.2363634, 2192517385.309091, 2179236924.509091, 2177162551.8545456, 2175666334.2545457, 2162940763.927273, 2158326514.0363636, 2154864909.9636364, 2154351229.672727, 2146489630.2545455, 2138808876.2181818, 2135981037.3818183, 2129673162.4727273, 2124897033.3090909, 2121134787.490909, 2116573261.9636364, 2110138565.8181818, 2106172648.7272727, 2109096629.5272727, 2094370131.7818182, 2096250526.2545455, 2086790763.0545454, 2086616645.8181818, 2079857587.2, 2078293313.1636364, 2080430396.509091, 2072125537.7454545, 2063388674.3272727, 2067190532.6545455, 2057577835.0545454, 2050808236.2181818, 2051083394.3272727, 2048826258.6181817, 2042321354.4727273, 2040913640.7272727, 2038065340.509091, 2032978771.7818182, 2027692371.7818182, 2026462686.2545455, 2021676618.4727273, 2021548111.1272728, 2015193907.2, 2014491524.6545455, 2010791486.8363636, 2008216932.0727272, 2007154427.3454545, 1998847097.0181818, 1995601967.709091, 1992781258.4727273, 1992616857.6, 1985212038.9818182, 1986054998.1090908, 1981084321.7454545, 1979964732.509091, 1976166962.0363636, 1973666693.8181818, 1972828571.9272728, 1970650600.7272727, 1965455536.8727272, 1961169266.0363636, 1962086060.2181818, 1960416795.9272728, 1957055931.3454545, 1955048624.8727272, 1951084143.709091, 1950097030.9818182, 1951943177.3090909, 1948948903.5636363, 1946329984.0, 1945191214.5454545, 1938186432.0, 1941335312.290909, 1937092256.581818, 1935582936.4363637, 1930206566.4, 1931689141.5272727, 1928880840.1454546, 1927052360.1454546, 1928253807.709091, 1923334283.6363637, 1929297110.1090908, 1922098309.8181818, 1921459274.4727273, 1917421479.5636363, 1917194414.5454545, 1917876573.090909, 1913414412.8, 1912486445.3818183, 1913590795.6363637, 1909986145.7454545, 1916017929.3090909, 1907456194.3272727, 1904767657.8909092, 1906796129.7454545, 1907513704.7272727, 1902079052.8, 1902963190.6909091, 1902551891.7818182, 1901878811.9272728, 1897633852.509091, 1896818340.0727272, 1898151744.0, 1891240079.1272728, 1898413423.709091, 1892557623.8545454, 1894899388.509091, 1892352558.5454545, 1891675957.5272727, 1889716158.8363636], 'val_mean_squared_error': [3512184000.0, 3508052700.0, 3498504400.0, 3484236000.0, 3469899800.0, 3464370200.0, 3454100500.0, 3439698200.0, 3429033200.0, 3414870500.0, 3402844000.0, 3392344800.0, 3382645000.0, 3373086200.0, 3361120500.0, 3348722200.0, 3338221800.0, 3324999700.0, 3311869400.0, 3301302800.0, 3295666700.0, 3279099600.0, 3267171600.0, 3264992000.0, 3244429300.0, 3230511000.0, 3227881700.0, 3208892700.0, 3203138000.0, 3189631200.0, 3179228200.0, 3168645400.0, 3155349000.0, 3142611500.0, 3131638000.0, 3121729800.0, 3117018400.0, 3098883000.0, 3086121000.0, 3077748700.0, 3064921300.0, 3057097500.0, 3043433000.0, 3030336800.0, 3019844600.0, 3012671500.0, 3003047200.0, 2989211000.0, 2974801400.0, 2970745300.0, 2954842000.0, 2946707700.0, 2931832800.0, 2924213800.0, 2914654700.0, 2902751000.0, 2889222000.0, 2883741700.0, 2870337000.0, 2863246000.0, 2846445300.0, 2833223200.0, 2825640400.0, 2820717000.0, 2807380700.0, 2794247000.0, 2788930000.0, 2783284500.0, 2768821500.0, 2757497600.0, 2746682400.0, 2735008800.0, 2727353000.0, 2721529300.0, 2713094000.0, 2695544000.0, 2691067100.0, 2679756500.0, 2667800600.0, 2665158400.0, 2650412300.0, 2644135000.0, 2633318100.0, 2625526300.0, 2614724000.0, 2609832400.0, 2592990700.0, 2589821400.0, 2585214500.0, 2570046000.0, 2561824300.0, 2557451500.0, 2548675000.0, 2535757600.0, 2529036500.0, 2522742300.0, 2508316200.0, 2504535000.0, 2498207200.0, 2490688800.0, 2483852800.0, 2474878500.0, 2458554400.0, 2458399200.0, 2447273500.0, 2444225800.0, 2430049300.0, 2428586000.0, 2418245400.0, 2413747500.0, 2399706000.0, 2392449800.0, 2391857200.0, 2376240400.0, 2372210200.0, 2369545500.0, 2358371000.0, 2351873300.0, 2348384000.0, 2334206500.0, 2328294400.0, 2322976000.0, 2315406600.0, 2303874000.0, 2298749700.0, 2293590800.0, 2282887000.0, 2282667300.0, 2270661000.0, 2272739600.0, 2257539600.0, 2256426800.0, 2247929600.0, 2240658200.0, 2234461000.0, 2227515600.0, 2225277000.0, 2214437000.0, 2210683400.0, 2201816800.0, 2198730000.0, 2197650200.0, 2192517400.0, 2179236900.0, 2177162500.0, 2175666400.0, 2162940700.0, 2158326500.0, 2154865000.0, 2154351000.0, 2146489600.0, 2138809100.0, 2135980900.0, 2129673200.0, 2124897000.0, 2121134700.0, 2116573300.0, 2110138500.0, 2106172500.0, 2109096600.0, 2094370200.0, 2096250500.0, 2086790800.0, 2086616400.0, 2079857700.0, 2078293400.0, 2080430300.0, 2072125600.0, 2063388500.0, 2067190500.0, 2057577700.0, 2050808200.0, 2051083500.0, 2048826200.0, 2042321400.0, 2040913700.0, 2038065500.0, 2032978800.0, 2027692300.0, 2026462500.0, 2021676500.0, 2021548200.0, 2015193900.0, 2014491400.0, 2010791600.0, 2008217000.0, 2007154400.0, 1998847100.0, 1995601900.0, 1992781200.0, 1992616800.0, 1985212000.0, 1986054900.0, 1981084200.0, 1979964700.0, 1976166900.0, 1973666700.0, 1972828500.0, 1970650500.0, 1965455500.0, 1961169300.0, 1962086000.0, 1960416800.0, 1957056000.0, 1955048700.0, 1951084300.0, 1950097000.0, 1951943200.0, 1948948900.0, 1946329900.0, 1945191300.0, 1938186400.0, 1941335300.0, 1937092200.0, 1935583000.0, 1930206600.0, 1931689100.0, 1928880900.0, 1927052400.0, 1928253800.0, 1923334300.0, 1929297000.0, 1922098200.0, 1921459300.0, 1917421600.0, 1917194500.0, 1917876500.0, 1913414400.0, 1912486400.0, 1913590800.0, 1909986000.0, 1916017900.0, 1907456300.0, 1904767600.0, 1906796200.0, 1907513700.0, 1902079000.0, 1902963200.0, 1902551900.0, 1901878900.0, 1897633800.0, 1896818400.0, 1898151800.0, 1891240100.0, 1898413400.0, 1892557600.0, 1894899500.0, 1892352600.0, 1891675900.0, 1889716200.0]}\n"
     ]
    }
   ],
   "source": [
    "print('\\nhistory dict:', history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4560759408.276142,\n",
       " 4540851892.369543,\n",
       " 4529272637.3360405,\n",
       " 4513658013.498477,\n",
       " 4500250274.696446,\n",
       " 4486020668.556345,\n",
       " 4473076856.073096,\n",
       " 4458707273.551269,\n",
       " 4443677051.4517765,\n",
       " 4428913408.259898,\n",
       " 4415976157.9532995,\n",
       " 4401264911.33401,\n",
       " 4388170873.372589,\n",
       " 4372585793.494416,\n",
       " 4360320322.014214,\n",
       " 4345352176.925888,\n",
       " 4330433189.035533,\n",
       " 4318301761.494416,\n",
       " 4302362395.029442,\n",
       " 4288045200.2436547,\n",
       " 4274532154.737056,\n",
       " 4261010745.4375634,\n",
       " 4246572693.181726,\n",
       " 4234059698.290355,\n",
       " 4217359680.5847716,\n",
       " 4206845762.2741117,\n",
       " 4193093195.630457,\n",
       " 4175729569.6568527,\n",
       " 4161689471.870051,\n",
       " 4146699443.849746,\n",
       " 4133225866.26599,\n",
       " 4118670358.871066,\n",
       " 4104297097.746193,\n",
       " 4091405434.152284,\n",
       " 4077132827.9390864,\n",
       " 4062746316.8,\n",
       " 4050568597.181726,\n",
       " 4033786075.8741117,\n",
       " 4021295889.932995,\n",
       " 4008557967.204061,\n",
       " 3992977925.457868,\n",
       " 3979828203.9878173,\n",
       " 3965257973.8639593,\n",
       " 3953145980.4913707,\n",
       " 3940636222.3756347,\n",
       " 3924746054.9522843,\n",
       " 3912986511.723858,\n",
       " 3896661251.1187816,\n",
       " 3883617418.7857866,\n",
       " 3872367490.8588834,\n",
       " 3856827592.6416245,\n",
       " 3843887181.969543,\n",
       " 3829935001.6,\n",
       " 3816004088.203046,\n",
       " 3802939429.55533,\n",
       " 3789066141.238579,\n",
       " 3777252344.203046,\n",
       " 3764653645.969543,\n",
       " 3750524793.502538,\n",
       " 3741216741.230457,\n",
       " 3722404484.028426,\n",
       " 3712063990.123858,\n",
       " 3699931457.754315,\n",
       " 3686072667.7441626,\n",
       " 3672647337.323858,\n",
       " 3663225361.6730967,\n",
       " 3647346161.4456854,\n",
       " 3635805867.857868,\n",
       " 3622194651.094416,\n",
       " 3612448302.781726,\n",
       " 3597843906.1441627,\n",
       " 3585324656.536041,\n",
       " 3572903350.7086296,\n",
       " 3561947359.252792,\n",
       " 3549617611.11066,\n",
       " 3538330655.577665,\n",
       " 3525975373.449746,\n",
       " 3513989358.067005,\n",
       " 3502803142.5624366,\n",
       " 3492149320.7715735,\n",
       " 3478730109.920812,\n",
       " 3469151979.4680204,\n",
       " 3455738643.362437,\n",
       " 3444838350.8791876,\n",
       " 3434767767.001015,\n",
       " 3422690686.050761,\n",
       " 3410965154.696447,\n",
       " 3399300537.827411,\n",
       " 3389228756.0771575,\n",
       " 3377325019.8741117,\n",
       " 3365554052.8081217,\n",
       " 3354814348.3451777,\n",
       " 3346227579.4517765,\n",
       " 3332990132.369543,\n",
       " 3325363501.4822335,\n",
       " 3312306631.73198,\n",
       " 3301478616.1055837,\n",
       " 3290066435.37868,\n",
       " 3280257560.950254,\n",
       " 3270213090.3715734,\n",
       " 3259294255.3015227,\n",
       " 3248433080.7878175,\n",
       " 3238277021.7583756,\n",
       " 3227173626.542132,\n",
       " 3215908102.2375636,\n",
       " 3206188721.2507615,\n",
       " 3198414535.0822334,\n",
       " 3185569186.1766496,\n",
       " 3175422703.7563453,\n",
       " 3167531407.9837565,\n",
       " 3154217402.6071067,\n",
       " 3145074303.4152284,\n",
       " 3136975549.7258883,\n",
       " 3127604967.5695434,\n",
       " 3115918084.418274,\n",
       " 3106215947.435533,\n",
       " 3097201589.1492386,\n",
       " 3086396167.667005,\n",
       " 3078084921.3076143,\n",
       " 3067192230.594924,\n",
       " 3060362340.320812,\n",
       " 3048441424.5685277,\n",
       " 3041286945.5269036,\n",
       " 3030755232.3573604,\n",
       " 3021129445.7502537,\n",
       " 3012190384.7309647,\n",
       " 3003350532.8081217,\n",
       " 2993257438.4730964,\n",
       " 2986278191.4314723,\n",
       " 2977356712.1543145,\n",
       " 2971192526.3593907,\n",
       " 2959274515.752284,\n",
       " 2950105390.0020304,\n",
       " 2942018705.543147,\n",
       " 2934383078.1401014,\n",
       " 2926045605.6852794,\n",
       " 2918049069.4822335,\n",
       " 2909376335.528934,\n",
       " 2901309496.9177666,\n",
       " 2894777774.521827,\n",
       " 2886575213.4172587,\n",
       " 2878455336.0243654,\n",
       " 2870611219.362437,\n",
       " 2862890872.852792,\n",
       " 2855003110.2700505,\n",
       " 2848558020.22335,\n",
       " 2840186187.11066,\n",
       " 2834155865.924873,\n",
       " 2825524076.1177664,\n",
       " 2821587318.253807,\n",
       " 2811618152.2192893,\n",
       " 2804496146.7126904,\n",
       " 2797732848.9258885,\n",
       " 2791985020.4913707,\n",
       " 2788285097.7137055,\n",
       " 2777707132.231472,\n",
       " 2771582417.608122,\n",
       " 2766241270.383756,\n",
       " 2758383304.9015226,\n",
       " 2753659521.4294415,\n",
       " 2747432970.915736,\n",
       " 2739792140.3451777,\n",
       " 2733474154.298477,\n",
       " 2729566098.062944,\n",
       " 2720821233.7055836,\n",
       " 2714743384.2355328,\n",
       " 2710040369.6406093,\n",
       " 2704581637.7177663,\n",
       " 2697348382.408122,\n",
       " 2694914641.348223,\n",
       " 2684846172.523858,\n",
       " 2680741038.521827,\n",
       " 2674299517.790863,\n",
       " 2669699649.754315,\n",
       " 2662552434.550254,\n",
       " 2658146296.5928936,\n",
       " 2652110690.501523,\n",
       " 2647504818.810152,\n",
       " 2640686390.318782,\n",
       " 2636985440.1624365,\n",
       " 2631432995.9959393,\n",
       " 2624237271.4558377,\n",
       " 2622381082.639594,\n",
       " 2612959820.9299493,\n",
       " 2612079999.6101522,\n",
       " 2603444071.3096447,\n",
       " 2600746229.0192895,\n",
       " 2592226959.9837565,\n",
       " 2587652213.4741116,\n",
       " 2582337766.789848,\n",
       " 2577587197.271066,\n",
       " 2574774917.1979694,\n",
       " 2567321444.190863,\n",
       " 2563291822.651777,\n",
       " 2557457310.7979693,\n",
       " 2552791570.1928935,\n",
       " 2548701897.551269,\n",
       " 2543657322.0385785,\n",
       " 2540181118.5705585,\n",
       " 2534726808.3005075,\n",
       " 2531067831.2284265,\n",
       " 2525256432.66599,\n",
       " 2523419339.760406,\n",
       " 2517126995.297462,\n",
       " 2513281736.1218276,\n",
       " 2509528416.422335,\n",
       " 2506894673.738071,\n",
       " 2503620429.189848,\n",
       " 2501083424.4873095,\n",
       " 2495569939.232487,\n",
       " 2492034760.1218276,\n",
       " 2491002594.2416244,\n",
       " 2485880121.1776648,\n",
       " 2480144494.456853,\n",
       " 2477923228.97868,\n",
       " 2474042309.782741,\n",
       " 2472053808.2111673,\n",
       " 2468816746.298477,\n",
       " 2464585705.9086294,\n",
       " 2461714446.034518,\n",
       " 2458578648.4954314,\n",
       " 2456917916.4588833,\n",
       " 2452619925.701523,\n",
       " 2449928363.5329947,\n",
       " 2446425827.9309645,\n",
       " 2443685678.0020304,\n",
       " 2440630176.877157,\n",
       " 2438268254.0832486,\n",
       " 2435178493.6609135,\n",
       " 2431857607.0822334,\n",
       " 2430462310.6598983,\n",
       " 2429471059.0375633,\n",
       " 2424928272.1137056,\n",
       " 2422667614.3431473,\n",
       " 2418644777.323858,\n",
       " 2416300634.44467,\n",
       " 2415052983.2284265,\n",
       " 2411607262.4730964,\n",
       " 2409483890.7451777,\n",
       " 2406433167.9837565,\n",
       " 2406957227.013198,\n",
       " 2400990787.833503,\n",
       " 2400627532.6700506,\n",
       " 2397865007.5614214,\n",
       " 2396983112.381726,\n",
       " 2391831542.9035535,\n",
       " 2388920198.8873096,\n",
       " 2386475895.943147,\n",
       " 2383362865.5106597,\n",
       " 2381891791.0416245]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2381891791.0416245"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaxis = [num / 1000000000 for num in history.history['loss']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.560759408276142,\n",
       " 4.540851892369543,\n",
       " 4.529272637336041,\n",
       " 4.513658013498477,\n",
       " 4.500250274696446,\n",
       " 4.486020668556345,\n",
       " 4.473076856073097,\n",
       " 4.458707273551268,\n",
       " 4.443677051451776,\n",
       " 4.428913408259898,\n",
       " 4.4159761579533,\n",
       " 4.40126491133401,\n",
       " 4.388170873372589,\n",
       " 4.372585793494416,\n",
       " 4.360320322014213,\n",
       " 4.345352176925888,\n",
       " 4.330433189035533,\n",
       " 4.318301761494416,\n",
       " 4.302362395029442,\n",
       " 4.288045200243655,\n",
       " 4.274532154737056,\n",
       " 4.261010745437564,\n",
       " 4.246572693181726,\n",
       " 4.2340596982903556,\n",
       " 4.217359680584772,\n",
       " 4.206845762274112,\n",
       " 4.193093195630457,\n",
       " 4.175729569656853,\n",
       " 4.161689471870051,\n",
       " 4.146699443849746,\n",
       " 4.13322586626599,\n",
       " 4.118670358871066,\n",
       " 4.104297097746193,\n",
       " 4.091405434152284,\n",
       " 4.077132827939087,\n",
       " 4.0627463168,\n",
       " 4.050568597181726,\n",
       " 4.0337860758741115,\n",
       " 4.021295889932995,\n",
       " 4.008557967204061,\n",
       " 3.992977925457868,\n",
       " 3.9798282039878172,\n",
       " 3.9652579738639595,\n",
       " 3.953145980491371,\n",
       " 3.940636222375635,\n",
       " 3.9247460549522843,\n",
       " 3.912986511723858,\n",
       " 3.8966612511187817,\n",
       " 3.8836174187857866,\n",
       " 3.8723674908588834,\n",
       " 3.8568275926416247,\n",
       " 3.843887181969543,\n",
       " 3.8299350016,\n",
       " 3.8160040882030457,\n",
       " 3.80293942955533,\n",
       " 3.7890661412385787,\n",
       " 3.777252344203046,\n",
       " 3.7646536459695428,\n",
       " 3.7505247935025383,\n",
       " 3.7412167412304567,\n",
       " 3.7224044840284263,\n",
       " 3.712063990123858,\n",
       " 3.6999314577543148,\n",
       " 3.6860726677441624,\n",
       " 3.6726473373238577,\n",
       " 3.6632253616730965,\n",
       " 3.6473461614456855,\n",
       " 3.635805867857868,\n",
       " 3.622194651094416,\n",
       " 3.612448302781726,\n",
       " 3.5978439061441625,\n",
       " 3.5853246565360406,\n",
       " 3.5729033507086294,\n",
       " 3.561947359252792,\n",
       " 3.54961761111066,\n",
       " 3.538330655577665,\n",
       " 3.525975373449746,\n",
       " 3.513989358067005,\n",
       " 3.5028031425624366,\n",
       " 3.4921493207715737,\n",
       " 3.478730109920812,\n",
       " 3.4691519794680206,\n",
       " 3.455738643362437,\n",
       " 3.4448383508791878,\n",
       " 3.434767767001015,\n",
       " 3.4226906860507613,\n",
       " 3.410965154696447,\n",
       " 3.3993005378274113,\n",
       " 3.3892287560771575,\n",
       " 3.3773250198741116,\n",
       " 3.365554052808122,\n",
       " 3.3548143483451778,\n",
       " 3.3462275794517766,\n",
       " 3.332990132369543,\n",
       " 3.3253635014822334,\n",
       " 3.31230663173198,\n",
       " 3.301478616105584,\n",
       " 3.29006643537868,\n",
       " 3.280257560950254,\n",
       " 3.2702130903715734,\n",
       " 3.2592942553015227,\n",
       " 3.2484330807878177,\n",
       " 3.2382770217583756,\n",
       " 3.227173626542132,\n",
       " 3.2159081022375635,\n",
       " 3.2061887212507614,\n",
       " 3.1984145350822333,\n",
       " 3.1855691861766497,\n",
       " 3.1754227037563454,\n",
       " 3.1675314079837564,\n",
       " 3.154217402607107,\n",
       " 3.145074303415228,\n",
       " 3.1369755497258884,\n",
       " 3.1276049675695434,\n",
       " 3.115918084418274,\n",
       " 3.106215947435533,\n",
       " 3.0972015891492384,\n",
       " 3.086396167667005,\n",
       " 3.0780849213076142,\n",
       " 3.067192230594924,\n",
       " 3.0603623403208124,\n",
       " 3.048441424568528,\n",
       " 3.041286945526904,\n",
       " 3.0307552323573606,\n",
       " 3.0211294457502538,\n",
       " 3.012190384730965,\n",
       " 3.003350532808122,\n",
       " 2.9932574384730963,\n",
       " 2.9862781914314724,\n",
       " 2.9773567121543145,\n",
       " 2.9711925263593906,\n",
       " 2.959274515752284,\n",
       " 2.9501053900020304,\n",
       " 2.942018705543147,\n",
       " 2.9343830781401015,\n",
       " 2.9260456056852795,\n",
       " 2.9180490694822336,\n",
       " 2.909376335528934,\n",
       " 2.9013094969177664,\n",
       " 2.894777774521827,\n",
       " 2.8865752134172586,\n",
       " 2.8784553360243654,\n",
       " 2.870611219362437,\n",
       " 2.862890872852792,\n",
       " 2.8550031102700504,\n",
       " 2.8485580202233503,\n",
       " 2.84018618711066,\n",
       " 2.8341558659248727,\n",
       " 2.8255240761177665,\n",
       " 2.821587318253807,\n",
       " 2.8116181522192893,\n",
       " 2.8044961467126903,\n",
       " 2.7977328489258886,\n",
       " 2.7919850204913708,\n",
       " 2.7882850977137057,\n",
       " 2.777707132231472,\n",
       " 2.771582417608122,\n",
       " 2.7662412703837562,\n",
       " 2.7583833049015225,\n",
       " 2.7536595214294413,\n",
       " 2.747432970915736,\n",
       " 2.7397921403451777,\n",
       " 2.7334741542984773,\n",
       " 2.729566098062944,\n",
       " 2.7208212337055837,\n",
       " 2.7147433842355326,\n",
       " 2.7100403696406095,\n",
       " 2.7045816377177663,\n",
       " 2.697348382408122,\n",
       " 2.6949146413482232,\n",
       " 2.684846172523858,\n",
       " 2.6807410385218273,\n",
       " 2.674299517790863,\n",
       " 2.669699649754315,\n",
       " 2.662552434550254,\n",
       " 2.6581462965928937,\n",
       " 2.652110690501523,\n",
       " 2.647504818810152,\n",
       " 2.640686390318782,\n",
       " 2.6369854401624364,\n",
       " 2.6314329959959393,\n",
       " 2.6242372714558377,\n",
       " 2.622381082639594,\n",
       " 2.6129598209299494,\n",
       " 2.612079999610152,\n",
       " 2.6034440713096445,\n",
       " 2.6007462290192893,\n",
       " 2.5922269599837566,\n",
       " 2.5876522134741116,\n",
       " 2.582337766789848,\n",
       " 2.577587197271066,\n",
       " 2.5747749171979692,\n",
       " 2.567321444190863,\n",
       " 2.563291822651777,\n",
       " 2.5574573107979695,\n",
       " 2.5527915701928934,\n",
       " 2.548701897551269,\n",
       " 2.5436573220385785,\n",
       " 2.5401811185705587,\n",
       " 2.5347268083005075,\n",
       " 2.5310678312284263,\n",
       " 2.52525643266599,\n",
       " 2.523419339760406,\n",
       " 2.517126995297462,\n",
       " 2.5132817361218276,\n",
       " 2.5095284164223353,\n",
       " 2.506894673738071,\n",
       " 2.503620429189848,\n",
       " 2.5010834244873092,\n",
       " 2.495569939232487,\n",
       " 2.4920347601218276,\n",
       " 2.4910025942416243,\n",
       " 2.4858801211776647,\n",
       " 2.480144494456853,\n",
       " 2.47792322897868,\n",
       " 2.474042309782741,\n",
       " 2.472053808211167,\n",
       " 2.468816746298477,\n",
       " 2.4645857059086294,\n",
       " 2.461714446034518,\n",
       " 2.4585786484954313,\n",
       " 2.4569179164588832,\n",
       " 2.452619925701523,\n",
       " 2.4499283635329947,\n",
       " 2.4464258279309643,\n",
       " 2.4436856780020304,\n",
       " 2.440630176877157,\n",
       " 2.4382682540832485,\n",
       " 2.4351784936609135,\n",
       " 2.4318576070822333,\n",
       " 2.430462310659898,\n",
       " 2.429471059037563,\n",
       " 2.4249282721137058,\n",
       " 2.422667614343147,\n",
       " 2.4186447773238577,\n",
       " 2.4163006344446702,\n",
       " 2.4150529832284264,\n",
       " 2.411607262473096,\n",
       " 2.409483890745178,\n",
       " 2.4064331679837565,\n",
       " 2.406957227013198,\n",
       " 2.4009907878335026,\n",
       " 2.400627532670051,\n",
       " 2.397865007561421,\n",
       " 2.396983112381726,\n",
       " 2.3918315429035535,\n",
       " 2.3889201988873094,\n",
       " 2.386475895943147,\n",
       " 2.3833628655106596,\n",
       " 2.3818917910416246]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(yaxis)\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Mean squared error (in billions)')\n",
    "plt.title('Reduction of mean squared error through the epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "reg.fit(X_train_normalized, y_train)\n",
    "y_pred = reg.predict(X_test_normalized)\n",
    "\n",
    "lin_mse = mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regression's MSE is $2,758,222,467.30 lower than the neural network's MSE.\n"
     ]
    }
   ],
   "source": [
    "if lin_mse > scores[1]:\n",
    "    print(\"The neural network's MSE is ${:,.2f} lower than the regression's MSE.\".format(lin_mse - scores[1]))\n",
    "else:\n",
    "    print(\"The regression's MSE is ${:,.2f} lower than the neural network's MSE.\".format(scores[1] - lin_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_44 (Dense)             (None, 20)                1600      \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 3,301\n",
      "Trainable params: 3,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential(name=\"nn2\")\n",
    "model2.add(Dense(20, input_dim=79, activation='relu'))\n",
    "model2.add(Dense(20, activation='relu'))\n",
    "model2.add(Dense(20, activation='relu'))\n",
    "model2.add(Dense(20, activation='relu'))\n",
    "model2.add(Dense(20, activation='relu'))\n",
    "model2.add(Dense(1, activation='relu'))\n",
    "\n",
    "model2.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 985 samples, validate on 110 samples\n",
      "Epoch 1/250\n",
      "985/985 [==============================] - 1s 566us/sample - loss: 39057434143.7076 - mean_squared_error: 39057436672.0000 - val_loss: 36152687783.5636 - val_mean_squared_error: 36152684544.0000\n",
      "Epoch 2/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 39057210327.4558 - mean_squared_error: 39057211392.0000 - val_loss: 36152217078.6909 - val_mean_squared_error: 36152217600.0000\n",
      "Epoch 3/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 39055954955.4355 - mean_squared_error: 39055953920.0000 - val_loss: 36149595601.4545 - val_mean_squared_error: 36149596160.0000\n",
      "Epoch 4/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 39049389200.5035 - mean_squared_error: 39049388032.0000 - val_loss: 36136450904.4364 - val_mean_squared_error: 36136452096.0000\n",
      "Epoch 5/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 39019732334.9766 - mean_squared_error: 39019737088.0000 - val_loss: 36081239840.5818 - val_mean_squared_error: 36081242112.0000\n",
      "Epoch 6/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 38908915657.9411 - mean_squared_error: 38908919808.0000 - val_loss: 35892462089.3091 - val_mean_squared_error: 35892461568.0000\n",
      "Epoch 7/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 38566693909.8315 - mean_squared_error: 38566694912.0000 - val_loss: 35358146336.5818 - val_mean_squared_error: 35358146560.0000\n",
      "Epoch 8/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 37692630877.8233 - mean_squared_error: 37692633088.0000 - val_loss: 34077041235.7818 - val_mean_squared_error: 34077042688.0000\n",
      "Epoch 9/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 35749394874.8670 - mean_squared_error: 35749392384.0000 - val_loss: 31422143208.7273 - val_mean_squared_error: 31422142464.0000\n",
      "Epoch 10/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 32027105386.0386 - mean_squared_error: 32027101184.0000 - val_loss: 26709687612.5091 - val_mean_squared_error: 26709688320.0000\n",
      "Epoch 11/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 25947812803.7036 - mean_squared_error: 25947807744.0000 - val_loss: 19709906832.2909 - val_mean_squared_error: 19709906944.0000\n",
      "Epoch 12/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 18108297674.4609 - mean_squared_error: 18108297216.0000 - val_loss: 11866731222.1091 - val_mean_squared_error: 11866729472.0000\n",
      "Epoch 13/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 10931529089.9492 - mean_squared_error: 10931529728.0000 - val_loss: 6654086311.5636 - val_mean_squared_error: 6654086144.0000\n",
      "Epoch 14/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 7441033390.1320 - mean_squared_error: 7441033216.0000 - val_loss: 5378055084.2182 - val_mean_squared_error: 5378055168.0000\n",
      "Epoch 15/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 6903831364.8731 - mean_squared_error: 6903831552.0000 - val_loss: 5335706624.0000 - val_mean_squared_error: 5335706624.0000\n",
      "Epoch 16/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 6851887910.2051 - mean_squared_error: 6851888640.0000 - val_loss: 5296484594.0364 - val_mean_squared_error: 5296484864.0000\n",
      "Epoch 17/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 6810023733.2792 - mean_squared_error: 6810023424.0000 - val_loss: 5271016168.7273 - val_mean_squared_error: 5271016448.0000\n",
      "Epoch 18/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 6777557388.0853 - mean_squared_error: 6777558016.0000 - val_loss: 5243751163.3455 - val_mean_squared_error: 5243750912.0000\n",
      "Epoch 19/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 6739492290.6640 - mean_squared_error: 6739491840.0000 - val_loss: 5213503846.4000 - val_mean_squared_error: 5213504000.0000\n",
      "Epoch 20/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 6708089929.8112 - mean_squared_error: 6708090368.0000 - val_loss: 5186650516.9455 - val_mean_squared_error: 5186650624.0000\n",
      "Epoch 21/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 6668843988.8569 - mean_squared_error: 6668844032.0000 - val_loss: 5160027159.2727 - val_mean_squared_error: 5160027136.0000\n",
      "Epoch 22/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 6631961655.0985 - mean_squared_error: 6631961600.0000 - val_loss: 5135900895.4182 - val_mean_squared_error: 5135900672.0000\n",
      "Epoch 23/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 6597878355.6873 - mean_squared_error: 6597878784.0000 - val_loss: 5107062541.9636 - val_mean_squared_error: 5107062784.0000\n",
      "Epoch 24/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 6559120033.6569 - mean_squared_error: 6559119872.0000 - val_loss: 5078537099.6364 - val_mean_squared_error: 5078537216.0000\n",
      "Epoch 25/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 6529826663.4396 - mean_squared_error: 6529826816.0000 - val_loss: 5051247639.2727 - val_mean_squared_error: 5051248128.0000\n",
      "Epoch 26/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 6490571270.7574 - mean_squared_error: 6490572288.0000 - val_loss: 5022304265.3091 - val_mean_squared_error: 5022304256.0000\n",
      "Epoch 27/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 6461775336.6091 - mean_squared_error: 6461775360.0000 - val_loss: 4993478479.1273 - val_mean_squared_error: 4993478656.0000\n",
      "Epoch 28/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 6423972210.0954 - mean_squared_error: 6423972352.0000 - val_loss: 4968661699.4909 - val_mean_squared_error: 4968661504.0000\n",
      "Epoch 29/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 6390600503.2284 - mean_squared_error: 6390600192.0000 - val_loss: 4936943373.9636 - val_mean_squared_error: 4936943104.0000\n",
      "Epoch 30/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 6354681629.8883 - mean_squared_error: 6354681856.0000 - val_loss: 4917523921.4545 - val_mean_squared_error: 4917523968.0000\n",
      "Epoch 31/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 6320478298.9645 - mean_squared_error: 6320478720.0000 - val_loss: 4889955565.3818 - val_mean_squared_error: 4889955328.0000\n",
      "Epoch 32/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 6277772092.2964 - mean_squared_error: 6277772288.0000 - val_loss: 4870868936.1455 - val_mean_squared_error: 4870868992.0000\n",
      "Epoch 33/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 6243969448.6741 - mean_squared_error: 6243970048.0000 - val_loss: 4836791528.7273 - val_mean_squared_error: 4836791296.0000\n",
      "Epoch 34/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 6219732730.0223 - mean_squared_error: 6219733504.0000 - val_loss: 4805769360.2909 - val_mean_squared_error: 4805769728.0000\n",
      "Epoch 35/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 6176637555.9147 - mean_squared_error: 6176637440.0000 - val_loss: 4790616920.4364 - val_mean_squared_error: 4790617088.0000\n",
      "Epoch 36/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 6136151034.8020 - mean_squared_error: 6136150528.0000 - val_loss: 4751218799.7091 - val_mean_squared_error: 4751218688.0000\n",
      "Epoch 37/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 6102222168.1056 - mean_squared_error: 6102222336.0000 - val_loss: 4726297409.1636 - val_mean_squared_error: 4726297088.0000\n",
      "Epoch 38/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 6067426408.2193 - mean_squared_error: 6067425792.0000 - val_loss: 4706058500.6545 - val_mean_squared_error: 4706058240.0000\n",
      "Epoch 39/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 6029543774.3431 - mean_squared_error: 6029542912.0000 - val_loss: 4664836654.5455 - val_mean_squared_error: 4664837120.0000\n",
      "Epoch 40/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 5999638663.1472 - mean_squared_error: 5999639040.0000 - val_loss: 4642816539.9273 - val_mean_squared_error: 4642816512.0000\n",
      "Epoch 41/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 5961583360.7797 - mean_squared_error: 5961583616.0000 - val_loss: 4618127299.4909 - val_mean_squared_error: 4618126848.0000\n",
      "Epoch 42/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 5933278461.6609 - mean_squared_error: 5933278208.0000 - val_loss: 4590920769.1636 - val_mean_squared_error: 4590920704.0000\n",
      "Epoch 43/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 5888376280.4954 - mean_squared_error: 5888375808.0000 - val_loss: 4559539523.4909 - val_mean_squared_error: 4559539200.0000\n",
      "Epoch 44/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 5851592241.9005 - mean_squared_error: 5851592704.0000 - val_loss: 4533560031.4182 - val_mean_squared_error: 4533559808.0000\n",
      "Epoch 45/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 5817667528.1218 - mean_squared_error: 5817667072.0000 - val_loss: 4510002729.8909 - val_mean_squared_error: 4510002688.0000\n",
      "Epoch 46/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 5786859593.8112 - mean_squared_error: 5786860032.0000 - val_loss: 4478340407.8545 - val_mean_squared_error: 4478340096.0000\n",
      "Epoch 47/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 5758670393.1777 - mean_squared_error: 5758670336.0000 - val_loss: 4443092279.8545 - val_mean_squared_error: 4443091968.0000\n",
      "Epoch 48/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 5720511995.5817 - mean_squared_error: 5720510976.0000 - val_loss: 4432635001.0182 - val_mean_squared_error: 4432634880.0000\n",
      "Epoch 49/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 5673125203.9472 - mean_squared_error: 5673124352.0000 - val_loss: 4402932773.2364 - val_mean_squared_error: 4402932736.0000\n",
      "Epoch 50/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 5639654124.5076 - mean_squared_error: 5639654912.0000 - val_loss: 4368660363.6364 - val_mean_squared_error: 4368660480.0000\n",
      "Epoch 51/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 5607731165.1736 - mean_squared_error: 5607731200.0000 - val_loss: 4331917484.2182 - val_mean_squared_error: 4331917824.0000\n",
      "Epoch 52/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 5562255784.1543 - mean_squared_error: 5562255360.0000 - val_loss: 4307695434.4727 - val_mean_squared_error: 4307695104.0000\n",
      "Epoch 53/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 5529009723.2569 - mean_squared_error: 5529009664.0000 - val_loss: 4280269824.0000 - val_mean_squared_error: 4280270080.0000\n",
      "Epoch 54/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 5490706194.9726 - mean_squared_error: 5490706432.0000 - val_loss: 4243338509.9636 - val_mean_squared_error: 4243338752.0000\n",
      "Epoch 55/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 5453269667.2162 - mean_squared_error: 5453269504.0000 - val_loss: 4220099742.2545 - val_mean_squared_error: 4220099584.0000\n",
      "Epoch 56/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 5411379100.7188 - mean_squared_error: 5411379200.0000 - val_loss: 4204200070.9818 - val_mean_squared_error: 4204199936.0000\n",
      "Epoch 57/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 5373440801.5269 - mean_squared_error: 5373441024.0000 - val_loss: 4160245313.1636 - val_mean_squared_error: 4160245248.0000\n",
      "Epoch 58/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 5337095478.8386 - mean_squared_error: 5337095168.0000 - val_loss: 4131020143.7091 - val_mean_squared_error: 4131020032.0000\n",
      "Epoch 59/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 5318189618.9401 - mean_squared_error: 5318189568.0000 - val_loss: 4100147944.7273 - val_mean_squared_error: 4100147712.0000\n",
      "Epoch 60/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 5260573760.4548 - mean_squared_error: 5260573696.0000 - val_loss: 4066125023.4182 - val_mean_squared_error: 4066125056.0000\n",
      "Epoch 61/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 5220016105.6487 - mean_squared_error: 5220016128.0000 - val_loss: 4040658329.6000 - val_mean_squared_error: 4040658432.0000\n",
      "Epoch 62/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 5180763384.9827 - mean_squared_error: 5180763648.0000 - val_loss: 4007316517.2364 - val_mean_squared_error: 4007316736.0000\n",
      "Epoch 63/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 5141858298.8020 - mean_squared_error: 5141857792.0000 - val_loss: 3981368180.3636 - val_mean_squared_error: 3981368064.0000\n",
      "Epoch 64/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 5101298968.4305 - mean_squared_error: 5101299200.0000 - val_loss: 3954012378.7636 - val_mean_squared_error: 3954012416.0000\n",
      "Epoch 65/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 5064365993.1939 - mean_squared_error: 5064365568.0000 - val_loss: 3907717106.0364 - val_mean_squared_error: 3907717120.0000\n",
      "Epoch 66/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 5019830314.7533 - mean_squared_error: 5019830272.0000 - val_loss: 3889694771.2000 - val_mean_squared_error: 3889694720.0000\n",
      "Epoch 67/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 4985742721.9492 - mean_squared_error: 4985742336.0000 - val_loss: 3856816993.7455 - val_mean_squared_error: 3856816896.0000\n",
      "Epoch 68/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 4940687610.6721 - mean_squared_error: 4940686848.0000 - val_loss: 3821295460.0727 - val_mean_squared_error: 3821295360.0000\n",
      "Epoch 69/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 4895165439.7401 - mean_squared_error: 4895165952.0000 - val_loss: 3787028363.6364 - val_mean_squared_error: 3787028480.0000\n",
      "Epoch 70/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 4852108283.3218 - mean_squared_error: 4852107776.0000 - val_loss: 3748278365.0909 - val_mean_squared_error: 3748278272.0000\n",
      "Epoch 71/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 4810768609.5919 - mean_squared_error: 4810769408.0000 - val_loss: 3722081775.7091 - val_mean_squared_error: 3722081792.0000\n",
      "Epoch 72/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 4785105744.3086 - mean_squared_error: 4785105920.0000 - val_loss: 3701172992.0000 - val_mean_squared_error: 3701172992.0000\n",
      "Epoch 73/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 4721767389.4335 - mean_squared_error: 4721766912.0000 - val_loss: 3645601605.8182 - val_mean_squared_error: 3645601536.0000\n",
      "Epoch 74/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 4690696112.7310 - mean_squared_error: 4690696192.0000 - val_loss: 3617284263.5636 - val_mean_squared_error: 3617284352.0000\n",
      "Epoch 75/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 4641937238.5462 - mean_squared_error: 4641937920.0000 - val_loss: 3571109017.6000 - val_mean_squared_error: 3571108864.0000\n",
      "Epoch 76/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 4597621854.6030 - mean_squared_error: 4597621760.0000 - val_loss: 3539016650.4727 - val_mean_squared_error: 3539016704.0000\n",
      "Epoch 77/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 4548206999.0010 - mean_squared_error: 4548208128.0000 - val_loss: 3511340213.5273 - val_mean_squared_error: 3511340288.0000\n",
      "Epoch 78/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 4507778636.4102 - mean_squared_error: 4507778560.0000 - val_loss: 3476376198.9818 - val_mean_squared_error: 3476376064.0000\n",
      "Epoch 79/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 4462739045.8802 - mean_squared_error: 4462738944.0000 - val_loss: 3434562299.3455 - val_mean_squared_error: 3434562048.0000\n",
      "Epoch 80/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 4412101444.8731 - mean_squared_error: 4412101632.0000 - val_loss: 3392369999.1273 - val_mean_squared_error: 3392370176.0000\n",
      "Epoch 81/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 4366721203.8497 - mean_squared_error: 4366721536.0000 - val_loss: 3364396595.2000 - val_mean_squared_error: 3364396544.0000\n",
      "Epoch 82/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 4322545942.0914 - mean_squared_error: 4322545664.0000 - val_loss: 3322268492.8000 - val_mean_squared_error: 3322268672.0000\n",
      "Epoch 83/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 4272513197.6122 - mean_squared_error: 4272512768.0000 - val_loss: 3290914099.2000 - val_mean_squared_error: 3290914048.0000\n",
      "Epoch 84/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 4227117645.4497 - mean_squared_error: 4227117824.0000 - val_loss: 3255582459.3455 - val_mean_squared_error: 3255582464.0000\n",
      "Epoch 85/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 4179532172.8650 - mean_squared_error: 4179532288.0000 - val_loss: 3207140116.9455 - val_mean_squared_error: 3207140352.0000\n",
      "Epoch 86/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 4129624691.9147 - mean_squared_error: 4129624576.0000 - val_loss: 3174154942.8364 - val_mean_squared_error: 3174154752.0000\n",
      "Epoch 87/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 4083261782.2863 - mean_squared_error: 4083262208.0000 - val_loss: 3143232239.7091 - val_mean_squared_error: 3143232000.0000\n",
      "Epoch 88/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 4042479398.7249 - mean_squared_error: 4042479616.0000 - val_loss: 3077678391.8545 - val_mean_squared_error: 3077678592.0000\n",
      "Epoch 89/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 3993205422.9117 - mean_squared_error: 3993204992.0000 - val_loss: 3054937753.6000 - val_mean_squared_error: 3054937600.0000\n",
      "Epoch 90/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 3942063772.9787 - mean_squared_error: 3942063616.0000 - val_loss: 3018983163.3455 - val_mean_squared_error: 3018983168.0000\n",
      "Epoch 91/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 3894676051.4274 - mean_squared_error: 3894675968.0000 - val_loss: 2976269891.4909 - val_mean_squared_error: 2976270080.0000\n",
      "Epoch 92/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 3849482624.1299 - mean_squared_error: 3849482496.0000 - val_loss: 2935302777.0182 - val_mean_squared_error: 2935302912.0000\n",
      "Epoch 93/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 3805956403.2000 - mean_squared_error: 3805956352.0000 - val_loss: 2907402337.7455 - val_mean_squared_error: 2907402496.0000\n",
      "Epoch 94/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 3760272577.3645 - mean_squared_error: 3760272896.0000 - val_loss: 2848622643.2000 - val_mean_squared_error: 2848622592.0000\n",
      "Epoch 95/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 3714548224.5198 - mean_squared_error: 3714547968.0000 - val_loss: 2832836968.7273 - val_mean_squared_error: 2832837120.0000\n",
      "Epoch 96/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 3659196972.7025 - mean_squared_error: 3659197184.0000 - val_loss: 2786947730.6182 - val_mean_squared_error: 2786947840.0000\n",
      "Epoch 97/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 3622376223.4477 - mean_squared_error: 3622376192.0000 - val_loss: 2767987879.5636 - val_mean_squared_error: 2767987968.0000\n",
      "Epoch 98/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 3578167562.7858 - mean_squared_error: 3578168320.0000 - val_loss: 2711702737.4545 - val_mean_squared_error: 2711702784.0000\n",
      "Epoch 99/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 3529526545.4132 - mean_squared_error: 3529526528.0000 - val_loss: 2673424975.1273 - val_mean_squared_error: 2673424896.0000\n",
      "Epoch 100/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 3479651841.5594 - mean_squared_error: 3479651584.0000 - val_loss: 2637557915.9273 - val_mean_squared_error: 2637557760.0000\n",
      "Epoch 101/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 3434446553.7949 - mean_squared_error: 3434446848.0000 - val_loss: 2606702952.7273 - val_mean_squared_error: 2606703104.0000\n",
      "Epoch 102/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 3389127610.3472 - mean_squared_error: 3389127424.0000 - val_loss: 2567495377.4545 - val_mean_squared_error: 2567495168.0000\n",
      "Epoch 103/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 3343473950.4081 - mean_squared_error: 3343473920.0000 - val_loss: 2543639581.0909 - val_mean_squared_error: 2543639552.0000\n",
      "Epoch 104/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 3294656131.3787 - mean_squared_error: 3294656512.0000 - val_loss: 2478047311.1273 - val_mean_squared_error: 2478047232.0000\n",
      "Epoch 105/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 3257107984.6335 - mean_squared_error: 3257107712.0000 - val_loss: 2451750185.8909 - val_mean_squared_error: 2451750400.0000\n",
      "Epoch 106/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 3207632755.5249 - mean_squared_error: 3207633152.0000 - val_loss: 2420595174.4000 - val_mean_squared_error: 2420595200.0000\n",
      "Epoch 107/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 3155179925.3117 - mean_squared_error: 3155179776.0000 - val_loss: 2384712594.6182 - val_mean_squared_error: 2384712448.0000\n",
      "Epoch 108/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 3126441296.8284 - mean_squared_error: 3126441216.0000 - val_loss: 2338210681.0182 - val_mean_squared_error: 2338210560.0000\n",
      "Epoch 109/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 3070488315.1919 - mean_squared_error: 3070488320.0000 - val_loss: 2320229241.0182 - val_mean_squared_error: 2320229120.0000\n",
      "Epoch 110/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 3031562337.2020 - mean_squared_error: 3031562752.0000 - val_loss: 2286491606.1091 - val_mean_squared_error: 2286491648.0000\n",
      "Epoch 111/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2992415564.1503 - mean_squared_error: 2992416000.0000 - val_loss: 2266800821.5273 - val_mean_squared_error: 2266800896.0000\n",
      "Epoch 112/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 2940254502.9848 - mean_squared_error: 2940253952.0000 - val_loss: 2241153515.0545 - val_mean_squared_error: 2241153536.0000\n",
      "Epoch 113/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 2904647110.3025 - mean_squared_error: 2904646912.0000 - val_loss: 2215118578.0364 - val_mean_squared_error: 2215118592.0000\n",
      "Epoch 114/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 2867428235.8254 - mean_squared_error: 2867428352.0000 - val_loss: 2193203078.9818 - val_mean_squared_error: 2193203200.0000\n",
      "Epoch 115/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 2826442708.2071 - mean_squared_error: 2826443008.0000 - val_loss: 2156343817.3091 - val_mean_squared_error: 2156343808.0000\n",
      "Epoch 116/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 2795004276.4345 - mean_squared_error: 2795004416.0000 - val_loss: 2132554558.8364 - val_mean_squared_error: 2132554496.0000\n",
      "Epoch 117/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 2759532297.0964 - mean_squared_error: 2759532032.0000 - val_loss: 2117635016.1455 - val_mean_squared_error: 2117635072.0000\n",
      "Epoch 118/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 2725765465.2751 - mean_squared_error: 2725765120.0000 - val_loss: 2090339428.0727 - val_mean_squared_error: 2090339200.0000\n",
      "Epoch 119/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2696035596.9949 - mean_squared_error: 2696035328.0000 - val_loss: 2089930000.2909 - val_mean_squared_error: 2089930112.0000\n",
      "Epoch 120/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2669091679.2528 - mean_squared_error: 2669091584.0000 - val_loss: 2063870435.4909 - val_mean_squared_error: 2063870464.0000\n",
      "Epoch 121/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2641376372.9543 - mean_squared_error: 2641375488.0000 - val_loss: 2024860043.6364 - val_mean_squared_error: 2024860160.0000\n",
      "Epoch 122/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 2609027410.6477 - mean_squared_error: 2609027328.0000 - val_loss: 2015767053.9636 - val_mean_squared_error: 2015767040.0000\n",
      "Epoch 123/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2572049373.1736 - mean_squared_error: 2572049664.0000 - val_loss: 1993045904.2909 - val_mean_squared_error: 1993045888.0000\n",
      "Epoch 124/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2535969488.4386 - mean_squared_error: 2535970048.0000 - val_loss: 1982520790.1091 - val_mean_squared_error: 1982520960.0000\n",
      "Epoch 125/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2509516410.6071 - mean_squared_error: 2509516288.0000 - val_loss: 1966878447.7091 - val_mean_squared_error: 1966878464.0000\n",
      "Epoch 126/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 2483384673.1371 - mean_squared_error: 2483384576.0000 - val_loss: 1969683940.0727 - val_mean_squared_error: 1969683840.0000\n",
      "Epoch 127/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2451216269.1249 - mean_squared_error: 2451215872.0000 - val_loss: 1937314280.7273 - val_mean_squared_error: 1937314176.0000\n",
      "Epoch 128/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 2420691306.2985 - mean_squared_error: 2420691456.0000 - val_loss: 1905934131.2000 - val_mean_squared_error: 1905934208.0000\n",
      "Epoch 129/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2387640622.2619 - mean_squared_error: 2387640320.0000 - val_loss: 1914932668.5091 - val_mean_squared_error: 1914932608.0000\n",
      "Epoch 130/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2361205659.4193 - mean_squared_error: 2361205504.0000 - val_loss: 1882192318.8364 - val_mean_squared_error: 1882192384.0000\n",
      "Epoch 131/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2332548820.0772 - mean_squared_error: 2332548864.0000 - val_loss: 1865412733.6727 - val_mean_squared_error: 1865412736.0000\n",
      "Epoch 132/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2308721383.8294 - mean_squared_error: 2308721664.0000 - val_loss: 1849118138.1818 - val_mean_squared_error: 1849118208.0000\n",
      "Epoch 133/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2278398591.6102 - mean_squared_error: 2278398720.0000 - val_loss: 1848477965.9636 - val_mean_squared_error: 1848478080.0000\n",
      "Epoch 134/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 2258762788.6457 - mean_squared_error: 2258762752.0000 - val_loss: 1838024517.8182 - val_mean_squared_error: 1838024448.0000\n",
      "Epoch 135/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2231454454.3838 - mean_squared_error: 2231454720.0000 - val_loss: 1814449361.4545 - val_mean_squared_error: 1814449408.0000\n",
      "Epoch 136/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 2216604234.3310 - mean_squared_error: 2216604416.0000 - val_loss: 1808094538.4727 - val_mean_squared_error: 1808094592.0000\n",
      "Epoch 137/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2207060185.0152 - mean_squared_error: 2207060224.0000 - val_loss: 1787074727.5636 - val_mean_squared_error: 1787074688.0000\n",
      "Epoch 138/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2165263276.9624 - mean_squared_error: 2165263360.0000 - val_loss: 1798060775.5636 - val_mean_squared_error: 1798060928.0000\n",
      "Epoch 139/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2152607956.0772 - mean_squared_error: 2152608000.0000 - val_loss: 1770061250.3273 - val_mean_squared_error: 1770061184.0000\n",
      "Epoch 140/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2129434189.4497 - mean_squared_error: 2129434112.0000 - val_loss: 1781962884.6545 - val_mean_squared_error: 1781962880.0000\n",
      "Epoch 141/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2124915845.9777 - mean_squared_error: 2124915968.0000 - val_loss: 1771671670.6909 - val_mean_squared_error: 1771671680.0000\n",
      "Epoch 142/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2108508460.0528 - mean_squared_error: 2108508672.0000 - val_loss: 1754176321.1636 - val_mean_squared_error: 1754176384.0000\n",
      "Epoch 143/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2093501424.6660 - mean_squared_error: 2093501056.0000 - val_loss: 1747923347.7818 - val_mean_squared_error: 1747923328.0000\n",
      "Epoch 144/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2074280471.2609 - mean_squared_error: 2074280704.0000 - val_loss: 1734461100.2182 - val_mean_squared_error: 1734461184.0000\n",
      "Epoch 145/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 2060691979.0457 - mean_squared_error: 2060691968.0000 - val_loss: 1743562193.4545 - val_mean_squared_error: 1743562368.0000\n",
      "Epoch 146/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2053910402.7289 - mean_squared_error: 2053910528.0000 - val_loss: 1728922200.4364 - val_mean_squared_error: 1728922240.0000\n",
      "Epoch 147/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 2040301441.9492 - mean_squared_error: 2040301440.0000 - val_loss: 1729060505.6000 - val_mean_squared_error: 1729060608.0000\n",
      "Epoch 148/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2033576960.3898 - mean_squared_error: 2033576960.0000 - val_loss: 1737944347.9273 - val_mean_squared_error: 1737944320.0000\n",
      "Epoch 149/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2013939924.8569 - mean_squared_error: 2013939968.0000 - val_loss: 1707270539.6364 - val_mean_squared_error: 1707270400.0000\n",
      "Epoch 150/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1998301120.3249 - mean_squared_error: 1998300928.0000 - val_loss: 1715828386.9091 - val_mean_squared_error: 1715828352.0000\n",
      "Epoch 151/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1990405852.8487 - mean_squared_error: 1990406016.0000 - val_loss: 1716557160.7273 - val_mean_squared_error: 1716557056.0000\n",
      "Epoch 152/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1982810646.8061 - mean_squared_error: 1982810880.0000 - val_loss: 1698228393.8909 - val_mean_squared_error: 1698228224.0000\n",
      "Epoch 153/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 1973483016.3168 - mean_squared_error: 1973483008.0000 - val_loss: 1712906910.2545 - val_mean_squared_error: 1712906880.0000\n",
      "Epoch 154/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1960435935.1228 - mean_squared_error: 1960435840.0000 - val_loss: 1710189982.2545 - val_mean_squared_error: 1710189952.0000\n",
      "Epoch 155/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1955588570.0548 - mean_squared_error: 1955588608.0000 - val_loss: 1707091188.3636 - val_mean_squared_error: 1707091200.0000\n",
      "Epoch 156/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1940082339.0863 - mean_squared_error: 1940082560.0000 - val_loss: 1696878801.4545 - val_mean_squared_error: 1696878848.0000\n",
      "Epoch 157/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1926359491.8985 - mean_squared_error: 1926359424.0000 - val_loss: 1692147218.6182 - val_mean_squared_error: 1692147328.0000\n",
      "Epoch 158/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1919399296.9096 - mean_squared_error: 1919399040.0000 - val_loss: 1689662340.6545 - val_mean_squared_error: 1689662464.0000\n",
      "Epoch 159/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1908409184.2924 - mean_squared_error: 1908408832.0000 - val_loss: 1695786639.1273 - val_mean_squared_error: 1695786624.0000\n",
      "Epoch 160/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1902631182.6843 - mean_squared_error: 1902631296.0000 - val_loss: 1688840983.2727 - val_mean_squared_error: 1688840960.0000\n",
      "Epoch 161/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1889619684.7107 - mean_squared_error: 1889619456.0000 - val_loss: 1693432932.0727 - val_mean_squared_error: 1693432832.0000\n",
      "Epoch 162/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 1879466204.6538 - mean_squared_error: 1879466112.0000 - val_loss: 1688113837.3818 - val_mean_squared_error: 1688113792.0000\n",
      "Epoch 163/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1877402308.2234 - mean_squared_error: 1877402240.0000 - val_loss: 1699521026.3273 - val_mean_squared_error: 1699520896.0000\n",
      "Epoch 164/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1869650166.3838 - mean_squared_error: 1869650432.0000 - val_loss: 1688417702.9818 - val_mean_squared_error: 1688417536.0000\n",
      "Epoch 165/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1860465534.8305 - mean_squared_error: 1860465536.0000 - val_loss: 1683213977.6000 - val_mean_squared_error: 1683213824.0000\n",
      "Epoch 166/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1842006519.4234 - mean_squared_error: 1842006400.0000 - val_loss: 1688385580.2182 - val_mean_squared_error: 1688385664.0000\n",
      "Epoch 167/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1836275009.1046 - mean_squared_error: 1836275072.0000 - val_loss: 1692319138.9091 - val_mean_squared_error: 1692319104.0000\n",
      "Epoch 168/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1819727712.8122 - mean_squared_error: 1819727872.0000 - val_loss: 1687254942.2545 - val_mean_squared_error: 1687255040.0000\n",
      "Epoch 169/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1810138998.1239 - mean_squared_error: 1810139136.0000 - val_loss: 1684159981.3818 - val_mean_squared_error: 1684160128.0000\n",
      "Epoch 170/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1795798105.9898 - mean_squared_error: 1795797888.0000 - val_loss: 1686462537.3091 - val_mean_squared_error: 1686462464.0000\n",
      "Epoch 171/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1794658632.2518 - mean_squared_error: 1794658688.0000 - val_loss: 1681231569.4545 - val_mean_squared_error: 1681231616.0000\n",
      "Epoch 172/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1779922414.4569 - mean_squared_error: 1779922176.0000 - val_loss: 1684073067.0545 - val_mean_squared_error: 1684073088.0000\n",
      "Epoch 173/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1770058119.2122 - mean_squared_error: 1770058240.0000 - val_loss: 1687528448.0000 - val_mean_squared_error: 1687528448.0000\n",
      "Epoch 174/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1751666268.6538 - mean_squared_error: 1751666304.0000 - val_loss: 1680648009.3091 - val_mean_squared_error: 1680647936.0000\n",
      "Epoch 175/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1748593113.2102 - mean_squared_error: 1748593408.0000 - val_loss: 1679743567.1273 - val_mean_squared_error: 1679743488.0000\n",
      "Epoch 176/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1730180236.8650 - mean_squared_error: 1730180224.0000 - val_loss: 1683259447.8545 - val_mean_squared_error: 1683259392.0000\n",
      "Epoch 177/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1739342392.6579 - mean_squared_error: 1739342464.0000 - val_loss: 1683403387.3455 - val_mean_squared_error: 1683403392.0000\n",
      "Epoch 178/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1713040391.7970 - mean_squared_error: 1713040128.0000 - val_loss: 1685431291.3455 - val_mean_squared_error: 1685431296.0000\n",
      "Epoch 179/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 1700419817.3888 - mean_squared_error: 1700419712.0000 - val_loss: 1681713264.8727 - val_mean_squared_error: 1681713152.0000\n",
      "Epoch 180/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1696846958.8467 - mean_squared_error: 1696846976.0000 - val_loss: 1684763728.2909 - val_mean_squared_error: 1684763648.0000\n",
      "Epoch 181/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1683075689.7787 - mean_squared_error: 1683075712.0000 - val_loss: 1682390541.9636 - val_mean_squared_error: 1682390656.0000\n",
      "Epoch 182/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1668977840.2112 - mean_squared_error: 1668977792.0000 - val_loss: 1691111947.6364 - val_mean_squared_error: 1691111936.0000\n",
      "Epoch 183/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 1658296289.5919 - mean_squared_error: 1658296320.0000 - val_loss: 1686475892.3636 - val_mean_squared_error: 1686475904.0000\n",
      "Epoch 184/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1654900416.3249 - mean_squared_error: 1654900736.0000 - val_loss: 1684541178.1818 - val_mean_squared_error: 1684541184.0000\n",
      "Epoch 185/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1643273779.7523 - mean_squared_error: 1643274112.0000 - val_loss: 1687266077.6727 - val_mean_squared_error: 1687266176.0000\n",
      "Epoch 186/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1636265358.4244 - mean_squared_error: 1636265344.0000 - val_loss: 1687369120.5818 - val_mean_squared_error: 1687369216.0000\n",
      "Epoch 187/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1623676606.5706 - mean_squared_error: 1623676544.0000 - val_loss: 1698078885.2364 - val_mean_squared_error: 1698078848.0000\n",
      "Epoch 188/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1618077538.4365 - mean_squared_error: 1618077440.0000 - val_loss: 1696576463.1273 - val_mean_squared_error: 1696576256.0000\n",
      "Epoch 189/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1609558763.7279 - mean_squared_error: 1609558656.0000 - val_loss: 1696114869.5273 - val_mean_squared_error: 1696114816.0000\n",
      "Epoch 190/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1601726445.3523 - mean_squared_error: 1601726208.0000 - val_loss: 1696863284.3636 - val_mean_squared_error: 1696863360.0000\n",
      "Epoch 191/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1597151935.2853 - mean_squared_error: 1597151872.0000 - val_loss: 1700206759.5636 - val_mean_squared_error: 1700206720.0000\n",
      "Epoch 192/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1594609138.0954 - mean_squared_error: 1594609408.0000 - val_loss: 1701985107.7818 - val_mean_squared_error: 1701985152.0000\n",
      "Epoch 193/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1592167796.6944 - mean_squared_error: 1592167552.0000 - val_loss: 1726117897.8909 - val_mean_squared_error: 1726118016.0000\n",
      "Epoch 194/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1576500244.7919 - mean_squared_error: 1576500352.0000 - val_loss: 1713064811.0545 - val_mean_squared_error: 1713064832.0000\n",
      "Epoch 195/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1569840803.8660 - mean_squared_error: 1569840640.0000 - val_loss: 1715798420.9455 - val_mean_squared_error: 1715798400.0000\n",
      "Epoch 196/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1567437180.2315 - mean_squared_error: 1567437312.0000 - val_loss: 1713102142.8364 - val_mean_squared_error: 1713102080.0000\n",
      "Epoch 197/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1562573431.6832 - mean_squared_error: 1562573440.0000 - val_loss: 1715831577.6000 - val_mean_squared_error: 1715831552.0000\n",
      "Epoch 198/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1558015489.6893 - mean_squared_error: 1558015360.0000 - val_loss: 1722330790.4000 - val_mean_squared_error: 1722330752.0000\n",
      "Epoch 199/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1552404763.8091 - mean_squared_error: 1552404736.0000 - val_loss: 1734731519.4182 - val_mean_squared_error: 1734731520.0000\n",
      "Epoch 200/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1542019008.7147 - mean_squared_error: 1542018944.0000 - val_loss: 1727714298.1818 - val_mean_squared_error: 1727714432.0000\n",
      "Epoch 201/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1542361448.2193 - mean_squared_error: 1542361344.0000 - val_loss: 1736011926.6909 - val_mean_squared_error: 1736012032.0000\n",
      "Epoch 202/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1534192793.4701 - mean_squared_error: 1534192896.0000 - val_loss: 1736502337.1636 - val_mean_squared_error: 1736502272.0000\n",
      "Epoch 203/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1533082737.1858 - mean_squared_error: 1533082880.0000 - val_loss: 1736041001.8909 - val_mean_squared_error: 1736041088.0000\n",
      "Epoch 204/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1519953552.3736 - mean_squared_error: 1519953664.0000 - val_loss: 1761623320.4364 - val_mean_squared_error: 1761623168.0000\n",
      "Epoch 205/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1518107515.7117 - mean_squared_error: 1518107392.0000 - val_loss: 1747111740.5091 - val_mean_squared_error: 1747111680.0000\n",
      "Epoch 206/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1517472684.5076 - mean_squared_error: 1517472768.0000 - val_loss: 1754405480.7273 - val_mean_squared_error: 1754405376.0000\n",
      "Epoch 207/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1506881178.9645 - mean_squared_error: 1506881280.0000 - val_loss: 1760332255.4182 - val_mean_squared_error: 1760332288.0000\n",
      "Epoch 208/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1504064256.0000 - mean_squared_error: 1504064384.0000 - val_loss: 1763128366.5455 - val_mean_squared_error: 1763128448.0000\n",
      "Epoch 209/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 1500941142.8061 - mean_squared_error: 1500941312.0000 - val_loss: 1770899347.7818 - val_mean_squared_error: 1770899328.0000\n",
      "Epoch 210/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1509838562.7614 - mean_squared_error: 1509838592.0000 - val_loss: 1770987663.1273 - val_mean_squared_error: 1770987648.0000\n",
      "Epoch 211/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 1501234003.2975 - mean_squared_error: 1501234048.0000 - val_loss: 1780866599.5636 - val_mean_squared_error: 1780866688.0000\n",
      "Epoch 212/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1491148571.5492 - mean_squared_error: 1491148544.0000 - val_loss: 1781267806.2545 - val_mean_squared_error: 1781267712.0000\n",
      "Epoch 213/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1486976033.3970 - mean_squared_error: 1486975744.0000 - val_loss: 1786614569.8909 - val_mean_squared_error: 1786614528.0000\n",
      "Epoch 214/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1489348103.4721 - mean_squared_error: 1489347968.0000 - val_loss: 1790024750.5455 - val_mean_squared_error: 1790024704.0000\n",
      "Epoch 215/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1489289941.2467 - mean_squared_error: 1489289984.0000 - val_loss: 1799125008.8727 - val_mean_squared_error: 1799125120.0000\n",
      "Epoch 216/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1480910702.0183 - mean_squared_error: 1480910848.0000 - val_loss: 1803225967.7091 - val_mean_squared_error: 1803225856.0000\n",
      "Epoch 217/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1475194339.6061 - mean_squared_error: 1475194240.0000 - val_loss: 1799289686.1091 - val_mean_squared_error: 1799289728.0000\n",
      "Epoch 218/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1476224666.2497 - mean_squared_error: 1476224512.0000 - val_loss: 1816786034.0364 - val_mean_squared_error: 1816786048.0000\n",
      "Epoch 219/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1480316499.1675 - mean_squared_error: 1480316544.0000 - val_loss: 1816704991.4182 - val_mean_squared_error: 1816705024.0000\n",
      "Epoch 220/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1466499926.7411 - mean_squared_error: 1466499968.0000 - val_loss: 1806949521.4545 - val_mean_squared_error: 1806949504.0000\n",
      "Epoch 221/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1466066533.2305 - mean_squared_error: 1466066688.0000 - val_loss: 1814618935.8545 - val_mean_squared_error: 1814618880.0000\n",
      "Epoch 222/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1460821735.0497 - mean_squared_error: 1460821504.0000 - val_loss: 1814918864.2909 - val_mean_squared_error: 1814918784.0000\n",
      "Epoch 223/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1457913103.2041 - mean_squared_error: 1457913216.0000 - val_loss: 1820514163.2000 - val_mean_squared_error: 1820514304.0000\n",
      "Epoch 224/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1457170270.7980 - mean_squared_error: 1457170432.0000 - val_loss: 1827385350.9818 - val_mean_squared_error: 1827385216.0000\n",
      "Epoch 225/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1466779403.3056 - mean_squared_error: 1466779392.0000 - val_loss: 1841896274.6182 - val_mean_squared_error: 1841896320.0000\n",
      "Epoch 226/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1452030145.1046 - mean_squared_error: 1452030336.0000 - val_loss: 1825600194.3273 - val_mean_squared_error: 1825600128.0000\n",
      "Epoch 227/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1461788717.1574 - mean_squared_error: 1461788672.0000 - val_loss: 1826211039.4182 - val_mean_squared_error: 1826210944.0000\n",
      "Epoch 228/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1443752502.3188 - mean_squared_error: 1443752576.0000 - val_loss: 1828759339.6364 - val_mean_squared_error: 1828759296.0000\n",
      "Epoch 229/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1445364432.1787 - mean_squared_error: 1445364480.0000 - val_loss: 1836893486.5455 - val_mean_squared_error: 1836893312.0000\n",
      "Epoch 230/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1441853859.3462 - mean_squared_error: 1441853824.0000 - val_loss: 1832599012.0727 - val_mean_squared_error: 1832599168.0000\n",
      "Epoch 231/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1432875225.6650 - mean_squared_error: 1432875136.0000 - val_loss: 1834757988.0727 - val_mean_squared_error: 1834758016.0000\n",
      "Epoch 232/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1438435792.9584 - mean_squared_error: 1438435584.0000 - val_loss: 1850972919.8545 - val_mean_squared_error: 1850972928.0000\n",
      "Epoch 233/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1430088895.8701 - mean_squared_error: 1430088960.0000 - val_loss: 1844813621.5273 - val_mean_squared_error: 1844813696.0000\n",
      "Epoch 234/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1423784195.5086 - mean_squared_error: 1423784192.0000 - val_loss: 1850794486.6909 - val_mean_squared_error: 1850794368.0000\n",
      "Epoch 235/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1425330853.8152 - mean_squared_error: 1425330688.0000 - val_loss: 1854832444.5091 - val_mean_squared_error: 1854832384.0000\n",
      "Epoch 236/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1423874389.9614 - mean_squared_error: 1423874304.0000 - val_loss: 1854964271.7091 - val_mean_squared_error: 1854964352.0000\n",
      "Epoch 237/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1428302749.2386 - mean_squared_error: 1428302848.0000 - val_loss: 1857901819.3455 - val_mean_squared_error: 1857901824.0000\n",
      "Epoch 238/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1423024387.6386 - mean_squared_error: 1423024384.0000 - val_loss: 1863046355.7818 - val_mean_squared_error: 1863046400.0000\n",
      "Epoch 239/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1418143193.5350 - mean_squared_error: 1418143232.0000 - val_loss: 1861023411.2000 - val_mean_squared_error: 1861023360.0000\n",
      "Epoch 240/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1422113682.9726 - mean_squared_error: 1422113792.0000 - val_loss: 1867846516.3636 - val_mean_squared_error: 1867846656.0000\n",
      "Epoch 241/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1427264845.4497 - mean_squared_error: 1427264640.0000 - val_loss: 1861275129.0182 - val_mean_squared_error: 1861275136.0000\n",
      "Epoch 242/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1419407544.7878 - mean_squared_error: 1419407616.0000 - val_loss: 1864624892.5091 - val_mean_squared_error: 1864624896.0000\n",
      "Epoch 243/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1405185154.7289 - mean_squared_error: 1405185408.0000 - val_loss: 1884953837.3818 - val_mean_squared_error: 1884953856.0000\n",
      "Epoch 244/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1404155037.1086 - mean_squared_error: 1404155008.0000 - val_loss: 1874678381.3818 - val_mean_squared_error: 1874678272.0000\n",
      "Epoch 245/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1410252761.2102 - mean_squared_error: 1410252672.0000 - val_loss: 1893638612.9455 - val_mean_squared_error: 1893638528.0000\n",
      "Epoch 246/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1424400337.9330 - mean_squared_error: 1424400384.0000 - val_loss: 1890018656.5818 - val_mean_squared_error: 1890018560.0000\n",
      "Epoch 247/250\n",
      "985/985 [==============================] - 0s 49us/sample - loss: 1403473628.5563 - mean_squared_error: 1403473792.0000 - val_loss: 1887800406.1091 - val_mean_squared_error: 1887800320.0000\n",
      "Epoch 248/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1402914841.7299 - mean_squared_error: 1402914688.0000 - val_loss: 1888852656.8727 - val_mean_squared_error: 1888852736.0000\n",
      "Epoch 249/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1399462601.5513 - mean_squared_error: 1399462528.0000 - val_loss: 1895386263.2727 - val_mean_squared_error: 1895386112.0000\n",
      "Epoch 250/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1397660792.5929 - mean_squared_error: 1397660928.0000 - val_loss: 1894065474.9091 - val_mean_squared_error: 1894065664.0000\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(X_train_normalized, y_train, epochs=250, validation_split=.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 29us/sample - loss: 2062225476.0329 - mean_squared_error: 2062225664.0000\n"
     ]
    }
   ],
   "source": [
    "scores2 = model2.evaluate(X_test_normalized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regression's MSE is $577,580,931.30 lower than the neural network's MSE.\n"
     ]
    }
   ],
   "source": [
    "if lin_mse > scores2[1]:\n",
    "    print(\"The neural network's MSE is ${:,.2f} lower than the regression's MSE.\".format(lin_mse - scores2[1]))\n",
    "else:\n",
    "    print(\"The regression's MSE is ${:,.2f} lower than the neural network's MSE.\".format(scores2[1] - lin_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street', 'Alley',\n",
       "       'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n",
       "       'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n",
       "       'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle',\n",
       "       'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea',\n",
       "       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n",
       "       'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2',\n",
       "       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC',\n",
       "       'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n",
       "       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
       "       'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n",
       "       'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageYrBlt',\n",
       "       'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond',\n",
       "       'PavedDrive', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n",
       "       'ScreenPorch', 'PoolArea', 'PoolQC', 'Fence', 'MiscFeature', 'MiscVal',\n",
       "       'MoSold', 'YrSold', 'SaleType', 'SaleCondition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1095, 81)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>TotalBath</th>\n",
       "      <th>YearsBeforeSell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>96.0</td>\n",
       "      <td>12444</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2008</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>50</td>\n",
       "      <td>RM</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6120</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>10041</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Missing</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>30</td>\n",
       "      <td>RM</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass MSZoning  LotFrontage  LotArea Street    Alley LotShape  \\\n",
       "1437          20       RL         96.0    12444   Pave  Missing      Reg   \n",
       "584           50       RM         51.0     6120   Pave  Missing      Reg   \n",
       "1261          20       RL         80.0     9600   Pave  Missing      Reg   \n",
       "602           60       RL         80.0    10041   Pave  Missing      IR1   \n",
       "1063          30       RM         50.0     6000   Pave  Missing      Reg   \n",
       "\n",
       "     LandContour Utilities LotConfig  ...   PoolQC    Fence MiscFeature  \\\n",
       "1437         Lvl    AllPub       FR2  ...  Missing  Missing     Missing   \n",
       "584          Lvl    AllPub    Inside  ...  Missing  Missing     Missing   \n",
       "1261         Lvl    AllPub    Inside  ...  Missing  Missing     Missing   \n",
       "602          Lvl    AllPub    Inside  ...  Missing  Missing     Missing   \n",
       "1063         Lvl    AllPub    Inside  ...  Missing    MnPrv     Missing   \n",
       "\n",
       "     MiscVal MoSold YrSold  SaleType  SaleCondition  TotalBath  \\\n",
       "1437       0     11   2008       New        Partial          2   \n",
       "584        0      7   2009        WD         Normal          1   \n",
       "1261       0      6   2009        WD         Normal          1   \n",
       "602        0      2   2006        WD        Abnorml          3   \n",
       "1063       0      7   2006        WD         Normal          1   \n",
       "\n",
       "      YearsBeforeSell  \n",
       "1437                0  \n",
       "584                74  \n",
       "1261               53  \n",
       "602                14  \n",
       "1063               81  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_engineering(X):\n",
    "    Y = X.copy()\n",
    "    Y['TotalBath'] = Y['FullBath'] + Y['HalfBath']\n",
    "    Y['YearsBeforeSell'] = Y['YrSold'] - Y['YearBuilt']\n",
    "    \n",
    "    return Y\n",
    "    \n",
    "X_train2 = feature_engineering(X_train)\n",
    "X_test2 = feature_engineering(X_test)\n",
    "print(X_train2.shape)\n",
    "X_train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YearsBeforeSell    0\n",
       "HeatingQC          0\n",
       "RoofMatl           0\n",
       "Exterior1st        0\n",
       "Exterior2nd        0\n",
       "MasVnrType         0\n",
       "MasVnrArea         0\n",
       "ExterQual          0\n",
       "ExterCond          0\n",
       "Foundation         0\n",
       "BsmtQual           0\n",
       "BsmtCond           0\n",
       "BsmtExposure       0\n",
       "BsmtFinType1       0\n",
       "BsmtFinSF1         0\n",
       "BsmtFinType2       0\n",
       "BsmtFinSF2         0\n",
       "BsmtUnfSF          0\n",
       "TotalBsmtSF        0\n",
       "RoofStyle          0\n",
       "YearRemodAdd       0\n",
       "YearBuilt          0\n",
       "Utilities          0\n",
       "MSZoning           0\n",
       "LotFrontage        0\n",
       "LotArea            0\n",
       "Street             0\n",
       "Alley              0\n",
       "LotShape           0\n",
       "LandContour        0\n",
       "                  ..\n",
       "PoolArea           0\n",
       "PoolQC             0\n",
       "Fence              0\n",
       "MiscFeature        0\n",
       "MiscVal            0\n",
       "MoSold             0\n",
       "YrSold             0\n",
       "SaleType           0\n",
       "SaleCondition      0\n",
       "GarageArea         0\n",
       "GarageCars         0\n",
       "GarageFinish       0\n",
       "HalfBath           0\n",
       "1stFlrSF           0\n",
       "2ndFlrSF           0\n",
       "LowQualFinSF       0\n",
       "GrLivArea          0\n",
       "BsmtFullBath       0\n",
       "BsmtHalfBath       0\n",
       "FullBath           0\n",
       "BedroomAbvGr       0\n",
       "GarageYrBlt        0\n",
       "KitchenAbvGr       0\n",
       "KitchenQual        0\n",
       "TotRmsAbvGrd       0\n",
       "Functional         0\n",
       "Fireplaces         0\n",
       "FireplaceQu        0\n",
       "GarageType         0\n",
       "MSSubClass         0\n",
       "Length: 81, dtype: int64"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded2 = encoder.fit_transform(X_train2)\n",
    "X_test_encoded2 = encoder.transform(X_test2)\n",
    "\n",
    "X_train_normalized2 = norm.fit_transform(X_train_encoded2)\n",
    "X_test_normalized2 = norm.transform(X_test_encoded2)\n",
    "\n",
    "reg2 = LinearRegression()\n",
    "\n",
    "reg2.fit(X_train_normalized2, y_train)\n",
    "y_pred2 = reg2.predict(X_test_normalized2)\n",
    "\n",
    "lin_mse2 = mean_squared_error(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_50 (Dense)             (None, 20)                1640      \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 3,341\n",
      "Trainable params: 3,341\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential(name=\"nn3\")\n",
    "model3.add(Dense(20, input_dim=81, activation='relu'))\n",
    "model3.add(Dense(20, activation='relu'))\n",
    "model3.add(Dense(20, activation='relu'))\n",
    "model3.add(Dense(20, activation='relu'))\n",
    "model3.add(Dense(20, activation='relu'))\n",
    "model3.add(Dense(1, activation='relu'))\n",
    "\n",
    "model3.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 985 samples, validate on 110 samples\n",
      "Epoch 1/250\n",
      "985/985 [==============================] - 1s 578us/sample - loss: 39114978226.0305 - mean_squared_error: 39114981376.0000 - val_loss: 39183804006.4000 - val_mean_squared_error: 39183802368.0000\n",
      "Epoch 2/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 39114700968.4142 - mean_squared_error: 39114698752.0000 - val_loss: 39183129618.6182 - val_mean_squared_error: 39183130624.0000\n",
      "Epoch 3/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 39112651571.2000 - mean_squared_error: 39112646656.0000 - val_loss: 39178541093.2364 - val_mean_squared_error: 39178539008.0000\n",
      "Epoch 4/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 39100531814.9198 - mean_squared_error: 39100526592.0000 - val_loss: 39153808216.4364 - val_mean_squared_error: 39153807360.0000\n",
      "Epoch 5/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 39045554255.0091 - mean_squared_error: 39045554176.0000 - val_loss: 39052220900.0727 - val_mean_squared_error: 39052222464.0000\n",
      "Epoch 6/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 38848879088.9259 - mean_squared_error: 38848876544.0000 - val_loss: 38724256414.2545 - val_mean_squared_error: 38724255744.0000\n",
      "Epoch 7/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 38282828938.2660 - mean_squared_error: 38282825728.0000 - val_loss: 37844781204.9455 - val_mean_squared_error: 37844779008.0000\n",
      "Epoch 8/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 36874774817.0071 - mean_squared_error: 36874780672.0000 - val_loss: 35818033449.8909 - val_mean_squared_error: 35818033152.0000\n",
      "Epoch 9/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 33920467301.6203 - mean_squared_error: 33920466944.0000 - val_loss: 31814983195.9273 - val_mean_squared_error: 31814981632.0000\n",
      "Epoch 10/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 28593949630.5056 - mean_squared_error: 28593950720.0000 - val_loss: 25273450235.3455 - val_mean_squared_error: 25273450496.0000\n",
      "Epoch 11/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 20845120953.8274 - mean_squared_error: 20845121536.0000 - val_loss: 16905934503.5636 - val_mean_squared_error: 16905934848.0000\n",
      "Epoch 12/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 12605558458.3472 - mean_squared_error: 12605558784.0000 - val_loss: 10197625707.0545 - val_mean_squared_error: 10197625856.0000\n",
      "Epoch 13/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 7931593343.3503 - mean_squared_error: 7931593728.0000 - val_loss: 8028557358.5455 - val_mean_squared_error: 8028557312.0000\n",
      "Epoch 14/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 6941760113.3157 - mean_squared_error: 6941759488.0000 - val_loss: 7915674568.1455 - val_mean_squared_error: 7915675136.0000\n",
      "Epoch 15/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 6861095313.8030 - mean_squared_error: 6861095424.0000 - val_loss: 7883803326.8364 - val_mean_squared_error: 7883803136.0000\n",
      "Epoch 16/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 6823562866.3553 - mean_squared_error: 6823562240.0000 - val_loss: 7847746094.5455 - val_mean_squared_error: 7847746560.0000\n",
      "Epoch 17/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 6774729766.4650 - mean_squared_error: 6774729728.0000 - val_loss: 7816975723.0545 - val_mean_squared_error: 7816976384.0000\n",
      "Epoch 18/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 6734394856.0893 - mean_squared_error: 6734394880.0000 - val_loss: 7784181015.2727 - val_mean_squared_error: 7784180736.0000\n",
      "Epoch 19/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 6696895270.7249 - mean_squared_error: 6696894976.0000 - val_loss: 7751519474.0364 - val_mean_squared_error: 7751519232.0000\n",
      "Epoch 20/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 6653369413.6528 - mean_squared_error: 6653371392.0000 - val_loss: 7717946312.1455 - val_mean_squared_error: 7717945856.0000\n",
      "Epoch 21/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 6616535418.4122 - mean_squared_error: 6616535040.0000 - val_loss: 7681629761.1636 - val_mean_squared_error: 7681630208.0000\n",
      "Epoch 22/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 6585791544.1381 - mean_squared_error: 6585791488.0000 - val_loss: 7651288999.5636 - val_mean_squared_error: 7651288576.0000\n",
      "Epoch 23/250\n",
      "985/985 [==============================] - 0s 55us/sample - loss: 6528998396.8812 - mean_squared_error: 6528997888.0000 - val_loss: 7616472724.9455 - val_mean_squared_error: 7616473088.0000\n",
      "Epoch 24/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 6500914136.4954 - mean_squared_error: 6500914176.0000 - val_loss: 7583329629.0909 - val_mean_squared_error: 7583329280.0000\n",
      "Epoch 25/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 6453518345.8761 - mean_squared_error: 6453517312.0000 - val_loss: 7557710531.4909 - val_mean_squared_error: 7557710336.0000\n",
      "Epoch 26/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 6413469718.3513 - mean_squared_error: 6413468672.0000 - val_loss: 7525759227.3455 - val_mean_squared_error: 7525759488.0000\n",
      "Epoch 27/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 6373116570.8995 - mean_squared_error: 6373115904.0000 - val_loss: 7488504515.4909 - val_mean_squared_error: 7488504320.0000\n",
      "Epoch 28/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 6334142211.3787 - mean_squared_error: 6334141952.0000 - val_loss: 7466500905.8909 - val_mean_squared_error: 7466501120.0000\n",
      "Epoch 29/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 6296508153.5025 - mean_squared_error: 6296508928.0000 - val_loss: 7425349259.6364 - val_mean_squared_error: 7425349120.0000\n",
      "Epoch 30/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 6269176145.3482 - mean_squared_error: 6269176320.0000 - val_loss: 7396421054.8364 - val_mean_squared_error: 7396421120.0000\n",
      "Epoch 31/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 6224139431.3746 - mean_squared_error: 6224139776.0000 - val_loss: 7363410068.9455 - val_mean_squared_error: 7363409920.0000\n",
      "Epoch 32/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 6184036102.4975 - mean_squared_error: 6184035328.0000 - val_loss: 7329550112.5818 - val_mean_squared_error: 7329550336.0000\n",
      "Epoch 33/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 6139276269.2873 - mean_squared_error: 6139276288.0000 - val_loss: 7304810007.2727 - val_mean_squared_error: 7304809984.0000\n",
      "Epoch 34/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 6106805967.3990 - mean_squared_error: 6106806272.0000 - val_loss: 7276473814.1091 - val_mean_squared_error: 7276473856.0000\n",
      "Epoch 35/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 6062565594.3147 - mean_squared_error: 6062565376.0000 - val_loss: 7237818330.7636 - val_mean_squared_error: 7237818368.0000\n",
      "Epoch 36/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 6026112883.1350 - mean_squared_error: 6026113536.0000 - val_loss: 7206136613.2364 - val_mean_squared_error: 7206136832.0000\n",
      "Epoch 37/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 5985446265.8924 - mean_squared_error: 5985445888.0000 - val_loss: 7185036483.4909 - val_mean_squared_error: 7185036288.0000\n",
      "Epoch 38/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 5957503604.1746 - mean_squared_error: 5957503488.0000 - val_loss: 7144338813.6727 - val_mean_squared_error: 7144338432.0000\n",
      "Epoch 39/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 5910310080.3249 - mean_squared_error: 5910309888.0000 - val_loss: 7112002257.4545 - val_mean_squared_error: 7112002048.0000\n",
      "Epoch 40/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 5871770715.4843 - mean_squared_error: 5871770624.0000 - val_loss: 7093486566.4000 - val_mean_squared_error: 7093486592.0000\n",
      "Epoch 41/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 5839243257.2426 - mean_squared_error: 5839242240.0000 - val_loss: 7053047787.0545 - val_mean_squared_error: 7053048320.0000\n",
      "Epoch 42/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 5798525377.1046 - mean_squared_error: 5798525952.0000 - val_loss: 7023532646.4000 - val_mean_squared_error: 7023532544.0000\n",
      "Epoch 43/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 5768261911.9107 - mean_squared_error: 5768262144.0000 - val_loss: 6994745734.9818 - val_mean_squared_error: 6994745856.0000\n",
      "Epoch 44/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 5725164186.8995 - mean_squared_error: 5725165056.0000 - val_loss: 6959281757.0909 - val_mean_squared_error: 6959281664.0000\n",
      "Epoch 45/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 5684992943.4315 - mean_squared_error: 5684993024.0000 - val_loss: 6920488010.4727 - val_mean_squared_error: 6920488448.0000\n",
      "Epoch 46/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 5642693448.2518 - mean_squared_error: 5642694144.0000 - val_loss: 6894710141.6727 - val_mean_squared_error: 6894710272.0000\n",
      "Epoch 47/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 5608925783.8457 - mean_squared_error: 5608925184.0000 - val_loss: 6865449741.9636 - val_mean_squared_error: 6865449984.0000\n",
      "Epoch 48/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 5567567039.2853 - mean_squared_error: 5567567360.0000 - val_loss: 6828625533.6727 - val_mean_squared_error: 6828625408.0000\n",
      "Epoch 49/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 5537269541.1655 - mean_squared_error: 5537269760.0000 - val_loss: 6790355018.4727 - val_mean_squared_error: 6790355456.0000\n",
      "Epoch 50/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 5490769877.3766 - mean_squared_error: 5490769408.0000 - val_loss: 6765749643.6364 - val_mean_squared_error: 6765749760.0000\n",
      "Epoch 51/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 5451422423.1959 - mean_squared_error: 5451422208.0000 - val_loss: 6735458494.8364 - val_mean_squared_error: 6735458816.0000\n",
      "Epoch 52/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 5415810577.4132 - mean_squared_error: 5415810560.0000 - val_loss: 6697382083.4909 - val_mean_squared_error: 6697382400.0000\n",
      "Epoch 53/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 5370854977.4944 - mean_squared_error: 5370855936.0000 - val_loss: 6666369473.1636 - val_mean_squared_error: 6666369536.0000\n",
      "Epoch 54/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 5332510262.0589 - mean_squared_error: 5332510720.0000 - val_loss: 6629291464.1455 - val_mean_squared_error: 6629291520.0000\n",
      "Epoch 55/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 5297325587.7523 - mean_squared_error: 5297325568.0000 - val_loss: 6591282688.0000 - val_mean_squared_error: 6591282688.0000\n",
      "Epoch 56/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 5257125114.8020 - mean_squared_error: 5257125376.0000 - val_loss: 6561702106.7636 - val_mean_squared_error: 6561701888.0000\n",
      "Epoch 57/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 5214305099.3706 - mean_squared_error: 5214304768.0000 - val_loss: 6524740682.4727 - val_mean_squared_error: 6524740096.0000\n",
      "Epoch 58/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 5175200700.9462 - mean_squared_error: 5175200768.0000 - val_loss: 6484775154.0364 - val_mean_squared_error: 6484775424.0000\n",
      "Epoch 59/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 5135592599.0010 - mean_squared_error: 5135593472.0000 - val_loss: 6447955148.8000 - val_mean_squared_error: 6447955456.0000\n",
      "Epoch 60/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 5091698508.1503 - mean_squared_error: 5091698176.0000 - val_loss: 6415033995.6364 - val_mean_squared_error: 6415034368.0000\n",
      "Epoch 61/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 5050398001.1208 - mean_squared_error: 5050398208.0000 - val_loss: 6387466081.7455 - val_mean_squared_error: 6387466240.0000\n",
      "Epoch 62/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 5013793611.1107 - mean_squared_error: 5013794304.0000 - val_loss: 6344196286.8364 - val_mean_squared_error: 6344196608.0000\n",
      "Epoch 63/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 4990325765.9777 - mean_squared_error: 4990325248.0000 - val_loss: 6314508520.7273 - val_mean_squared_error: 6314508800.0000\n",
      "Epoch 64/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 4929222870.6761 - mean_squared_error: 4929222144.0000 - val_loss: 6273619579.3455 - val_mean_squared_error: 6273619968.0000\n",
      "Epoch 65/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 4888160335.6589 - mean_squared_error: 4888160256.0000 - val_loss: 6228917704.1455 - val_mean_squared_error: 6228917760.0000\n",
      "Epoch 66/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 4837907202.3391 - mean_squared_error: 4837907456.0000 - val_loss: 6196169895.5636 - val_mean_squared_error: 6196170240.0000\n",
      "Epoch 67/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 4796358038.4812 - mean_squared_error: 4796357632.0000 - val_loss: 6164237028.0727 - val_mean_squared_error: 6164236800.0000\n",
      "Epoch 68/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 4750857805.9695 - mean_squared_error: 4750857728.0000 - val_loss: 6105494928.2909 - val_mean_squared_error: 6105494528.0000\n",
      "Epoch 69/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 4711008567.8782 - mean_squared_error: 4711008768.0000 - val_loss: 6083509825.1636 - val_mean_squared_error: 6083510272.0000\n",
      "Epoch 70/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 4666995054.9766 - mean_squared_error: 4666995200.0000 - val_loss: 6041404257.7455 - val_mean_squared_error: 6041404416.0000\n",
      "Epoch 71/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 4622084952.6254 - mean_squared_error: 4622084608.0000 - val_loss: 5995508549.8182 - val_mean_squared_error: 5995508736.0000\n",
      "Epoch 72/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 4579064162.5015 - mean_squared_error: 4579063808.0000 - val_loss: 5946315654.9818 - val_mean_squared_error: 5946315264.0000\n",
      "Epoch 73/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 4538698611.6548 - mean_squared_error: 4538699264.0000 - val_loss: 5901207156.3636 - val_mean_squared_error: 5901207040.0000\n",
      "Epoch 74/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 4492360712.3168 - mean_squared_error: 4492360704.0000 - val_loss: 5873847247.1273 - val_mean_squared_error: 5873847296.0000\n",
      "Epoch 75/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 4438703410.6802 - mean_squared_error: 4438703104.0000 - val_loss: 5812098513.4545 - val_mean_squared_error: 5812098048.0000\n",
      "Epoch 76/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 4401366947.4761 - mean_squared_error: 4401366528.0000 - val_loss: 5774734931.7818 - val_mean_squared_error: 5774734848.0000\n",
      "Epoch 77/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 4357705317.8802 - mean_squared_error: 4357705216.0000 - val_loss: 5737828575.4182 - val_mean_squared_error: 5737828864.0000\n",
      "Epoch 78/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 4309416613.8152 - mean_squared_error: 4309416448.0000 - val_loss: 5681829678.5455 - val_mean_squared_error: 5681829376.0000\n",
      "Epoch 79/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 4258864642.5990 - mean_squared_error: 4258864640.0000 - val_loss: 5649288159.4182 - val_mean_squared_error: 5649287680.0000\n",
      "Epoch 80/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 4214769091.7036 - mean_squared_error: 4214768896.0000 - val_loss: 5602675134.8364 - val_mean_squared_error: 5602675200.0000\n",
      "Epoch 81/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 4169269238.1239 - mean_squared_error: 4169268736.0000 - val_loss: 5544304905.3091 - val_mean_squared_error: 5544305152.0000\n",
      "Epoch 82/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 4123289895.2447 - mean_squared_error: 4123289600.0000 - val_loss: 5510509884.5091 - val_mean_squared_error: 5510510080.0000\n",
      "Epoch 83/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 4084189884.1665 - mean_squared_error: 4084190464.0000 - val_loss: 5447578302.8364 - val_mean_squared_error: 5447578624.0000\n",
      "Epoch 84/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 4027079733.0193 - mean_squared_error: 4027079424.0000 - val_loss: 5418298861.3818 - val_mean_squared_error: 5418298880.0000\n",
      "Epoch 85/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 3981000489.3239 - mean_squared_error: 3981000448.0000 - val_loss: 5363435803.9273 - val_mean_squared_error: 5363435008.0000\n",
      "Epoch 86/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 3933183085.6772 - mean_squared_error: 3933182976.0000 - val_loss: 5305107637.5273 - val_mean_squared_error: 5305107456.0000\n",
      "Epoch 87/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 3887690591.3827 - mean_squared_error: 3887690496.0000 - val_loss: 5255023439.1273 - val_mean_squared_error: 5255023616.0000\n",
      "Epoch 88/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 3847379450.4122 - mean_squared_error: 3847379712.0000 - val_loss: 5209007243.6364 - val_mean_squared_error: 5209007616.0000\n",
      "Epoch 89/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 3798374460.8162 - mean_squared_error: 3798374912.0000 - val_loss: 5156664920.4364 - val_mean_squared_error: 5156664832.0000\n",
      "Epoch 90/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 3757435362.6315 - mean_squared_error: 3757435392.0000 - val_loss: 5098459485.0909 - val_mean_squared_error: 5098459648.0000\n",
      "Epoch 91/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 3712933040.3411 - mean_squared_error: 3712933120.0000 - val_loss: 5056515127.8545 - val_mean_squared_error: 5056515072.0000\n",
      "Epoch 92/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 3666270498.3066 - mean_squared_error: 3666270720.0000 - val_loss: 5004709143.2727 - val_mean_squared_error: 5004708864.0000\n",
      "Epoch 93/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 3619890542.9766 - mean_squared_error: 3619890688.0000 - val_loss: 4950764869.8182 - val_mean_squared_error: 4950765056.0000\n",
      "Epoch 94/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 3578344990.4081 - mean_squared_error: 3578344960.0000 - val_loss: 4904042288.8727 - val_mean_squared_error: 4904041984.0000\n",
      "Epoch 95/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 3534053509.0680 - mean_squared_error: 3534053376.0000 - val_loss: 4853179136.0000 - val_mean_squared_error: 4853179392.0000\n",
      "Epoch 96/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 3497269187.5736 - mean_squared_error: 3497269760.0000 - val_loss: 4791559000.4364 - val_mean_squared_error: 4791558656.0000\n",
      "Epoch 97/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 3459637282.5665 - mean_squared_error: 3459637504.0000 - val_loss: 4745973038.5455 - val_mean_squared_error: 4745973248.0000\n",
      "Epoch 98/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 3409740922.6721 - mean_squared_error: 3409741568.0000 - val_loss: 4698747298.9091 - val_mean_squared_error: 4698747392.0000\n",
      "Epoch 99/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 3364012200.9340 - mean_squared_error: 3364012288.0000 - val_loss: 4661264335.1273 - val_mean_squared_error: 4661263872.0000\n",
      "Epoch 100/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 3331535514.8995 - mean_squared_error: 3331535360.0000 - val_loss: 4601479773.0909 - val_mean_squared_error: 4601479680.0000\n",
      "Epoch 101/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 3288774286.6843 - mean_squared_error: 3288773888.0000 - val_loss: 4564289624.4364 - val_mean_squared_error: 4564289536.0000\n",
      "Epoch 102/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 3254880743.3096 - mean_squared_error: 3254880512.0000 - val_loss: 4517011339.6364 - val_mean_squared_error: 4517010944.0000\n",
      "Epoch 103/250\n",
      "985/985 [==============================] - 0s 55us/sample - loss: 3213209686.1563 - mean_squared_error: 3213209600.0000 - val_loss: 4455159074.9091 - val_mean_squared_error: 4455158784.0000\n",
      "Epoch 104/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 3175277444.5482 - mean_squared_error: 3175277568.0000 - val_loss: 4407388867.4909 - val_mean_squared_error: 4407389184.0000\n",
      "Epoch 105/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 3140876977.7706 - mean_squared_error: 3140877056.0000 - val_loss: 4355521703.5636 - val_mean_squared_error: 4355521536.0000\n",
      "Epoch 106/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 3103620278.4487 - mean_squared_error: 3103620608.0000 - val_loss: 4313232067.4909 - val_mean_squared_error: 4313232384.0000\n",
      "Epoch 107/250\n",
      "985/985 [==============================] - 0s 55us/sample - loss: 3075069183.3503 - mean_squared_error: 3075068928.0000 - val_loss: 4286115006.8364 - val_mean_squared_error: 4286114816.0000\n",
      "Epoch 108/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 3047098230.2538 - mean_squared_error: 3047098624.0000 - val_loss: 4202717272.4364 - val_mean_squared_error: 4202717440.0000\n",
      "Epoch 109/250\n",
      "985/985 [==============================] - 0s 55us/sample - loss: 2997741335.7807 - mean_squared_error: 2997741568.0000 - val_loss: 4179531841.1636 - val_mean_squared_error: 4179531776.0000\n",
      "Epoch 110/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 2969218523.3543 - mean_squared_error: 2969219072.0000 - val_loss: 4119823662.5455 - val_mean_squared_error: 4119823616.0000\n",
      "Epoch 111/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2936018418.7452 - mean_squared_error: 2936018432.0000 - val_loss: 4069461085.0909 - val_mean_squared_error: 4069460992.0000\n",
      "Epoch 112/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 2903266039.9431 - mean_squared_error: 2903265792.0000 - val_loss: 4047087616.0000 - val_mean_squared_error: 4047087616.0000\n",
      "Epoch 113/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 2871731989.5716 - mean_squared_error: 2871732224.0000 - val_loss: 3991728141.9636 - val_mean_squared_error: 3991728128.0000\n",
      "Epoch 114/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 2843007611.1919 - mean_squared_error: 2843007744.0000 - val_loss: 3944072992.5818 - val_mean_squared_error: 3944072960.0000\n",
      "Epoch 115/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 2810405381.8477 - mean_squared_error: 2810405376.0000 - val_loss: 3909874203.9273 - val_mean_squared_error: 3909874176.0000\n",
      "Epoch 116/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2788465588.8893 - mean_squared_error: 2788465664.0000 - val_loss: 3877192047.7091 - val_mean_squared_error: 3877191936.0000\n",
      "Epoch 117/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2749750074.9970 - mean_squared_error: 2749750016.0000 - val_loss: 3811747539.7818 - val_mean_squared_error: 3811747840.0000\n",
      "Epoch 118/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2729520259.8985 - mean_squared_error: 2729519872.0000 - val_loss: 3785807990.6909 - val_mean_squared_error: 3785808384.0000\n",
      "Epoch 119/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2700920373.7990 - mean_squared_error: 2700920576.0000 - val_loss: 3730452489.3091 - val_mean_squared_error: 3730452480.0000\n",
      "Epoch 120/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2666799099.8416 - mean_squared_error: 2666799360.0000 - val_loss: 3704242750.8364 - val_mean_squared_error: 3704242944.0000\n",
      "Epoch 121/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2645843377.2508 - mean_squared_error: 2645843456.0000 - val_loss: 3668808424.7273 - val_mean_squared_error: 3668808448.0000\n",
      "Epoch 122/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2614498911.3827 - mean_squared_error: 2614499072.0000 - val_loss: 3616332290.3273 - val_mean_squared_error: 3616332544.0000\n",
      "Epoch 123/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2592373022.4081 - mean_squared_error: 2592372992.0000 - val_loss: 3577818112.0000 - val_mean_squared_error: 3577818112.0000\n",
      "Epoch 124/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2558372797.9858 - mean_squared_error: 2558372864.0000 - val_loss: 3544281311.4182 - val_mean_squared_error: 3544281344.0000\n",
      "Epoch 125/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2530000792.8203 - mean_squared_error: 2530000896.0000 - val_loss: 3506112972.8000 - val_mean_squared_error: 3506112768.0000\n",
      "Epoch 126/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 2501566183.5695 - mean_squared_error: 2501565952.0000 - val_loss: 3463553419.6364 - val_mean_squared_error: 3463553536.0000\n",
      "Epoch 127/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 2470888532.2071 - mean_squared_error: 2470888192.0000 - val_loss: 3418269208.4364 - val_mean_squared_error: 3418269184.0000\n",
      "Epoch 128/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2440532411.9066 - mean_squared_error: 2440532480.0000 - val_loss: 3386163055.7091 - val_mean_squared_error: 3386162944.0000\n",
      "Epoch 129/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2399610117.0680 - mean_squared_error: 2399610368.0000 - val_loss: 3344581199.1273 - val_mean_squared_error: 3344581120.0000\n",
      "Epoch 130/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2365849059.4112 - mean_squared_error: 2365849088.0000 - val_loss: 3295350649.0182 - val_mean_squared_error: 3295350784.0000\n",
      "Epoch 131/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2335964210.9401 - mean_squared_error: 2335964160.0000 - val_loss: 3255026743.8545 - val_mean_squared_error: 3255026432.0000\n",
      "Epoch 132/250\n",
      "985/985 [==============================] - 0s 55us/sample - loss: 2301944332.7350 - mean_squared_error: 2301944576.0000 - val_loss: 3220976747.0545 - val_mean_squared_error: 3220976640.0000\n",
      "Epoch 133/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2271206789.7178 - mean_squared_error: 2271206912.0000 - val_loss: 3179763763.2000 - val_mean_squared_error: 3179763712.0000\n",
      "Epoch 134/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 2242348473.8274 - mean_squared_error: 2242348288.0000 - val_loss: 3133087699.7818 - val_mean_squared_error: 3133088000.0000\n",
      "Epoch 135/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2211661709.1249 - mean_squared_error: 2211661824.0000 - val_loss: 3102213434.1818 - val_mean_squared_error: 3102213376.0000\n",
      "Epoch 136/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2181235641.0477 - mean_squared_error: 2181235712.0000 - val_loss: 3056926892.2182 - val_mean_squared_error: 3056926720.0000\n",
      "Epoch 137/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2156922995.3949 - mean_squared_error: 2156923136.0000 - val_loss: 3030869171.2000 - val_mean_squared_error: 3030869248.0000\n",
      "Epoch 138/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2131851907.2487 - mean_squared_error: 2131851776.0000 - val_loss: 2988759326.2545 - val_mean_squared_error: 2988759552.0000\n",
      "Epoch 139/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 2114484110.2294 - mean_squared_error: 2114483968.0000 - val_loss: 2956655811.4909 - val_mean_squared_error: 2956655872.0000\n",
      "Epoch 140/250\n",
      "985/985 [==============================] - 0s 55us/sample - loss: 2088001512.3492 - mean_squared_error: 2088001536.0000 - val_loss: 2929544524.8000 - val_mean_squared_error: 2929544448.0000\n",
      "Epoch 141/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2074723572.8893 - mean_squared_error: 2074723456.0000 - val_loss: 2900685972.9455 - val_mean_squared_error: 2900685824.0000\n",
      "Epoch 142/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 2051395853.3848 - mean_squared_error: 2051395712.0000 - val_loss: 2887012649.8909 - val_mean_squared_error: 2887012608.0000\n",
      "Epoch 143/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 2039463614.7655 - mean_squared_error: 2039463808.0000 - val_loss: 2849008849.4545 - val_mean_squared_error: 2849008640.0000\n",
      "Epoch 144/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 2020809320.8690 - mean_squared_error: 2020809472.0000 - val_loss: 2831133605.2364 - val_mean_squared_error: 2831133696.0000\n",
      "Epoch 145/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1999038412.5401 - mean_squared_error: 1999038464.0000 - val_loss: 2795103923.2000 - val_mean_squared_error: 2795103744.0000\n",
      "Epoch 146/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1982647233.8843 - mean_squared_error: 1982646912.0000 - val_loss: 2783100723.2000 - val_mean_squared_error: 2783100672.0000\n",
      "Epoch 147/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1982102324.1096 - mean_squared_error: 1982102016.0000 - val_loss: 2769869998.5455 - val_mean_squared_error: 2769869824.0000\n",
      "Epoch 148/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1953765749.6041 - mean_squared_error: 1953766016.0000 - val_loss: 2724722045.6727 - val_mean_squared_error: 2724722432.0000\n",
      "Epoch 149/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1944656162.0467 - mean_squared_error: 1944656128.0000 - val_loss: 2710435032.4364 - val_mean_squared_error: 2710434816.0000\n",
      "Epoch 150/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1930197118.4406 - mean_squared_error: 1930197248.0000 - val_loss: 2680072506.1818 - val_mean_squared_error: 2680072704.0000\n",
      "Epoch 151/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1913900624.5685 - mean_squared_error: 1913900416.0000 - val_loss: 2661980881.4545 - val_mean_squared_error: 2661980928.0000\n",
      "Epoch 152/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1898562236.0365 - mean_squared_error: 1898561920.0000 - val_loss: 2639223956.9455 - val_mean_squared_error: 2639224064.0000\n",
      "Epoch 153/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1888308414.2457 - mean_squared_error: 1888308736.0000 - val_loss: 2618417906.0364 - val_mean_squared_error: 2618417920.0000\n",
      "Epoch 154/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1880350758.4650 - mean_squared_error: 1880350720.0000 - val_loss: 2598528812.2182 - val_mean_squared_error: 2598529024.0000\n",
      "Epoch 155/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1869928124.2964 - mean_squared_error: 1869928064.0000 - val_loss: 2587170041.0182 - val_mean_squared_error: 2587170048.0000\n",
      "Epoch 156/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1852386318.6843 - mean_squared_error: 1852386304.0000 - val_loss: 2569771952.8727 - val_mean_squared_error: 2569772032.0000\n",
      "Epoch 157/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1841761517.3523 - mean_squared_error: 1841761408.0000 - val_loss: 2547085507.4909 - val_mean_squared_error: 2547085568.0000\n",
      "Epoch 158/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1831336244.2396 - mean_squared_error: 1831336192.0000 - val_loss: 2535947994.7636 - val_mean_squared_error: 2535948032.0000\n",
      "Epoch 159/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1822174705.3157 - mean_squared_error: 1822174848.0000 - val_loss: 2521521084.5091 - val_mean_squared_error: 2521521152.0000\n",
      "Epoch 160/250\n",
      "985/985 [==============================] - 0s 55us/sample - loss: 1814080490.1685 - mean_squared_error: 1814080256.0000 - val_loss: 2507376619.0545 - val_mean_squared_error: 2507376640.0000\n",
      "Epoch 161/250\n",
      "985/985 [==============================] - 0s 56us/sample - loss: 1804518968.3980 - mean_squared_error: 1804518912.0000 - val_loss: 2490261429.5273 - val_mean_squared_error: 2490261248.0000\n",
      "Epoch 162/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1797695904.3574 - mean_squared_error: 1797696000.0000 - val_loss: 2470526994.6182 - val_mean_squared_error: 2470526976.0000\n",
      "Epoch 163/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1790638009.2751 - mean_squared_error: 1790638080.0000 - val_loss: 2457966438.4000 - val_mean_squared_error: 2457966336.0000\n",
      "Epoch 164/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1780633564.0041 - mean_squared_error: 1780633600.0000 - val_loss: 2442576481.7455 - val_mean_squared_error: 2442576640.0000\n",
      "Epoch 165/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1777525348.7107 - mean_squared_error: 1777525248.0000 - val_loss: 2438832505.0182 - val_mean_squared_error: 2438832640.0000\n",
      "Epoch 166/250\n",
      "985/985 [==============================] - 0s 55us/sample - loss: 1774522070.1563 - mean_squared_error: 1774522112.0000 - val_loss: 2414076087.8545 - val_mean_squared_error: 2414076160.0000\n",
      "Epoch 167/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1760260946.5827 - mean_squared_error: 1760260864.0000 - val_loss: 2403524313.6000 - val_mean_squared_error: 2403524352.0000\n",
      "Epoch 168/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1751576201.6812 - mean_squared_error: 1751576192.0000 - val_loss: 2394904636.5091 - val_mean_squared_error: 2394904576.0000\n",
      "Epoch 169/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1745717199.9188 - mean_squared_error: 1745716992.0000 - val_loss: 2381416822.6909 - val_mean_squared_error: 2381416704.0000\n",
      "Epoch 170/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1743620206.9766 - mean_squared_error: 1743620224.0000 - val_loss: 2379896420.0727 - val_mean_squared_error: 2379896320.0000\n",
      "Epoch 171/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1733266473.7137 - mean_squared_error: 1733266304.0000 - val_loss: 2355514603.0545 - val_mean_squared_error: 2355514624.0000\n",
      "Epoch 172/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1726362185.5513 - mean_squared_error: 1726362240.0000 - val_loss: 2338976470.1091 - val_mean_squared_error: 2338976512.0000\n",
      "Epoch 173/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1719438621.9533 - mean_squared_error: 1719438464.0000 - val_loss: 2334744652.8000 - val_mean_squared_error: 2334744576.0000\n",
      "Epoch 174/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1727114552.1381 - mean_squared_error: 1727114752.0000 - val_loss: 2336006239.4182 - val_mean_squared_error: 2336006144.0000\n",
      "Epoch 175/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1709179737.5350 - mean_squared_error: 1709179776.0000 - val_loss: 2308740544.0000 - val_mean_squared_error: 2308740608.0000\n",
      "Epoch 176/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1704243207.2772 - mean_squared_error: 1704243072.0000 - val_loss: 2302837953.1636 - val_mean_squared_error: 2302838016.0000\n",
      "Epoch 177/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1709871179.1756 - mean_squared_error: 1709870976.0000 - val_loss: 2287638188.2182 - val_mean_squared_error: 2287638016.0000\n",
      "Epoch 178/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1690744622.0020 - mean_squared_error: 1690744448.0000 - val_loss: 2300069222.4000 - val_mean_squared_error: 2300069120.0000\n",
      "Epoch 179/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1704896140.2152 - mean_squared_error: 1704896256.0000 - val_loss: 2265123714.3273 - val_mean_squared_error: 2265123840.0000\n",
      "Epoch 180/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1696178144.5848 - mean_squared_error: 1696178048.0000 - val_loss: 2264011913.3091 - val_mean_squared_error: 2264011776.0000\n",
      "Epoch 181/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1684344062.3756 - mean_squared_error: 1684343808.0000 - val_loss: 2254758070.6909 - val_mean_squared_error: 2254758144.0000\n",
      "Epoch 182/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1677302715.1269 - mean_squared_error: 1677302784.0000 - val_loss: 2226098859.0545 - val_mean_squared_error: 2226098944.0000\n",
      "Epoch 183/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1667039125.7015 - mean_squared_error: 1667039104.0000 - val_loss: 2248748143.7091 - val_mean_squared_error: 2248748288.0000\n",
      "Epoch 184/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1682136093.3685 - mean_squared_error: 1682136320.0000 - val_loss: 2207700766.2545 - val_mean_squared_error: 2207700736.0000\n",
      "Epoch 185/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1666433943.5208 - mean_squared_error: 1666433792.0000 - val_loss: 2193030116.0727 - val_mean_squared_error: 2193030400.0000\n",
      "Epoch 186/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1656642575.4640 - mean_squared_error: 1656642816.0000 - val_loss: 2195905405.6727 - val_mean_squared_error: 2195905536.0000\n",
      "Epoch 187/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1654196423.4071 - mean_squared_error: 1654196224.0000 - val_loss: 2185383780.0727 - val_mean_squared_error: 2185383680.0000\n",
      "Epoch 188/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1648471280.4711 - mean_squared_error: 1648471040.0000 - val_loss: 2175292097.1636 - val_mean_squared_error: 2175292160.0000\n",
      "Epoch 189/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1640806093.8396 - mean_squared_error: 1640806144.0000 - val_loss: 2166015362.3273 - val_mean_squared_error: 2166015232.0000\n",
      "Epoch 190/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1639797478.4650 - mean_squared_error: 1639797632.0000 - val_loss: 2158769682.6182 - val_mean_squared_error: 2158769920.0000\n",
      "Epoch 191/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1636584418.2416 - mean_squared_error: 1636584576.0000 - val_loss: 2156061903.1273 - val_mean_squared_error: 2156061952.0000\n",
      "Epoch 192/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1635620844.2477 - mean_squared_error: 1635620736.0000 - val_loss: 2142865286.9818 - val_mean_squared_error: 2142865152.0000\n",
      "Epoch 193/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1627247758.8142 - mean_squared_error: 1627248000.0000 - val_loss: 2134030412.8000 - val_mean_squared_error: 2134030336.0000\n",
      "Epoch 194/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1627004703.7076 - mean_squared_error: 1627004672.0000 - val_loss: 2119882156.2182 - val_mean_squared_error: 2119882240.0000\n",
      "Epoch 195/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1619948740.2234 - mean_squared_error: 1619948800.0000 - val_loss: 2134370934.6909 - val_mean_squared_error: 2134371072.0000\n",
      "Epoch 196/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1643471318.9360 - mean_squared_error: 1643471232.0000 - val_loss: 2118107920.2909 - val_mean_squared_error: 2118107904.0000\n",
      "Epoch 197/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1613160985.2102 - mean_squared_error: 1613160960.0000 - val_loss: 2104412941.9636 - val_mean_squared_error: 2104412928.0000\n",
      "Epoch 198/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1612187620.9706 - mean_squared_error: 1612187648.0000 - val_loss: 2085133451.6364 - val_mean_squared_error: 2085133568.0000\n",
      "Epoch 199/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1613132418.9239 - mean_squared_error: 1613132288.0000 - val_loss: 2076972683.6364 - val_mean_squared_error: 2076972544.0000\n",
      "Epoch 200/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1602122677.4091 - mean_squared_error: 1602122624.0000 - val_loss: 2068304502.6909 - val_mean_squared_error: 2068304640.0000\n",
      "Epoch 201/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1596514283.9878 - mean_squared_error: 1596514432.0000 - val_loss: 2060449475.4909 - val_mean_squared_error: 2060449408.0000\n",
      "Epoch 202/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1594419401.9411 - mean_squared_error: 1594419584.0000 - val_loss: 2054784965.8182 - val_mean_squared_error: 2054784896.0000\n",
      "Epoch 203/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1593318199.3584 - mean_squared_error: 1593318016.0000 - val_loss: 2049240942.5455 - val_mean_squared_error: 2049240960.0000\n",
      "Epoch 204/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1585452691.4924 - mean_squared_error: 1585452672.0000 - val_loss: 2029266537.8909 - val_mean_squared_error: 2029266560.0000\n",
      "Epoch 205/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1576939018.2660 - mean_squared_error: 1576939136.0000 - val_loss: 2030215263.4182 - val_mean_squared_error: 2030215168.0000\n",
      "Epoch 206/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1574586162.8751 - mean_squared_error: 1574586112.0000 - val_loss: 2026753937.4545 - val_mean_squared_error: 2026753920.0000\n",
      "Epoch 207/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1573206699.3381 - mean_squared_error: 1573206784.0000 - val_loss: 2009958095.1273 - val_mean_squared_error: 2009958144.0000\n",
      "Epoch 208/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1579556131.6061 - mean_squared_error: 1579556224.0000 - val_loss: 2009897874.6182 - val_mean_squared_error: 2009897984.0000\n",
      "Epoch 209/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1566192054.8386 - mean_squared_error: 1566192128.0000 - val_loss: 1996149140.9455 - val_mean_squared_error: 1996149248.0000\n",
      "Epoch 210/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1562373755.4518 - mean_squared_error: 1562373888.0000 - val_loss: 1991628156.5091 - val_mean_squared_error: 1991628288.0000\n",
      "Epoch 211/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1557554119.2122 - mean_squared_error: 1557554304.0000 - val_loss: 1983772766.2545 - val_mean_squared_error: 1983772800.0000\n",
      "Epoch 212/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1565472908.2152 - mean_squared_error: 1565473024.0000 - val_loss: 1975662962.0364 - val_mean_squared_error: 1975662848.0000\n",
      "Epoch 213/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1552538285.6122 - mean_squared_error: 1552537984.0000 - val_loss: 1973808858.7636 - val_mean_squared_error: 1973808768.0000\n",
      "Epoch 214/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1553641088.7797 - mean_squared_error: 1553640960.0000 - val_loss: 1968691097.6000 - val_mean_squared_error: 1968691072.0000\n",
      "Epoch 215/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1550885738.1685 - mean_squared_error: 1550885632.0000 - val_loss: 1954901842.6182 - val_mean_squared_error: 1954901888.0000\n",
      "Epoch 216/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1560830530.4690 - mean_squared_error: 1560830336.0000 - val_loss: 1953381248.0000 - val_mean_squared_error: 1953381248.0000\n",
      "Epoch 217/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1545488420.2558 - mean_squared_error: 1545488640.0000 - val_loss: 1953905196.2182 - val_mean_squared_error: 1953905152.0000\n",
      "Epoch 218/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1546569547.5005 - mean_squared_error: 1546569472.0000 - val_loss: 1938919698.6182 - val_mean_squared_error: 1938919808.0000\n",
      "Epoch 219/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1537395011.5086 - mean_squared_error: 1537394688.0000 - val_loss: 1933195337.3091 - val_mean_squared_error: 1933195392.0000\n",
      "Epoch 220/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1541096539.2244 - mean_squared_error: 1541096704.0000 - val_loss: 1926962867.2000 - val_mean_squared_error: 1926962944.0000\n",
      "Epoch 221/250\n",
      "985/985 [==============================] - 0s 55us/sample - loss: 1533935388.0690 - mean_squared_error: 1533935360.0000 - val_loss: 1924744664.4364 - val_mean_squared_error: 1924744704.0000\n",
      "Epoch 222/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1532713303.3909 - mean_squared_error: 1532713216.0000 - val_loss: 1932854998.1091 - val_mean_squared_error: 1932855040.0000\n",
      "Epoch 223/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1536327015.4396 - mean_squared_error: 1536327168.0000 - val_loss: 1912196978.0364 - val_mean_squared_error: 1912196864.0000\n",
      "Epoch 224/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1542509017.7949 - mean_squared_error: 1542508928.0000 - val_loss: 1911589140.9455 - val_mean_squared_error: 1911589248.0000\n",
      "Epoch 225/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1527845564.8162 - mean_squared_error: 1527845632.0000 - val_loss: 1926427893.5273 - val_mean_squared_error: 1926427904.0000\n",
      "Epoch 226/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1537999740.5563 - mean_squared_error: 1537999744.0000 - val_loss: 1904412290.3273 - val_mean_squared_error: 1904412288.0000\n",
      "Epoch 227/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1525772983.8782 - mean_squared_error: 1525772928.0000 - val_loss: 1893076440.4364 - val_mean_squared_error: 1893076480.0000\n",
      "Epoch 228/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1526653079.1310 - mean_squared_error: 1526652800.0000 - val_loss: 1899318309.2364 - val_mean_squared_error: 1899318272.0000\n",
      "Epoch 229/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1524608015.0741 - mean_squared_error: 1524607872.0000 - val_loss: 1887315018.4727 - val_mean_squared_error: 1887315072.0000\n",
      "Epoch 230/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1520112341.0518 - mean_squared_error: 1520112256.0000 - val_loss: 1895916227.4909 - val_mean_squared_error: 1895916288.0000\n",
      "Epoch 231/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1518639731.2650 - mean_squared_error: 1518639488.0000 - val_loss: 1881389030.4000 - val_mean_squared_error: 1881389056.0000\n",
      "Epoch 232/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1513166108.8487 - mean_squared_error: 1513166080.0000 - val_loss: 1896399234.3273 - val_mean_squared_error: 1896399104.0000\n",
      "Epoch 233/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1516464705.7543 - mean_squared_error: 1516464768.0000 - val_loss: 1870383443.7818 - val_mean_squared_error: 1870383488.0000\n",
      "Epoch 234/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1514317645.1898 - mean_squared_error: 1514317824.0000 - val_loss: 1875915722.4727 - val_mean_squared_error: 1875915776.0000\n",
      "Epoch 235/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1512403811.5411 - mean_squared_error: 1512403712.0000 - val_loss: 1871753541.8182 - val_mean_squared_error: 1871753472.0000\n",
      "Epoch 236/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1510780991.6102 - mean_squared_error: 1510780928.0000 - val_loss: 1863457982.8364 - val_mean_squared_error: 1863457920.0000\n",
      "Epoch 237/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1513240497.1208 - mean_squared_error: 1513240704.0000 - val_loss: 1856361522.0364 - val_mean_squared_error: 1856361472.0000\n",
      "Epoch 238/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1506521721.8274 - mean_squared_error: 1506521728.0000 - val_loss: 1858704842.4727 - val_mean_squared_error: 1858704768.0000\n",
      "Epoch 239/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1509888475.2244 - mean_squared_error: 1509888512.0000 - val_loss: 1855398867.7818 - val_mean_squared_error: 1855398912.0000\n",
      "Epoch 240/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1509540570.1198 - mean_squared_error: 1509540608.0000 - val_loss: 1850873188.0727 - val_mean_squared_error: 1850873216.0000\n",
      "Epoch 241/250\n",
      "985/985 [==============================] - 0s 56us/sample - loss: 1505454345.3563 - mean_squared_error: 1505454464.0000 - val_loss: 1848066250.4727 - val_mean_squared_error: 1848066176.0000\n",
      "Epoch 242/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1506476790.1888 - mean_squared_error: 1506476544.0000 - val_loss: 1847286150.9818 - val_mean_squared_error: 1847286144.0000\n",
      "Epoch 243/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1507391213.9371 - mean_squared_error: 1507391232.0000 - val_loss: 1844092844.8000 - val_mean_squared_error: 1844092800.0000\n",
      "Epoch 244/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1503383435.5330 - mean_squared_error: 1503383296.0000 - val_loss: 1841312051.2000 - val_mean_squared_error: 1841312128.0000\n",
      "Epoch 245/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1497966171.3543 - mean_squared_error: 1497966080.0000 - val_loss: 1843426716.5091 - val_mean_squared_error: 1843426688.0000\n",
      "Epoch 246/250\n",
      "985/985 [==============================] - 0s 52us/sample - loss: 1500941523.1675 - mean_squared_error: 1500941568.0000 - val_loss: 1834090572.8000 - val_mean_squared_error: 1834090624.0000\n",
      "Epoch 247/250\n",
      "985/985 [==============================] - 0s 53us/sample - loss: 1495381727.1228 - mean_squared_error: 1495381504.0000 - val_loss: 1844204655.7091 - val_mean_squared_error: 1844204672.0000\n",
      "Epoch 248/250\n",
      "985/985 [==============================] - 0s 54us/sample - loss: 1499757368.3330 - mean_squared_error: 1499757440.0000 - val_loss: 1840765740.2182 - val_mean_squared_error: 1840765824.0000\n",
      "Epoch 249/250\n",
      "985/985 [==============================] - 0s 50us/sample - loss: 1498138378.5259 - mean_squared_error: 1498138240.0000 - val_loss: 1829054436.0727 - val_mean_squared_error: 1829054464.0000\n",
      "Epoch 250/250\n",
      "985/985 [==============================] - 0s 51us/sample - loss: 1498361080.1381 - mean_squared_error: 1498360960.0000 - val_loss: 1823313340.5091 - val_mean_squared_error: 1823313408.0000\n"
     ]
    }
   ],
   "source": [
    "history3 = model3.fit(X_train_normalized2, y_train, epochs=250, validation_split=.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 30us/sample - loss: 1522597145.9507 - mean_squared_error: 1522597120.0000\n"
     ]
    }
   ],
   "source": [
    "scores3 = model3.evaluate(X_test_normalized2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After feature engineering, the regression's MSE improved by $-631,583,988.09\n",
      "After feature engineering, the neural network's MSE improved by $539,628,544.00\n"
     ]
    }
   ],
   "source": [
    "print(\"After feature engineering, the regression's MSE improved by ${:,.2f}\"\n",
    "      .format(lin_mse - lin_mse2))\n",
    "\n",
    "print(\"After feature engineering, the neural network's MSE improved by ${:,.2f}\"\n",
    "      .format(scores2[1] - scores3[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "\n",
    "Interesting; the new features significantly hurt the regression but significantly helped the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SfcFnOONyuNm"
   },
   "source": [
    "## Use the Keras Library to build an image recognition network using the Fashion-MNIST dataset (also comes with keras)\n",
    "\n",
    "- Load and preprocess the image data similar to how we preprocessed the MNIST data in class.\n",
    "- Make sure to one-hot encode your category labels\n",
    "- Make sure to have your final layer have as many nodes as the number of classes that you want to predict.\n",
    "- Try different hyperparameters. What is the highest accuracy that you are able to achieve.\n",
    "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
    "- Remember that neural networks fall prey to randomness so you may need to run your model multiple times (or use Cross Validation) in order to tell if a change to a hyperparameter is truly producing better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szi6-IpuzaH1"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(60000, 28*28)\n",
    "X_test = X_test.reshape(10000, 28*28)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train2 = to_categorical(y_train, num_classes)\n",
    "y_test2 = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_62 (Dense)             (None, 30)                23550     \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 10)                310       \n",
      "=================================================================\n",
      "Total params: 27,580\n",
      "Trainable params: 27,580\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnist_model = Sequential(name='mnist_model')\n",
    "\n",
    "mnist_model.add(Dense(30, input_dim=784, activation='relu'))\n",
    "mnist_model.add(Dense(30, activation='relu'))\n",
    "mnist_model.add(Dense(30, activation='relu'))\n",
    "mnist_model.add(Dense(30, activation='relu'))\n",
    "mnist_model.add(Dense(30, activation='relu'))\n",
    "mnist_model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "mnist_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "mnist_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 49us/sample - loss: nan - acc: 0.1000\n",
      "acc: 10.000000149011612\n"
     ]
    }
   ],
   "source": [
    "history = mnist_model.fit(X_train, y_train2, batch_size=batch_size, epochs=epochs, verbose=False)\n",
    "scores = mnist_model.evaluate(X_test, y_test2)\n",
    "print(f'{mnist_model.metrics_names[1]}: {scores[1]*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_68 (Dense)             (None, 20)                15700     \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 16,750\n",
      "Trainable params: 16,750\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnist_model2 = Sequential(name='mnist_model2')\n",
    "\n",
    "mnist_model2.add(Dense(20, input_dim=784, activation='relu'))\n",
    "mnist_model2.add(Dense(20, activation='relu'))\n",
    "mnist_model2.add(Dense(20, activation='relu'))\n",
    "mnist_model2.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "mnist_model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "mnist_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 51us/sample - loss: nan - acc: 0.1000\n",
      "acc: 10.000000149011612%\n"
     ]
    }
   ],
   "source": [
    "history2 = mnist_model2.fit(X_train, y_train2, batch_size=batch_size, epochs=epochs, verbose=False)\n",
    "scores2 = mnist_model2.evaluate(X_test, y_test2)\n",
    "print(f'{mnist_model2.metrics_names[1]}: {scores2[1]*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 53us/sample - loss: 8.5885 - acc: 0.1995\n",
      "acc: 19.949999451637268%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_classes = 20\n",
    "epochs = 250\n",
    "\n",
    "y_train3 = to_categorical(y_train, num_classes)\n",
    "y_test3 = to_categorical(y_test, num_classes)\n",
    "\n",
    "mnist_model3 = Sequential(name='mnist_model2')\n",
    "mnist_model3.add(Dense(30, input_dim=784, activation='relu'))\n",
    "mnist_model3.add(Dense(30, activation='relu'))\n",
    "mnist_model3.add(Dense(30, activation='relu'))\n",
    "mnist_model3.add(Dense(20, activation='sigmoid'))\n",
    "mnist_model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history3 = mnist_model3.fit(X_train, y_train3, batch_size=batch_size, epochs=epochs, verbose=False)\n",
    "scores3 = mnist_model3.evaluate(X_test, y_test3)\n",
    "print(f'{mnist_model3.metrics_names[1]}: {scores3[1]*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 54us/sample - loss: 0.5181 - acc: 0.8520\n",
      "acc: 19.949999451637268%\n"
     ]
    }
   ],
   "source": [
    "mnist_model4 = Sequential(name='mnist_model4')\n",
    "mnist_model4.add(Dense(30, input_dim=784, activation='relu'))\n",
    "mnist_model4.add(Dense(30, activation='relu'))\n",
    "mnist_model4.add(Dense(30, activation='relu'))\n",
    "mnist_model4.add(Dense(20, activation='softmax'))\n",
    "mnist_model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history4 = mnist_model4.fit(X_train, y_train3, batch_size=batch_size, epochs=epochs, verbose=False)\n",
    "scores4 = mnist_model4.evaluate(X_test, y_test3)\n",
    "#print(f'{mnist_model4.metrics_names[1]}: {scores3[1]*100}%')\n",
    "# forgot to change scores 3 to scores4; keeping the above line to explain why there's \"acc\" in the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 85.19999980926514%\n"
     ]
    }
   ],
   "source": [
    "print(f'{mnist_model4.metrics_names[1]}: {scores4[1]*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 60us/sample - loss: 12.8961 - acc: 0.1999\n",
      "acc: 19.990000128746033%\n"
     ]
    }
   ],
   "source": [
    "mnist_model5 = Sequential(name='mnist_model5')\n",
    "mnist_model5.add(Dense(50, input_dim=784, activation='relu'))\n",
    "mnist_model5.add(Dense(50, activation='relu'))\n",
    "mnist_model5.add(Dense(20, activation='softmax'))\n",
    "mnist_model5.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history5 = mnist_model5.fit(X_train, y_train3, batch_size=batch_size, epochs=epochs, verbose=False)\n",
    "scores5 = mnist_model5.evaluate(X_test, y_test3)\n",
    "print(f'{mnist_model5.metrics_names[1]}: {scores5[1]*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [10.104350356547037,\n",
       "  5.5239645332336424,\n",
       "  0.7280191445350647,\n",
       "  0.6416097206115723,\n",
       "  0.5999556479136149,\n",
       "  0.57086315685908,\n",
       "  0.5423135063648223,\n",
       "  0.5378928355852763,\n",
       "  0.5110366497516632,\n",
       "  0.49652727201779684,\n",
       "  0.48621878379185995,\n",
       "  0.47341205151875815,\n",
       "  0.4681453092892965,\n",
       "  0.4590261271635691,\n",
       "  0.45309892950057984,\n",
       "  0.4277035462061564,\n",
       "  0.41895921177864076,\n",
       "  0.4119960872809092,\n",
       "  0.40830311415990195,\n",
       "  0.39940396370887754,\n",
       "  0.39650303513209023,\n",
       "  0.3899669487953186,\n",
       "  0.3901788347005844,\n",
       "  0.3812743240515391,\n",
       "  0.3842205726146698,\n",
       "  0.3804528474966685,\n",
       "  0.3710465505599976,\n",
       "  0.37413855996131895,\n",
       "  0.37271419808069867,\n",
       "  0.3701269857724508,\n",
       "  0.36411271575291954,\n",
       "  0.3635120357831319,\n",
       "  0.36043160746097563,\n",
       "  0.36380989145437875,\n",
       "  0.35949290974934894,\n",
       "  0.3578373213926951,\n",
       "  0.3538252984682719,\n",
       "  0.35221810002326964,\n",
       "  0.3572491289218267,\n",
       "  0.3552642683426539,\n",
       "  0.3561127767165502,\n",
       "  0.3457362171888351,\n",
       "  0.3470778883695602,\n",
       "  0.3480693128108978,\n",
       "  0.3458678532123566,\n",
       "  0.3485951348781586,\n",
       "  0.34201757400433225,\n",
       "  0.3482985583464305,\n",
       "  0.3436438460667928,\n",
       "  0.3357832374016444,\n",
       "  0.34329626231193544,\n",
       "  0.335304030585289,\n",
       "  0.3354350578069687,\n",
       "  0.33209636437098183,\n",
       "  0.33205113128821057,\n",
       "  0.33660463450749717,\n",
       "  0.3298119934956233,\n",
       "  0.3333078904946645,\n",
       "  0.3289498834689458,\n",
       "  0.33338331010341643,\n",
       "  0.32940389142036436,\n",
       "  0.32924790828227996,\n",
       "  0.3298134144385656,\n",
       "  0.3310762050549189,\n",
       "  0.32404521955649057,\n",
       "  0.3248516383806864,\n",
       "  0.33427712847391766,\n",
       "  0.3205611809571584,\n",
       "  0.3240060384432475,\n",
       "  0.32531941253344215,\n",
       "  0.32650933126608533,\n",
       "  0.31579955271085103,\n",
       "  0.3204864918629328,\n",
       "  0.3169654178619385,\n",
       "  0.3197861184199651,\n",
       "  0.32005332231521605,\n",
       "  0.31614049021402996,\n",
       "  0.3166862366755803,\n",
       "  0.31812480628490447,\n",
       "  0.3179654893716176,\n",
       "  0.32384554198582965,\n",
       "  0.31918312119642894,\n",
       "  0.31657504450480145,\n",
       "  0.3120422287225723,\n",
       "  0.31778755530516306,\n",
       "  0.3147565460642179,\n",
       "  0.3141290539741516,\n",
       "  0.315448052628835,\n",
       "  0.31989130942821503,\n",
       "  0.30996152389844256,\n",
       "  0.3081188581943512,\n",
       "  0.30824640939235687,\n",
       "  0.3101937718391418,\n",
       "  0.31278103398482004,\n",
       "  0.30867843619187674,\n",
       "  0.3076662102222443,\n",
       "  0.30587351807753244,\n",
       "  0.3068160412232081,\n",
       "  0.31254987115859983,\n",
       "  0.30799889663855234,\n",
       "  0.3065721020857493,\n",
       "  0.3068287027756373,\n",
       "  0.3051083079576492,\n",
       "  0.3154201268831889,\n",
       "  0.3079397381703059,\n",
       "  0.30339818823337555,\n",
       "  0.3095643561522166,\n",
       "  0.31374820887247723,\n",
       "  0.3077287195245425,\n",
       "  0.31342606423695885,\n",
       "  0.30209711105823517,\n",
       "  0.3102228803475698,\n",
       "  0.31265712542533874,\n",
       "  0.2971990173459053,\n",
       "  0.301554572057724,\n",
       "  0.3026484750350316,\n",
       "  0.3052585581461589,\n",
       "  0.3047909649292628,\n",
       "  0.30581779407660165,\n",
       "  0.2991512370109558,\n",
       "  0.30295094221433005,\n",
       "  0.31045025585492453,\n",
       "  0.30205792366663614,\n",
       "  0.3004401867787043,\n",
       "  0.3019670962254206,\n",
       "  0.29837099287509916,\n",
       "  0.30763510286013285,\n",
       "  0.2967289761543274,\n",
       "  0.29983416129748025,\n",
       "  0.3049112981478373,\n",
       "  0.3017489206234614,\n",
       "  0.3055392489671707,\n",
       "  0.30116532675822577,\n",
       "  0.299536318953832,\n",
       "  0.306526099729538,\n",
       "  0.30078989230791725,\n",
       "  0.3010386643330256,\n",
       "  0.2935292400121689,\n",
       "  0.29942944676876065,\n",
       "  0.29448145116964974,\n",
       "  0.2991919287443161,\n",
       "  0.3004777222951253,\n",
       "  0.3007984878063202,\n",
       "  0.30752525289058685,\n",
       "  0.31125926010608673,\n",
       "  0.2971809096415838,\n",
       "  0.3012513489087423,\n",
       "  0.29029719398021697,\n",
       "  0.299152850095431,\n",
       "  0.29763768088420234,\n",
       "  0.2969632116238276,\n",
       "  0.2950320030768712,\n",
       "  0.31343448152542114,\n",
       "  0.2929091104189555,\n",
       "  0.294008488257726,\n",
       "  0.3015872835556666,\n",
       "  0.2955257798751195,\n",
       "  0.29208974668184917,\n",
       "  0.2989120992422104,\n",
       "  0.3001371733268102,\n",
       "  0.2952088574409485,\n",
       "  0.29126359883149466,\n",
       "  0.29952664801279705,\n",
       "  0.29605435293515525,\n",
       "  0.2949445694287618,\n",
       "  0.2891397475719452,\n",
       "  0.2997619929790497,\n",
       "  0.30071910496552784,\n",
       "  0.29415165849526725,\n",
       "  0.3103124599297841,\n",
       "  0.294001616191864,\n",
       "  0.29967311038970945,\n",
       "  0.2922277028083801,\n",
       "  0.2996582621534665,\n",
       "  0.2976377678076426,\n",
       "  0.3029329823652903,\n",
       "  0.2949222422758738,\n",
       "  0.2878358173131943,\n",
       "  0.29415545814037325,\n",
       "  0.29866625340779623,\n",
       "  0.29703329570293424,\n",
       "  0.2896354910055796,\n",
       "  0.2968624130964279,\n",
       "  0.29387093489964805,\n",
       "  0.2988148910721143,\n",
       "  0.29166576030254365,\n",
       "  0.2885978883266449,\n",
       "  0.29325672620137533,\n",
       "  0.2931760874589284,\n",
       "  0.2882249717076619,\n",
       "  0.2963499382257462,\n",
       "  0.30182553547223406,\n",
       "  0.28281115884780883,\n",
       "  0.29135779661337535,\n",
       "  0.2901459070046743,\n",
       "  0.2915347562233607,\n",
       "  0.2845893203655879,\n",
       "  0.289902215218544,\n",
       "  0.29797969665527346,\n",
       "  0.30061098667383196,\n",
       "  0.3069288574854533,\n",
       "  0.2960551416476568,\n",
       "  0.31297055570284527,\n",
       "  0.29246076622009276,\n",
       "  0.28735815119743346,\n",
       "  0.28951175702412923,\n",
       "  0.291619151310126,\n",
       "  0.3065580498615901,\n",
       "  0.2817745386362076,\n",
       "  0.2846479341506958,\n",
       "  0.2851077470779419,\n",
       "  0.29991739591757455,\n",
       "  0.30909914077917733,\n",
       "  0.30012872428099313,\n",
       "  0.2964788176059723,\n",
       "  0.3024814179897308,\n",
       "  0.28796209280490875,\n",
       "  0.2910531721194585,\n",
       "  0.28597829418182374,\n",
       "  0.2977689562082291,\n",
       "  0.29952360752423607,\n",
       "  0.28661585830052694,\n",
       "  0.2891166306257248,\n",
       "  0.28899629400571186,\n",
       "  0.29516312397321065,\n",
       "  0.2976844652891159,\n",
       "  0.29210873161156975,\n",
       "  0.2911809489091237,\n",
       "  0.3174928077618281,\n",
       "  0.28911193838914234,\n",
       "  0.2852309035936991,\n",
       "  0.29297397869428,\n",
       "  0.29763733282089233,\n",
       "  0.2824543833335241,\n",
       "  0.28364070615768433,\n",
       "  0.2792910146633784,\n",
       "  0.29689017090797426,\n",
       "  0.29163819632530213,\n",
       "  0.2872860672791799,\n",
       "  0.30152403457164767,\n",
       "  0.293221856212616,\n",
       "  0.29091867334047955,\n",
       "  0.2810412581205368,\n",
       "  0.28354953087965645,\n",
       "  0.2861883640607198,\n",
       "  0.30464657042821247,\n",
       "  0.29032453294595084,\n",
       "  0.290547537557284,\n",
       "  0.28308533191680907,\n",
       "  0.30521998832623165],\n",
       " 'acc': [0.36888334,\n",
       "  0.5339,\n",
       "  0.7450167,\n",
       "  0.7741333,\n",
       "  0.78718334,\n",
       "  0.79511666,\n",
       "  0.80403334,\n",
       "  0.8038333,\n",
       "  0.81083333,\n",
       "  0.8150667,\n",
       "  0.81731665,\n",
       "  0.82093334,\n",
       "  0.82096666,\n",
       "  0.8242667,\n",
       "  0.83386666,\n",
       "  0.8451167,\n",
       "  0.84681666,\n",
       "  0.8502833,\n",
       "  0.85095,\n",
       "  0.85433334,\n",
       "  0.8562833,\n",
       "  0.85751665,\n",
       "  0.85835,\n",
       "  0.86115,\n",
       "  0.8585167,\n",
       "  0.86165,\n",
       "  0.8629,\n",
       "  0.8623667,\n",
       "  0.86356664,\n",
       "  0.86385,\n",
       "  0.86721665,\n",
       "  0.8669,\n",
       "  0.86808336,\n",
       "  0.86745,\n",
       "  0.8691667,\n",
       "  0.8685833,\n",
       "  0.8713167,\n",
       "  0.8723,\n",
       "  0.8685833,\n",
       "  0.86986667,\n",
       "  0.8693,\n",
       "  0.87268335,\n",
       "  0.8728,\n",
       "  0.8725333,\n",
       "  0.8736167,\n",
       "  0.8724333,\n",
       "  0.8750167,\n",
       "  0.8732333,\n",
       "  0.8746167,\n",
       "  0.87696666,\n",
       "  0.8746,\n",
       "  0.8756,\n",
       "  0.87633336,\n",
       "  0.8781,\n",
       "  0.8781667,\n",
       "  0.87658334,\n",
       "  0.87901664,\n",
       "  0.87666667,\n",
       "  0.87838334,\n",
       "  0.87656665,\n",
       "  0.8789833,\n",
       "  0.87925,\n",
       "  0.87805,\n",
       "  0.8792,\n",
       "  0.88161665,\n",
       "  0.88165,\n",
       "  0.87765,\n",
       "  0.88275,\n",
       "  0.8814333,\n",
       "  0.8821167,\n",
       "  0.8794,\n",
       "  0.88233334,\n",
       "  0.8818,\n",
       "  0.88385,\n",
       "  0.8821833,\n",
       "  0.88285,\n",
       "  0.88451666,\n",
       "  0.8835667,\n",
       "  0.88325,\n",
       "  0.8838,\n",
       "  0.88103336,\n",
       "  0.88156664,\n",
       "  0.88376665,\n",
       "  0.8854333,\n",
       "  0.8828667,\n",
       "  0.8840333,\n",
       "  0.8839667,\n",
       "  0.88225,\n",
       "  0.8832333,\n",
       "  0.88558334,\n",
       "  0.88645,\n",
       "  0.88673335,\n",
       "  0.88625,\n",
       "  0.88448334,\n",
       "  0.88668334,\n",
       "  0.88605,\n",
       "  0.8882333,\n",
       "  0.88678336,\n",
       "  0.885,\n",
       "  0.8857333,\n",
       "  0.8878667,\n",
       "  0.88671666,\n",
       "  0.88935,\n",
       "  0.8848,\n",
       "  0.88561666,\n",
       "  0.8889,\n",
       "  0.8847833,\n",
       "  0.88395,\n",
       "  0.8871,\n",
       "  0.88481665,\n",
       "  0.88916665,\n",
       "  0.8857167,\n",
       "  0.8839833,\n",
       "  0.8889667,\n",
       "  0.8890333,\n",
       "  0.88886666,\n",
       "  0.8872,\n",
       "  0.88736665,\n",
       "  0.8864,\n",
       "  0.88983333,\n",
       "  0.8885,\n",
       "  0.8852,\n",
       "  0.88876665,\n",
       "  0.88918334,\n",
       "  0.8886833,\n",
       "  0.88986665,\n",
       "  0.88768333,\n",
       "  0.89005,\n",
       "  0.88911664,\n",
       "  0.88835,\n",
       "  0.88843334,\n",
       "  0.8872,\n",
       "  0.88806665,\n",
       "  0.89045,\n",
       "  0.8873,\n",
       "  0.88916665,\n",
       "  0.88801664,\n",
       "  0.89105,\n",
       "  0.8886167,\n",
       "  0.8912333,\n",
       "  0.88905,\n",
       "  0.8904833,\n",
       "  0.88825,\n",
       "  0.88376665,\n",
       "  0.8868833,\n",
       "  0.8907833,\n",
       "  0.8894,\n",
       "  0.89285,\n",
       "  0.88975,\n",
       "  0.8905333,\n",
       "  0.89066666,\n",
       "  0.89143336,\n",
       "  0.88268334,\n",
       "  0.8914667,\n",
       "  0.8901333,\n",
       "  0.88818336,\n",
       "  0.8908,\n",
       "  0.89243335,\n",
       "  0.88983333,\n",
       "  0.88888335,\n",
       "  0.8903,\n",
       "  0.8928,\n",
       "  0.8886833,\n",
       "  0.8915,\n",
       "  0.8918833,\n",
       "  0.8929333,\n",
       "  0.89021665,\n",
       "  0.8895,\n",
       "  0.8926833,\n",
       "  0.88158333,\n",
       "  0.89215,\n",
       "  0.88948333,\n",
       "  0.89266664,\n",
       "  0.8894167,\n",
       "  0.89105,\n",
       "  0.8878,\n",
       "  0.8889167,\n",
       "  0.89353335,\n",
       "  0.8921,\n",
       "  0.8907,\n",
       "  0.8901,\n",
       "  0.89428335,\n",
       "  0.8890333,\n",
       "  0.8928667,\n",
       "  0.8881,\n",
       "  0.89173335,\n",
       "  0.8936667,\n",
       "  0.89166665,\n",
       "  0.89213336,\n",
       "  0.89348334,\n",
       "  0.89275,\n",
       "  0.89021665,\n",
       "  0.8955,\n",
       "  0.89351666,\n",
       "  0.8926167,\n",
       "  0.89381665,\n",
       "  0.8954,\n",
       "  0.89355,\n",
       "  0.89105,\n",
       "  0.89103335,\n",
       "  0.8837,\n",
       "  0.8903,\n",
       "  0.87691665,\n",
       "  0.89038336,\n",
       "  0.8932667,\n",
       "  0.89311665,\n",
       "  0.89241666,\n",
       "  0.88456666,\n",
       "  0.89645,\n",
       "  0.8964,\n",
       "  0.89536667,\n",
       "  0.89208335,\n",
       "  0.8872667,\n",
       "  0.8893667,\n",
       "  0.88988334,\n",
       "  0.88885,\n",
       "  0.89376664,\n",
       "  0.8933333,\n",
       "  0.89533335,\n",
       "  0.8908833,\n",
       "  0.8904167,\n",
       "  0.8950167,\n",
       "  0.89316666,\n",
       "  0.8943,\n",
       "  0.89353335,\n",
       "  0.8926333,\n",
       "  0.8917,\n",
       "  0.89345,\n",
       "  0.8767167,\n",
       "  0.89313334,\n",
       "  0.8950167,\n",
       "  0.8926333,\n",
       "  0.8902,\n",
       "  0.89475,\n",
       "  0.8965833,\n",
       "  0.89778334,\n",
       "  0.89155,\n",
       "  0.8908833,\n",
       "  0.8939,\n",
       "  0.88588333,\n",
       "  0.89098334,\n",
       "  0.8922833,\n",
       "  0.8965833,\n",
       "  0.8954667,\n",
       "  0.8944,\n",
       "  0.888,\n",
       "  0.89341664,\n",
       "  0.8933667,\n",
       "  0.8968167,\n",
       "  0.88525]}"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history4.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(history4.history['acc'])\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Accuracy score')\n",
    "plt.title('Accuracy score increase through the epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zv_3xNMjzdLI"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Use Hyperparameter Tuning to make the accuracy of your models as high as possible. (error as low as possible)\n",
    "- Use Cross Validation techniques to get more consistent results with your model.\n",
    "- Use GridSearchCV to try different combinations of hyperparameters. \n",
    "- Start looking into other types of Keras layers for CNNs and RNNs maybe try and build a CNN model for fashion-MNIST to see how the results compare."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_433_Keras_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
