{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBQsZEJmubLs"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Neural Network Framework (Keras)\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignmnet 3*\n",
    "\n",
    "## Use the Keras Library to build a Multi-Layer Perceptron Model on the Boston Housing dataset\n",
    "\n",
    "- The Boston Housing dataset comes with the Keras library so use Keras to import it into your notebook. \n",
    "- Normalize the data (all features should have roughly the same scale)\n",
    "- Import the type of model and layers that you will need from Keras.\n",
    "- Instantiate a model object and use `model.add()` to add layers to your model\n",
    "- Since this is a regression model you will have a single output node in the final layer.\n",
    "- Use activation functions that are appropriate for this task\n",
    "- Compile your model\n",
    "- Fit your model and report its accuracy in terms of Mean Squared Error\n",
    "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
    "- Run this same data through a linear regression model. Which achieves higher accuracy?\n",
    "- Do a little bit of feature engineering and see how that affects your neural network model. (you will need to change your model to accept more inputs)\n",
    "- After feature engineering, which model sees a greater accuracy boost due to the new features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NLTAR87uYJ-"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import normalize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (404,), (102, 13), (102,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize(\n",
    "    X_train,\n",
    "    axis=1,\n",
    "    order=2\n",
    ")\n",
    "\n",
    "X_test = normalize(\n",
    "    X_test,\n",
    "    axis=1,\n",
    "    order=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (404,), (102, 13), (102,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.resize(404, 1)\n",
    "y_test.resize(102, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (404, 1), (102, 13), (102, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 270 samples, validate on 134 samples\n",
      "Epoch 1/50\n",
      "270/270 [==============================] - 0s 2ms/sample - loss: 531.8965 - acc: 0.0000e+00 - val_loss: 619.4525 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 531.2578 - acc: 0.0000e+00 - val_loss: 618.7400 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "270/270 [==============================] - 0s 106us/sample - loss: 530.5840 - acc: 0.0000e+00 - val_loss: 617.9804 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "270/270 [==============================] - 0s 127us/sample - loss: 529.8540 - acc: 0.0000e+00 - val_loss: 617.1792 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "270/270 [==============================] - 0s 85us/sample - loss: 529.1088 - acc: 0.0000e+00 - val_loss: 616.3413 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "270/270 [==============================] - 0s 98us/sample - loss: 528.3149 - acc: 0.0000e+00 - val_loss: 615.4940 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "270/270 [==============================] - 0s 123us/sample - loss: 527.5156 - acc: 0.0000e+00 - val_loss: 614.6431 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 526.7175 - acc: 0.0000e+00 - val_loss: 613.7854 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "270/270 [==============================] - 0s 112us/sample - loss: 525.9101 - acc: 0.0000e+00 - val_loss: 612.9405 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "270/270 [==============================] - 0s 98us/sample - loss: 525.1175 - acc: 0.0000e+00 - val_loss: 612.1016 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "270/270 [==============================] - 0s 120us/sample - loss: 524.3482 - acc: 0.0000e+00 - val_loss: 611.2786 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "270/270 [==============================] - 0s 119us/sample - loss: 523.5823 - acc: 0.0000e+00 - val_loss: 610.4813 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "270/270 [==============================] - 0s 111us/sample - loss: 522.8462 - acc: 0.0000e+00 - val_loss: 609.6979 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "270/270 [==============================] - 0s 127us/sample - loss: 522.1285 - acc: 0.0000e+00 - val_loss: 608.9537 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "270/270 [==============================] - 0s 115us/sample - loss: 521.4532 - acc: 0.0000e+00 - val_loss: 608.2484 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "270/270 [==============================] - 0s 102us/sample - loss: 520.8067 - acc: 0.0000e+00 - val_loss: 607.5884 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "270/270 [==============================] - 0s 111us/sample - loss: 520.2085 - acc: 0.0000e+00 - val_loss: 606.9701 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "270/270 [==============================] - 0s 118us/sample - loss: 519.6525 - acc: 0.0000e+00 - val_loss: 606.3951 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "270/270 [==============================] - 0s 98us/sample - loss: 519.1300 - acc: 0.0000e+00 - val_loss: 605.8703 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "270/270 [==============================] - 0s 101us/sample - loss: 518.6515 - acc: 0.0000e+00 - val_loss: 605.3905 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "270/270 [==============================] - 0s 117us/sample - loss: 518.2209 - acc: 0.0000e+00 - val_loss: 604.9485 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "270/270 [==============================] - 0s 104us/sample - loss: 517.8239 - acc: 0.0000e+00 - val_loss: 604.5478 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "270/270 [==============================] - 0s 107us/sample - loss: 517.4654 - acc: 0.0000e+00 - val_loss: 604.1842 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "270/270 [==============================] - 0s 91us/sample - loss: 517.1409 - acc: 0.0000e+00 - val_loss: 603.8565 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 516.8434 - acc: 0.0000e+00 - val_loss: 603.5623 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "270/270 [==============================] - 0s 99us/sample - loss: 516.5796 - acc: 0.0000e+00 - val_loss: 603.2962 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "270/270 [==============================] - 0s 98us/sample - loss: 516.3411 - acc: 0.0000e+00 - val_loss: 603.0554 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "270/270 [==============================] - 0s 93us/sample - loss: 516.1257 - acc: 0.0000e+00 - val_loss: 602.8384 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "270/270 [==============================] - 0s 107us/sample - loss: 515.9299 - acc: 0.0000e+00 - val_loss: 602.6441 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "270/270 [==============================] - 0s 107us/sample - loss: 515.7564 - acc: 0.0000e+00 - val_loss: 602.4677 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "270/270 [==============================] - 0s 102us/sample - loss: 515.5992 - acc: 0.0000e+00 - val_loss: 602.3078 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 515.4560 - acc: 0.0000e+00 - val_loss: 602.1642 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "270/270 [==============================] - 0s 127us/sample - loss: 515.3266 - acc: 0.0000e+00 - val_loss: 602.0344 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "270/270 [==============================] - 0s 134us/sample - loss: 515.2101 - acc: 0.0000e+00 - val_loss: 601.9159 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "270/270 [==============================] - 0s 123us/sample - loss: 515.1043 - acc: 0.0000e+00 - val_loss: 601.8086 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "270/270 [==============================] - 0s 117us/sample - loss: 515.0076 - acc: 0.0000e+00 - val_loss: 601.7113 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "270/270 [==============================] - 0s 107us/sample - loss: 514.9197 - acc: 0.0000e+00 - val_loss: 601.6230 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "270/270 [==============================] - 0s 126us/sample - loss: 514.8413 - acc: 0.0000e+00 - val_loss: 601.5417 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "270/270 [==============================] - 0s 109us/sample - loss: 514.7674 - acc: 0.0000e+00 - val_loss: 601.4679 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 514.7006 - acc: 0.0000e+00 - val_loss: 601.4003 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "270/270 [==============================] - 0s 104us/sample - loss: 514.6400 - acc: 0.0000e+00 - val_loss: 601.3376 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "270/270 [==============================] - 0s 86us/sample - loss: 514.5829 - acc: 0.0000e+00 - val_loss: 601.2805 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "270/270 [==============================] - 0s 104us/sample - loss: 514.5315 - acc: 0.0000e+00 - val_loss: 601.2273 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "270/270 [==============================] - 0s 94us/sample - loss: 514.4837 - acc: 0.0000e+00 - val_loss: 601.1779 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "270/270 [==============================] - 0s 91us/sample - loss: 514.4388 - acc: 0.0000e+00 - val_loss: 601.1325 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "270/270 [==============================] - 0s 86us/sample - loss: 514.3976 - acc: 0.0000e+00 - val_loss: 601.0903 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "270/270 [==============================] - 0s 106us/sample - loss: 514.3596 - acc: 0.0000e+00 - val_loss: 601.0511 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "270/270 [==============================] - 0s 109us/sample - loss: 514.3240 - acc: 0.0000e+00 - val_loss: 601.0147 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "270/270 [==============================] - 0s 105us/sample - loss: 514.2910 - acc: 0.0000e+00 - val_loss: 600.9808 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "270/270 [==============================] - 0s 108us/sample - loss: 514.2604 - acc: 0.0000e+00 - val_loss: 600.9490 - val_acc: 0.0000e+00\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 14        \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 198\n",
      "Trainable params: 198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "boston_housing = Sequential()\n",
    " \n",
    "boston_housing.add(Dense(13, input_dim=13, activation='relu'))\n",
    "boston_housing.add(Dense(1, activation='relu'))\n",
    "boston_housing.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "boston_housing.compile(loss='mean_squared_error', \n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "boston_housing.fit(X_train, y_train, validation_split=0.33, epochs=50, batch_size=50, verbose=1)\n",
    "\n",
    "boston_housing.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 84us/sample - loss: 592.4653 - acc: 0.0000e+00\n",
      "acc: 0.0\n"
     ]
    }
   ],
   "source": [
    "scores = boston_housing.evaluate(X_test, y_test)\n",
    "print(f'{boston_housing.metrics_names[1]}: {scores[1]*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SfcFnOONyuNm"
   },
   "source": [
    "## Use the Keras Library to build an image recognition network using the Fashion-MNIST dataset (also comes with keras)\n",
    "\n",
    "- Load and preprocess the image data similar to how we preprocessed the MNIST data in class.\n",
    "- Make sure to one-hot encode your category labels\n",
    "- Make sure to have your final layer have as many nodes as the number of classes that you want to predict.\n",
    "- Try different hyperparameters. What is the highest accuracy that you are able to achieve.\n",
    "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
    "- Remember that neural networks fall prey to randomness so you may need to run your model multiple times (or use Cross Validation) in order to tell if a change to a hyperparameter is truly producing better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szi6-IpuzaH1"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zv_3xNMjzdLI"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Use Hyperparameter Tuning to make the accuracy of your models as high as possible. (error as low as possible)\n",
    "- Use Cross Validation techniques to get more consistent results with your model.\n",
    "- Use GridSearchCV to try different combinations of hyperparameters. \n",
    "- Start looking into other types of Keras layers for CNNs and RNNs maybe try and build a CNN model for fashion-MNIST to see how the results compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_433_Keras_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
