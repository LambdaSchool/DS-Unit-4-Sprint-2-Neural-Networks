{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBQsZEJmubLs"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Neural Network Framework (Keras)\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignmnet 3*\n",
    "\n",
    "## Use the Keras Library to build a Multi-Layer Perceptron Model on the Boston Housing dataset\n",
    "\n",
    "- The Boston Housing dataset comes with the Keras library so use Keras to import it into your notebook. \n",
    "- Normalize the data (all features should have roughly the same scale)\n",
    "- Import the type of model and layers that you will need from Keras.\n",
    "- Instantiate a model object and use `model.add()` to add layers to your model\n",
    "- Since this is a regression model you will have a single output node in the final layer.\n",
    "- Use activation functions that are appropriate for this task\n",
    "- Compile your model\n",
    "- Fit your model and report its accuracy in terms of Mean Squared Error\n",
    "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
    "- Run this same data through a linear regression model. Which achieves higher accuracy?\n",
    "- Do a little bit of feature engineering and see how that affects your neural network model. (you will need to change your model to accept more inputs)\n",
    "- After feature engineering, which model sees a greater accuracy boost due to the new features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NLTAR87uYJ-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.23247,   0.     ,   8.14   ,   0.     ,   0.538  ,   6.142  ,\n",
       "        91.7    ,   3.9769 ,   4.     , 307.     ,  21.     , 396.9    ,\n",
       "        18.72   ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['mean_squared_error'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_model()\n",
    "epochs = 2000\n",
    "model.fit(train_data, train_targets, batch_size=32, epochs=epochs, validation_split=.1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SfcFnOONyuNm"
   },
   "source": [
    "## Use the Keras Library to build an image recognition network using the Fashion-MNIST dataset (also comes with keras)\n",
    "\n",
    "- Load and preprocess the image data similar to how we preprocessed the MNIST data in class.\n",
    "- Make sure to one-hot encode your category labels\n",
    "- Make sure to have your final layer have as many nodes as the number of classes that you want to predict.\n",
    "- Try different hyperparameters. What is the highest accuracy that you are able to achieve.\n",
    "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
    "- Remember that neural networks fall prey to randomness so you may need to run your model multiple times (or use Cross Validation) in order to tell if a change to a hyperparameter is truly producing better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szi6-IpuzaH1"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_train.reshape(60000, 784)\n",
    "X_test = x_test.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mnist_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=784, activation='relu'))\n",
    "    model.add(Dense(48, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/20\n",
      "54000/54000 [==============================] - 0s 7us/sample - loss: 0.2890 - accuracy: 0.8951 - val_loss: 0.4100 - val_accuracy: 0.8715\n",
      "Epoch 2/20\n",
      "54000/54000 [==============================] - 0s 7us/sample - loss: 0.2839 - accuracy: 0.8963 - val_loss: 0.4139 - val_accuracy: 0.8675\n",
      "Epoch 3/20\n",
      "54000/54000 [==============================] - 0s 7us/sample - loss: 0.2891 - accuracy: 0.8964 - val_loss: 0.4351 - val_accuracy: 0.8545\n",
      "Epoch 4/20\n",
      "54000/54000 [==============================] - 0s 7us/sample - loss: 0.2868 - accuracy: 0.8960 - val_loss: 0.4082 - val_accuracy: 0.8713\n",
      "Epoch 5/20\n",
      "54000/54000 [==============================] - 0s 7us/sample - loss: 0.2786 - accuracy: 0.8988 - val_loss: 0.4152 - val_accuracy: 0.8673\n",
      "Epoch 6/20\n",
      "54000/54000 [==============================] - 0s 8us/sample - loss: 0.2796 - accuracy: 0.8976 - val_loss: 0.4280 - val_accuracy: 0.8655\n",
      "Epoch 7/20\n",
      "54000/54000 [==============================] - 0s 7us/sample - loss: 0.2793 - accuracy: 0.8988 - val_loss: 0.4198 - val_accuracy: 0.8693\n",
      "Epoch 8/20\n",
      "54000/54000 [==============================] - 0s 8us/sample - loss: 0.2769 - accuracy: 0.8999 - val_loss: 0.4264 - val_accuracy: 0.8657\n",
      "Epoch 9/20\n",
      "54000/54000 [==============================] - 0s 7us/sample - loss: 0.2763 - accuracy: 0.8987 - val_loss: 0.4106 - val_accuracy: 0.8680\n",
      "Epoch 10/20\n",
      "54000/54000 [==============================] - 0s 7us/sample - loss: 0.2667 - accuracy: 0.9027 - val_loss: 0.4142 - val_accuracy: 0.8683\n",
      "Epoch 11/20\n",
      "54000/54000 [==============================] - 0s 7us/sample - loss: 0.2656 - accuracy: 0.9026 - val_loss: 0.4178 - val_accuracy: 0.8707\n",
      "Epoch 12/20\n",
      "54000/54000 [==============================] - 0s 7us/sample - loss: 0.2754 - accuracy: 0.8990 - val_loss: 0.4409 - val_accuracy: 0.8602\n",
      "Epoch 13/20\n",
      "54000/54000 [==============================] - 1s 10us/sample - loss: 0.2671 - accuracy: 0.9029 - val_loss: 0.4249 - val_accuracy: 0.8685\n",
      "Epoch 14/20\n",
      "54000/54000 [==============================] - 0s 8us/sample - loss: 0.2727 - accuracy: 0.9012 - val_loss: 0.4117 - val_accuracy: 0.8652\n",
      "Epoch 15/20\n",
      "54000/54000 [==============================] - 0s 7us/sample - loss: 0.2649 - accuracy: 0.9030 - val_loss: 0.4234 - val_accuracy: 0.8652\n",
      "Epoch 16/20\n",
      "54000/54000 [==============================] - 0s 7us/sample - loss: 0.2655 - accuracy: 0.9030 - val_loss: 0.4652 - val_accuracy: 0.8548\n",
      "Epoch 17/20\n",
      "54000/54000 [==============================] - 0s 7us/sample - loss: 0.2681 - accuracy: 0.9015 - val_loss: 0.4309 - val_accuracy: 0.8678\n",
      "Epoch 18/20\n",
      "54000/54000 [==============================] - 0s 7us/sample - loss: 0.2611 - accuracy: 0.9035 - val_loss: 0.4348 - val_accuracy: 0.8665\n",
      "Epoch 19/20\n",
      "54000/54000 [==============================] - 0s 7us/sample - loss: 0.2579 - accuracy: 0.9054 - val_loss: 0.4442 - val_accuracy: 0.8628\n",
      "Epoch 20/20\n",
      "54000/54000 [==============================] - 0s 8us/sample - loss: 0.2618 - accuracy: 0.9033 - val_loss: 0.4418 - val_accuracy: 0.8653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8ccaa53c88>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = create_mnist_model()\n",
    "epochs = 20\n",
    "model.fit(X_train, y_train, batch_size=200, epochs=epochs, validation_split=.1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zv_3xNMjzdLI"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Use Hyperparameter Tuning to make the accuracy of your models as high as possible. (error as low as possible)\n",
    "- Use Cross Validation techniques to get more consistent results with your model.\n",
    "- Use GridSearchCV to try different combinations of hyperparameters. \n",
    "- Start looking into other types of Keras layers for CNNs and RNNs maybe try and build a CNN model for fashion-MNIST to see how the results compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 28, 28, 1)\n",
    "x_test = x_test.reshape(10000, 28, 28, 1)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "def cnn_mnist_model():\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "#     model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'],\n",
    "             optimizer='adam')\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/200\n",
      "60000/60000 [==============================] - 30s 500us/sample - loss: 0.5568 - accuracy: 0.7972\n",
      "Epoch 2/200\n",
      "60000/60000 [==============================] - 32s 529us/sample - loss: 0.3934 - accuracy: 0.8566\n",
      "Epoch 3/200\n",
      "60000/60000 [==============================] - 31s 522us/sample - loss: 0.3488 - accuracy: 0.8728\n",
      "Epoch 4/200\n",
      "60000/60000 [==============================] - 32s 538us/sample - loss: 0.3257 - accuracy: 0.8812\n",
      "Epoch 5/200\n",
      "60000/60000 [==============================] - 34s 561us/sample - loss: 0.3076 - accuracy: 0.8875\n",
      "Epoch 6/200\n",
      "60000/60000 [==============================] - 34s 575us/sample - loss: 0.2944 - accuracy: 0.8915\n",
      "Epoch 7/200\n",
      "60000/60000 [==============================] - 35s 588us/sample - loss: 0.2863 - accuracy: 0.8945\n",
      "Epoch 8/200\n",
      "60000/60000 [==============================] - 34s 569us/sample - loss: 0.2735 - accuracy: 0.8982\n",
      "Epoch 9/200\n",
      "60000/60000 [==============================] - 39s 643us/sample - loss: 0.2690 - accuracy: 0.9004\n",
      "Epoch 10/200\n",
      "60000/60000 [==============================] - 43s 722us/sample - loss: 0.2593 - accuracy: 0.9033\n",
      "Epoch 11/200\n",
      "60000/60000 [==============================] - 46s 762us/sample - loss: 0.2567 - accuracy: 0.9046\n",
      "Epoch 12/200\n",
      "60000/60000 [==============================] - 41s 681us/sample - loss: 0.2518 - accuracy: 0.9051\n",
      "Epoch 13/200\n",
      "60000/60000 [==============================] - 44s 741us/sample - loss: 0.2497 - accuracy: 0.9066\n",
      "Epoch 14/200\n",
      "60000/60000 [==============================] - 50s 831us/sample - loss: 0.2426 - accuracy: 0.9098\n",
      "Epoch 15/200\n",
      "60000/60000 [==============================] - 50s 841us/sample - loss: 0.2380 - accuracy: 0.9107\n",
      "Epoch 16/200\n",
      "60000/60000 [==============================] - 50s 830us/sample - loss: 0.2369 - accuracy: 0.9131\n",
      "Epoch 17/200\n",
      "60000/60000 [==============================] - 45s 756us/sample - loss: 0.2336 - accuracy: 0.9122\n",
      "Epoch 18/200\n",
      "60000/60000 [==============================] - 51s 858us/sample - loss: 0.2324 - accuracy: 0.9130\n",
      "Epoch 19/200\n",
      "60000/60000 [==============================] - 49s 822us/sample - loss: 0.2293 - accuracy: 0.9148\n",
      "Epoch 20/200\n",
      "60000/60000 [==============================] - 46s 761us/sample - loss: 0.2264 - accuracy: 0.9160\n",
      "Epoch 21/200\n",
      "60000/60000 [==============================] - 48s 792us/sample - loss: 0.2238 - accuracy: 0.9169\n",
      "Epoch 22/200\n",
      "60000/60000 [==============================] - 44s 735us/sample - loss: 0.2230 - accuracy: 0.9168\n",
      "Epoch 23/200\n",
      "60000/60000 [==============================] - 44s 725us/sample - loss: 0.2174 - accuracy: 0.9185\n",
      "Epoch 24/200\n",
      "60000/60000 [==============================] - 43s 713us/sample - loss: 0.2164 - accuracy: 0.9197\n",
      "Epoch 25/200\n",
      "60000/60000 [==============================] - 45s 758us/sample - loss: 0.2159 - accuracy: 0.9211\n",
      "Epoch 26/200\n",
      "60000/60000 [==============================] - 47s 790us/sample - loss: 0.2141 - accuracy: 0.9197\n",
      "Epoch 27/200\n",
      "60000/60000 [==============================] - 43s 719us/sample - loss: 0.2109 - accuracy: 0.9220\n",
      "Epoch 28/200\n",
      "60000/60000 [==============================] - 44s 731us/sample - loss: 0.2134 - accuracy: 0.9208\n",
      "Epoch 29/200\n",
      "60000/60000 [==============================] - 43s 725us/sample - loss: 0.2103 - accuracy: 0.9215\n",
      "Epoch 30/200\n",
      "60000/60000 [==============================] - 41s 684us/sample - loss: 0.2091 - accuracy: 0.9220\n",
      "Epoch 31/200\n",
      "60000/60000 [==============================] - 46s 775us/sample - loss: 0.2071 - accuracy: 0.9226\n",
      "Epoch 32/200\n",
      "60000/60000 [==============================] - 50s 831us/sample - loss: 0.2033 - accuracy: 0.9244\n",
      "Epoch 33/200\n",
      "60000/60000 [==============================] - 48s 800us/sample - loss: 0.2027 - accuracy: 0.9245\n",
      "Epoch 34/200\n",
      "60000/60000 [==============================] - 51s 846us/sample - loss: 0.2015 - accuracy: 0.9255\n",
      "Epoch 35/200\n",
      "60000/60000 [==============================] - 46s 768us/sample - loss: 0.2014 - accuracy: 0.9251\n",
      "Epoch 36/200\n",
      "60000/60000 [==============================] - 56s 928us/sample - loss: 0.1995 - accuracy: 0.9246\n",
      "Epoch 37/200\n",
      "60000/60000 [==============================] - 53s 883us/sample - loss: 0.1967 - accuracy: 0.9261\n",
      "Epoch 38/200\n",
      "60000/60000 [==============================] - 50s 830us/sample - loss: 0.1957 - accuracy: 0.9268\n",
      "Epoch 39/200\n",
      "60000/60000 [==============================] - 47s 785us/sample - loss: 0.1968 - accuracy: 0.9255\n",
      "Epoch 40/200\n",
      "60000/60000 [==============================] - 49s 811us/sample - loss: 0.1955 - accuracy: 0.9268\n",
      "Epoch 41/200\n",
      "60000/60000 [==============================] - 42s 704us/sample - loss: 0.1937 - accuracy: 0.9273\n",
      "Epoch 42/200\n",
      "31008/60000 [==============>...............] - ETA: 21s - loss: 0.1992 - accuracy: 0.9255"
     ]
    }
   ],
   "source": [
    "model = cnn_mnist_model()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 275us/sample - loss: 0.2251 - accuracy: 0.9310\n",
      "accuracy: 93.09999942779541\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test)\n",
    "print(f'{model.metrics_names[1]}: {scores[1]*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_433_Keras_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
