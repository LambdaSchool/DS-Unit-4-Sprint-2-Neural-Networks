{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlJrdCkzqmcg"
   },
   "source": [
    "In this challenge we jump directly into building neural networks. We won't get into much theory or generality, but by the end of this exercise you'll build a very simple example of one, and in the mean time gain some intuition for how they work. First, we import numpy as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5wjq6pOrq8FN"
   },
   "outputs": [],
   "source": [
    "# LAMBDA SCHOOL\n",
    "#\n",
    "# MACHINE LEARNING\n",
    "#\n",
    "# MIT LICENSE\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0rnp70Yq9wg"
   },
   "source": [
    "Next, look at the Wikipedia article for the XOR function (https://en.wikipedia.org/wiki/XOR_gate). Basically, it's a function that takes in two truth values (aka booleans) x and y and spits out a third truth value f(x,y) according to the following rule: f(x,y) is true when x is true OR y is true, but not both. If we use the common representation wherein \"1\" means \"True\" and \"0\" means \"False\", this means that f(0,0) = f(1,1) = 0 and f(0,1) = f(1,0) = 1. Check that this makes sense!\n",
    "\n",
    "Your first task for today is to implement the XOR function. There are slick ways to do this using modular arithmetic (if you're in to that sort of thing), but implement it however you like. Check that it gives the right values for each of the inputs (0,0), (0,1), (1,0), and (1,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xoBdblQaq8tz"
   },
   "outputs": [],
   "source": [
    "def xorFunction(x, y):\n",
    "    return (x + y) % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for xi in x:\n",
    "    print(xorFunction(*xi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3k71Zs36slNz"
   },
   "source": [
    "Great. Now, define a function sigma(x) that acts the way that the sigmoid function does. If you don't remember exactly how this works, check Wikipedia or ask one of your classmates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MePiuRkbsP3O"
   },
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return (1 / (1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KuwFEeMmtU1d"
   },
   "source": [
    "Most machine learning algorithms have free parameters that we tweak to get the behavior we want, and this is no exception. Introduce two variables a and b and assign them both to the value 10 (for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JYgJFgIvtFt3"
   },
   "outputs": [],
   "source": [
    "a = 10\n",
    "b = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yyazrFrntlGM"
   },
   "source": [
    "Finally, here's our first neural network. Just like linear and logistic regression, it's nothing more than a function that takes in our inputs (x and y) and returns an output according to some prescribed rule. Today our rule consists of the following steps:\n",
    "\n",
    "Step 1: Take x and y and calculate ax + by.\n",
    "\n",
    "Step 2: Plug the result of step 1 into the sigma function we introduced earlier.\n",
    "\n",
    "Step 3: Take the result of step 2 and round it to the nearest whole number.\n",
    "\n",
    "Define a function NN(x,y) that takes in x and y and returns the result of performing these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0pjkc4h4tHOB"
   },
   "outputs": [],
   "source": [
    "def NN(x, y):\n",
    "    weighted_sum = a * x + b * y\n",
    "    print(f'Weighted sum: {weighted_sum}')\n",
    "    activated_sum = sigma(weighted_sum)\n",
    "    print(f'Activated sum: {activated_sum}')\n",
    "    return np.rint(activated_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GtOLsTqxub-n"
   },
   "source": [
    "See what happens when you plug the values (0,0), (0,1), (1,0), and (1,1) into NN. The last (and possible trickiest) part of this assignment is to try and find values of a and b such that NN and XOR give the same outputs on each of those inputs. If you find a solution, share it. If you can't, talk with your classmates and see how they do. Feel free to collaborate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted sum: 0\n",
      "Activated sum: 0.5\n",
      "0.0 \n",
      "\n",
      "Weighted sum: 10\n",
      "Activated sum: 0.9999546021312976\n",
      "1.0 \n",
      "\n",
      "Weighted sum: 10\n",
      "Activated sum: 0.9999546021312976\n",
      "1.0 \n",
      "\n",
      "Weighted sum: 20\n",
      "Activated sum: 0.9999999979388463\n",
      "1.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for xi in x:\n",
    "    print(NN(*xi), '\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "XOR (or not).ipynb",
   "provenance": [
    {
     "file_id": "1VS1iWZ83zwYXN0SMiuNWTfDKlqhlftHP",
     "timestamp": 1527603979636
    },
    {
     "file_id": "1oVcR7pelvCjc9mNnoX65_GLFXS5JZwEl",
     "timestamp": 1527603721468
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
