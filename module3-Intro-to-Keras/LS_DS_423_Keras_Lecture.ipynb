{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HJzTIkYAsLxw"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 2, Module 3*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HJzTIkYAsLxw"
   },
   "source": [
    "# Neural Network Frameworks (Prepare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HJzTIkYAsLxw"
   },
   "source": [
    "## Learning Objectives\n",
    "* <a href=\"#p1\">Part 1</a>: Introduce the Keras Sequential Model API\n",
    "* <a href=\"#p2\">Part 2</a>: Learn How to Select Model Architecture \n",
    "* <a href=\"#p3\">Part 3</a>: Discuss the trade-off between various activation functions\n",
    "\n",
    "## Lets Use Libraries!\n",
    "\n",
    "The objective of the last two days has been to familiarize you with the fundamentals of neural networks: terminology, structure of networks, forward propagation, error/cost functions, backpropagation, epochs, and gradient descent. We have tried to reinforce these topics by requiring to you code some of the simplest neural networks by hand including Perceptrons (single node neural networks) and Multi-Layer Perceptrons also known as Feed-Forward Neural Networks. Continuing to do things by hand would not be the best use of our limited time. You're ready to graduate from doing things by hand and start using some powerful libraries to build cutting-edge predictive models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Sequential API (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Overview\n",
    "\n",
    "> \"Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Use Keras if you need a deep learning library that:\n",
    "\n",
    "> Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
    "Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
    "Runs seamlessly on CPU and GPU.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AIJoRBxHy27n"
   },
   "source": [
    "### Keras Perceptron Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')\n",
    "X = df[['x1', 'x2']].values\n",
    "y = df['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5216
    },
    "colab_type": "code",
    "id": "TQxyONqKvFxB",
    "outputId": "12966e66-2297-4f82-85b3-c275a9c38563"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 0s 72ms/sample - loss: 0.7485 - acc: 0.2500\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 679us/sample - loss: 0.7482 - acc: 0.5000\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 331us/sample - loss: 0.7479 - acc: 0.5000\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 412us/sample - loss: 0.7476 - acc: 0.5000\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 347us/sample - loss: 0.7473 - acc: 0.5000\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 301us/sample - loss: 0.7470 - acc: 0.5000\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 305us/sample - loss: 0.7467 - acc: 0.5000\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 431us/sample - loss: 0.7464 - acc: 0.5000\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 933us/sample - loss: 0.7461 - acc: 0.5000\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 406us/sample - loss: 0.7458 - acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7f4c61b358>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# This is our perceptron from Monday's by-hand: \n",
    "model = Sequential()\n",
    "model.add(Dense(1,input_dim=2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X,y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "Z1wfKUxszPKa",
    "outputId": "0cdacd1d-6e5a-4bbe-fabb-568cd94724be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 20ms/sample - loss: 0.7455 - acc: 0.5000\n",
      "acc: 50.0\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, y)\n",
    "print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "In the `Sequential` api model, you specify a model architecture by 'sequentially specifying layers. This type of specification works well for feed forward neural networks in which the data flows in one direction (forward propagation) and the error flows in the opposite direction (backwards propagation). The Keras `Sequential` API follows a standardarized worklow to estimate a 'net: \n",
    "\n",
    "1. Load Data\n",
    "2. Define Model\n",
    "3. Compile Model\n",
    "4. Fit Model\n",
    "5. Evaluate Model\n",
    "\n",
    "You saw these steps in our Keras Perceptron Sample, but let's walk thru each step in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Md5D67XwqVAf",
    "toc-hr-collapsed": false
   },
   "source": [
    "### Load Data\n",
    "\n",
    "Our life is going to be easier if our data is already cleaned up and numeric, so lets use this dataset from Jason Brownlee that is already numeric and has no column headers so we'll need to slice off the last column of data to act as our y values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "bn09phMBpY1J",
    "outputId": "1c45fb6a-e3cb-4ec1-fb85-de52b3c60bae"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url =\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "\n",
    "dataset = pd.read_csv(url, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3    4     5      6   7  8\n",
       "0  6  148  72  35    0  33.6  0.627  50  1\n",
       "1  1   85  66  29    0  26.6  0.351  31  0\n",
       "2  8  183  64   0    0  23.3  0.672  32  1\n",
       "3  1   89  66  23   94  28.1  0.167  21  0\n",
       "4  0  137  40  35  168  43.1  2.288  33  1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "colab_type": "code",
    "id": "FKuofD3Pogil",
    "outputId": "16c2f4fa-93c8-491d-b339-9f35f7918621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8)\n",
      "[[  6.    148.     72.    ...  33.6     0.627  50.   ]\n",
      " [  1.     85.     66.    ...  26.6     0.351  31.   ]\n",
      " [  8.    183.     64.    ...  23.3     0.672  32.   ]\n",
      " ...\n",
      " [  5.    121.     72.    ...  26.2     0.245  30.   ]\n",
      " [  1.    126.     60.    ...  30.1     0.349  47.   ]\n",
      " [  1.     93.     70.    ...  30.4     0.315  23.   ]]\n"
     ]
    }
   ],
   "source": [
    "X = dataset.values[:,0:8]\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "[1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1.\n",
      " 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0.\n",
      " 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1.\n",
      " 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "y = dataset.values[:,-1]\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0xMqOyTs5xt"
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bp9USczrfu6M"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAzHLg27thoN"
   },
   "source": [
    "I'll instantiate my model as a \"sequential\" model. This just means that I'm going to tell Keras what my model's architecture should be one layer at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DSNsL49Xp6KI"
   },
   "outputs": [],
   "source": [
    "# https://keras.io/getting-started/sequential-model-guide/\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCYX6QzJtvpG"
   },
   "source": [
    "Adding a \"Dense\" layer to our model is how we add \"vanilla\" perceptron-based layers to our neural network. These are also called \"fully-connected\" or \"densely-connected\" layers. They're used as a layer type in lots of other Neural Net Architectures but they're not referred to as perceptrons or multi-layer perceptrons very often in those situations even though that's what they are.\n",
    "\n",
    " > [\"Just your regular densely-connected NN layer.\"](https://keras.io/layers/core/)\n",
    " \n",
    " The first argument is how many neurons we want to have in that layer. To create a perceptron model we will just set it to 1. We will tell it that there will be 8 inputs coming into this layer from our dataset and set it to use the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "GNzOLidxtvFa",
    "outputId": "35b1457d-0189-49f1-aa6d-3ef15b29bd6e"
   },
   "outputs": [],
   "source": [
    "model.add(Dense(4, input_dim=8, activation=\"relu\")) #Relu is valid option. \n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EnI3jwKMtBL2",
    "toc-hr-collapsed": false
   },
   "source": [
    "### Compile Model\n",
    "Using binary_crossentropy as the loss function here is just telling keras that I'm doing binary classification so that it can use the appropriate loss function accordingly. If we were predicting non-binary categories we might assign something like `categorical_crossentropy`. We're also telling keras that we want it to report model accuracy as our main error metric for each epoch. We will also be able to see the overall accuracy once the model has finished training.\n",
    "\n",
    "#### Adam Optimizer\n",
    "Check out this links for more background on the Adam optimizer and Stohastic Gradient Descent\n",
    "* [Adam Optimization Algorithm](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "* [Adam Optimizer - original paper](https://arxiv.org/abs/1412.6980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qp6xwYaqurRO"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5dW8SZ2Ls9SX",
    "toc-hr-collapsed": false
   },
   "source": [
    "### Fit Model\n",
    "\n",
    "Lets train it up! `model.fit()` has a `batch_size` parameter that we can use if we want to do mini-batch epochs, but since this tabular dataset is pretty small we're just going to delete that parameter. Keras' default `batch_size` is `None` so omiting it will tell Keras to do batch epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 614 samples, validate on 154 samples\n",
      "Epoch 1/850\n",
      "614/614 [==============================] - 1s 835us/sample - loss: 0.4746 - acc: 0.7736 - val_loss: 0.4877 - val_acc: 0.7532\n",
      "Epoch 2/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4673 - acc: 0.7801 - val_loss: 0.4869 - val_acc: 0.7662\n",
      "Epoch 3/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4737 - acc: 0.7801 - val_loss: 0.4928 - val_acc: 0.7922\n",
      "Epoch 4/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4813 - acc: 0.7687 - val_loss: 0.4924 - val_acc: 0.7987\n",
      "Epoch 5/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4765 - acc: 0.7638 - val_loss: 0.4850 - val_acc: 0.7727\n",
      "Epoch 6/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4706 - acc: 0.7785 - val_loss: 0.4934 - val_acc: 0.7597\n",
      "Epoch 7/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4730 - acc: 0.7687 - val_loss: 0.4880 - val_acc: 0.7792\n",
      "Epoch 8/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4688 - acc: 0.7736 - val_loss: 0.4857 - val_acc: 0.7792\n",
      "Epoch 9/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4686 - acc: 0.7818 - val_loss: 0.5034 - val_acc: 0.7597\n",
      "Epoch 10/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4814 - acc: 0.7736 - val_loss: 0.4921 - val_acc: 0.7597\n",
      "Epoch 11/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4765 - acc: 0.7769 - val_loss: 0.4886 - val_acc: 0.7597\n",
      "Epoch 12/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4660 - acc: 0.7818 - val_loss: 0.4909 - val_acc: 0.7662\n",
      "Epoch 13/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4677 - acc: 0.7785 - val_loss: 0.4886 - val_acc: 0.7792\n",
      "Epoch 14/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4709 - acc: 0.7785 - val_loss: 0.4888 - val_acc: 0.7922\n",
      "Epoch 15/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4664 - acc: 0.7785 - val_loss: 0.4942 - val_acc: 0.7532\n",
      "Epoch 16/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4685 - acc: 0.7752 - val_loss: 0.4879 - val_acc: 0.7662\n",
      "Epoch 17/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4708 - acc: 0.7638 - val_loss: 0.5039 - val_acc: 0.7857\n",
      "Epoch 18/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4678 - acc: 0.7834 - val_loss: 0.5282 - val_acc: 0.7338\n",
      "Epoch 19/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4744 - acc: 0.7736 - val_loss: 0.4861 - val_acc: 0.7857\n",
      "Epoch 20/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4657 - acc: 0.7883 - val_loss: 0.4883 - val_acc: 0.7857\n",
      "Epoch 21/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4673 - acc: 0.7704 - val_loss: 0.4914 - val_acc: 0.7792\n",
      "Epoch 22/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4699 - acc: 0.7785 - val_loss: 0.4991 - val_acc: 0.7532\n",
      "Epoch 23/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4680 - acc: 0.7752 - val_loss: 0.4989 - val_acc: 0.7987\n",
      "Epoch 24/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4837 - acc: 0.7687 - val_loss: 0.5006 - val_acc: 0.7468\n",
      "Epoch 25/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4733 - acc: 0.7818 - val_loss: 0.4954 - val_acc: 0.7597\n",
      "Epoch 26/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4713 - acc: 0.7850 - val_loss: 0.4957 - val_acc: 0.7532\n",
      "Epoch 27/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4771 - acc: 0.7671 - val_loss: 0.4919 - val_acc: 0.7922\n",
      "Epoch 28/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4713 - acc: 0.7785 - val_loss: 0.4906 - val_acc: 0.7987\n",
      "Epoch 29/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4736 - acc: 0.7801 - val_loss: 0.4913 - val_acc: 0.7597\n",
      "Epoch 30/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4651 - acc: 0.7769 - val_loss: 0.4920 - val_acc: 0.7792\n",
      "Epoch 31/850\n",
      "614/614 [==============================] - 0s 75us/sample - loss: 0.4679 - acc: 0.7785 - val_loss: 0.4868 - val_acc: 0.7922\n",
      "Epoch 32/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4657 - acc: 0.7850 - val_loss: 0.4924 - val_acc: 0.7597\n",
      "Epoch 33/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4670 - acc: 0.7769 - val_loss: 0.4944 - val_acc: 0.7597\n",
      "Epoch 34/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4654 - acc: 0.7785 - val_loss: 0.4911 - val_acc: 0.7662\n",
      "Epoch 35/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4649 - acc: 0.7801 - val_loss: 0.4896 - val_acc: 0.7792\n",
      "Epoch 36/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4791 - acc: 0.7704 - val_loss: 0.4877 - val_acc: 0.7857\n",
      "Epoch 37/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4668 - acc: 0.7818 - val_loss: 0.4898 - val_acc: 0.7792\n",
      "Epoch 38/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4651 - acc: 0.7850 - val_loss: 0.5007 - val_acc: 0.7532\n",
      "Epoch 39/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4671 - acc: 0.7720 - val_loss: 0.4923 - val_acc: 0.7662\n",
      "Epoch 40/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4659 - acc: 0.7801 - val_loss: 0.4913 - val_acc: 0.7857\n",
      "Epoch 41/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4714 - acc: 0.7801 - val_loss: 0.5000 - val_acc: 0.7532\n",
      "Epoch 42/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4664 - acc: 0.7850 - val_loss: 0.4925 - val_acc: 0.7727\n",
      "Epoch 43/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4648 - acc: 0.7850 - val_loss: 0.5011 - val_acc: 0.7532\n",
      "Epoch 44/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4694 - acc: 0.7834 - val_loss: 0.4967 - val_acc: 0.7922\n",
      "Epoch 45/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4663 - acc: 0.7752 - val_loss: 0.4989 - val_acc: 0.7532\n",
      "Epoch 46/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4647 - acc: 0.7801 - val_loss: 0.5068 - val_acc: 0.7532\n",
      "Epoch 47/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4763 - acc: 0.7834 - val_loss: 0.4958 - val_acc: 0.7597\n",
      "Epoch 48/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4662 - acc: 0.7818 - val_loss: 0.4958 - val_acc: 0.7597\n",
      "Epoch 49/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4670 - acc: 0.7736 - val_loss: 0.4926 - val_acc: 0.7857\n",
      "Epoch 50/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4644 - acc: 0.7883 - val_loss: 0.4941 - val_acc: 0.7662\n",
      "Epoch 51/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4632 - acc: 0.7785 - val_loss: 0.5017 - val_acc: 0.7532\n",
      "Epoch 52/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4645 - acc: 0.7818 - val_loss: 0.5043 - val_acc: 0.7468\n",
      "Epoch 53/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4656 - acc: 0.7785 - val_loss: 0.4922 - val_acc: 0.7792\n",
      "Epoch 54/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4768 - acc: 0.7769 - val_loss: 0.4992 - val_acc: 0.7532\n",
      "Epoch 55/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4643 - acc: 0.7834 - val_loss: 0.5017 - val_acc: 0.7532\n",
      "Epoch 56/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4659 - acc: 0.7785 - val_loss: 0.4987 - val_acc: 0.7597\n",
      "Epoch 57/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4658 - acc: 0.7785 - val_loss: 0.4981 - val_acc: 0.7597\n",
      "Epoch 58/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4676 - acc: 0.7801 - val_loss: 0.4947 - val_acc: 0.7987\n",
      "Epoch 59/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4691 - acc: 0.7818 - val_loss: 0.5117 - val_acc: 0.7403\n",
      "Epoch 60/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4766 - acc: 0.7671 - val_loss: 0.4934 - val_acc: 0.7857\n",
      "Epoch 61/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4658 - acc: 0.7769 - val_loss: 0.4910 - val_acc: 0.7727\n",
      "Epoch 62/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4659 - acc: 0.7769 - val_loss: 0.4930 - val_acc: 0.7922\n",
      "Epoch 63/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4672 - acc: 0.7801 - val_loss: 0.4959 - val_acc: 0.7987\n",
      "Epoch 64/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4728 - acc: 0.7801 - val_loss: 0.5018 - val_acc: 0.7532\n",
      "Epoch 65/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4681 - acc: 0.7736 - val_loss: 0.5034 - val_acc: 0.7532\n",
      "Epoch 66/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4601 - acc: 0.7818 - val_loss: 0.4965 - val_acc: 0.7922\n",
      "Epoch 67/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4685 - acc: 0.7899 - val_loss: 0.5002 - val_acc: 0.7597\n",
      "Epoch 68/850\n",
      "614/614 [==============================] - 0s 75us/sample - loss: 0.4632 - acc: 0.7801 - val_loss: 0.4950 - val_acc: 0.7727\n",
      "Epoch 69/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4646 - acc: 0.7769 - val_loss: 0.5010 - val_acc: 0.7532\n",
      "Epoch 70/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4653 - acc: 0.7818 - val_loss: 0.4935 - val_acc: 0.7857\n",
      "Epoch 71/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4647 - acc: 0.7850 - val_loss: 0.5096 - val_acc: 0.7532\n",
      "Epoch 72/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4664 - acc: 0.7752 - val_loss: 0.5171 - val_acc: 0.7727\n",
      "Epoch 73/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4758 - acc: 0.7704 - val_loss: 0.4949 - val_acc: 0.7662\n",
      "Epoch 74/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4663 - acc: 0.7785 - val_loss: 0.4936 - val_acc: 0.7857\n",
      "Epoch 75/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4685 - acc: 0.7915 - val_loss: 0.5030 - val_acc: 0.7532\n",
      "Epoch 76/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4645 - acc: 0.7883 - val_loss: 0.4984 - val_acc: 0.7597\n",
      "Epoch 77/850\n",
      "614/614 [==============================] - 0s 75us/sample - loss: 0.4647 - acc: 0.7785 - val_loss: 0.4969 - val_acc: 0.7662\n",
      "Epoch 78/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4649 - acc: 0.7932 - val_loss: 0.5069 - val_acc: 0.7532\n",
      "Epoch 79/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4663 - acc: 0.7785 - val_loss: 0.5000 - val_acc: 0.7532\n",
      "Epoch 80/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4633 - acc: 0.7736 - val_loss: 0.4963 - val_acc: 0.7922\n",
      "Epoch 81/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4625 - acc: 0.7752 - val_loss: 0.5201 - val_acc: 0.7403\n",
      "Epoch 82/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4700 - acc: 0.7801 - val_loss: 0.4929 - val_acc: 0.7857\n",
      "Epoch 83/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4666 - acc: 0.7752 - val_loss: 0.4960 - val_acc: 0.7727\n",
      "Epoch 84/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4684 - acc: 0.7785 - val_loss: 0.4948 - val_acc: 0.7987\n",
      "Epoch 85/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4654 - acc: 0.7818 - val_loss: 0.4960 - val_acc: 0.7727\n",
      "Epoch 86/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4662 - acc: 0.7785 - val_loss: 0.4983 - val_acc: 0.7662\n",
      "Epoch 87/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4664 - acc: 0.7769 - val_loss: 0.4975 - val_acc: 0.7662\n",
      "Epoch 88/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4630 - acc: 0.7785 - val_loss: 0.4946 - val_acc: 0.7792\n",
      "Epoch 89/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4699 - acc: 0.7752 - val_loss: 0.4998 - val_acc: 0.7597\n",
      "Epoch 90/850\n",
      "614/614 [==============================] - 0s 76us/sample - loss: 0.4642 - acc: 0.7736 - val_loss: 0.4962 - val_acc: 0.7597\n",
      "Epoch 91/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4627 - acc: 0.7883 - val_loss: 0.4967 - val_acc: 0.7662\n",
      "Epoch 92/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4614 - acc: 0.7687 - val_loss: 0.4975 - val_acc: 0.7857\n",
      "Epoch 93/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4646 - acc: 0.7801 - val_loss: 0.4942 - val_acc: 0.7792\n",
      "Epoch 94/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4629 - acc: 0.7834 - val_loss: 0.4954 - val_acc: 0.7727\n",
      "Epoch 95/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4622 - acc: 0.7752 - val_loss: 0.4963 - val_acc: 0.7727\n",
      "Epoch 96/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4620 - acc: 0.7769 - val_loss: 0.4945 - val_acc: 0.7792\n",
      "Epoch 97/850\n",
      "614/614 [==============================] - 0s 75us/sample - loss: 0.4626 - acc: 0.7964 - val_loss: 0.5042 - val_acc: 0.7532\n",
      "Epoch 98/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4724 - acc: 0.7736 - val_loss: 0.5012 - val_acc: 0.7662\n",
      "Epoch 99/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4635 - acc: 0.7801 - val_loss: 0.4934 - val_acc: 0.7922\n",
      "Epoch 100/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4653 - acc: 0.7818 - val_loss: 0.5017 - val_acc: 0.7662\n",
      "Epoch 101/850\n",
      "614/614 [==============================] - 0s 76us/sample - loss: 0.4702 - acc: 0.7818 - val_loss: 0.4938 - val_acc: 0.7857\n",
      "Epoch 102/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4716 - acc: 0.7704 - val_loss: 0.4967 - val_acc: 0.7922\n",
      "Epoch 103/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4815 - acc: 0.7769 - val_loss: 0.4972 - val_acc: 0.7857\n",
      "Epoch 104/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4680 - acc: 0.7736 - val_loss: 0.4940 - val_acc: 0.7922\n",
      "Epoch 105/850\n",
      "614/614 [==============================] - 0s 75us/sample - loss: 0.4649 - acc: 0.7834 - val_loss: 0.5009 - val_acc: 0.7662\n",
      "Epoch 106/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4663 - acc: 0.7736 - val_loss: 0.4962 - val_acc: 0.7727\n",
      "Epoch 107/850\n",
      "614/614 [==============================] - 0s 77us/sample - loss: 0.4639 - acc: 0.7785 - val_loss: 0.4965 - val_acc: 0.7662\n",
      "Epoch 108/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4636 - acc: 0.7752 - val_loss: 0.4984 - val_acc: 0.7792\n",
      "Epoch 109/850\n",
      "614/614 [==============================] - 0s 76us/sample - loss: 0.4712 - acc: 0.7818 - val_loss: 0.4949 - val_acc: 0.7857\n",
      "Epoch 110/850\n",
      "614/614 [==============================] - 0s 75us/sample - loss: 0.4652 - acc: 0.7801 - val_loss: 0.5029 - val_acc: 0.7468\n",
      "Epoch 111/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4695 - acc: 0.7850 - val_loss: 0.5010 - val_acc: 0.7662\n",
      "Epoch 112/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4622 - acc: 0.7850 - val_loss: 0.4922 - val_acc: 0.7792\n",
      "Epoch 113/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4606 - acc: 0.7915 - val_loss: 0.5024 - val_acc: 0.7532\n",
      "Epoch 114/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4689 - acc: 0.7769 - val_loss: 0.5013 - val_acc: 0.7532\n",
      "Epoch 115/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4693 - acc: 0.7834 - val_loss: 0.5214 - val_acc: 0.7338\n",
      "Epoch 116/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4787 - acc: 0.7720 - val_loss: 0.5067 - val_acc: 0.7532\n",
      "Epoch 117/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4843 - acc: 0.7785 - val_loss: 0.4958 - val_acc: 0.7922\n",
      "Epoch 118/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4638 - acc: 0.7834 - val_loss: 0.4944 - val_acc: 0.7662\n",
      "Epoch 119/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4626 - acc: 0.7850 - val_loss: 0.4952 - val_acc: 0.7727\n",
      "Epoch 120/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4625 - acc: 0.7752 - val_loss: 0.4961 - val_acc: 0.7662\n",
      "Epoch 121/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4652 - acc: 0.7736 - val_loss: 0.4960 - val_acc: 0.7857\n",
      "Epoch 122/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4680 - acc: 0.7818 - val_loss: 0.4953 - val_acc: 0.7662\n",
      "Epoch 123/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4615 - acc: 0.7866 - val_loss: 0.4955 - val_acc: 0.7662\n",
      "Epoch 124/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4638 - acc: 0.7818 - val_loss: 0.4919 - val_acc: 0.7792\n",
      "Epoch 125/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4709 - acc: 0.7769 - val_loss: 0.5064 - val_acc: 0.7597\n",
      "Epoch 126/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4732 - acc: 0.7752 - val_loss: 0.5059 - val_acc: 0.7532\n",
      "Epoch 127/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4680 - acc: 0.7834 - val_loss: 0.5057 - val_acc: 0.7532\n",
      "Epoch 128/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4638 - acc: 0.7785 - val_loss: 0.4902 - val_acc: 0.7922\n",
      "Epoch 129/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4668 - acc: 0.7720 - val_loss: 0.4913 - val_acc: 0.7857\n",
      "Epoch 130/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4624 - acc: 0.7801 - val_loss: 0.4935 - val_acc: 0.7727\n",
      "Epoch 131/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4626 - acc: 0.7850 - val_loss: 0.4932 - val_acc: 0.7857\n",
      "Epoch 132/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4606 - acc: 0.7785 - val_loss: 0.4941 - val_acc: 0.7857\n",
      "Epoch 133/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4631 - acc: 0.7769 - val_loss: 0.5004 - val_acc: 0.7662\n",
      "Epoch 134/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4690 - acc: 0.7915 - val_loss: 0.4950 - val_acc: 0.7922\n",
      "Epoch 135/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4633 - acc: 0.7834 - val_loss: 0.5204 - val_acc: 0.7403\n",
      "Epoch 136/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4797 - acc: 0.7704 - val_loss: 0.4929 - val_acc: 0.7727\n",
      "Epoch 137/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4634 - acc: 0.7785 - val_loss: 0.4924 - val_acc: 0.7792\n",
      "Epoch 138/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4659 - acc: 0.7769 - val_loss: 0.4920 - val_acc: 0.7792\n",
      "Epoch 139/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4608 - acc: 0.7801 - val_loss: 0.5087 - val_acc: 0.7532\n",
      "Epoch 140/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4643 - acc: 0.7834 - val_loss: 0.4935 - val_acc: 0.7857\n",
      "Epoch 141/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4717 - acc: 0.7850 - val_loss: 0.4910 - val_acc: 0.7857\n",
      "Epoch 142/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4629 - acc: 0.7866 - val_loss: 0.4927 - val_acc: 0.7857\n",
      "Epoch 143/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4661 - acc: 0.7866 - val_loss: 0.5007 - val_acc: 0.7532\n",
      "Epoch 144/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4675 - acc: 0.7866 - val_loss: 0.4932 - val_acc: 0.7857\n",
      "Epoch 145/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4611 - acc: 0.7769 - val_loss: 0.4921 - val_acc: 0.7922\n",
      "Epoch 146/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4613 - acc: 0.7883 - val_loss: 0.4942 - val_acc: 0.7727\n",
      "Epoch 147/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4601 - acc: 0.7850 - val_loss: 0.4904 - val_acc: 0.7792\n",
      "Epoch 148/850\n",
      "614/614 [==============================] - 0s 78us/sample - loss: 0.4620 - acc: 0.7801 - val_loss: 0.4965 - val_acc: 0.7792\n",
      "Epoch 149/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4619 - acc: 0.7785 - val_loss: 0.4935 - val_acc: 0.7727\n",
      "Epoch 150/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4630 - acc: 0.7785 - val_loss: 0.4937 - val_acc: 0.7727\n",
      "Epoch 151/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4636 - acc: 0.7769 - val_loss: 0.4918 - val_acc: 0.7857\n",
      "Epoch 152/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4596 - acc: 0.7834 - val_loss: 0.4929 - val_acc: 0.7857\n",
      "Epoch 153/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4612 - acc: 0.7964 - val_loss: 0.5085 - val_acc: 0.7532\n",
      "Epoch 154/850\n",
      "614/614 [==============================] - 0s 76us/sample - loss: 0.4654 - acc: 0.7834 - val_loss: 0.4929 - val_acc: 0.7922\n",
      "Epoch 155/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4617 - acc: 0.7834 - val_loss: 0.4957 - val_acc: 0.7597\n",
      "Epoch 156/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4611 - acc: 0.7883 - val_loss: 0.4981 - val_acc: 0.7727\n",
      "Epoch 157/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4674 - acc: 0.7818 - val_loss: 0.4917 - val_acc: 0.7857\n",
      "Epoch 158/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4604 - acc: 0.7883 - val_loss: 0.4951 - val_acc: 0.7727\n",
      "Epoch 159/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4661 - acc: 0.7850 - val_loss: 0.4945 - val_acc: 0.7857\n",
      "Epoch 160/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4600 - acc: 0.7899 - val_loss: 0.4942 - val_acc: 0.7792\n",
      "Epoch 161/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4607 - acc: 0.7866 - val_loss: 0.4958 - val_acc: 0.7857\n",
      "Epoch 162/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4616 - acc: 0.7801 - val_loss: 0.4937 - val_acc: 0.7857\n",
      "Epoch 163/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4591 - acc: 0.7948 - val_loss: 0.5036 - val_acc: 0.7597\n",
      "Epoch 164/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4671 - acc: 0.7769 - val_loss: 0.4981 - val_acc: 0.7727\n",
      "Epoch 165/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4643 - acc: 0.7801 - val_loss: 0.4934 - val_acc: 0.7792\n",
      "Epoch 166/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4601 - acc: 0.7818 - val_loss: 0.4915 - val_acc: 0.7857\n",
      "Epoch 167/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4620 - acc: 0.7818 - val_loss: 0.4903 - val_acc: 0.7792\n",
      "Epoch 168/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4677 - acc: 0.7866 - val_loss: 0.5213 - val_acc: 0.7338\n",
      "Epoch 169/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4607 - acc: 0.7866 - val_loss: 0.4928 - val_acc: 0.7857\n",
      "Epoch 170/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4652 - acc: 0.7752 - val_loss: 0.4951 - val_acc: 0.7792\n",
      "Epoch 171/850\n",
      "614/614 [==============================] - 0s 75us/sample - loss: 0.4621 - acc: 0.7801 - val_loss: 0.4940 - val_acc: 0.7792\n",
      "Epoch 172/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4595 - acc: 0.7785 - val_loss: 0.4906 - val_acc: 0.7922\n",
      "Epoch 173/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4601 - acc: 0.7818 - val_loss: 0.4953 - val_acc: 0.7792\n",
      "Epoch 174/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4657 - acc: 0.7801 - val_loss: 0.5060 - val_acc: 0.7532\n",
      "Epoch 175/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4704 - acc: 0.7687 - val_loss: 0.4975 - val_acc: 0.7727\n",
      "Epoch 176/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4667 - acc: 0.7883 - val_loss: 0.5120 - val_acc: 0.7468\n",
      "Epoch 177/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4655 - acc: 0.7899 - val_loss: 0.4922 - val_acc: 0.7922\n",
      "Epoch 178/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4622 - acc: 0.7818 - val_loss: 0.4927 - val_acc: 0.7922\n",
      "Epoch 179/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4621 - acc: 0.7850 - val_loss: 0.4903 - val_acc: 0.7792\n",
      "Epoch 180/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4633 - acc: 0.7850 - val_loss: 0.4948 - val_acc: 0.7987\n",
      "Epoch 181/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4672 - acc: 0.7785 - val_loss: 0.4989 - val_acc: 0.7662\n",
      "Epoch 182/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4717 - acc: 0.7704 - val_loss: 0.4936 - val_acc: 0.7922\n",
      "Epoch 183/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4617 - acc: 0.7752 - val_loss: 0.4909 - val_acc: 0.7857\n",
      "Epoch 184/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4616 - acc: 0.7785 - val_loss: 0.4902 - val_acc: 0.7857\n",
      "Epoch 185/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4622 - acc: 0.7818 - val_loss: 0.4921 - val_acc: 0.7987\n",
      "Epoch 186/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4627 - acc: 0.7883 - val_loss: 0.4957 - val_acc: 0.7727\n",
      "Epoch 187/850\n",
      "614/614 [==============================] - 0s 88us/sample - loss: 0.4612 - acc: 0.7834 - val_loss: 0.4894 - val_acc: 0.7792\n",
      "Epoch 188/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4712 - acc: 0.7850 - val_loss: 0.4940 - val_acc: 0.7792\n",
      "Epoch 189/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4624 - acc: 0.7866 - val_loss: 0.4898 - val_acc: 0.7922\n",
      "Epoch 190/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4616 - acc: 0.7785 - val_loss: 0.4914 - val_acc: 0.7727\n",
      "Epoch 191/850\n",
      "614/614 [==============================] - 0s 76us/sample - loss: 0.4615 - acc: 0.7818 - val_loss: 0.4936 - val_acc: 0.7857\n",
      "Epoch 192/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4620 - acc: 0.7850 - val_loss: 0.4920 - val_acc: 0.7857\n",
      "Epoch 193/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4769 - acc: 0.7671 - val_loss: 0.4929 - val_acc: 0.7792\n",
      "Epoch 194/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4658 - acc: 0.7801 - val_loss: 0.4942 - val_acc: 0.7792\n",
      "Epoch 195/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4656 - acc: 0.7866 - val_loss: 0.5186 - val_acc: 0.7403\n",
      "Epoch 196/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4715 - acc: 0.7785 - val_loss: 0.4966 - val_acc: 0.7792\n",
      "Epoch 197/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4651 - acc: 0.7704 - val_loss: 0.4897 - val_acc: 0.7727\n",
      "Epoch 198/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4589 - acc: 0.7915 - val_loss: 0.5183 - val_acc: 0.7403\n",
      "Epoch 199/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4637 - acc: 0.7801 - val_loss: 0.4918 - val_acc: 0.7922\n",
      "Epoch 200/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4626 - acc: 0.7752 - val_loss: 0.4919 - val_acc: 0.7792\n",
      "Epoch 201/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4631 - acc: 0.7866 - val_loss: 0.4983 - val_acc: 0.7792\n",
      "Epoch 202/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4623 - acc: 0.7850 - val_loss: 0.5099 - val_acc: 0.7532\n",
      "Epoch 203/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4674 - acc: 0.7752 - val_loss: 0.4982 - val_acc: 0.7532\n",
      "Epoch 204/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4604 - acc: 0.7866 - val_loss: 0.4933 - val_acc: 0.7792\n",
      "Epoch 205/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4654 - acc: 0.7736 - val_loss: 0.4917 - val_acc: 0.7727\n",
      "Epoch 206/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4712 - acc: 0.7899 - val_loss: 0.5140 - val_acc: 0.7468\n",
      "Epoch 207/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4697 - acc: 0.7785 - val_loss: 0.5046 - val_acc: 0.7532\n",
      "Epoch 208/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4697 - acc: 0.7769 - val_loss: 0.4928 - val_acc: 0.7792\n",
      "Epoch 209/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4604 - acc: 0.7785 - val_loss: 0.4990 - val_acc: 0.7597\n",
      "Epoch 210/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4684 - acc: 0.7687 - val_loss: 0.4927 - val_acc: 0.7857\n",
      "Epoch 211/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4616 - acc: 0.7818 - val_loss: 0.4926 - val_acc: 0.7727\n",
      "Epoch 212/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4620 - acc: 0.7801 - val_loss: 0.4923 - val_acc: 0.7857\n",
      "Epoch 213/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4613 - acc: 0.7834 - val_loss: 0.4976 - val_acc: 0.7662\n",
      "Epoch 214/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4660 - acc: 0.7801 - val_loss: 0.4953 - val_acc: 0.7857\n",
      "Epoch 215/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4650 - acc: 0.7801 - val_loss: 0.4899 - val_acc: 0.7857\n",
      "Epoch 216/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4785 - acc: 0.7850 - val_loss: 0.5044 - val_acc: 0.7597\n",
      "Epoch 217/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4668 - acc: 0.7801 - val_loss: 0.5030 - val_acc: 0.7597\n",
      "Epoch 218/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4638 - acc: 0.7834 - val_loss: 0.4911 - val_acc: 0.7792\n",
      "Epoch 219/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4605 - acc: 0.7899 - val_loss: 0.4902 - val_acc: 0.7727\n",
      "Epoch 220/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4660 - acc: 0.7899 - val_loss: 0.5210 - val_acc: 0.7468\n",
      "Epoch 221/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4681 - acc: 0.7932 - val_loss: 0.4964 - val_acc: 0.7727\n",
      "Epoch 222/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4630 - acc: 0.7801 - val_loss: 0.4913 - val_acc: 0.7792\n",
      "Epoch 223/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4610 - acc: 0.7785 - val_loss: 0.4917 - val_acc: 0.7727\n",
      "Epoch 224/850\n",
      "614/614 [==============================] - 0s 76us/sample - loss: 0.4634 - acc: 0.7736 - val_loss: 0.4913 - val_acc: 0.7792\n",
      "Epoch 225/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4624 - acc: 0.7866 - val_loss: 0.4907 - val_acc: 0.7792\n",
      "Epoch 226/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4651 - acc: 0.7915 - val_loss: 0.5033 - val_acc: 0.7597\n",
      "Epoch 227/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4675 - acc: 0.7801 - val_loss: 0.4997 - val_acc: 0.7532\n",
      "Epoch 228/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4633 - acc: 0.7834 - val_loss: 0.5006 - val_acc: 0.7468\n",
      "Epoch 229/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4585 - acc: 0.7834 - val_loss: 0.4893 - val_acc: 0.7792\n",
      "Epoch 230/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4619 - acc: 0.7801 - val_loss: 0.4997 - val_acc: 0.7597\n",
      "Epoch 231/850\n",
      "614/614 [==============================] - 0s 77us/sample - loss: 0.4615 - acc: 0.7899 - val_loss: 0.4961 - val_acc: 0.7597\n",
      "Epoch 232/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4611 - acc: 0.7801 - val_loss: 0.4911 - val_acc: 0.7792\n",
      "Epoch 233/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4596 - acc: 0.7785 - val_loss: 0.4993 - val_acc: 0.7662\n",
      "Epoch 234/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4607 - acc: 0.7850 - val_loss: 0.4936 - val_acc: 0.7662\n",
      "Epoch 235/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4565 - acc: 0.7834 - val_loss: 0.4945 - val_acc: 0.7727\n",
      "Epoch 236/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4575 - acc: 0.7818 - val_loss: 0.4908 - val_acc: 0.7792\n",
      "Epoch 237/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4579 - acc: 0.7866 - val_loss: 0.4951 - val_acc: 0.7727\n",
      "Epoch 238/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4615 - acc: 0.7834 - val_loss: 0.4915 - val_acc: 0.7792\n",
      "Epoch 239/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4580 - acc: 0.7850 - val_loss: 0.4922 - val_acc: 0.7792\n",
      "Epoch 240/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4602 - acc: 0.7883 - val_loss: 0.4907 - val_acc: 0.7792\n",
      "Epoch 241/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4580 - acc: 0.7801 - val_loss: 0.4905 - val_acc: 0.7857\n",
      "Epoch 242/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4599 - acc: 0.7736 - val_loss: 0.4948 - val_acc: 0.7727\n",
      "Epoch 243/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4617 - acc: 0.7769 - val_loss: 0.4906 - val_acc: 0.7857\n",
      "Epoch 244/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4651 - acc: 0.7866 - val_loss: 0.4970 - val_acc: 0.7662\n",
      "Epoch 245/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4579 - acc: 0.7850 - val_loss: 0.4938 - val_acc: 0.7922\n",
      "Epoch 246/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4562 - acc: 0.7818 - val_loss: 0.4917 - val_acc: 0.7792\n",
      "Epoch 247/850\n",
      "614/614 [==============================] - 0s 75us/sample - loss: 0.4600 - acc: 0.7899 - val_loss: 0.5005 - val_acc: 0.7662\n",
      "Epoch 248/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4633 - acc: 0.7818 - val_loss: 0.4929 - val_acc: 0.7727\n",
      "Epoch 249/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4598 - acc: 0.7818 - val_loss: 0.4925 - val_acc: 0.7662\n",
      "Epoch 250/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4575 - acc: 0.7899 - val_loss: 0.4972 - val_acc: 0.7727\n",
      "Epoch 251/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4568 - acc: 0.7834 - val_loss: 0.4932 - val_acc: 0.7727\n",
      "Epoch 252/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4606 - acc: 0.7850 - val_loss: 0.4926 - val_acc: 0.7857\n",
      "Epoch 253/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4579 - acc: 0.7818 - val_loss: 0.4953 - val_acc: 0.7662\n",
      "Epoch 254/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4623 - acc: 0.7866 - val_loss: 0.4967 - val_acc: 0.7727\n",
      "Epoch 255/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4618 - acc: 0.7866 - val_loss: 0.4915 - val_acc: 0.7727\n",
      "Epoch 256/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4597 - acc: 0.7769 - val_loss: 0.4936 - val_acc: 0.7922\n",
      "Epoch 257/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4591 - acc: 0.7850 - val_loss: 0.5034 - val_acc: 0.7922\n",
      "Epoch 258/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4694 - acc: 0.7866 - val_loss: 0.4975 - val_acc: 0.7662\n",
      "Epoch 259/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4608 - acc: 0.7834 - val_loss: 0.4969 - val_acc: 0.7662\n",
      "Epoch 260/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4625 - acc: 0.7834 - val_loss: 0.4906 - val_acc: 0.7727\n",
      "Epoch 261/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4574 - acc: 0.7818 - val_loss: 0.4884 - val_acc: 0.7662\n",
      "Epoch 262/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4597 - acc: 0.7883 - val_loss: 0.4916 - val_acc: 0.7662\n",
      "Epoch 263/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4569 - acc: 0.7834 - val_loss: 0.4921 - val_acc: 0.7792\n",
      "Epoch 264/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4571 - acc: 0.7801 - val_loss: 0.4983 - val_acc: 0.7662\n",
      "Epoch 265/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4592 - acc: 0.7883 - val_loss: 0.4944 - val_acc: 0.7727\n",
      "Epoch 266/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4595 - acc: 0.7948 - val_loss: 0.4983 - val_acc: 0.7662\n",
      "Epoch 267/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4580 - acc: 0.7785 - val_loss: 0.4921 - val_acc: 0.7727\n",
      "Epoch 268/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4692 - acc: 0.7720 - val_loss: 0.4903 - val_acc: 0.7792\n",
      "Epoch 269/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4585 - acc: 0.7883 - val_loss: 0.4920 - val_acc: 0.7727\n",
      "Epoch 270/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4565 - acc: 0.7769 - val_loss: 0.4893 - val_acc: 0.7597\n",
      "Epoch 271/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4582 - acc: 0.7818 - val_loss: 0.4879 - val_acc: 0.7662\n",
      "Epoch 272/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4575 - acc: 0.7834 - val_loss: 0.4914 - val_acc: 0.7662\n",
      "Epoch 273/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4640 - acc: 0.7850 - val_loss: 0.4896 - val_acc: 0.7792\n",
      "Epoch 274/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4616 - acc: 0.7785 - val_loss: 0.4929 - val_acc: 0.7922\n",
      "Epoch 275/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4626 - acc: 0.7785 - val_loss: 0.4983 - val_acc: 0.7662\n",
      "Epoch 276/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4571 - acc: 0.7850 - val_loss: 0.4936 - val_acc: 0.7662\n",
      "Epoch 277/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4578 - acc: 0.7834 - val_loss: 0.5013 - val_acc: 0.7532\n",
      "Epoch 278/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4597 - acc: 0.7834 - val_loss: 0.4924 - val_acc: 0.7857\n",
      "Epoch 279/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4659 - acc: 0.7769 - val_loss: 0.4907 - val_acc: 0.7792\n",
      "Epoch 280/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4607 - acc: 0.7866 - val_loss: 0.4956 - val_acc: 0.7662\n",
      "Epoch 281/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4674 - acc: 0.7850 - val_loss: 0.4964 - val_acc: 0.7727\n",
      "Epoch 282/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4565 - acc: 0.7850 - val_loss: 0.4890 - val_acc: 0.7662\n",
      "Epoch 283/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4586 - acc: 0.7785 - val_loss: 0.4874 - val_acc: 0.7597\n",
      "Epoch 284/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4588 - acc: 0.7883 - val_loss: 0.5011 - val_acc: 0.7662\n",
      "Epoch 285/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4566 - acc: 0.7932 - val_loss: 0.4911 - val_acc: 0.7857\n",
      "Epoch 286/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4581 - acc: 0.7834 - val_loss: 0.4896 - val_acc: 0.7727\n",
      "Epoch 287/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4578 - acc: 0.7932 - val_loss: 0.4964 - val_acc: 0.7662\n",
      "Epoch 288/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4564 - acc: 0.7883 - val_loss: 0.4929 - val_acc: 0.7727\n",
      "Epoch 289/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4550 - acc: 0.7883 - val_loss: 0.4970 - val_acc: 0.7597\n",
      "Epoch 290/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4627 - acc: 0.7850 - val_loss: 0.5012 - val_acc: 0.7857\n",
      "Epoch 291/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4635 - acc: 0.7834 - val_loss: 0.5085 - val_acc: 0.7468\n",
      "Epoch 292/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4620 - acc: 0.7834 - val_loss: 0.4954 - val_acc: 0.7662\n",
      "Epoch 293/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4613 - acc: 0.7915 - val_loss: 0.4909 - val_acc: 0.7727\n",
      "Epoch 294/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4620 - acc: 0.7866 - val_loss: 0.4945 - val_acc: 0.7662\n",
      "Epoch 295/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4539 - acc: 0.7866 - val_loss: 0.4942 - val_acc: 0.7792\n",
      "Epoch 296/850\n",
      "614/614 [==============================] - 0s 77us/sample - loss: 0.4638 - acc: 0.7866 - val_loss: 0.5119 - val_acc: 0.7403\n",
      "Epoch 297/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4593 - acc: 0.7801 - val_loss: 0.4979 - val_acc: 0.7662\n",
      "Epoch 298/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4669 - acc: 0.7818 - val_loss: 0.4892 - val_acc: 0.7857\n",
      "Epoch 299/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4591 - acc: 0.7850 - val_loss: 0.4886 - val_acc: 0.7792\n",
      "Epoch 300/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4583 - acc: 0.7932 - val_loss: 0.5043 - val_acc: 0.7532\n",
      "Epoch 301/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4564 - acc: 0.7818 - val_loss: 0.4912 - val_acc: 0.7727\n",
      "Epoch 302/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4619 - acc: 0.7915 - val_loss: 0.4906 - val_acc: 0.7727\n",
      "Epoch 303/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4547 - acc: 0.7818 - val_loss: 0.4886 - val_acc: 0.7727\n",
      "Epoch 304/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4583 - acc: 0.7883 - val_loss: 0.4956 - val_acc: 0.7857\n",
      "Epoch 305/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4570 - acc: 0.7948 - val_loss: 0.4932 - val_acc: 0.7727\n",
      "Epoch 306/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4541 - acc: 0.7834 - val_loss: 0.4925 - val_acc: 0.7857\n",
      "Epoch 307/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4572 - acc: 0.7834 - val_loss: 0.4957 - val_acc: 0.7662\n",
      "Epoch 308/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4624 - acc: 0.7834 - val_loss: 0.4931 - val_acc: 0.7857\n",
      "Epoch 309/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4611 - acc: 0.7850 - val_loss: 0.4911 - val_acc: 0.7792\n",
      "Epoch 310/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4554 - acc: 0.7834 - val_loss: 0.4937 - val_acc: 0.7662\n",
      "Epoch 311/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4576 - acc: 0.7866 - val_loss: 0.4934 - val_acc: 0.7662\n",
      "Epoch 312/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4617 - acc: 0.7948 - val_loss: 0.4874 - val_acc: 0.7792\n",
      "Epoch 313/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4547 - acc: 0.7850 - val_loss: 0.4915 - val_acc: 0.7662\n",
      "Epoch 314/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4637 - acc: 0.7834 - val_loss: 0.5065 - val_acc: 0.7468\n",
      "Epoch 315/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4653 - acc: 0.7834 - val_loss: 0.4970 - val_acc: 0.7662\n",
      "Epoch 316/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4569 - acc: 0.7850 - val_loss: 0.4897 - val_acc: 0.7792\n",
      "Epoch 317/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4569 - acc: 0.7866 - val_loss: 0.4933 - val_acc: 0.7857\n",
      "Epoch 318/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4620 - acc: 0.7818 - val_loss: 0.4920 - val_acc: 0.7792\n",
      "Epoch 319/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4825 - acc: 0.7671 - val_loss: 0.5101 - val_acc: 0.7403\n",
      "Epoch 320/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4628 - acc: 0.7883 - val_loss: 0.5004 - val_acc: 0.7597\n",
      "Epoch 321/850\n",
      "614/614 [==============================] - 0s 78us/sample - loss: 0.4588 - acc: 0.7932 - val_loss: 0.4939 - val_acc: 0.7727\n",
      "Epoch 322/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4615 - acc: 0.7736 - val_loss: 0.4920 - val_acc: 0.7922\n",
      "Epoch 323/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4589 - acc: 0.7834 - val_loss: 0.4897 - val_acc: 0.7792\n",
      "Epoch 324/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4552 - acc: 0.7915 - val_loss: 0.4927 - val_acc: 0.7727\n",
      "Epoch 325/850\n",
      "614/614 [==============================] - 0s 94us/sample - loss: 0.4554 - acc: 0.7964 - val_loss: 0.4927 - val_acc: 0.7727\n",
      "Epoch 326/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4540 - acc: 0.7850 - val_loss: 0.4939 - val_acc: 0.7792\n",
      "Epoch 327/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4630 - acc: 0.7801 - val_loss: 0.4897 - val_acc: 0.7727\n",
      "Epoch 328/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4588 - acc: 0.7899 - val_loss: 0.5037 - val_acc: 0.7468\n",
      "Epoch 329/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4585 - acc: 0.7915 - val_loss: 0.4918 - val_acc: 0.7857\n",
      "Epoch 330/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4582 - acc: 0.7915 - val_loss: 0.4905 - val_acc: 0.7662\n",
      "Epoch 331/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4646 - acc: 0.7834 - val_loss: 0.4933 - val_acc: 0.7792\n",
      "Epoch 332/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4614 - acc: 0.7720 - val_loss: 0.4930 - val_acc: 0.7727\n",
      "Epoch 333/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4616 - acc: 0.7834 - val_loss: 0.4958 - val_acc: 0.7727\n",
      "Epoch 334/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4586 - acc: 0.7818 - val_loss: 0.5008 - val_acc: 0.7792\n",
      "Epoch 335/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4589 - acc: 0.7899 - val_loss: 0.4923 - val_acc: 0.7922\n",
      "Epoch 336/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4558 - acc: 0.7964 - val_loss: 0.4987 - val_acc: 0.7727\n",
      "Epoch 337/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4552 - acc: 0.7980 - val_loss: 0.4919 - val_acc: 0.7662\n",
      "Epoch 338/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4553 - acc: 0.7899 - val_loss: 0.4960 - val_acc: 0.7792\n",
      "Epoch 339/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4590 - acc: 0.7769 - val_loss: 0.4975 - val_acc: 0.7662\n",
      "Epoch 340/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4587 - acc: 0.7801 - val_loss: 0.4934 - val_acc: 0.7857\n",
      "Epoch 341/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4649 - acc: 0.7883 - val_loss: 0.4942 - val_acc: 0.7662\n",
      "Epoch 342/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4564 - acc: 0.7948 - val_loss: 0.4907 - val_acc: 0.7597\n",
      "Epoch 343/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4548 - acc: 0.7883 - val_loss: 0.4992 - val_acc: 0.7597\n",
      "Epoch 344/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4648 - acc: 0.7818 - val_loss: 0.4942 - val_acc: 0.7727\n",
      "Epoch 345/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4579 - acc: 0.7818 - val_loss: 0.4917 - val_acc: 0.7662\n",
      "Epoch 346/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4597 - acc: 0.7866 - val_loss: 0.4942 - val_acc: 0.7792\n",
      "Epoch 347/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4577 - acc: 0.7883 - val_loss: 0.4944 - val_acc: 0.7662\n",
      "Epoch 348/850\n",
      "614/614 [==============================] - 0s 76us/sample - loss: 0.4557 - acc: 0.7866 - val_loss: 0.4949 - val_acc: 0.7662\n",
      "Epoch 349/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4564 - acc: 0.7915 - val_loss: 0.4965 - val_acc: 0.7727\n",
      "Epoch 350/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4548 - acc: 0.7883 - val_loss: 0.4927 - val_acc: 0.7727\n",
      "Epoch 351/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4598 - acc: 0.7932 - val_loss: 0.4932 - val_acc: 0.7727\n",
      "Epoch 352/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4553 - acc: 0.7915 - val_loss: 0.5014 - val_acc: 0.7662\n",
      "Epoch 353/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4626 - acc: 0.7883 - val_loss: 0.4942 - val_acc: 0.7792\n",
      "Epoch 354/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4654 - acc: 0.7850 - val_loss: 0.5055 - val_acc: 0.7792\n",
      "Epoch 355/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4623 - acc: 0.7785 - val_loss: 0.4914 - val_acc: 0.7662\n",
      "Epoch 356/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4547 - acc: 0.7883 - val_loss: 0.4893 - val_acc: 0.7532\n",
      "Epoch 357/850\n",
      "614/614 [==============================] - 0s 77us/sample - loss: 0.4609 - acc: 0.7915 - val_loss: 0.5022 - val_acc: 0.7662\n",
      "Epoch 358/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4603 - acc: 0.7866 - val_loss: 0.4939 - val_acc: 0.7727\n",
      "Epoch 359/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4563 - acc: 0.7899 - val_loss: 0.5013 - val_acc: 0.7597\n",
      "Epoch 360/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4568 - acc: 0.7915 - val_loss: 0.4926 - val_acc: 0.7792\n",
      "Epoch 361/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4557 - acc: 0.7899 - val_loss: 0.5143 - val_acc: 0.7338\n",
      "Epoch 362/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4616 - acc: 0.7948 - val_loss: 0.4952 - val_acc: 0.7792\n",
      "Epoch 363/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4564 - acc: 0.7866 - val_loss: 0.4911 - val_acc: 0.7532\n",
      "Epoch 364/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4556 - acc: 0.7866 - val_loss: 0.4922 - val_acc: 0.7792\n",
      "Epoch 365/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4570 - acc: 0.7752 - val_loss: 0.4967 - val_acc: 0.7662\n",
      "Epoch 366/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4538 - acc: 0.7980 - val_loss: 0.4913 - val_acc: 0.7727\n",
      "Epoch 367/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4543 - acc: 0.7899 - val_loss: 0.4931 - val_acc: 0.7597\n",
      "Epoch 368/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4534 - acc: 0.7801 - val_loss: 0.4930 - val_acc: 0.7792\n",
      "Epoch 369/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4615 - acc: 0.7964 - val_loss: 0.5088 - val_acc: 0.7468\n",
      "Epoch 370/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4553 - acc: 0.7834 - val_loss: 0.4937 - val_acc: 0.7727\n",
      "Epoch 371/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4580 - acc: 0.7866 - val_loss: 0.4931 - val_acc: 0.7727\n",
      "Epoch 372/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4568 - acc: 0.7948 - val_loss: 0.5005 - val_acc: 0.7792\n",
      "Epoch 373/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4592 - acc: 0.7915 - val_loss: 0.4970 - val_acc: 0.7662\n",
      "Epoch 374/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4536 - acc: 0.7883 - val_loss: 0.5047 - val_acc: 0.7857\n",
      "Epoch 375/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4647 - acc: 0.7948 - val_loss: 0.4926 - val_acc: 0.7792\n",
      "Epoch 376/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4568 - acc: 0.7899 - val_loss: 0.4999 - val_acc: 0.7662\n",
      "Epoch 377/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4610 - acc: 0.7834 - val_loss: 0.5025 - val_acc: 0.7597\n",
      "Epoch 378/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4548 - acc: 0.7801 - val_loss: 0.4958 - val_acc: 0.7792\n",
      "Epoch 379/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4563 - acc: 0.7915 - val_loss: 0.4957 - val_acc: 0.7727\n",
      "Epoch 380/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4573 - acc: 0.7801 - val_loss: 0.4930 - val_acc: 0.7792\n",
      "Epoch 381/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4537 - acc: 0.7964 - val_loss: 0.5006 - val_acc: 0.7662\n",
      "Epoch 382/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4610 - acc: 0.7834 - val_loss: 0.4913 - val_acc: 0.7597\n",
      "Epoch 383/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4524 - acc: 0.7883 - val_loss: 0.5027 - val_acc: 0.7662\n",
      "Epoch 384/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4613 - acc: 0.7932 - val_loss: 0.4951 - val_acc: 0.7857\n",
      "Epoch 385/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4806 - acc: 0.7736 - val_loss: 0.4947 - val_acc: 0.7792\n",
      "Epoch 386/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4574 - acc: 0.7769 - val_loss: 0.4940 - val_acc: 0.7597\n",
      "Epoch 387/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4537 - acc: 0.7866 - val_loss: 0.4944 - val_acc: 0.7662\n",
      "Epoch 388/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4541 - acc: 0.7866 - val_loss: 0.4913 - val_acc: 0.7532\n",
      "Epoch 389/850\n",
      "614/614 [==============================] - 0s 77us/sample - loss: 0.4551 - acc: 0.7932 - val_loss: 0.4940 - val_acc: 0.7792\n",
      "Epoch 390/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4571 - acc: 0.7883 - val_loss: 0.4988 - val_acc: 0.7727\n",
      "Epoch 391/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4554 - acc: 0.7883 - val_loss: 0.4929 - val_acc: 0.7922\n",
      "Epoch 392/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4558 - acc: 0.7801 - val_loss: 0.4937 - val_acc: 0.7857\n",
      "Epoch 393/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4542 - acc: 0.7850 - val_loss: 0.4914 - val_acc: 0.7597\n",
      "Epoch 394/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4537 - acc: 0.7932 - val_loss: 0.4977 - val_acc: 0.7792\n",
      "Epoch 395/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4552 - acc: 0.7866 - val_loss: 0.5018 - val_acc: 0.7662\n",
      "Epoch 396/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4663 - acc: 0.7638 - val_loss: 0.4960 - val_acc: 0.7727\n",
      "Epoch 397/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4583 - acc: 0.7915 - val_loss: 0.4937 - val_acc: 0.7727\n",
      "Epoch 398/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4525 - acc: 0.7866 - val_loss: 0.4976 - val_acc: 0.7727\n",
      "Epoch 399/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4528 - acc: 0.7818 - val_loss: 0.4942 - val_acc: 0.7792\n",
      "Epoch 400/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4547 - acc: 0.7948 - val_loss: 0.5254 - val_acc: 0.7273\n",
      "Epoch 401/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4706 - acc: 0.7801 - val_loss: 0.4920 - val_acc: 0.7727\n",
      "Epoch 402/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4559 - acc: 0.7866 - val_loss: 0.4941 - val_acc: 0.7857\n",
      "Epoch 403/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4582 - acc: 0.7801 - val_loss: 0.4952 - val_acc: 0.7662\n",
      "Epoch 404/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4545 - acc: 0.7850 - val_loss: 0.4998 - val_acc: 0.7727\n",
      "Epoch 405/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4542 - acc: 0.7866 - val_loss: 0.4918 - val_acc: 0.7597\n",
      "Epoch 406/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4535 - acc: 0.7818 - val_loss: 0.5049 - val_acc: 0.7597\n",
      "Epoch 407/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4631 - acc: 0.7769 - val_loss: 0.4922 - val_acc: 0.7792\n",
      "Epoch 408/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4548 - acc: 0.7866 - val_loss: 0.4966 - val_acc: 0.7597\n",
      "Epoch 409/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4573 - acc: 0.7850 - val_loss: 0.4913 - val_acc: 0.7727\n",
      "Epoch 410/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4538 - acc: 0.7948 - val_loss: 0.4937 - val_acc: 0.7662\n",
      "Epoch 411/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4525 - acc: 0.7866 - val_loss: 0.4956 - val_acc: 0.7792\n",
      "Epoch 412/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4528 - acc: 0.7866 - val_loss: 0.4918 - val_acc: 0.7532\n",
      "Epoch 413/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4554 - acc: 0.7883 - val_loss: 0.4967 - val_acc: 0.7792\n",
      "Epoch 414/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4617 - acc: 0.7850 - val_loss: 0.4952 - val_acc: 0.7792\n",
      "Epoch 415/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4569 - acc: 0.7866 - val_loss: 0.4948 - val_acc: 0.7792\n",
      "Epoch 416/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4555 - acc: 0.7801 - val_loss: 0.4940 - val_acc: 0.7597\n",
      "Epoch 417/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4547 - acc: 0.7834 - val_loss: 0.4933 - val_acc: 0.7662\n",
      "Epoch 418/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4563 - acc: 0.7850 - val_loss: 0.5064 - val_acc: 0.7597\n",
      "Epoch 419/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4556 - acc: 0.7899 - val_loss: 0.4931 - val_acc: 0.7727\n",
      "Epoch 420/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4528 - acc: 0.7883 - val_loss: 0.4927 - val_acc: 0.7662\n",
      "Epoch 421/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4551 - acc: 0.7866 - val_loss: 0.5104 - val_acc: 0.7532\n",
      "Epoch 422/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4524 - acc: 0.7850 - val_loss: 0.4973 - val_acc: 0.7792\n",
      "Epoch 423/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4587 - acc: 0.7834 - val_loss: 0.4957 - val_acc: 0.7727\n",
      "Epoch 424/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4532 - acc: 0.7866 - val_loss: 0.4930 - val_acc: 0.7727\n",
      "Epoch 425/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4510 - acc: 0.7883 - val_loss: 0.5020 - val_acc: 0.7792\n",
      "Epoch 426/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4690 - acc: 0.7655 - val_loss: 0.4931 - val_acc: 0.7857\n",
      "Epoch 427/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4607 - acc: 0.7964 - val_loss: 0.4939 - val_acc: 0.7857\n",
      "Epoch 428/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4649 - acc: 0.7932 - val_loss: 0.5390 - val_acc: 0.7338\n",
      "Epoch 429/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4658 - acc: 0.7785 - val_loss: 0.4923 - val_acc: 0.7792\n",
      "Epoch 430/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4646 - acc: 0.7850 - val_loss: 0.4970 - val_acc: 0.7857\n",
      "Epoch 431/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4560 - acc: 0.7834 - val_loss: 0.4947 - val_acc: 0.7792\n",
      "Epoch 432/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4547 - acc: 0.7915 - val_loss: 0.4924 - val_acc: 0.7662\n",
      "Epoch 433/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4557 - acc: 0.7899 - val_loss: 0.4934 - val_acc: 0.7727\n",
      "Epoch 434/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4514 - acc: 0.7948 - val_loss: 0.5041 - val_acc: 0.7597\n",
      "Epoch 435/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4585 - acc: 0.7818 - val_loss: 0.4914 - val_acc: 0.7597\n",
      "Epoch 436/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4597 - acc: 0.7883 - val_loss: 0.4973 - val_acc: 0.7857\n",
      "Epoch 437/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4545 - acc: 0.7834 - val_loss: 0.4944 - val_acc: 0.7662\n",
      "Epoch 438/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4543 - acc: 0.7866 - val_loss: 0.4958 - val_acc: 0.7857\n",
      "Epoch 439/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4615 - acc: 0.7964 - val_loss: 0.5238 - val_acc: 0.7273\n",
      "Epoch 440/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4694 - acc: 0.7785 - val_loss: 0.4995 - val_acc: 0.7662\n",
      "Epoch 441/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4614 - acc: 0.7834 - val_loss: 0.4936 - val_acc: 0.7662\n",
      "Epoch 442/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4582 - acc: 0.7850 - val_loss: 0.4949 - val_acc: 0.7792\n",
      "Epoch 443/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4564 - acc: 0.7818 - val_loss: 0.4935 - val_acc: 0.7727\n",
      "Epoch 444/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4537 - acc: 0.7818 - val_loss: 0.4956 - val_acc: 0.7857\n",
      "Epoch 445/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4632 - acc: 0.7866 - val_loss: 0.4908 - val_acc: 0.7727\n",
      "Epoch 446/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4540 - acc: 0.7818 - val_loss: 0.4974 - val_acc: 0.7792\n",
      "Epoch 447/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4571 - acc: 0.7915 - val_loss: 0.4920 - val_acc: 0.7922\n",
      "Epoch 448/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4553 - acc: 0.7883 - val_loss: 0.5103 - val_acc: 0.7532\n",
      "Epoch 449/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4547 - acc: 0.7785 - val_loss: 0.4919 - val_acc: 0.7857\n",
      "Epoch 450/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4542 - acc: 0.7818 - val_loss: 0.4928 - val_acc: 0.7662\n",
      "Epoch 451/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4538 - acc: 0.7948 - val_loss: 0.4919 - val_acc: 0.7597\n",
      "Epoch 452/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4561 - acc: 0.7899 - val_loss: 0.5271 - val_acc: 0.7273\n",
      "Epoch 453/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4603 - acc: 0.7980 - val_loss: 0.4921 - val_acc: 0.7857\n",
      "Epoch 454/850\n",
      "614/614 [==============================] - 0s 75us/sample - loss: 0.4607 - acc: 0.7915 - val_loss: 0.4893 - val_acc: 0.7597\n",
      "Epoch 455/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4534 - acc: 0.7883 - val_loss: 0.5128 - val_acc: 0.7403\n",
      "Epoch 456/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4678 - acc: 0.7720 - val_loss: 0.5011 - val_acc: 0.7662\n",
      "Epoch 457/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4574 - acc: 0.7850 - val_loss: 0.5042 - val_acc: 0.7597\n",
      "Epoch 458/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4581 - acc: 0.7818 - val_loss: 0.5005 - val_acc: 0.7727\n",
      "Epoch 459/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4524 - acc: 0.7834 - val_loss: 0.4946 - val_acc: 0.7597\n",
      "Epoch 460/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4528 - acc: 0.7883 - val_loss: 0.4952 - val_acc: 0.7597\n",
      "Epoch 461/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4542 - acc: 0.7899 - val_loss: 0.4939 - val_acc: 0.7727\n",
      "Epoch 462/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4574 - acc: 0.7801 - val_loss: 0.5079 - val_acc: 0.7857\n",
      "Epoch 463/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4586 - acc: 0.7850 - val_loss: 0.4949 - val_acc: 0.7597\n",
      "Epoch 464/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4572 - acc: 0.7883 - val_loss: 0.4947 - val_acc: 0.7597\n",
      "Epoch 465/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4623 - acc: 0.7801 - val_loss: 0.5023 - val_acc: 0.7662\n",
      "Epoch 466/850\n",
      "614/614 [==============================] - 0s 75us/sample - loss: 0.4533 - acc: 0.7915 - val_loss: 0.4943 - val_acc: 0.7727\n",
      "Epoch 467/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4542 - acc: 0.7899 - val_loss: 0.4931 - val_acc: 0.7662\n",
      "Epoch 468/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4543 - acc: 0.7915 - val_loss: 0.4968 - val_acc: 0.7727\n",
      "Epoch 469/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4523 - acc: 0.7948 - val_loss: 0.4956 - val_acc: 0.7857\n",
      "Epoch 470/850\n",
      "614/614 [==============================] - 0s 79us/sample - loss: 0.4539 - acc: 0.7915 - val_loss: 0.4965 - val_acc: 0.7597\n",
      "Epoch 471/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4567 - acc: 0.7899 - val_loss: 0.4942 - val_acc: 0.7792\n",
      "Epoch 472/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4540 - acc: 0.7915 - val_loss: 0.4933 - val_acc: 0.7597\n",
      "Epoch 473/850\n",
      "614/614 [==============================] - 0s 76us/sample - loss: 0.4552 - acc: 0.7866 - val_loss: 0.4951 - val_acc: 0.7727\n",
      "Epoch 474/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4598 - acc: 0.7850 - val_loss: 0.5272 - val_acc: 0.7273\n",
      "Epoch 475/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4623 - acc: 0.7801 - val_loss: 0.5027 - val_acc: 0.7662\n",
      "Epoch 476/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4564 - acc: 0.7834 - val_loss: 0.4929 - val_acc: 0.7727\n",
      "Epoch 477/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4539 - acc: 0.7850 - val_loss: 0.4939 - val_acc: 0.7662\n",
      "Epoch 478/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4562 - acc: 0.7834 - val_loss: 0.4967 - val_acc: 0.7597\n",
      "Epoch 479/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4518 - acc: 0.7866 - val_loss: 0.4963 - val_acc: 0.7727\n",
      "Epoch 480/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4530 - acc: 0.7964 - val_loss: 0.4967 - val_acc: 0.7662\n",
      "Epoch 481/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4516 - acc: 0.7866 - val_loss: 0.4965 - val_acc: 0.7857\n",
      "Epoch 482/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4534 - acc: 0.7899 - val_loss: 0.4972 - val_acc: 0.7597\n",
      "Epoch 483/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4611 - acc: 0.7850 - val_loss: 0.4956 - val_acc: 0.7662\n",
      "Epoch 484/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4549 - acc: 0.7818 - val_loss: 0.4919 - val_acc: 0.7727\n",
      "Epoch 485/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4529 - acc: 0.7932 - val_loss: 0.4934 - val_acc: 0.7532\n",
      "Epoch 486/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4531 - acc: 0.7834 - val_loss: 0.4939 - val_acc: 0.7662\n",
      "Epoch 487/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4521 - acc: 0.7932 - val_loss: 0.4943 - val_acc: 0.7727\n",
      "Epoch 488/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4551 - acc: 0.7883 - val_loss: 0.4945 - val_acc: 0.7727\n",
      "Epoch 489/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4528 - acc: 0.7932 - val_loss: 0.4957 - val_acc: 0.7597\n",
      "Epoch 490/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4537 - acc: 0.7932 - val_loss: 0.4936 - val_acc: 0.7662\n",
      "Epoch 491/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4529 - acc: 0.7915 - val_loss: 0.4953 - val_acc: 0.7857\n",
      "Epoch 492/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4559 - acc: 0.7899 - val_loss: 0.4964 - val_acc: 0.7727\n",
      "Epoch 493/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4552 - acc: 0.7801 - val_loss: 0.4946 - val_acc: 0.7597\n",
      "Epoch 494/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4574 - acc: 0.7866 - val_loss: 0.5033 - val_acc: 0.7727\n",
      "Epoch 495/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4527 - acc: 0.7948 - val_loss: 0.4939 - val_acc: 0.7857\n",
      "Epoch 496/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4538 - acc: 0.7932 - val_loss: 0.5006 - val_acc: 0.7662\n",
      "Epoch 497/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4535 - acc: 0.7818 - val_loss: 0.4952 - val_acc: 0.7662\n",
      "Epoch 498/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4566 - acc: 0.7866 - val_loss: 0.5003 - val_acc: 0.7857\n",
      "Epoch 499/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4567 - acc: 0.7834 - val_loss: 0.4915 - val_acc: 0.7727\n",
      "Epoch 500/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4521 - acc: 0.7899 - val_loss: 0.4941 - val_acc: 0.7662\n",
      "Epoch 501/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4549 - acc: 0.7964 - val_loss: 0.4941 - val_acc: 0.7792\n",
      "Epoch 502/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4608 - acc: 0.7785 - val_loss: 0.4947 - val_acc: 0.7727\n",
      "Epoch 503/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4540 - acc: 0.7883 - val_loss: 0.4999 - val_acc: 0.7468\n",
      "Epoch 504/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4534 - acc: 0.7850 - val_loss: 0.4926 - val_acc: 0.7597\n",
      "Epoch 505/850\n",
      "614/614 [==============================] - 0s 77us/sample - loss: 0.4538 - acc: 0.7899 - val_loss: 0.4954 - val_acc: 0.7662\n",
      "Epoch 506/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4554 - acc: 0.7915 - val_loss: 0.5023 - val_acc: 0.7727\n",
      "Epoch 507/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4629 - acc: 0.7769 - val_loss: 0.5041 - val_acc: 0.7727\n",
      "Epoch 508/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4490 - acc: 0.7915 - val_loss: 0.4967 - val_acc: 0.7857\n",
      "Epoch 509/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4540 - acc: 0.7948 - val_loss: 0.4953 - val_acc: 0.7662\n",
      "Epoch 510/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4533 - acc: 0.7834 - val_loss: 0.4959 - val_acc: 0.7727\n",
      "Epoch 511/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4531 - acc: 0.7899 - val_loss: 0.4987 - val_acc: 0.7727\n",
      "Epoch 512/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4546 - acc: 0.7834 - val_loss: 0.4946 - val_acc: 0.7792\n",
      "Epoch 513/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4550 - acc: 0.7801 - val_loss: 0.4964 - val_acc: 0.7792\n",
      "Epoch 514/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4556 - acc: 0.7866 - val_loss: 0.5078 - val_acc: 0.7597\n",
      "Epoch 515/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4614 - acc: 0.7883 - val_loss: 0.5164 - val_acc: 0.7403\n",
      "Epoch 516/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4677 - acc: 0.7915 - val_loss: 0.5000 - val_acc: 0.7857\n",
      "Epoch 517/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4523 - acc: 0.7899 - val_loss: 0.5148 - val_acc: 0.7403\n",
      "Epoch 518/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4616 - acc: 0.7866 - val_loss: 0.5086 - val_acc: 0.7597\n",
      "Epoch 519/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4612 - acc: 0.7850 - val_loss: 0.4943 - val_acc: 0.7727\n",
      "Epoch 520/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4579 - acc: 0.7866 - val_loss: 0.4948 - val_acc: 0.7532\n",
      "Epoch 521/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4518 - acc: 0.7932 - val_loss: 0.4955 - val_acc: 0.7792\n",
      "Epoch 522/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4549 - acc: 0.8013 - val_loss: 0.5008 - val_acc: 0.7727\n",
      "Epoch 523/850\n",
      "614/614 [==============================] - 0s 75us/sample - loss: 0.4535 - acc: 0.7866 - val_loss: 0.4940 - val_acc: 0.7727\n",
      "Epoch 524/850\n",
      "614/614 [==============================] - 0s 75us/sample - loss: 0.4537 - acc: 0.7801 - val_loss: 0.5002 - val_acc: 0.7857\n",
      "Epoch 525/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4565 - acc: 0.7818 - val_loss: 0.4973 - val_acc: 0.7662\n",
      "Epoch 526/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4534 - acc: 0.7997 - val_loss: 0.4995 - val_acc: 0.7727\n",
      "Epoch 527/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4556 - acc: 0.7866 - val_loss: 0.4958 - val_acc: 0.7727\n",
      "Epoch 528/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4507 - acc: 0.7915 - val_loss: 0.4965 - val_acc: 0.7857\n",
      "Epoch 529/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4578 - acc: 0.7997 - val_loss: 0.5002 - val_acc: 0.7792\n",
      "Epoch 530/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4594 - acc: 0.7801 - val_loss: 0.4950 - val_acc: 0.7857\n",
      "Epoch 531/850\n",
      "614/614 [==============================] - 0s 90us/sample - loss: 0.4551 - acc: 0.7899 - val_loss: 0.4954 - val_acc: 0.7662\n",
      "Epoch 532/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4540 - acc: 0.7866 - val_loss: 0.5035 - val_acc: 0.7857\n",
      "Epoch 533/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4562 - acc: 0.7964 - val_loss: 0.4981 - val_acc: 0.7597\n",
      "Epoch 534/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4629 - acc: 0.7785 - val_loss: 0.4956 - val_acc: 0.7532\n",
      "Epoch 535/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4714 - acc: 0.7818 - val_loss: 0.5146 - val_acc: 0.7403\n",
      "Epoch 536/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4516 - acc: 0.7948 - val_loss: 0.4977 - val_acc: 0.7857\n",
      "Epoch 537/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4559 - acc: 0.7948 - val_loss: 0.5170 - val_acc: 0.7403\n",
      "Epoch 538/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4620 - acc: 0.7818 - val_loss: 0.4990 - val_acc: 0.7727\n",
      "Epoch 539/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4537 - acc: 0.7915 - val_loss: 0.5012 - val_acc: 0.7857\n",
      "Epoch 540/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4545 - acc: 0.7883 - val_loss: 0.5112 - val_acc: 0.7597\n",
      "Epoch 541/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4531 - acc: 0.7866 - val_loss: 0.4955 - val_acc: 0.7792\n",
      "Epoch 542/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4632 - acc: 0.8029 - val_loss: 0.5050 - val_acc: 0.7662\n",
      "Epoch 543/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4545 - acc: 0.7980 - val_loss: 0.5034 - val_acc: 0.7597\n",
      "Epoch 544/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4579 - acc: 0.7834 - val_loss: 0.5010 - val_acc: 0.7792\n",
      "Epoch 545/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4563 - acc: 0.7801 - val_loss: 0.4939 - val_acc: 0.7727\n",
      "Epoch 546/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4551 - acc: 0.7850 - val_loss: 0.4939 - val_acc: 0.7727\n",
      "Epoch 547/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4614 - acc: 0.7850 - val_loss: 0.4964 - val_acc: 0.7597\n",
      "Epoch 548/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4529 - acc: 0.7899 - val_loss: 0.4925 - val_acc: 0.7792\n",
      "Epoch 549/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4528 - acc: 0.7899 - val_loss: 0.4937 - val_acc: 0.7597\n",
      "Epoch 550/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4537 - acc: 0.7866 - val_loss: 0.4948 - val_acc: 0.7857\n",
      "Epoch 551/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4581 - acc: 0.7818 - val_loss: 0.4932 - val_acc: 0.7662\n",
      "Epoch 552/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4529 - acc: 0.7948 - val_loss: 0.4945 - val_acc: 0.7597\n",
      "Epoch 553/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4554 - acc: 0.7964 - val_loss: 0.4994 - val_acc: 0.7532\n",
      "Epoch 554/850\n",
      "614/614 [==============================] - 0s 78us/sample - loss: 0.4528 - acc: 0.7932 - val_loss: 0.4947 - val_acc: 0.7532\n",
      "Epoch 555/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4659 - acc: 0.7801 - val_loss: 0.4960 - val_acc: 0.7597\n",
      "Epoch 556/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4587 - acc: 0.7818 - val_loss: 0.4943 - val_acc: 0.7792\n",
      "Epoch 557/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4633 - acc: 0.7752 - val_loss: 0.5020 - val_acc: 0.7792\n",
      "Epoch 558/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4605 - acc: 0.7866 - val_loss: 0.4924 - val_acc: 0.7792\n",
      "Epoch 559/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4551 - acc: 0.7785 - val_loss: 0.4993 - val_acc: 0.7792\n",
      "Epoch 560/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4527 - acc: 0.7866 - val_loss: 0.5021 - val_acc: 0.7662\n",
      "Epoch 561/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4637 - acc: 0.7752 - val_loss: 0.5044 - val_acc: 0.7662\n",
      "Epoch 562/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4551 - acc: 0.7866 - val_loss: 0.4964 - val_acc: 0.7597\n",
      "Epoch 563/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4525 - acc: 0.7948 - val_loss: 0.4936 - val_acc: 0.7727\n",
      "Epoch 564/850\n",
      "614/614 [==============================] - 0s 76us/sample - loss: 0.4546 - acc: 0.7866 - val_loss: 0.4978 - val_acc: 0.7597\n",
      "Epoch 565/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4532 - acc: 0.7932 - val_loss: 0.4953 - val_acc: 0.7727\n",
      "Epoch 566/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4625 - acc: 0.7915 - val_loss: 0.5034 - val_acc: 0.7792\n",
      "Epoch 567/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4611 - acc: 0.7915 - val_loss: 0.4934 - val_acc: 0.7727\n",
      "Epoch 568/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4525 - acc: 0.7850 - val_loss: 0.5071 - val_acc: 0.7597\n",
      "Epoch 569/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4531 - acc: 0.7899 - val_loss: 0.4951 - val_acc: 0.7857\n",
      "Epoch 570/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4538 - acc: 0.7883 - val_loss: 0.4952 - val_acc: 0.7662\n",
      "Epoch 571/850\n",
      "614/614 [==============================] - 0s 77us/sample - loss: 0.4559 - acc: 0.7883 - val_loss: 0.4977 - val_acc: 0.7662\n",
      "Epoch 572/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4869 - acc: 0.7752 - val_loss: 0.4963 - val_acc: 0.7662\n",
      "Epoch 573/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4752 - acc: 0.7801 - val_loss: 0.5124 - val_acc: 0.7792\n",
      "Epoch 574/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4531 - acc: 0.7850 - val_loss: 0.4991 - val_acc: 0.7662\n",
      "Epoch 575/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4513 - acc: 0.7932 - val_loss: 0.4924 - val_acc: 0.7597\n",
      "Epoch 576/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4530 - acc: 0.7866 - val_loss: 0.4917 - val_acc: 0.7597\n",
      "Epoch 577/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4522 - acc: 0.7932 - val_loss: 0.4940 - val_acc: 0.7727\n",
      "Epoch 578/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4576 - acc: 0.7801 - val_loss: 0.5039 - val_acc: 0.7857\n",
      "Epoch 579/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4502 - acc: 0.7980 - val_loss: 0.5134 - val_acc: 0.7532\n",
      "Epoch 580/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4564 - acc: 0.7834 - val_loss: 0.4968 - val_acc: 0.7727\n",
      "Epoch 581/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4549 - acc: 0.7769 - val_loss: 0.4952 - val_acc: 0.7727\n",
      "Epoch 582/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4658 - acc: 0.7980 - val_loss: 0.5337 - val_acc: 0.7338\n",
      "Epoch 583/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4638 - acc: 0.7801 - val_loss: 0.4931 - val_acc: 0.7597\n",
      "Epoch 584/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4538 - acc: 0.7818 - val_loss: 0.4981 - val_acc: 0.7727\n",
      "Epoch 585/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4552 - acc: 0.7899 - val_loss: 0.4977 - val_acc: 0.7597\n",
      "Epoch 586/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4557 - acc: 0.7932 - val_loss: 0.5008 - val_acc: 0.7792\n",
      "Epoch 587/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4557 - acc: 0.7980 - val_loss: 0.4998 - val_acc: 0.7597\n",
      "Epoch 588/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4533 - acc: 0.7883 - val_loss: 0.4942 - val_acc: 0.7662\n",
      "Epoch 589/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4539 - acc: 0.7850 - val_loss: 0.4966 - val_acc: 0.7662\n",
      "Epoch 590/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4663 - acc: 0.7655 - val_loss: 0.4958 - val_acc: 0.7792\n",
      "Epoch 591/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4515 - acc: 0.7866 - val_loss: 0.5074 - val_acc: 0.7922\n",
      "Epoch 592/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4667 - acc: 0.7769 - val_loss: 0.4927 - val_acc: 0.7792\n",
      "Epoch 593/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4615 - acc: 0.7801 - val_loss: 0.4970 - val_acc: 0.7662\n",
      "Epoch 594/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4514 - acc: 0.7915 - val_loss: 0.4934 - val_acc: 0.7727\n",
      "Epoch 595/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4520 - acc: 0.7948 - val_loss: 0.4936 - val_acc: 0.7792\n",
      "Epoch 596/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4544 - acc: 0.7883 - val_loss: 0.4959 - val_acc: 0.7662\n",
      "Epoch 597/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4564 - acc: 0.7866 - val_loss: 0.4997 - val_acc: 0.7857\n",
      "Epoch 598/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4510 - acc: 0.7785 - val_loss: 0.4974 - val_acc: 0.7662\n",
      "Epoch 599/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4534 - acc: 0.7834 - val_loss: 0.4948 - val_acc: 0.7662\n",
      "Epoch 600/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4526 - acc: 0.7785 - val_loss: 0.4956 - val_acc: 0.7662\n",
      "Epoch 601/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4563 - acc: 0.7915 - val_loss: 0.5174 - val_acc: 0.7403\n",
      "Epoch 602/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4569 - acc: 0.7801 - val_loss: 0.4922 - val_acc: 0.7792\n",
      "Epoch 603/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4507 - acc: 0.7932 - val_loss: 0.5020 - val_acc: 0.7597\n",
      "Epoch 604/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4579 - acc: 0.7915 - val_loss: 0.4941 - val_acc: 0.7662\n",
      "Epoch 605/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4558 - acc: 0.7850 - val_loss: 0.4932 - val_acc: 0.7727\n",
      "Epoch 606/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4559 - acc: 0.7818 - val_loss: 0.4956 - val_acc: 0.7597\n",
      "Epoch 607/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4612 - acc: 0.7818 - val_loss: 0.5052 - val_acc: 0.7662\n",
      "Epoch 608/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4527 - acc: 0.7915 - val_loss: 0.5028 - val_acc: 0.7922\n",
      "Epoch 609/850\n",
      "614/614 [==============================] - 0s 77us/sample - loss: 0.4635 - acc: 0.7834 - val_loss: 0.4943 - val_acc: 0.7792\n",
      "Epoch 610/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4525 - acc: 0.7834 - val_loss: 0.4996 - val_acc: 0.7727\n",
      "Epoch 611/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4707 - acc: 0.7866 - val_loss: 0.5007 - val_acc: 0.7597\n",
      "Epoch 612/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4618 - acc: 0.7899 - val_loss: 0.5210 - val_acc: 0.7338\n",
      "Epoch 613/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4692 - acc: 0.7818 - val_loss: 0.4945 - val_acc: 0.7792\n",
      "Epoch 614/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4548 - acc: 0.7769 - val_loss: 0.5045 - val_acc: 0.7792\n",
      "Epoch 615/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4613 - acc: 0.7834 - val_loss: 0.5028 - val_acc: 0.7662\n",
      "Epoch 616/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4548 - acc: 0.7883 - val_loss: 0.4961 - val_acc: 0.7662\n",
      "Epoch 617/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4558 - acc: 0.7883 - val_loss: 0.4928 - val_acc: 0.7792\n",
      "Epoch 618/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4542 - acc: 0.7866 - val_loss: 0.4975 - val_acc: 0.7597\n",
      "Epoch 619/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4534 - acc: 0.7801 - val_loss: 0.4942 - val_acc: 0.7857\n",
      "Epoch 620/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4644 - acc: 0.7736 - val_loss: 0.4975 - val_acc: 0.7857\n",
      "Epoch 621/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4687 - acc: 0.7801 - val_loss: 0.5012 - val_acc: 0.7597\n",
      "Epoch 622/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4506 - acc: 0.7915 - val_loss: 0.4996 - val_acc: 0.7662\n",
      "Epoch 623/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4526 - acc: 0.7850 - val_loss: 0.4957 - val_acc: 0.7662\n",
      "Epoch 624/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4550 - acc: 0.7850 - val_loss: 0.4953 - val_acc: 0.7727\n",
      "Epoch 625/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4606 - acc: 0.7866 - val_loss: 0.4959 - val_acc: 0.7662\n",
      "Epoch 626/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4551 - acc: 0.7915 - val_loss: 0.4987 - val_acc: 0.7597\n",
      "Epoch 627/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4583 - acc: 0.7834 - val_loss: 0.4947 - val_acc: 0.7857\n",
      "Epoch 628/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4563 - acc: 0.7866 - val_loss: 0.5014 - val_acc: 0.7857\n",
      "Epoch 629/850\n",
      "614/614 [==============================] - 0s 75us/sample - loss: 0.4548 - acc: 0.7866 - val_loss: 0.4958 - val_acc: 0.7597\n",
      "Epoch 630/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4529 - acc: 0.7866 - val_loss: 0.4954 - val_acc: 0.7662\n",
      "Epoch 631/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4528 - acc: 0.7883 - val_loss: 0.4939 - val_acc: 0.7857\n",
      "Epoch 632/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4506 - acc: 0.7980 - val_loss: 0.4999 - val_acc: 0.7727\n",
      "Epoch 633/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4518 - acc: 0.7883 - val_loss: 0.4996 - val_acc: 0.7792\n",
      "Epoch 634/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4574 - acc: 0.7850 - val_loss: 0.4934 - val_acc: 0.7727\n",
      "Epoch 635/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4561 - acc: 0.7915 - val_loss: 0.4943 - val_acc: 0.7662\n",
      "Epoch 636/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4508 - acc: 0.7899 - val_loss: 0.4958 - val_acc: 0.7792\n",
      "Epoch 637/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4528 - acc: 0.7932 - val_loss: 0.5031 - val_acc: 0.7662\n",
      "Epoch 638/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4517 - acc: 0.7785 - val_loss: 0.4932 - val_acc: 0.7727\n",
      "Epoch 639/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4554 - acc: 0.7834 - val_loss: 0.4962 - val_acc: 0.7922\n",
      "Epoch 640/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4529 - acc: 0.7932 - val_loss: 0.4980 - val_acc: 0.7597\n",
      "Epoch 641/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4536 - acc: 0.7915 - val_loss: 0.4989 - val_acc: 0.7662\n",
      "Epoch 642/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4526 - acc: 0.7850 - val_loss: 0.4939 - val_acc: 0.7727\n",
      "Epoch 643/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4517 - acc: 0.7997 - val_loss: 0.5250 - val_acc: 0.7338\n",
      "Epoch 644/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4680 - acc: 0.7769 - val_loss: 0.4923 - val_acc: 0.7662\n",
      "Epoch 645/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4530 - acc: 0.7850 - val_loss: 0.5060 - val_acc: 0.7662\n",
      "Epoch 646/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4606 - acc: 0.8062 - val_loss: 0.5199 - val_acc: 0.7403\n",
      "Epoch 647/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4541 - acc: 0.7850 - val_loss: 0.4952 - val_acc: 0.7922\n",
      "Epoch 648/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4608 - acc: 0.7834 - val_loss: 0.5003 - val_acc: 0.7597\n",
      "Epoch 649/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4554 - acc: 0.7866 - val_loss: 0.4954 - val_acc: 0.7727\n",
      "Epoch 650/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4578 - acc: 0.7818 - val_loss: 0.4997 - val_acc: 0.7857\n",
      "Epoch 651/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4660 - acc: 0.7866 - val_loss: 0.5119 - val_acc: 0.7857\n",
      "Epoch 652/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4564 - acc: 0.8013 - val_loss: 0.5010 - val_acc: 0.7662\n",
      "Epoch 653/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4552 - acc: 0.7801 - val_loss: 0.4941 - val_acc: 0.7727\n",
      "Epoch 654/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4504 - acc: 0.7915 - val_loss: 0.4993 - val_acc: 0.7662\n",
      "Epoch 655/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4507 - acc: 0.7818 - val_loss: 0.4944 - val_acc: 0.7792\n",
      "Epoch 656/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4521 - acc: 0.8013 - val_loss: 0.5028 - val_acc: 0.7727\n",
      "Epoch 657/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4515 - acc: 0.7866 - val_loss: 0.5007 - val_acc: 0.7662\n",
      "Epoch 658/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4515 - acc: 0.7850 - val_loss: 0.4961 - val_acc: 0.7727\n",
      "Epoch 659/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4526 - acc: 0.7932 - val_loss: 0.4930 - val_acc: 0.7727\n",
      "Epoch 660/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4542 - acc: 0.7997 - val_loss: 0.5164 - val_acc: 0.7468\n",
      "Epoch 661/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4529 - acc: 0.7834 - val_loss: 0.4950 - val_acc: 0.7727\n",
      "Epoch 662/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4521 - acc: 0.7883 - val_loss: 0.4943 - val_acc: 0.7727\n",
      "Epoch 663/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4526 - acc: 0.7834 - val_loss: 0.4959 - val_acc: 0.7727\n",
      "Epoch 664/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4541 - acc: 0.7834 - val_loss: 0.5003 - val_acc: 0.7792\n",
      "Epoch 665/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4518 - acc: 0.7980 - val_loss: 0.5112 - val_acc: 0.7662\n",
      "Epoch 666/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4510 - acc: 0.7899 - val_loss: 0.4946 - val_acc: 0.7857\n",
      "Epoch 667/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4563 - acc: 0.7932 - val_loss: 0.5171 - val_acc: 0.7403\n",
      "Epoch 668/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4541 - acc: 0.7948 - val_loss: 0.4947 - val_acc: 0.7792\n",
      "Epoch 669/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4561 - acc: 0.7834 - val_loss: 0.4925 - val_acc: 0.7727\n",
      "Epoch 670/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4533 - acc: 0.7850 - val_loss: 0.4938 - val_acc: 0.7857\n",
      "Epoch 671/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4503 - acc: 0.7883 - val_loss: 0.4972 - val_acc: 0.7597\n",
      "Epoch 672/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4541 - acc: 0.7818 - val_loss: 0.4994 - val_acc: 0.7922\n",
      "Epoch 673/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4721 - acc: 0.7769 - val_loss: 0.5026 - val_acc: 0.7662\n",
      "Epoch 674/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4632 - acc: 0.7883 - val_loss: 0.5145 - val_acc: 0.7597\n",
      "Epoch 675/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4525 - acc: 0.8029 - val_loss: 0.4996 - val_acc: 0.7727\n",
      "Epoch 676/850\n",
      "614/614 [==============================] - 0s 75us/sample - loss: 0.4626 - acc: 0.7850 - val_loss: 0.4971 - val_acc: 0.7532\n",
      "Epoch 677/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4519 - acc: 0.7866 - val_loss: 0.5029 - val_acc: 0.7792\n",
      "Epoch 678/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4567 - acc: 0.7818 - val_loss: 0.5011 - val_acc: 0.7792\n",
      "Epoch 679/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4521 - acc: 0.7899 - val_loss: 0.5108 - val_acc: 0.7597\n",
      "Epoch 680/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4525 - acc: 0.7866 - val_loss: 0.4938 - val_acc: 0.7727\n",
      "Epoch 681/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4573 - acc: 0.7801 - val_loss: 0.4956 - val_acc: 0.7597\n",
      "Epoch 682/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4522 - acc: 0.7899 - val_loss: 0.5081 - val_acc: 0.7662\n",
      "Epoch 683/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4556 - acc: 0.7883 - val_loss: 0.4944 - val_acc: 0.7727\n",
      "Epoch 684/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4505 - acc: 0.7883 - val_loss: 0.4960 - val_acc: 0.7597\n",
      "Epoch 685/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4552 - acc: 0.7850 - val_loss: 0.4973 - val_acc: 0.7597\n",
      "Epoch 686/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4510 - acc: 0.7899 - val_loss: 0.4971 - val_acc: 0.7792\n",
      "Epoch 687/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4521 - acc: 0.7899 - val_loss: 0.4983 - val_acc: 0.7792\n",
      "Epoch 688/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4584 - acc: 0.7850 - val_loss: 0.4984 - val_acc: 0.7597\n",
      "Epoch 689/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4571 - acc: 0.7915 - val_loss: 0.5030 - val_acc: 0.7597\n",
      "Epoch 690/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4567 - acc: 0.7834 - val_loss: 0.5005 - val_acc: 0.7532\n",
      "Epoch 691/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4495 - acc: 0.7866 - val_loss: 0.4953 - val_acc: 0.7727\n",
      "Epoch 692/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4517 - acc: 0.7899 - val_loss: 0.4991 - val_acc: 0.7792\n",
      "Epoch 693/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4537 - acc: 0.7866 - val_loss: 0.4973 - val_acc: 0.7597\n",
      "Epoch 694/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4556 - acc: 0.7883 - val_loss: 0.5149 - val_acc: 0.7597\n",
      "Epoch 695/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4620 - acc: 0.7866 - val_loss: 0.4974 - val_acc: 0.7792\n",
      "Epoch 696/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4515 - acc: 0.7850 - val_loss: 0.4979 - val_acc: 0.7597\n",
      "Epoch 697/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4493 - acc: 0.7850 - val_loss: 0.4959 - val_acc: 0.7727\n",
      "Epoch 698/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4502 - acc: 0.7899 - val_loss: 0.5029 - val_acc: 0.7727\n",
      "Epoch 699/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4521 - acc: 0.7866 - val_loss: 0.4924 - val_acc: 0.7727\n",
      "Epoch 700/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4560 - acc: 0.8078 - val_loss: 0.5043 - val_acc: 0.7662\n",
      "Epoch 701/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4598 - acc: 0.7769 - val_loss: 0.4987 - val_acc: 0.7597\n",
      "Epoch 702/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4575 - acc: 0.7834 - val_loss: 0.4958 - val_acc: 0.7662\n",
      "Epoch 703/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4506 - acc: 0.7883 - val_loss: 0.4953 - val_acc: 0.7792\n",
      "Epoch 704/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4549 - acc: 0.7866 - val_loss: 0.4958 - val_acc: 0.7597\n",
      "Epoch 705/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4572 - acc: 0.7850 - val_loss: 0.4969 - val_acc: 0.7727\n",
      "Epoch 706/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4597 - acc: 0.7866 - val_loss: 0.4991 - val_acc: 0.7857\n",
      "Epoch 707/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4569 - acc: 0.7915 - val_loss: 0.4931 - val_acc: 0.7662\n",
      "Epoch 708/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4551 - acc: 0.7932 - val_loss: 0.5082 - val_acc: 0.7662\n",
      "Epoch 709/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4561 - acc: 0.7801 - val_loss: 0.4951 - val_acc: 0.7662\n",
      "Epoch 710/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4527 - acc: 0.7915 - val_loss: 0.4936 - val_acc: 0.7662\n",
      "Epoch 711/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4605 - acc: 0.7834 - val_loss: 0.5176 - val_acc: 0.7857\n",
      "Epoch 712/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4695 - acc: 0.7883 - val_loss: 0.4939 - val_acc: 0.7662\n",
      "Epoch 713/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4649 - acc: 0.7948 - val_loss: 0.5028 - val_acc: 0.7727\n",
      "Epoch 714/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4516 - acc: 0.7964 - val_loss: 0.4953 - val_acc: 0.7597\n",
      "Epoch 715/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4604 - acc: 0.7964 - val_loss: 0.5168 - val_acc: 0.7922\n",
      "Epoch 716/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4674 - acc: 0.7736 - val_loss: 0.4964 - val_acc: 0.7662\n",
      "Epoch 717/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4661 - acc: 0.7752 - val_loss: 0.4970 - val_acc: 0.7597\n",
      "Epoch 718/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4519 - acc: 0.7850 - val_loss: 0.4956 - val_acc: 0.7792\n",
      "Epoch 719/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4538 - acc: 0.7883 - val_loss: 0.4940 - val_acc: 0.7662\n",
      "Epoch 720/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4594 - acc: 0.7769 - val_loss: 0.4933 - val_acc: 0.7727\n",
      "Epoch 721/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4537 - acc: 0.7801 - val_loss: 0.4995 - val_acc: 0.7857\n",
      "Epoch 722/850\n",
      "614/614 [==============================] - 0s 75us/sample - loss: 0.4562 - acc: 0.7997 - val_loss: 0.5175 - val_acc: 0.7468\n",
      "Epoch 723/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4587 - acc: 0.7915 - val_loss: 0.4933 - val_acc: 0.7662\n",
      "Epoch 724/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4557 - acc: 0.7818 - val_loss: 0.5044 - val_acc: 0.7792\n",
      "Epoch 725/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4559 - acc: 0.7818 - val_loss: 0.4961 - val_acc: 0.7727\n",
      "Epoch 726/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4621 - acc: 0.7883 - val_loss: 0.5061 - val_acc: 0.7532\n",
      "Epoch 727/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4514 - acc: 0.7997 - val_loss: 0.5009 - val_acc: 0.7662\n",
      "Epoch 728/850\n",
      "614/614 [==============================] - 0s 90us/sample - loss: 0.4583 - acc: 0.7785 - val_loss: 0.5006 - val_acc: 0.7662\n",
      "Epoch 729/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4544 - acc: 0.7818 - val_loss: 0.4974 - val_acc: 0.7792\n",
      "Epoch 730/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4529 - acc: 0.7883 - val_loss: 0.4957 - val_acc: 0.7727\n",
      "Epoch 731/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4559 - acc: 0.7818 - val_loss: 0.4987 - val_acc: 0.7792\n",
      "Epoch 732/850\n",
      "614/614 [==============================] - 0s 75us/sample - loss: 0.4568 - acc: 0.7752 - val_loss: 0.5047 - val_acc: 0.7532\n",
      "Epoch 733/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4476 - acc: 0.7915 - val_loss: 0.5004 - val_acc: 0.7792\n",
      "Epoch 734/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4643 - acc: 0.7915 - val_loss: 0.4978 - val_acc: 0.7597\n",
      "Epoch 735/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4540 - acc: 0.7899 - val_loss: 0.4970 - val_acc: 0.7532\n",
      "Epoch 736/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4556 - acc: 0.7818 - val_loss: 0.4979 - val_acc: 0.7597\n",
      "Epoch 737/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4751 - acc: 0.7866 - val_loss: 0.5039 - val_acc: 0.7792\n",
      "Epoch 738/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4557 - acc: 0.7850 - val_loss: 0.4951 - val_acc: 0.7662\n",
      "Epoch 739/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4505 - acc: 0.7997 - val_loss: 0.5184 - val_acc: 0.7468\n",
      "Epoch 740/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4583 - acc: 0.7818 - val_loss: 0.5010 - val_acc: 0.7792\n",
      "Epoch 741/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4529 - acc: 0.7964 - val_loss: 0.5005 - val_acc: 0.7597\n",
      "Epoch 742/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4518 - acc: 0.7899 - val_loss: 0.4940 - val_acc: 0.7662\n",
      "Epoch 743/850\n",
      "614/614 [==============================] - 0s 75us/sample - loss: 0.4512 - acc: 0.7850 - val_loss: 0.4938 - val_acc: 0.7727\n",
      "Epoch 744/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4509 - acc: 0.7883 - val_loss: 0.4981 - val_acc: 0.7792\n",
      "Epoch 745/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4506 - acc: 0.7866 - val_loss: 0.4971 - val_acc: 0.7727\n",
      "Epoch 746/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4505 - acc: 0.7866 - val_loss: 0.4942 - val_acc: 0.7662\n",
      "Epoch 747/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4538 - acc: 0.7883 - val_loss: 0.4948 - val_acc: 0.7857\n",
      "Epoch 748/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4513 - acc: 0.7850 - val_loss: 0.4951 - val_acc: 0.7727\n",
      "Epoch 749/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4515 - acc: 0.7899 - val_loss: 0.4933 - val_acc: 0.7792\n",
      "Epoch 750/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4590 - acc: 0.7834 - val_loss: 0.4962 - val_acc: 0.7792\n",
      "Epoch 751/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4510 - acc: 0.7899 - val_loss: 0.5029 - val_acc: 0.7727\n",
      "Epoch 752/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4577 - acc: 0.7834 - val_loss: 0.4937 - val_acc: 0.7662\n",
      "Epoch 753/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4609 - acc: 0.7720 - val_loss: 0.4964 - val_acc: 0.7727\n",
      "Epoch 754/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4492 - acc: 0.7785 - val_loss: 0.4964 - val_acc: 0.7662\n",
      "Epoch 755/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4589 - acc: 0.7899 - val_loss: 0.4942 - val_acc: 0.7792\n",
      "Epoch 756/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4587 - acc: 0.7818 - val_loss: 0.4954 - val_acc: 0.7727\n",
      "Epoch 757/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4661 - acc: 0.7752 - val_loss: 0.5034 - val_acc: 0.7597\n",
      "Epoch 758/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4494 - acc: 0.7915 - val_loss: 0.4937 - val_acc: 0.7662\n",
      "Epoch 759/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4521 - acc: 0.7850 - val_loss: 0.4965 - val_acc: 0.7597\n",
      "Epoch 760/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4504 - acc: 0.7899 - val_loss: 0.4965 - val_acc: 0.7727\n",
      "Epoch 761/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4505 - acc: 0.7899 - val_loss: 0.4926 - val_acc: 0.7662\n",
      "Epoch 762/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4540 - acc: 0.7980 - val_loss: 0.5110 - val_acc: 0.7662\n",
      "Epoch 763/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4582 - acc: 0.7834 - val_loss: 0.4947 - val_acc: 0.7662\n",
      "Epoch 764/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4711 - acc: 0.7915 - val_loss: 0.5353 - val_acc: 0.7792\n",
      "Epoch 765/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4694 - acc: 0.7997 - val_loss: 0.5006 - val_acc: 0.7597\n",
      "Epoch 766/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4520 - acc: 0.7980 - val_loss: 0.5071 - val_acc: 0.7597\n",
      "Epoch 767/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4522 - acc: 0.7834 - val_loss: 0.4957 - val_acc: 0.7727\n",
      "Epoch 768/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4538 - acc: 0.7866 - val_loss: 0.4986 - val_acc: 0.7727\n",
      "Epoch 769/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4509 - acc: 0.7883 - val_loss: 0.4944 - val_acc: 0.7597\n",
      "Epoch 770/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4517 - acc: 0.7883 - val_loss: 0.4973 - val_acc: 0.7662\n",
      "Epoch 771/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4535 - acc: 0.7834 - val_loss: 0.5016 - val_acc: 0.7597\n",
      "Epoch 772/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4497 - acc: 0.7883 - val_loss: 0.4979 - val_acc: 0.7662\n",
      "Epoch 773/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4501 - acc: 0.7850 - val_loss: 0.4976 - val_acc: 0.7662\n",
      "Epoch 774/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4570 - acc: 0.7850 - val_loss: 0.5014 - val_acc: 0.7597\n",
      "Epoch 775/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4501 - acc: 0.7980 - val_loss: 0.4948 - val_acc: 0.7727\n",
      "Epoch 776/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4500 - acc: 0.7883 - val_loss: 0.4948 - val_acc: 0.7662\n",
      "Epoch 777/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4524 - acc: 0.7866 - val_loss: 0.4976 - val_acc: 0.7597\n",
      "Epoch 778/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4506 - acc: 0.7834 - val_loss: 0.4953 - val_acc: 0.7792\n",
      "Epoch 779/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4528 - acc: 0.7883 - val_loss: 0.4947 - val_acc: 0.7662\n",
      "Epoch 780/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4532 - acc: 0.7948 - val_loss: 0.5032 - val_acc: 0.7727\n",
      "Epoch 781/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4523 - acc: 0.7866 - val_loss: 0.4953 - val_acc: 0.7597\n",
      "Epoch 782/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4559 - acc: 0.7932 - val_loss: 0.4961 - val_acc: 0.7532\n",
      "Epoch 783/850\n",
      "614/614 [==============================] - 0s 77us/sample - loss: 0.4547 - acc: 0.7883 - val_loss: 0.5010 - val_acc: 0.7662\n",
      "Epoch 784/850\n",
      "614/614 [==============================] - 0s 77us/sample - loss: 0.4486 - acc: 0.7932 - val_loss: 0.4970 - val_acc: 0.7792\n",
      "Epoch 785/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4544 - acc: 0.7785 - val_loss: 0.4959 - val_acc: 0.7727\n",
      "Epoch 786/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4497 - acc: 0.7834 - val_loss: 0.4991 - val_acc: 0.7597\n",
      "Epoch 787/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4524 - acc: 0.7850 - val_loss: 0.5030 - val_acc: 0.7857\n",
      "Epoch 788/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4565 - acc: 0.7834 - val_loss: 0.4985 - val_acc: 0.7597\n",
      "Epoch 789/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4503 - acc: 0.7866 - val_loss: 0.4977 - val_acc: 0.7662\n",
      "Epoch 790/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4511 - acc: 0.7850 - val_loss: 0.4991 - val_acc: 0.7792\n",
      "Epoch 791/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4500 - acc: 0.7866 - val_loss: 0.5009 - val_acc: 0.7597\n",
      "Epoch 792/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4510 - acc: 0.7866 - val_loss: 0.4955 - val_acc: 0.7597\n",
      "Epoch 793/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4510 - acc: 0.7899 - val_loss: 0.4948 - val_acc: 0.7532\n",
      "Epoch 794/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4521 - acc: 0.7899 - val_loss: 0.4960 - val_acc: 0.7597\n",
      "Epoch 795/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4542 - acc: 0.7834 - val_loss: 0.5004 - val_acc: 0.7597\n",
      "Epoch 796/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4608 - acc: 0.7769 - val_loss: 0.5243 - val_acc: 0.7403\n",
      "Epoch 797/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4658 - acc: 0.7834 - val_loss: 0.5029 - val_acc: 0.7662\n",
      "Epoch 798/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4508 - acc: 0.8013 - val_loss: 0.5003 - val_acc: 0.7532\n",
      "Epoch 799/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4587 - acc: 0.7866 - val_loss: 0.4948 - val_acc: 0.7727\n",
      "Epoch 800/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4550 - acc: 0.7818 - val_loss: 0.4980 - val_acc: 0.7792\n",
      "Epoch 801/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4546 - acc: 0.7850 - val_loss: 0.4950 - val_acc: 0.7727\n",
      "Epoch 802/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4524 - acc: 0.7899 - val_loss: 0.4990 - val_acc: 0.7597\n",
      "Epoch 803/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4529 - acc: 0.7915 - val_loss: 0.5031 - val_acc: 0.7857\n",
      "Epoch 804/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4645 - acc: 0.7801 - val_loss: 0.5024 - val_acc: 0.7792\n",
      "Epoch 805/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4618 - acc: 0.7834 - val_loss: 0.5002 - val_acc: 0.7857\n",
      "Epoch 806/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4581 - acc: 0.7883 - val_loss: 0.4957 - val_acc: 0.7662\n",
      "Epoch 807/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4545 - acc: 0.7915 - val_loss: 0.4929 - val_acc: 0.7662\n",
      "Epoch 808/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4590 - acc: 0.7899 - val_loss: 0.4989 - val_acc: 0.7532\n",
      "Epoch 809/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4505 - acc: 0.7801 - val_loss: 0.5001 - val_acc: 0.7792\n",
      "Epoch 810/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4563 - acc: 0.7850 - val_loss: 0.4985 - val_acc: 0.7597\n",
      "Epoch 811/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4521 - acc: 0.7834 - val_loss: 0.4983 - val_acc: 0.7857\n",
      "Epoch 812/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4665 - acc: 0.7769 - val_loss: 0.4938 - val_acc: 0.7662\n",
      "Epoch 813/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4509 - acc: 0.7883 - val_loss: 0.4990 - val_acc: 0.7597\n",
      "Epoch 814/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4507 - acc: 0.7883 - val_loss: 0.4986 - val_acc: 0.7727\n",
      "Epoch 815/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4524 - acc: 0.7866 - val_loss: 0.4950 - val_acc: 0.7597\n",
      "Epoch 816/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4555 - acc: 0.7818 - val_loss: 0.4967 - val_acc: 0.7662\n",
      "Epoch 817/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4513 - acc: 0.8029 - val_loss: 0.5102 - val_acc: 0.7532\n",
      "Epoch 818/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4553 - acc: 0.7964 - val_loss: 0.5052 - val_acc: 0.7922\n",
      "Epoch 819/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4564 - acc: 0.7899 - val_loss: 0.4947 - val_acc: 0.7727\n",
      "Epoch 820/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4496 - acc: 0.7980 - val_loss: 0.5150 - val_acc: 0.7662\n",
      "Epoch 821/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4552 - acc: 0.7866 - val_loss: 0.4936 - val_acc: 0.7597\n",
      "Epoch 822/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4597 - acc: 0.7915 - val_loss: 0.5173 - val_acc: 0.7468\n",
      "Epoch 823/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4527 - acc: 0.7915 - val_loss: 0.4976 - val_acc: 0.7727\n",
      "Epoch 824/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4535 - acc: 0.7801 - val_loss: 0.4968 - val_acc: 0.7662\n",
      "Epoch 825/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4506 - acc: 0.7801 - val_loss: 0.4998 - val_acc: 0.7662\n",
      "Epoch 826/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4528 - acc: 0.7866 - val_loss: 0.4971 - val_acc: 0.7857\n",
      "Epoch 827/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4522 - acc: 0.7818 - val_loss: 0.4931 - val_acc: 0.7727\n",
      "Epoch 828/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4715 - acc: 0.7785 - val_loss: 0.5191 - val_acc: 0.7468\n",
      "Epoch 829/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4543 - acc: 0.7948 - val_loss: 0.4984 - val_acc: 0.7597\n",
      "Epoch 830/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4520 - acc: 0.7899 - val_loss: 0.4993 - val_acc: 0.7597\n",
      "Epoch 831/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4578 - acc: 0.7801 - val_loss: 0.5008 - val_acc: 0.7857\n",
      "Epoch 832/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4508 - acc: 0.7866 - val_loss: 0.4925 - val_acc: 0.7662\n",
      "Epoch 833/850\n",
      "614/614 [==============================] - 0s 68us/sample - loss: 0.4511 - acc: 0.7932 - val_loss: 0.5067 - val_acc: 0.7597\n",
      "Epoch 834/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4541 - acc: 0.7801 - val_loss: 0.5011 - val_acc: 0.7857\n",
      "Epoch 835/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4569 - acc: 0.7785 - val_loss: 0.4951 - val_acc: 0.7662\n",
      "Epoch 836/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4550 - acc: 0.7850 - val_loss: 0.4961 - val_acc: 0.7792\n",
      "Epoch 837/850\n",
      "614/614 [==============================] - 0s 76us/sample - loss: 0.4507 - acc: 0.7818 - val_loss: 0.4993 - val_acc: 0.7792\n",
      "Epoch 838/850\n",
      "614/614 [==============================] - 0s 72us/sample - loss: 0.4556 - acc: 0.7834 - val_loss: 0.5035 - val_acc: 0.7792\n",
      "Epoch 839/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4643 - acc: 0.7785 - val_loss: 0.4953 - val_acc: 0.7727\n",
      "Epoch 840/850\n",
      "614/614 [==============================] - 0s 73us/sample - loss: 0.4563 - acc: 0.7818 - val_loss: 0.5003 - val_acc: 0.7857\n",
      "Epoch 841/850\n",
      "614/614 [==============================] - 0s 74us/sample - loss: 0.4550 - acc: 0.7899 - val_loss: 0.4938 - val_acc: 0.7727\n",
      "Epoch 842/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4530 - acc: 0.7899 - val_loss: 0.4956 - val_acc: 0.7662\n",
      "Epoch 843/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4633 - acc: 0.7915 - val_loss: 0.5011 - val_acc: 0.7792\n",
      "Epoch 844/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4522 - acc: 0.7915 - val_loss: 0.4974 - val_acc: 0.7662\n",
      "Epoch 845/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4640 - acc: 0.7948 - val_loss: 0.5049 - val_acc: 0.7662\n",
      "Epoch 846/850\n",
      "614/614 [==============================] - 0s 69us/sample - loss: 0.4621 - acc: 0.7883 - val_loss: 0.5213 - val_acc: 0.7468\n",
      "Epoch 847/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4632 - acc: 0.7883 - val_loss: 0.5014 - val_acc: 0.7532\n",
      "Epoch 848/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4518 - acc: 0.7980 - val_loss: 0.4986 - val_acc: 0.7792\n",
      "Epoch 849/850\n",
      "614/614 [==============================] - 0s 71us/sample - loss: 0.4517 - acc: 0.7883 - val_loss: 0.5191 - val_acc: 0.7532\n",
      "Epoch 850/850\n",
      "614/614 [==============================] - 0s 70us/sample - loss: 0.4611 - acc: 0.7801 - val_loss: 0.5015 - val_acc: 0.7727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7ef9d10908>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=850, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3489583333333333"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting never diabetes\n",
    "sum(y) / len(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 126us/sample - loss: 0.5750 - acc: 0.7305\n",
      "acc: 73.046875\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X,y)\n",
    "print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zHYB7k9q3O8T"
   },
   "source": [
    "### Unstable Results\n",
    "\n",
    "You'll notice that if we rerun the results might differ from the origin run. This can be explain by a bunch of factors. Check out some of them in this article: \n",
    "\n",
    "<https://machinelearningmastery.com/randomness-in-machine-learning/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to leverage the Keras `Sequential`api to estimate a feed forward neural networks on a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ueDVpctAzvy8"
   },
   "source": [
    "# Choosing Architecture (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Choosing an architecture for a neural network is almost more an art than a science. Let's do a few experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6W2Sc7-LzQo_",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"3LayerJunk\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Dense1 (Dense)               (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 49\n",
      "Trainable params: 49\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Tell me your ideas\n",
    "\n",
    "model_improved = Sequential(name=\"3LayerJunk\")\n",
    "\n",
    "model_improved.add(Dense(4, input_dim=8, activation='sigmoid', name=\"Dense1\"))\n",
    "model_improved.add(Dense(2, activation='relu'))\n",
    "model_improved.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_improved.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Let's inspect our new architecture\n",
    "model_improved.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7f09a63400>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_improved.fit(X, y, epochs=200, verbose=False) # What parameters can I specify here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 244us/sample - loss: 0.5898 - acc: 0.6510\n",
      "acc: 65.10416865348816\n"
     ]
    }
   ],
   "source": [
    "scores = model_improved.evaluate(X,y)\n",
    "print(f\"{model_improved.metrics_names[1]}: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with 1 Additional Layer & Change Activation F(x)s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"4LayerJunk\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Dense1 (Dense)               (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 3)                 15        \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 67\n",
      "Trainable params: 67\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Tell me your ideas\n",
    "\n",
    "model_improved = Sequential(name=\"4LayerJunk\")\n",
    "\n",
    "model_improved.add(Dense(4, input_dim=8, activation='relu', name=\"Dense1\"))\n",
    "model_improved.add(Dense(3, activation='relu'))\n",
    "model_improved.add(Dense(3, activation='relu'))\n",
    "model_improved.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_improved.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Let's inspect our new architecture\n",
    "model_improved.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7f0870c860>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_improved.fit(X,y, epochs=250, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 262us/sample - loss: 0.5175 - acc: 0.7474\n",
      "acc: 65.10416865348816\n"
     ]
    }
   ],
   "source": [
    "model_improved.evaluate(X,y)\n",
    "print(f\"{model_improved.metrics_names[1]}: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Experiment with Identical Neuron Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"EvenNuerons4LayerJunk\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Dense1 (Dense)               (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 345\n",
      "Trainable params: 345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Tell me your ideas\n",
    "\n",
    "model_improved = Sequential(name=\"EvenNuerons4LayerJunk\")\n",
    "\n",
    "model_improved.add(Dense(8, input_dim=8, activation='relu', name=\"Dense1\"))\n",
    "model_improved.add(Dense(12, activation='relu'))\n",
    "model_improved.add(Dense(8, activation='relu'))\n",
    "model_improved.add(Dense(6, activation='relu'))\n",
    "model_improved.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_improved.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Let's inspect our new architecture\n",
    "model_improved.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 691 samples, validate on 77 samples\n",
      "Epoch 1/300\n",
      "691/691 [==============================] - 1s 1ms/sample - loss: 1.8184 - acc: 0.6643 - val_loss: 1.2766 - val_acc: 0.5714\n",
      "Epoch 2/300\n",
      "691/691 [==============================] - 0s 82us/sample - loss: 0.9270 - acc: 0.6541 - val_loss: 0.7721 - val_acc: 0.5844\n",
      "Epoch 3/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.6905 - acc: 0.6512 - val_loss: 0.6894 - val_acc: 0.5584\n",
      "Epoch 4/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.6574 - acc: 0.6165 - val_loss: 0.6853 - val_acc: 0.5714\n",
      "Epoch 5/300\n",
      "691/691 [==============================] - 0s 78us/sample - loss: 0.6386 - acc: 0.6454 - val_loss: 0.6944 - val_acc: 0.6104\n",
      "Epoch 6/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.6312 - acc: 0.6570 - val_loss: 0.6895 - val_acc: 0.6104\n",
      "Epoch 7/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.6216 - acc: 0.6643 - val_loss: 0.6958 - val_acc: 0.6234\n",
      "Epoch 8/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.6152 - acc: 0.6599 - val_loss: 0.7018 - val_acc: 0.5974\n",
      "Epoch 9/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.6105 - acc: 0.6411 - val_loss: 0.7026 - val_acc: 0.6104\n",
      "Epoch 10/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.6078 - acc: 0.6744 - val_loss: 0.7071 - val_acc: 0.6234\n",
      "Epoch 11/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.6052 - acc: 0.6700 - val_loss: 0.7064 - val_acc: 0.5844\n",
      "Epoch 12/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.6018 - acc: 0.6874 - val_loss: 0.7111 - val_acc: 0.6234\n",
      "Epoch 13/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5999 - acc: 0.6816 - val_loss: 0.7132 - val_acc: 0.6234\n",
      "Epoch 14/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.6003 - acc: 0.6802 - val_loss: 0.7055 - val_acc: 0.6494\n",
      "Epoch 15/300\n",
      "691/691 [==============================] - 0s 77us/sample - loss: 0.5952 - acc: 0.6831 - val_loss: 0.7076 - val_acc: 0.6494\n",
      "Epoch 16/300\n",
      "691/691 [==============================] - 0s 78us/sample - loss: 0.5948 - acc: 0.6729 - val_loss: 0.7088 - val_acc: 0.6623\n",
      "Epoch 17/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5887 - acc: 0.6932 - val_loss: 0.7048 - val_acc: 0.6753\n",
      "Epoch 18/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5882 - acc: 0.6932 - val_loss: 0.7090 - val_acc: 0.6234\n",
      "Epoch 19/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5872 - acc: 0.6961 - val_loss: 0.7046 - val_acc: 0.6753\n",
      "Epoch 20/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5855 - acc: 0.6932 - val_loss: 0.7002 - val_acc: 0.6494\n",
      "Epoch 21/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5824 - acc: 0.6975 - val_loss: 0.6969 - val_acc: 0.6364\n",
      "Epoch 22/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5823 - acc: 0.6816 - val_loss: 0.7037 - val_acc: 0.6364\n",
      "Epoch 23/300\n",
      "691/691 [==============================] - 0s 78us/sample - loss: 0.5793 - acc: 0.6932 - val_loss: 0.7037 - val_acc: 0.6364\n",
      "Epoch 24/300\n",
      "691/691 [==============================] - 0s 78us/sample - loss: 0.5821 - acc: 0.6961 - val_loss: 0.6864 - val_acc: 0.6234\n",
      "Epoch 25/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5870 - acc: 0.6975 - val_loss: 0.6972 - val_acc: 0.6234\n",
      "Epoch 26/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5822 - acc: 0.7019 - val_loss: 0.6848 - val_acc: 0.6234\n",
      "Epoch 27/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5757 - acc: 0.6946 - val_loss: 0.6935 - val_acc: 0.6364\n",
      "Epoch 28/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5792 - acc: 0.6932 - val_loss: 0.6821 - val_acc: 0.6494\n",
      "Epoch 29/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5736 - acc: 0.6918 - val_loss: 0.6963 - val_acc: 0.6364\n",
      "Epoch 30/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5725 - acc: 0.6990 - val_loss: 0.6758 - val_acc: 0.6104\n",
      "Epoch 31/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5708 - acc: 0.6975 - val_loss: 0.7000 - val_acc: 0.6494\n",
      "Epoch 32/300\n",
      "691/691 [==============================] - 0s 84us/sample - loss: 0.5749 - acc: 0.6889 - val_loss: 0.6859 - val_acc: 0.6623\n",
      "Epoch 33/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5691 - acc: 0.6961 - val_loss: 0.6797 - val_acc: 0.6494\n",
      "Epoch 34/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5747 - acc: 0.6932 - val_loss: 0.7082 - val_acc: 0.6234\n",
      "Epoch 35/300\n",
      "691/691 [==============================] - 0s 76us/sample - loss: 0.5694 - acc: 0.6932 - val_loss: 0.6721 - val_acc: 0.6364\n",
      "Epoch 36/300\n",
      "691/691 [==============================] - 0s 82us/sample - loss: 0.5663 - acc: 0.6975 - val_loss: 0.6738 - val_acc: 0.6364\n",
      "Epoch 37/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5667 - acc: 0.6860 - val_loss: 0.6790 - val_acc: 0.6623\n",
      "Epoch 38/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5676 - acc: 0.6946 - val_loss: 0.7053 - val_acc: 0.6364\n",
      "Epoch 39/300\n",
      "691/691 [==============================] - 0s 77us/sample - loss: 0.5785 - acc: 0.6946 - val_loss: 0.6712 - val_acc: 0.6104\n",
      "Epoch 40/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5655 - acc: 0.6975 - val_loss: 0.6772 - val_acc: 0.6494\n",
      "Epoch 41/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5649 - acc: 0.7019 - val_loss: 0.6889 - val_acc: 0.6494\n",
      "Epoch 42/300\n",
      "691/691 [==============================] - 0s 83us/sample - loss: 0.5645 - acc: 0.6946 - val_loss: 0.6707 - val_acc: 0.6234\n",
      "Epoch 43/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5639 - acc: 0.7004 - val_loss: 0.7026 - val_acc: 0.6494\n",
      "Epoch 44/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5646 - acc: 0.6990 - val_loss: 0.6740 - val_acc: 0.6234\n",
      "Epoch 45/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5586 - acc: 0.6918 - val_loss: 0.6820 - val_acc: 0.6364\n",
      "Epoch 46/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5601 - acc: 0.6961 - val_loss: 0.6792 - val_acc: 0.6623\n",
      "Epoch 47/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5582 - acc: 0.6975 - val_loss: 0.6701 - val_acc: 0.6234\n",
      "Epoch 48/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5560 - acc: 0.7004 - val_loss: 0.6801 - val_acc: 0.6623\n",
      "Epoch 49/300\n",
      "691/691 [==============================] - 0s 76us/sample - loss: 0.5566 - acc: 0.6975 - val_loss: 0.6915 - val_acc: 0.6364\n",
      "Epoch 50/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5585 - acc: 0.7004 - val_loss: 0.6766 - val_acc: 0.6364\n",
      "Epoch 51/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5597 - acc: 0.6889 - val_loss: 0.6730 - val_acc: 0.6494\n",
      "Epoch 52/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5571 - acc: 0.6961 - val_loss: 0.6848 - val_acc: 0.6364\n",
      "Epoch 53/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5507 - acc: 0.6932 - val_loss: 0.6560 - val_acc: 0.6104\n",
      "Epoch 54/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5604 - acc: 0.6946 - val_loss: 0.6978 - val_acc: 0.6364\n",
      "Epoch 55/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5552 - acc: 0.7077 - val_loss: 0.6715 - val_acc: 0.6364\n",
      "Epoch 56/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5578 - acc: 0.7106 - val_loss: 0.6929 - val_acc: 0.6364\n",
      "Epoch 57/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5514 - acc: 0.7106 - val_loss: 0.6602 - val_acc: 0.6104\n",
      "Epoch 58/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5516 - acc: 0.7062 - val_loss: 0.6901 - val_acc: 0.6494\n",
      "Epoch 59/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5494 - acc: 0.6918 - val_loss: 0.6584 - val_acc: 0.6234\n",
      "Epoch 60/300\n",
      "691/691 [==============================] - 0s 76us/sample - loss: 0.5525 - acc: 0.7033 - val_loss: 0.6851 - val_acc: 0.6494\n",
      "Epoch 61/300\n",
      "691/691 [==============================] - 0s 78us/sample - loss: 0.5522 - acc: 0.6990 - val_loss: 0.6763 - val_acc: 0.6364\n",
      "Epoch 62/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5482 - acc: 0.7019 - val_loss: 0.6789 - val_acc: 0.6494\n",
      "Epoch 63/300\n",
      "691/691 [==============================] - 0s 82us/sample - loss: 0.5482 - acc: 0.7048 - val_loss: 0.6671 - val_acc: 0.6364\n",
      "Epoch 64/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5470 - acc: 0.7004 - val_loss: 0.6643 - val_acc: 0.6494\n",
      "Epoch 65/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5455 - acc: 0.7004 - val_loss: 0.6540 - val_acc: 0.6234\n",
      "Epoch 66/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5533 - acc: 0.7019 - val_loss: 0.6736 - val_acc: 0.6234\n",
      "Epoch 67/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5480 - acc: 0.7106 - val_loss: 0.6605 - val_acc: 0.6494\n",
      "Epoch 68/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5512 - acc: 0.6961 - val_loss: 0.6498 - val_acc: 0.6234\n",
      "Epoch 69/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5474 - acc: 0.7091 - val_loss: 0.6799 - val_acc: 0.6364\n",
      "Epoch 70/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5481 - acc: 0.7062 - val_loss: 0.6654 - val_acc: 0.6623\n",
      "Epoch 71/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5456 - acc: 0.7106 - val_loss: 0.6767 - val_acc: 0.6234\n",
      "Epoch 72/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5414 - acc: 0.7120 - val_loss: 0.6588 - val_acc: 0.6234\n",
      "Epoch 73/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5428 - acc: 0.7077 - val_loss: 0.6874 - val_acc: 0.6623\n",
      "Epoch 74/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5388 - acc: 0.7033 - val_loss: 0.6591 - val_acc: 0.6104\n",
      "Epoch 75/300\n",
      "691/691 [==============================] - 0s 78us/sample - loss: 0.5409 - acc: 0.7120 - val_loss: 0.6689 - val_acc: 0.6494\n",
      "Epoch 76/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5405 - acc: 0.7048 - val_loss: 0.6696 - val_acc: 0.6364\n",
      "Epoch 77/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5382 - acc: 0.7192 - val_loss: 0.6870 - val_acc: 0.6494\n",
      "Epoch 78/300\n",
      "691/691 [==============================] - 0s 77us/sample - loss: 0.5406 - acc: 0.7033 - val_loss: 0.6586 - val_acc: 0.6494\n",
      "Epoch 79/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5398 - acc: 0.7164 - val_loss: 0.6592 - val_acc: 0.6234\n",
      "Epoch 80/300\n",
      "691/691 [==============================] - 0s 83us/sample - loss: 0.5369 - acc: 0.7062 - val_loss: 0.6705 - val_acc: 0.6494\n",
      "Epoch 81/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5389 - acc: 0.7077 - val_loss: 0.6691 - val_acc: 0.6104\n",
      "Epoch 82/300\n",
      "691/691 [==============================] - 0s 78us/sample - loss: 0.5421 - acc: 0.6975 - val_loss: 0.6500 - val_acc: 0.6364\n",
      "Epoch 83/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5414 - acc: 0.7091 - val_loss: 0.6595 - val_acc: 0.5974\n",
      "Epoch 84/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5436 - acc: 0.7192 - val_loss: 0.6788 - val_acc: 0.6364\n",
      "Epoch 85/300\n",
      "691/691 [==============================] - 0s 77us/sample - loss: 0.5346 - acc: 0.7062 - val_loss: 0.6497 - val_acc: 0.6364\n",
      "Epoch 86/300\n",
      "691/691 [==============================] - 0s 83us/sample - loss: 0.5385 - acc: 0.7091 - val_loss: 0.6726 - val_acc: 0.6364\n",
      "Epoch 87/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5363 - acc: 0.7062 - val_loss: 0.6644 - val_acc: 0.6364\n",
      "Epoch 88/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5324 - acc: 0.7120 - val_loss: 0.6708 - val_acc: 0.6623\n",
      "Epoch 89/300\n",
      "691/691 [==============================] - 0s 78us/sample - loss: 0.5352 - acc: 0.7062 - val_loss: 0.6688 - val_acc: 0.6364\n",
      "Epoch 90/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5328 - acc: 0.7250 - val_loss: 0.6799 - val_acc: 0.6364\n",
      "Epoch 91/300\n",
      "691/691 [==============================] - 0s 77us/sample - loss: 0.5348 - acc: 0.7004 - val_loss: 0.6563 - val_acc: 0.6494\n",
      "Epoch 92/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5347 - acc: 0.7106 - val_loss: 0.6592 - val_acc: 0.5844\n",
      "Epoch 93/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5334 - acc: 0.7120 - val_loss: 0.6664 - val_acc: 0.6364\n",
      "Epoch 94/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5313 - acc: 0.7149 - val_loss: 0.6550 - val_acc: 0.6623\n",
      "Epoch 95/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5324 - acc: 0.7106 - val_loss: 0.6687 - val_acc: 0.6364\n",
      "Epoch 96/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5361 - acc: 0.7279 - val_loss: 0.6641 - val_acc: 0.6104\n",
      "Epoch 97/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5319 - acc: 0.7062 - val_loss: 0.6478 - val_acc: 0.6623\n",
      "Epoch 98/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5365 - acc: 0.7207 - val_loss: 0.7147 - val_acc: 0.6364\n",
      "Epoch 99/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5292 - acc: 0.7265 - val_loss: 0.6716 - val_acc: 0.6104\n",
      "Epoch 100/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5290 - acc: 0.7164 - val_loss: 0.6638 - val_acc: 0.6364\n",
      "Epoch 101/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5284 - acc: 0.7221 - val_loss: 0.6887 - val_acc: 0.6364\n",
      "Epoch 102/300\n",
      "691/691 [==============================] - 0s 84us/sample - loss: 0.5288 - acc: 0.7221 - val_loss: 0.6422 - val_acc: 0.6364\n",
      "Epoch 103/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5299 - acc: 0.7048 - val_loss: 0.6760 - val_acc: 0.6623\n",
      "Epoch 104/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5262 - acc: 0.7164 - val_loss: 0.6663 - val_acc: 0.5844\n",
      "Epoch 105/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5286 - acc: 0.7106 - val_loss: 0.6564 - val_acc: 0.6494\n",
      "Epoch 106/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5285 - acc: 0.7207 - val_loss: 0.6624 - val_acc: 0.6234\n",
      "Epoch 107/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5241 - acc: 0.7250 - val_loss: 0.6547 - val_acc: 0.6364\n",
      "Epoch 108/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5274 - acc: 0.7323 - val_loss: 0.6735 - val_acc: 0.6623\n",
      "Epoch 109/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5287 - acc: 0.7077 - val_loss: 0.6660 - val_acc: 0.6364\n",
      "Epoch 110/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5255 - acc: 0.7265 - val_loss: 0.7091 - val_acc: 0.6234\n",
      "Epoch 111/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5374 - acc: 0.7236 - val_loss: 0.6536 - val_acc: 0.6494\n",
      "Epoch 112/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5258 - acc: 0.7250 - val_loss: 0.6718 - val_acc: 0.6494\n",
      "Epoch 113/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5213 - acc: 0.7221 - val_loss: 0.6661 - val_acc: 0.6753\n",
      "Epoch 114/300\n",
      "691/691 [==============================] - 0s 77us/sample - loss: 0.5247 - acc: 0.7337 - val_loss: 0.6874 - val_acc: 0.6364\n",
      "Epoch 115/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5249 - acc: 0.7236 - val_loss: 0.6433 - val_acc: 0.6753\n",
      "Epoch 116/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5236 - acc: 0.7236 - val_loss: 0.6907 - val_acc: 0.5974\n",
      "Epoch 117/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5193 - acc: 0.7294 - val_loss: 0.6512 - val_acc: 0.6364\n",
      "Epoch 118/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5226 - acc: 0.7265 - val_loss: 0.6509 - val_acc: 0.6494\n",
      "Epoch 119/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5202 - acc: 0.7366 - val_loss: 0.6545 - val_acc: 0.6753\n",
      "Epoch 120/300\n",
      "691/691 [==============================] - 0s 77us/sample - loss: 0.5207 - acc: 0.7250 - val_loss: 0.6514 - val_acc: 0.6623\n",
      "Epoch 121/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5206 - acc: 0.7337 - val_loss: 0.6839 - val_acc: 0.6623\n",
      "Epoch 122/300\n",
      "691/691 [==============================] - 0s 76us/sample - loss: 0.5169 - acc: 0.7366 - val_loss: 0.6557 - val_acc: 0.6753\n",
      "Epoch 123/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5173 - acc: 0.7192 - val_loss: 0.7046 - val_acc: 0.6753\n",
      "Epoch 124/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5243 - acc: 0.7366 - val_loss: 0.6601 - val_acc: 0.6364\n",
      "Epoch 125/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5171 - acc: 0.7308 - val_loss: 0.6773 - val_acc: 0.6623\n",
      "Epoch 126/300\n",
      "691/691 [==============================] - 0s 84us/sample - loss: 0.5192 - acc: 0.7294 - val_loss: 0.6555 - val_acc: 0.6623\n",
      "Epoch 127/300\n",
      "691/691 [==============================] - 0s 87us/sample - loss: 0.5136 - acc: 0.7294 - val_loss: 0.6662 - val_acc: 0.6753\n",
      "Epoch 128/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5147 - acc: 0.7352 - val_loss: 0.6833 - val_acc: 0.6364\n",
      "Epoch 129/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5154 - acc: 0.7366 - val_loss: 0.6848 - val_acc: 0.6494\n",
      "Epoch 130/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5169 - acc: 0.7178 - val_loss: 0.6729 - val_acc: 0.6753\n",
      "Epoch 131/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5126 - acc: 0.7395 - val_loss: 0.6586 - val_acc: 0.6364\n",
      "Epoch 132/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5190 - acc: 0.7294 - val_loss: 0.6420 - val_acc: 0.6623\n",
      "Epoch 133/300\n",
      "691/691 [==============================] - 0s 82us/sample - loss: 0.5160 - acc: 0.7250 - val_loss: 0.6578 - val_acc: 0.6753\n",
      "Epoch 134/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5175 - acc: 0.7294 - val_loss: 0.6616 - val_acc: 0.6234\n",
      "Epoch 135/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5105 - acc: 0.7424 - val_loss: 0.6842 - val_acc: 0.6753\n",
      "Epoch 136/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5122 - acc: 0.7323 - val_loss: 0.6844 - val_acc: 0.6753\n",
      "Epoch 137/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5108 - acc: 0.7482 - val_loss: 0.6743 - val_acc: 0.6234\n",
      "Epoch 138/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5134 - acc: 0.7410 - val_loss: 0.6704 - val_acc: 0.6104\n",
      "Epoch 139/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5102 - acc: 0.7467 - val_loss: 0.6652 - val_acc: 0.6364\n",
      "Epoch 140/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5102 - acc: 0.7410 - val_loss: 0.6664 - val_acc: 0.6364\n",
      "Epoch 141/300\n",
      "691/691 [==============================] - 0s 84us/sample - loss: 0.5091 - acc: 0.7395 - val_loss: 0.6760 - val_acc: 0.6364\n",
      "Epoch 142/300\n",
      "691/691 [==============================] - 0s 87us/sample - loss: 0.5113 - acc: 0.7366 - val_loss: 0.6456 - val_acc: 0.6234\n",
      "Epoch 143/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5082 - acc: 0.7438 - val_loss: 0.6539 - val_acc: 0.6883\n",
      "Epoch 144/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5171 - acc: 0.7381 - val_loss: 0.7291 - val_acc: 0.5974\n",
      "Epoch 145/300\n",
      "691/691 [==============================] - 0s 83us/sample - loss: 0.5229 - acc: 0.7453 - val_loss: 0.6642 - val_acc: 0.6883\n",
      "Epoch 146/300\n",
      "691/691 [==============================] - 0s 83us/sample - loss: 0.5109 - acc: 0.7352 - val_loss: 0.6525 - val_acc: 0.6494\n",
      "Epoch 147/300\n",
      "691/691 [==============================] - 0s 82us/sample - loss: 0.5078 - acc: 0.7294 - val_loss: 0.6558 - val_acc: 0.6623\n",
      "Epoch 148/300\n",
      "691/691 [==============================] - 0s 82us/sample - loss: 0.5022 - acc: 0.7453 - val_loss: 0.6679 - val_acc: 0.6364\n",
      "Epoch 149/300\n",
      "691/691 [==============================] - 0s 78us/sample - loss: 0.5099 - acc: 0.7279 - val_loss: 0.6575 - val_acc: 0.6494\n",
      "Epoch 150/300\n",
      "691/691 [==============================] - 0s 82us/sample - loss: 0.5122 - acc: 0.7207 - val_loss: 0.6467 - val_acc: 0.6494\n",
      "Epoch 151/300\n",
      "691/691 [==============================] - 0s 77us/sample - loss: 0.5050 - acc: 0.7438 - val_loss: 0.6688 - val_acc: 0.6494\n",
      "Epoch 152/300\n",
      "691/691 [==============================] - 0s 83us/sample - loss: 0.5026 - acc: 0.7496 - val_loss: 0.6782 - val_acc: 0.6623\n",
      "Epoch 153/300\n",
      "691/691 [==============================] - 0s 94us/sample - loss: 0.5100 - acc: 0.7424 - val_loss: 0.6500 - val_acc: 0.6494\n",
      "Epoch 154/300\n",
      "691/691 [==============================] - 0s 89us/sample - loss: 0.5077 - acc: 0.7525 - val_loss: 0.6644 - val_acc: 0.6753\n",
      "Epoch 155/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5107 - acc: 0.7381 - val_loss: 0.6379 - val_acc: 0.6623\n",
      "Epoch 156/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.5079 - acc: 0.7467 - val_loss: 0.6902 - val_acc: 0.6494\n",
      "Epoch 157/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5081 - acc: 0.7381 - val_loss: 0.7004 - val_acc: 0.6494\n",
      "Epoch 158/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5030 - acc: 0.7627 - val_loss: 0.6472 - val_acc: 0.6494\n",
      "Epoch 159/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5042 - acc: 0.7395 - val_loss: 0.6613 - val_acc: 0.6623\n",
      "Epoch 160/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5016 - acc: 0.7525 - val_loss: 0.6855 - val_acc: 0.6753\n",
      "Epoch 161/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4988 - acc: 0.7627 - val_loss: 0.6767 - val_acc: 0.6494\n",
      "Epoch 162/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4974 - acc: 0.7656 - val_loss: 0.6714 - val_acc: 0.6494\n",
      "Epoch 163/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4980 - acc: 0.7583 - val_loss: 0.6640 - val_acc: 0.6623\n",
      "Epoch 164/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5014 - acc: 0.7554 - val_loss: 0.6712 - val_acc: 0.6494\n",
      "Epoch 165/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.5015 - acc: 0.7569 - val_loss: 0.6517 - val_acc: 0.6623\n",
      "Epoch 166/300\n",
      "691/691 [==============================] - 0s 84us/sample - loss: 0.5006 - acc: 0.7569 - val_loss: 0.6784 - val_acc: 0.6753\n",
      "Epoch 167/300\n",
      "691/691 [==============================] - 0s 82us/sample - loss: 0.5003 - acc: 0.7598 - val_loss: 0.6679 - val_acc: 0.6883\n",
      "Epoch 168/300\n",
      "691/691 [==============================] - 0s 83us/sample - loss: 0.5010 - acc: 0.7511 - val_loss: 0.6637 - val_acc: 0.6494\n",
      "Epoch 169/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4994 - acc: 0.7467 - val_loss: 0.6778 - val_acc: 0.6883\n",
      "Epoch 170/300\n",
      "691/691 [==============================] - 0s 82us/sample - loss: 0.5005 - acc: 0.7656 - val_loss: 0.6737 - val_acc: 0.6883\n",
      "Epoch 171/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.5103 - acc: 0.7641 - val_loss: 0.6729 - val_acc: 0.6494\n",
      "Epoch 172/300\n",
      "691/691 [==============================] - 0s 83us/sample - loss: 0.5087 - acc: 0.7598 - val_loss: 0.6757 - val_acc: 0.6883\n",
      "Epoch 173/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4953 - acc: 0.7728 - val_loss: 0.6856 - val_acc: 0.6623\n",
      "Epoch 174/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.4946 - acc: 0.7612 - val_loss: 0.6562 - val_acc: 0.7143\n",
      "Epoch 175/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4967 - acc: 0.7685 - val_loss: 0.6769 - val_acc: 0.6883\n",
      "Epoch 176/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4937 - acc: 0.7598 - val_loss: 0.6888 - val_acc: 0.6623\n",
      "Epoch 177/300\n",
      "691/691 [==============================] - 0s 83us/sample - loss: 0.4923 - acc: 0.7641 - val_loss: 0.7028 - val_acc: 0.6753\n",
      "Epoch 178/300\n",
      "691/691 [==============================] - 0s 84us/sample - loss: 0.4896 - acc: 0.7670 - val_loss: 0.6849 - val_acc: 0.6883\n",
      "Epoch 179/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.4887 - acc: 0.7627 - val_loss: 0.6566 - val_acc: 0.6883\n",
      "Epoch 180/300\n",
      "691/691 [==============================] - 0s 77us/sample - loss: 0.4938 - acc: 0.7699 - val_loss: 0.6924 - val_acc: 0.6883\n",
      "Epoch 181/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4931 - acc: 0.7670 - val_loss: 0.6795 - val_acc: 0.6623\n",
      "Epoch 182/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4874 - acc: 0.7627 - val_loss: 0.6775 - val_acc: 0.7013\n",
      "Epoch 183/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4944 - acc: 0.7569 - val_loss: 0.6968 - val_acc: 0.6623\n",
      "Epoch 184/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4878 - acc: 0.7583 - val_loss: 0.6687 - val_acc: 0.6883\n",
      "Epoch 185/300\n",
      "691/691 [==============================] - 0s 83us/sample - loss: 0.4902 - acc: 0.7583 - val_loss: 0.6713 - val_acc: 0.6883\n",
      "Epoch 186/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4937 - acc: 0.7757 - val_loss: 0.6785 - val_acc: 0.6494\n",
      "Epoch 187/300\n",
      "691/691 [==============================] - 0s 85us/sample - loss: 0.4876 - acc: 0.7627 - val_loss: 0.7100 - val_acc: 0.6753\n",
      "Epoch 188/300\n",
      "691/691 [==============================] - 0s 83us/sample - loss: 0.4930 - acc: 0.7569 - val_loss: 0.7197 - val_acc: 0.6623\n",
      "Epoch 189/300\n",
      "691/691 [==============================] - 0s 82us/sample - loss: 0.4895 - acc: 0.7656 - val_loss: 0.6782 - val_acc: 0.6753\n",
      "Epoch 190/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4839 - acc: 0.7612 - val_loss: 0.6901 - val_acc: 0.7013\n",
      "Epoch 191/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4911 - acc: 0.7540 - val_loss: 0.6782 - val_acc: 0.7013\n",
      "Epoch 192/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4839 - acc: 0.7742 - val_loss: 0.7023 - val_acc: 0.7143\n",
      "Epoch 193/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4876 - acc: 0.7583 - val_loss: 0.6814 - val_acc: 0.6623\n",
      "Epoch 194/300\n",
      "691/691 [==============================] - 0s 83us/sample - loss: 0.4870 - acc: 0.7713 - val_loss: 0.7053 - val_acc: 0.7013\n",
      "Epoch 195/300\n",
      "691/691 [==============================] - 0s 84us/sample - loss: 0.4846 - acc: 0.7583 - val_loss: 0.6697 - val_acc: 0.6623\n",
      "Epoch 196/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4816 - acc: 0.7670 - val_loss: 0.6795 - val_acc: 0.6623\n",
      "Epoch 197/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4851 - acc: 0.7569 - val_loss: 0.6709 - val_acc: 0.6753\n",
      "Epoch 198/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4848 - acc: 0.7713 - val_loss: 0.6552 - val_acc: 0.6883\n",
      "Epoch 199/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4821 - acc: 0.7728 - val_loss: 0.6836 - val_acc: 0.7532\n",
      "Epoch 200/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4887 - acc: 0.7713 - val_loss: 0.6831 - val_acc: 0.7013\n",
      "Epoch 201/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4865 - acc: 0.7670 - val_loss: 0.6917 - val_acc: 0.6753\n",
      "Epoch 202/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4813 - acc: 0.7699 - val_loss: 0.6892 - val_acc: 0.7143\n",
      "Epoch 203/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4810 - acc: 0.7742 - val_loss: 0.6762 - val_acc: 0.6883\n",
      "Epoch 204/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4823 - acc: 0.7699 - val_loss: 0.6815 - val_acc: 0.6753\n",
      "Epoch 205/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4849 - acc: 0.7685 - val_loss: 0.6953 - val_acc: 0.6494\n",
      "Epoch 206/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4801 - acc: 0.7786 - val_loss: 0.6798 - val_acc: 0.6753\n",
      "Epoch 207/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4796 - acc: 0.7641 - val_loss: 0.7077 - val_acc: 0.7013\n",
      "Epoch 208/300\n",
      "691/691 [==============================] - 0s 78us/sample - loss: 0.4752 - acc: 0.7627 - val_loss: 0.6977 - val_acc: 0.7143\n",
      "Epoch 209/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4833 - acc: 0.7627 - val_loss: 0.6863 - val_acc: 0.7143\n",
      "Epoch 210/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4746 - acc: 0.7685 - val_loss: 0.6711 - val_acc: 0.7013\n",
      "Epoch 211/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4732 - acc: 0.7699 - val_loss: 0.7083 - val_acc: 0.6883\n",
      "Epoch 212/300\n",
      "691/691 [==============================] - 0s 77us/sample - loss: 0.4788 - acc: 0.7699 - val_loss: 0.6962 - val_acc: 0.7013\n",
      "Epoch 213/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4881 - acc: 0.7713 - val_loss: 0.7057 - val_acc: 0.7143\n",
      "Epoch 214/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4759 - acc: 0.7786 - val_loss: 0.6806 - val_acc: 0.6494\n",
      "Epoch 215/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4822 - acc: 0.7829 - val_loss: 0.6738 - val_acc: 0.6753\n",
      "Epoch 216/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4821 - acc: 0.7829 - val_loss: 0.7575 - val_acc: 0.6623\n",
      "Epoch 217/300\n",
      "691/691 [==============================] - 0s 82us/sample - loss: 0.4797 - acc: 0.7757 - val_loss: 0.7064 - val_acc: 0.7013\n",
      "Epoch 218/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4733 - acc: 0.7656 - val_loss: 0.6713 - val_acc: 0.6753\n",
      "Epoch 219/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.4761 - acc: 0.7873 - val_loss: 0.6801 - val_acc: 0.6883\n",
      "Epoch 220/300\n",
      "691/691 [==============================] - 0s 83us/sample - loss: 0.4734 - acc: 0.7699 - val_loss: 0.6662 - val_acc: 0.6883\n",
      "Epoch 221/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4772 - acc: 0.7728 - val_loss: 0.7098 - val_acc: 0.6623\n",
      "Epoch 222/300\n",
      "691/691 [==============================] - 0s 82us/sample - loss: 0.4718 - acc: 0.7742 - val_loss: 0.6843 - val_acc: 0.7013\n",
      "Epoch 223/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4714 - acc: 0.7815 - val_loss: 0.6906 - val_acc: 0.7143\n",
      "Epoch 224/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.4720 - acc: 0.7742 - val_loss: 0.6836 - val_acc: 0.7273\n",
      "Epoch 225/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4744 - acc: 0.7713 - val_loss: 0.7404 - val_acc: 0.6883\n",
      "Epoch 226/300\n",
      "691/691 [==============================] - 0s 78us/sample - loss: 0.4780 - acc: 0.7742 - val_loss: 0.6879 - val_acc: 0.7273\n",
      "Epoch 227/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.4947 - acc: 0.7771 - val_loss: 0.6570 - val_acc: 0.6753\n",
      "Epoch 228/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4761 - acc: 0.7800 - val_loss: 0.6788 - val_acc: 0.7143\n",
      "Epoch 229/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4709 - acc: 0.7627 - val_loss: 0.7046 - val_acc: 0.7273\n",
      "Epoch 230/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4677 - acc: 0.7815 - val_loss: 0.6616 - val_acc: 0.6753\n",
      "Epoch 231/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4694 - acc: 0.7829 - val_loss: 0.6730 - val_acc: 0.7143\n",
      "Epoch 232/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4669 - acc: 0.7815 - val_loss: 0.6911 - val_acc: 0.6623\n",
      "Epoch 233/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4660 - acc: 0.7786 - val_loss: 0.7211 - val_acc: 0.7013\n",
      "Epoch 234/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.4670 - acc: 0.7858 - val_loss: 0.6781 - val_acc: 0.7013\n",
      "Epoch 235/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4639 - acc: 0.7873 - val_loss: 0.6861 - val_acc: 0.7013\n",
      "Epoch 236/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4730 - acc: 0.7554 - val_loss: 0.6582 - val_acc: 0.6623\n",
      "Epoch 237/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4709 - acc: 0.7670 - val_loss: 0.6703 - val_acc: 0.6753\n",
      "Epoch 238/300\n",
      "691/691 [==============================] - 0s 77us/sample - loss: 0.4678 - acc: 0.7771 - val_loss: 0.6796 - val_acc: 0.7143\n",
      "Epoch 239/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.4646 - acc: 0.7902 - val_loss: 0.6882 - val_acc: 0.7273\n",
      "Epoch 240/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.4657 - acc: 0.7815 - val_loss: 0.7144 - val_acc: 0.6753\n",
      "Epoch 241/300\n",
      "691/691 [==============================] - 0s 77us/sample - loss: 0.4737 - acc: 0.7713 - val_loss: 0.6853 - val_acc: 0.7143\n",
      "Epoch 242/300\n",
      "691/691 [==============================] - 0s 86us/sample - loss: 0.4691 - acc: 0.7771 - val_loss: 0.6748 - val_acc: 0.7013\n",
      "Epoch 243/300\n",
      "691/691 [==============================] - 0s 82us/sample - loss: 0.4667 - acc: 0.7829 - val_loss: 0.6880 - val_acc: 0.7143\n",
      "Epoch 244/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4647 - acc: 0.7945 - val_loss: 0.6650 - val_acc: 0.6623\n",
      "Epoch 245/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4646 - acc: 0.7786 - val_loss: 0.7133 - val_acc: 0.7013\n",
      "Epoch 246/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4661 - acc: 0.7858 - val_loss: 0.6717 - val_acc: 0.6883\n",
      "Epoch 247/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4667 - acc: 0.7757 - val_loss: 0.7034 - val_acc: 0.6753\n",
      "Epoch 248/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.4665 - acc: 0.7800 - val_loss: 0.7273 - val_acc: 0.7143\n",
      "Epoch 249/300\n",
      "691/691 [==============================] - 0s 77us/sample - loss: 0.4625 - acc: 0.7771 - val_loss: 0.6784 - val_acc: 0.7273\n",
      "Epoch 250/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.4638 - acc: 0.7757 - val_loss: 0.6981 - val_acc: 0.6883\n",
      "Epoch 251/300\n",
      "691/691 [==============================] - 0s 78us/sample - loss: 0.4619 - acc: 0.7902 - val_loss: 0.7114 - val_acc: 0.6753\n",
      "Epoch 252/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.4742 - acc: 0.7713 - val_loss: 0.6946 - val_acc: 0.7013\n",
      "Epoch 253/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4638 - acc: 0.7945 - val_loss: 0.6885 - val_acc: 0.7013\n",
      "Epoch 254/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4585 - acc: 0.7887 - val_loss: 0.6891 - val_acc: 0.7143\n",
      "Epoch 255/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4630 - acc: 0.7887 - val_loss: 0.6981 - val_acc: 0.7273\n",
      "Epoch 256/300\n",
      "691/691 [==============================] - 0s 81us/sample - loss: 0.4635 - acc: 0.7945 - val_loss: 0.6869 - val_acc: 0.7013\n",
      "Epoch 257/300\n",
      "691/691 [==============================] - 0s 82us/sample - loss: 0.4742 - acc: 0.7829 - val_loss: 0.6664 - val_acc: 0.7273\n",
      "Epoch 258/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4637 - acc: 0.7902 - val_loss: 0.6881 - val_acc: 0.7143\n",
      "Epoch 259/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4677 - acc: 0.7742 - val_loss: 0.7269 - val_acc: 0.6623\n",
      "Epoch 260/300\n",
      "691/691 [==============================] - 0s 83us/sample - loss: 0.4639 - acc: 0.7844 - val_loss: 0.6778 - val_acc: 0.7143\n",
      "Epoch 261/300\n",
      "691/691 [==============================] - 0s 82us/sample - loss: 0.4599 - acc: 0.7887 - val_loss: 0.6935 - val_acc: 0.6883\n",
      "Epoch 262/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4592 - acc: 0.7858 - val_loss: 0.6812 - val_acc: 0.7143\n",
      "Epoch 263/300\n",
      "691/691 [==============================] - 0s 77us/sample - loss: 0.4632 - acc: 0.7902 - val_loss: 0.6971 - val_acc: 0.7273\n",
      "Epoch 264/300\n",
      "691/691 [==============================] - 0s 83us/sample - loss: 0.4590 - acc: 0.7815 - val_loss: 0.6613 - val_acc: 0.7013\n",
      "Epoch 265/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4573 - acc: 0.7873 - val_loss: 0.6748 - val_acc: 0.6883\n",
      "Epoch 266/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4543 - acc: 0.7974 - val_loss: 0.6867 - val_acc: 0.7143\n",
      "Epoch 267/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4622 - acc: 0.7771 - val_loss: 0.6724 - val_acc: 0.7273\n",
      "Epoch 268/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4605 - acc: 0.7844 - val_loss: 0.6555 - val_acc: 0.7273\n",
      "Epoch 269/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4599 - acc: 0.7844 - val_loss: 0.6724 - val_acc: 0.6753\n",
      "Epoch 270/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4591 - acc: 0.7873 - val_loss: 0.6716 - val_acc: 0.7143\n",
      "Epoch 271/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4572 - acc: 0.7931 - val_loss: 0.6730 - val_acc: 0.7143\n",
      "Epoch 272/300\n",
      "691/691 [==============================] - 0s 82us/sample - loss: 0.4551 - acc: 0.7931 - val_loss: 0.6802 - val_acc: 0.7403\n",
      "Epoch 273/300\n",
      "691/691 [==============================] - 0s 78us/sample - loss: 0.4575 - acc: 0.7988 - val_loss: 0.6649 - val_acc: 0.7273\n",
      "Epoch 274/300\n",
      "691/691 [==============================] - 0s 83us/sample - loss: 0.4550 - acc: 0.7931 - val_loss: 0.6744 - val_acc: 0.7273\n",
      "Epoch 275/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4559 - acc: 0.7844 - val_loss: 0.6980 - val_acc: 0.6883\n",
      "Epoch 276/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4558 - acc: 0.7771 - val_loss: 0.6895 - val_acc: 0.7143\n",
      "Epoch 277/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4617 - acc: 0.7873 - val_loss: 0.6785 - val_acc: 0.7143\n",
      "Epoch 278/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4568 - acc: 0.8017 - val_loss: 0.6669 - val_acc: 0.7013\n",
      "Epoch 279/300\n",
      "691/691 [==============================] - 0s 78us/sample - loss: 0.4571 - acc: 0.7902 - val_loss: 0.6607 - val_acc: 0.6753\n",
      "Epoch 280/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4629 - acc: 0.7873 - val_loss: 0.6988 - val_acc: 0.7403\n",
      "Epoch 281/300\n",
      "691/691 [==============================] - 0s 77us/sample - loss: 0.4555 - acc: 0.7916 - val_loss: 0.6996 - val_acc: 0.7143\n",
      "Epoch 282/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4598 - acc: 0.7959 - val_loss: 0.6701 - val_acc: 0.7143\n",
      "Epoch 283/300\n",
      "691/691 [==============================] - 0s 77us/sample - loss: 0.4586 - acc: 0.7887 - val_loss: 0.6382 - val_acc: 0.7273\n",
      "Epoch 284/300\n",
      "691/691 [==============================] - 0s 83us/sample - loss: 0.4551 - acc: 0.8003 - val_loss: 0.7080 - val_acc: 0.7143\n",
      "Epoch 285/300\n",
      "691/691 [==============================] - 0s 83us/sample - loss: 0.4744 - acc: 0.7670 - val_loss: 0.6610 - val_acc: 0.7013\n",
      "Epoch 286/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4583 - acc: 0.7873 - val_loss: 0.6559 - val_acc: 0.7403\n",
      "Epoch 287/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4592 - acc: 0.7815 - val_loss: 0.7056 - val_acc: 0.7273\n",
      "Epoch 288/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4516 - acc: 0.7786 - val_loss: 0.6977 - val_acc: 0.7013\n",
      "Epoch 289/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4606 - acc: 0.7902 - val_loss: 0.6621 - val_acc: 0.7013\n",
      "Epoch 290/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4509 - acc: 0.7959 - val_loss: 0.6865 - val_acc: 0.7273\n",
      "Epoch 291/300\n",
      "691/691 [==============================] - 0s 79us/sample - loss: 0.4507 - acc: 0.7916 - val_loss: 0.6616 - val_acc: 0.7273\n",
      "Epoch 292/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4628 - acc: 0.7685 - val_loss: 0.6414 - val_acc: 0.7013\n",
      "Epoch 293/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4496 - acc: 0.7988 - val_loss: 0.7021 - val_acc: 0.7273\n",
      "Epoch 294/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4503 - acc: 0.7959 - val_loss: 0.6738 - val_acc: 0.7143\n",
      "Epoch 295/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4593 - acc: 0.7902 - val_loss: 0.6590 - val_acc: 0.6883\n",
      "Epoch 296/300\n",
      "691/691 [==============================] - 0s 78us/sample - loss: 0.4524 - acc: 0.7945 - val_loss: 0.6839 - val_acc: 0.7403\n",
      "Epoch 297/300\n",
      "691/691 [==============================] - 0s 83us/sample - loss: 0.4544 - acc: 0.7931 - val_loss: 0.6975 - val_acc: 0.7273\n",
      "Epoch 298/300\n",
      "691/691 [==============================] - 0s 78us/sample - loss: 0.4584 - acc: 0.7844 - val_loss: 0.6675 - val_acc: 0.6623\n",
      "Epoch 299/300\n",
      "691/691 [==============================] - 0s 80us/sample - loss: 0.4565 - acc: 0.7887 - val_loss: 0.6614 - val_acc: 0.7403\n",
      "Epoch 300/300\n",
      "691/691 [==============================] - 0s 78us/sample - loss: 0.4549 - acc: 0.7887 - val_loss: 0.6779 - val_acc: 0.7403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7efaa21208>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_improved.fit(X,y, epochs=300, validation_split=.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 161us/sample - loss: 0.4706 - accuracy: 0.7630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4735589859386285, 0.7630208]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_improved.evaluate(X,y)\n",
    "#print(f\"{model_improved.metrics_names[1]}: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will have to choose your own architectures in today's module project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tcjMuxtn6wIQ",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Activation Functions (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tcjMuxtn6wIQ",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Overview\n",
    "What is an activation function and how does it work?\n",
    "\n",
    "- Takes in a weighted sum of inputs + a bias from the previous layer and outputs an \"activation\" value.\n",
    "- Based its inputs the neuron decides how 'activated' it should be. This can be thought of as the neuron deciding how strongly to fire. You can also think of it as if the neuron is deciding how much of the signal that it has received to pass onto the next layer. \n",
    "- Our choice of activation function does not only affect signal that is passed forward but also affects the backpropagation algorithm. It affects how we update weights in reverse order since activated weight/input sums become the inputs of the next layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_b0u8Ch60bA"
   },
   "source": [
    "### Step Function\n",
    "\n",
    "![Heaviside Step Function](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Dirac_distribution_CDF.svg/325px-Dirac_distribution_CDF.svg.png)\n",
    "\n",
    "All or nothing, a little extreme, which is fine, but makes updating weights through backpropagation impossible. Why? remember that during backpropagation we use derivatives in order to determine how much to update or not update weights. What is the derivative of the step function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vKR0YhIVEnXZ"
   },
   "source": [
    "### Linear Function\n",
    "\n",
    "![Linear Function](http://www.roconnell.net/Parent%20function/linear.gif)\n",
    "\n",
    "The linear function takes the opposite tact from the step function and passes the signal onto the next layer by a constant factor. There are problems with this but the biggest problems again lie in backpropagation. The derivative of any linear function is a horizontal line which would indicate that we should update all weights by a constant amount every time -which on balance wouldn't change the behavior of our network. Linear functions are typically only used for very simple tasks where interpretability is important, but if interpretability is your highest priority, you probably shouldn't be using neural networks in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JFurIVL6EkQ8"
   },
   "source": [
    "### Sigmoid Function\n",
    "\n",
    "![Sigmoid Function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/480px-Logistic-curve.svg.png)\n",
    "\n",
    "The sigmoid function works great as an activation function! it's continuously differentiable, its derivative doesn't have a constant slope, and having the higher slope in the middle pushes y value predictions towards extremes which is particularly useful for binary classification problems. I mean, this is why we use it as the squishifier in logistic regression as well. It constrains output, but over repeated epochs pushes predictions towards a strong binary prediction. \n",
    "\n",
    "What's the biggest problem with the sigmoid function? The fact that its slope gets pretty flat so quickly after its departure from zero. This means that updating weights based on its gradient really diminishes the size of our weight updates as our model gets more confident about its classifications. This is why even after so many iterations with our test score example we couldn't reach the levels of fit that our gradient descent based model could reach in just a few epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hm6p1HWbEhYi"
   },
   "source": [
    "### Tanh Function\n",
    "\n",
    "![Tanh Function](http://mathworld.wolfram.com/images/interactive/TanhReal.gif)\n",
    "\n",
    "What if the sigmoid function didn't get so flat quite as soon when moving away from zero and was a little bit steeper in the middle? That's basically the Tanh function. The Tanh function can actually be created by scaling the sigmoid function by 2 in the y dimension and subtracting 1 from all values. It has basically the same properties as the sigmoid, still struggles from diminishingly flat gradients as we move away from 0, but its derivative is higher around 0 causing weights to move to the extremes a little faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sFOn_L6gEcz1"
   },
   "source": [
    "### ReLU Function\n",
    "\n",
    "![ReLU Function](https://cdn-images-1.medium.com/max/937/1*oePAhrm74RNnNEolprmTaQ.png)\n",
    "\n",
    "ReLU stands for Rectified Linear Units it is by far the most commonly used activation function in modern neural networks. It doesn't activate neurons that are being passed a negative signal and passes on positive signals. Think about why this might be useful. Remember how a lot of our initial weights got set to negative numbers by chance? This would have dealt with those negative weights a lot faster than the sigmoid function updating. What does the derivative of this function look like? It looks like the step function! This means that not all neurons are activated. With sigmoid basically all of our neurons are passing some amount of signal even if it's small making it hard for the network to differentiate important and less important connections. ReLU turns off a portion of our less important neurons which decreases computational load, but also helps the network learn what the most important connections are faster. \n",
    "\n",
    "What's the problem with relu? Well the left half of its derivative function shows that for neurons that are initialized with weights that cause them to have no activation, our gradient will not update those neuron's weights, this can lead to dead neurons that never fire and whose weights never get updated. We would probably want to update the weights of neurons that didn't fire even if it's just by a little bit in case we got unlucky with our initial weights and want to give those neurons a chance of turning back on in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XWdvWOBIETwk"
   },
   "source": [
    "### Leaky ReLU\n",
    "\n",
    "![Leaky ReLU](https://cdn-images-1.medium.com/max/1600/1*ypsvQH7kvtI2BhzR2eT_Sw.png)\n",
    "\n",
    "Leaky ReLU accomplishes exactly that! it avoids having a gradient of 0 on the left side of its derivative function. This means that even \"dead\" neurons have a chance of being revived over enough iterations. In some specifications the slope of the leaky left-hand side can also be experimented with as a hyperparameter of the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FcAxkNFREMFb"
   },
   "source": [
    "### Softmax Function\n",
    "\n",
    "![Softmax Function](https://cdn-images-1.medium.com/max/800/1*670CdxchunD-yAuUWdI7Bw.png)\n",
    "\n",
    "Like the sigmoid function but more useful for multi-class classification problems. The softmax function can take any set of inputs and translate them into probabilities that sum up to 1. This means that we can throw any list of outputs at it and it will translate them into probabilities, this is extremely useful for multi-class classification problems. Like MNIST for example..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23-XRRXKHs34"
   },
   "source": [
    "### Major takeaways\n",
    "\n",
    "- ReLU is generally better at obtaining the optimal model fit.\n",
    "- Sigmoid and its derivatives are usually better at classification problems.\n",
    "- Softmax for multi-class classification problems. \n",
    "\n",
    "You'll typically see ReLU used for all initial layers and then the final layer being sigmoid or softmax for classification problems. But you can experiment and tune these selections as hyperparameters as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TWuoXZCCKCI7"
   },
   "source": [
    "### MNIST with Keras \n",
    "\n",
    "#### This will be a good chance to bring up dropout regularization. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jmJ_5azs04pU"
   },
   "outputs": [],
   "source": [
    "### Let's do it!\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Stretch - use dropout \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the Data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X Variable Types\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct Encoding on Y\n",
    "# What softmax expects = [0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 13,546\n",
      "Trainable params: 13,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnist_model = Sequential()\n",
    "\n",
    "# Input => Hidden\n",
    "mnist_model.add(Dense(16, input_dim=784, activation='relu'))\n",
    "# Hidden\n",
    "mnist_model.add(Dense(16, activation='relu'))\n",
    "# Hidden\n",
    "mnist_model.add(Dense(16, activation='relu'))\n",
    "# Hidden\n",
    "mnist_model.add(Dense(16, activation='relu'))\n",
    "# Output\n",
    "mnist_model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "#Compile\n",
    "mnist_model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer='adam',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "mnist_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(X):\n",
    "    \n",
    "    dense1 = Dense(8, activation='relu', input=X)\n",
    "    dense2 = Dense(8, activation='relu')(X)\n",
    "    \n",
    "    \n",
    "    return dense2 #or return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "16 *  784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-fd0808510929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(f'{mnist_model.metrics_names[1]}: {scores[1]*100}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = mnist_model.fit(X_train, y_train, batch_size=32, epochs=100, verbose=False)\n",
    "scores = mnist_model.evaluate(X_test, y_test)\n",
    "#print(f'{mnist_model.metrics_names[1]}: {scores[1]*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CKmx8153w9Ci"
   },
   "source": [
    "### Dropout Regularization\n",
    "\n",
    "![Regularization](https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Regularization.svg/354px-Regularization.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's do it!\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "mnist_model = Sequential()\n",
    "\n",
    "# Hidden\n",
    "mnist_model.add(Dense(32, input_dim=784, activation='relu'))\n",
    "mnist_model.add(Dropout(0.2))\n",
    "mnist_model.add(Dense(16, activation='relu'))\n",
    "mnist_model.add(Dropout(0.2))\n",
    "# Output Layer\n",
    "mnist_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "mnist_model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer='adam', \n",
    "                    metrics=['accuracy'])\n",
    "mnist_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = mnist_model.fit(X_train, y_train, batch_size=32, epochs=epochs, validation_split=.1, verbose=0)\n",
    "scores = mnist_model.evaluate(X_test, y_test)\n",
    "print(f'{mnist_model.metrics_names[1]}: {scores[1]*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will apply your choice of activation function inside two Keras Seqeuntial models today. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_433_Keras_Lecture.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
