{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valogonor/DS-Unit-4-Sprint-3-Neural-Networks/blob/master/LS_DS_431_Intro_to_NN_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "dVfaLrjLvxvQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Intro to Neural Networks Assignment"
      ]
    },
    {
      "metadata": {
        "id": "wxtoY12mwmih",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the Following:\n",
        "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
        "\n",
        "### Input Layer:\n",
        "\n",
        "The Input Layer is what receives input from our dataset. Sometimes it is called the visible layer because it's the only part that is exposed to our data and that our data interacts with directly. Typically node maps are drawn with one input node for each of the different inputs/features/columns of our dataset that will be passed to the network.\n",
        "\n",
        "### Hidden Layer:\n",
        "\n",
        "Layers after the input layer are called Hidden Layers. This is because they cannot be accessed except through the input layer. They're inside of the network and they perform their functions, but we don't directly interact with them. The simplest possible network is to have a single neuron in the hidden layer that just outputs the value. \"Deep Learning\" apart from being a big buzzword simply means that we are using a Neural Network that has multiple hidden layers. \"Deep Learning\" is a big part of the renewed hype around ANNs because it allows networks that are structured in specific ways to accomplish tasks that were previously out of reach (image recognition for example).\n",
        "\n",
        "### Output Layer:\n",
        "\n",
        "The final layer is called the Output Layer. The purpose of the output layer is to output a vector of values that is in a format that is suitable for the type of problem that we're trying to address. Typically the output value is modified by an \"activation function\" to transform it into a format that makes sense for our context.\n",
        "\n",
        "### Neuron:\n",
        "\n",
        "In Artificial Neural Networks the neurons or \"nodes\" are similar to neurons in brains in that they receive inputs and pass on their signal to the next layer of nodes if a certain threshold is reached.\n",
        "\n",
        "### Weight:\n",
        "\n",
        "In a neural network, each input gets modified by a weight, and these weighted inputs are used to calculate the final output.\n",
        "\n",
        "### Activation Function:\n",
        "\n",
        "Typically the output value is modified by an \"activation function\" to transform it into a format that makes sense for our context. Here's a few of examples:\n",
        "\n",
        "    NNs applied to a regression problem might have a single output node with no activation function because what we want is an unbounded continuous value.\n",
        "\n",
        "    NNS applied to a binary classification problem might use a sigmoid function as its activation function in order to squishify values down to represent a probability. Outputs in this case would represent the probability of predicting the primary class of interest. We can turn this into a class-specific prediction by rounding the outputted sigmoid probability up to 1 or down to 0.\n",
        "\n",
        "    NNS applied to multiclass classification problems might have multiple output nodes in the output layer, one for each class that we're trying to predict. This output layer would probably employ what's called a \"softmax function\" for accomplishing this.\n",
        "    \n",
        "### Node Map:\n",
        "\n",
        "A visual diagram of the architecture or \"topology\" of a neural network.\n",
        "\n",
        "### Perceptron:\n",
        "\n",
        "The first and simplest kind of neural network that we could talk about is the perceptron. A perceptron is just a single node or neuron of a neural network with nothing else. It can take any number of inputs and spit out an output. What a neuron does is it takes each of the input values, multplies each of them by a weight, sums all of these products up, and then passes the sum through what is called an \"activation function\" the result of which is the final value.\n",
        "\n",
        "### Bias\n",
        "\n",
        "Bias is a constant which helps to give a neural network model, in combination with the weights for each input, the full range of motion that it needs to find the best way to explain the patterns in the data."
      ]
    },
    {
      "metadata": {
        "id": "NXuy9WcWzxa4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inputs -> Outputs\n",
        "\n",
        "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
      ]
    },
    {
      "metadata": {
        "id": "PlSwIJMC0A8F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each layer of a neural network includes a bias term, which will be the output when all the inputs are zero. The first layer consists of inputs or features of the model. Each input is weighted, and each of the inputs plus the bias go into calculating the outputs. An activation function may be performed prior to giving the output in order to put the output in a form that makes sense for our context."
      ]
    },
    {
      "metadata": {
        "id": "6sWR43PTwhSk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Write your own perceptron code that can correctly classify a NAND gate. \n",
        "\n",
        "| x1 | x2 | y |\n",
        "|----|----|---|\n",
        "| 0  | 0  | 1 |\n",
        "| 1  | 0  | 1 |\n",
        "| 0  | 1  | 1 |\n",
        "| 1  | 1  | 0 |"
      ]
    },
    {
      "metadata": {
        "id": "Sgh7VFGwnXGH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "\n",
        "inputs = np.array([[0,0,1],\n",
        "                   [1,1,1],\n",
        "                   [1,0,1],\n",
        "                   [0,1,1]])\n",
        "# Expected output for NAND (!AND)\n",
        "correct_outputs = [[1],\n",
        "                  [1],\n",
        "                  [1],\n",
        "                  [0]]\n",
        "\n",
        "# Initial weights\n",
        "weights = 2 * np.random.random((3,1)) - 1\n",
        "# weights = np.random.random((3,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0YtmdChAzm48",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  return sigmoid(x) * (1 - sigmoid(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yw3amshJzuTy",
        "colab_type": "code",
        "outputId": "a192fb35-6ccb-48b8-b141-759092ab8f35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "cell_type": "code",
      "source": [
        "for iteration in range(10000):\n",
        "  \n",
        "  # Weighted sum of inputs and weights\n",
        "  weighted_sum = np.dot(inputs, weights)\n",
        "  \n",
        "  # Activate with sigmoid function\n",
        "  activated_output = sigmoid(weighted_sum)\n",
        "  \n",
        "  # Calculate Error\n",
        "  error = correct_outputs - activated_output\n",
        "  \n",
        "  # Calculate weight adjustments with sigmoid_derivative\n",
        "  adjustments = error * sigmoid_derivative(activated_output)\n",
        "  \n",
        "  # Update weights\n",
        "  weights += np.dot(inputs.T, adjustments)\n",
        "  \n",
        "print('optimized weights after training: ')\n",
        "print(weights)\n",
        "\n",
        "print(\"Output After Training:\")\n",
        "print(activated_output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "optimized weights after training: \n",
            "[[ 13.31887341]\n",
            " [-12.91309088]\n",
            " [  6.48068717]]\n",
            "Output After Training:\n",
            "[[0.99846944]\n",
            " [0.99897943]\n",
            " [1.        ]\n",
            " [0.00160616]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xf7sdqVs0s4x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implement your own Perceptron Class and use it to classify a binary dataset like: \n",
        "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
        "- [Titanic](https://raw.githubusercontent.com/ryanleeallred/datasets/master/titanic.csv)\n",
        "- [A two-class version of the Iris dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/Iris.csv)\n",
        "\n",
        "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
      ]
    },
    {
      "metadata": {
        "id": "-W0tiX1F1hh2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6QR4oAW1xdyu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
        "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
        "- Try and implement your own backpropagation algorithm.\n",
        "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
      ]
    }
  ]
}