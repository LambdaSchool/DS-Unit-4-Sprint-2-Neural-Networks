{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of LS_DS_431_Intro_to_NN_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danhorsley/DS-Unit-4-Sprint-2-Neural-Networks/blob/master/Copy_of_LS_DS_431_Intro_to_NN_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dVfaLrjLvxvQ"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "# Neural Networks\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 1*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wxtoY12mwmih"
      },
      "source": [
        "## Define the Following:\n",
        "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
        "\n",
        "### Input Layer:\n",
        "The nodes of the input layer are passive, meaning they do not modify the data. They receive a single value on their input, and duplicate the value to their multiple outputs. For example, they may be pixel values from an image, samples from an audio signal, stock market prices on successive days, etc. They may also be the output of some other algorithm, such as the classifiers in our cancer detection example: diameter, brightness, edge sharpness, etc.  Each value from the input layer is duplicated and sent to all of the hidden nodes\n",
        "### Hidden Layer:\n",
        "In comparison, the nodes of the hidden and output layer are active. This means they modify the data.  The values entering a hidden node are multiplied by weights, a set of predetermined numbers stored in the program. The weighted inputs are then added to produce a single number.  \n",
        "### Output Layer:\n",
        "The purpose of the output layer is to output a vector of values that is in a format that is suitable for the type of problem that we're trying to address. Typically the output value is modified by an \"activation function\" to transform it into a format that makes sense for our context.  for examplebefore leaving the node, this vector can be passed through a nonlinear mathematical function called a sigmoid. This is an \"s\" shaped curve that limits the node's output. That is, the input to the sigmoid is a value between -∞ and +∞, while its output can only be between 0 and 1.\n",
        "### Neuron:\n",
        "Artificial Neural Networks are a computational model that was inspired by how neural networks in the brain process information. In the brain electrochemical signals flow from earlier neurons through the dendrites of the cell toward the cell body. If the received signals surpass a certain threshold with a given timing then the neuron fires sending a large spike of energy down the axon and through the axon terminals to other neurons down the line.\n",
        "\n",
        "In Artificial Neural Networks the neurons or \"nodes\" are similar in that they receive inputs and pass on their signal to the next layer of nodes if a certain threshold is reached\n",
        "What a neuron does is it takes each of the input values, multplies each of them by a weight, sums all of these products up, and then passes the sum through what is called an \"activation function\" the result of which is the final value\n",
        "### Weight:\n",
        "Weights are what each input is multiplied by in the hidden layer, not entirely unlike a linear regression\n",
        "### Activation Function:\n",
        "see above - for example sigmoid\n",
        "### Node Map:\n",
        "Node Map\" it's a visual diagram of the architecture or \"topology\" of our neural network. It's kind of like a flow chart in that it shows the path from inputs to outputs. They are usually color coded and help us understand at a very high level, some of the differences in architecture between kinds of neural networks. Just like with all the areas of machine learning that we have studied before there is a \"zoo\" of neural network architectures.\n",
        "### Perceptron:\n",
        "A perceptron is just a single node or neuron of a neural network with nothing else. It can take any number of inputs and spit out an output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NXuy9WcWzxa4"
      },
      "source": [
        "## Inputs -> Outputs\n",
        "\n",
        "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PlSwIJMC0A8F"
      },
      "source": [
        "#### Your Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6sWR43PTwhSk"
      },
      "source": [
        "## Write your own perceptron code that can correctly classify a NAND gate. \n",
        "\n",
        "| x1 | x2 | y |\n",
        "|----|----|---|\n",
        "| 0  | 0  | 1 |\n",
        "| 1  | 0  | 1 |\n",
        "| 0  | 1  | 1 |\n",
        "| 1  | 1  | 0 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sgh7VFGwnXGH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "58e6ff23-8d37-4564-ed5a-bca54a1135fb"
      },
      "source": [
        "##### Your Code Here #####\n",
        "import numpy as np\n",
        "np.random.seed(555)\n",
        "inputs = np.array([[1,0,0],\n",
        "                  [1,1,0],\n",
        "                  [1,0,1],\n",
        "                  [0,1,1]])\n",
        "\n",
        "ground_truth = [[1],[1],[1],[0]]\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_derivate(x):\n",
        "  sx = sigmoid(x)\n",
        "  return sx*(1-sx)\n",
        "\n",
        "weights = np.random.random((3,1))\n",
        "for iteration in range(10000):\n",
        "  #weighted sum of inputs/weights\n",
        "  weighted_sum = np.dot(inputs,weights)\n",
        "  #activate\n",
        "  activated_output = sigmoid(weighted_sum)\n",
        "  #calculate teh error\n",
        "  error = ground_truth - activated_output\n",
        "  #adjustments\n",
        "  adjustments = error * sigmoid_derivate(activated_output)\n",
        "  weights+=np.dot(inputs.T,adjustments)\n",
        " \n",
        "format_output = ['%.2f' % x for x in activated_output]\n",
        "print(\"weights and training\")\n",
        "print(weights)\n",
        "print(\"output after training\")\n",
        "print(format_output)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights and training\n",
            "[[11.69679857]\n",
            " [-3.81080473]\n",
            " [-3.80297293]]\n",
            "output after training\n",
            "['1.00', '1.00', '1.00', '0.00']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xf7sdqVs0s4x"
      },
      "source": [
        "## Implement your own Perceptron Class and use it to classify a binary dataset like: \n",
        "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
        "- [Titanic](https://raw.githubusercontent.com/ryanleeallred/datasets/master/titanic.csv)\n",
        "- [A two-class version of the Iris dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/Iris.csv)\n",
        "\n",
        "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-W0tiX1F1hh2",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####\n",
        "import pandas as pd\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "mms = MinMaxScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH1vv_DSFyLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()\n",
        "df['is_f']=np.where(df['sex']=='female',1,0)\n",
        "df['age']=np.where(df['age'].isna(),df['age'].mean(),df['age'])\n",
        "df['age']=mms.fit_transform(df[['age']])\n",
        "df['fare']=mms.fit_transform(df[['fare']])\n",
        "df['pclass']=mms.fit_transform(df[['pclass']])\n",
        "df['parch']=mms.fit_transform(df[['parch']])\n",
        "df[['alone']] *= 1\n",
        "features = ['survived','pclass','is_f','age','sibsp','parch','fare','alone']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX60WuQrGN8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 400\n",
        "train = df[:n]\n",
        "\n",
        "inputs = train[features].values\n",
        "ground_truth = df['survived'].values[:n].reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vWAKCd2TM-i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d7a6c2c-577f-48d8-b87d-392a8b224165"
      },
      "source": [
        "inputs.shape,ground_truth.shape"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((400, 8), (400, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS7Tdr31TzQ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "3a47c6a1-ad47-4059-e18e-3e4fb6278a07"
      },
      "source": [
        "np.array(ground_truth)[:5]"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZZ55yb6IGMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_derivate(x):\n",
        "  sx = sigmoid(x)\n",
        "  return sx*(1-sx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nmamlabIrTf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "4afdc8a3-9efb-46a8-b875-82a9f1862ecb"
      },
      "source": [
        "#apologies for all teh hashtags, i had a small problem on array shape which i fixed by reshaping\n",
        "#the ground truth\n",
        "weights = np.random.random((len(features),1))\n",
        "for iteration in range(100):\n",
        "  #print(iteration)\n",
        "  #weighted sum of inputs/weights\n",
        "  weighted_sum = np.dot(inputs,weights)\n",
        "  #print(weighted_sum.shape)\n",
        "  #activate\n",
        "  activated_output = sigmoid(weighted_sum)\n",
        "  #print(activated_output.shape)\n",
        "  #calculate teh error\n",
        "  error = ground_truth - activated_output\n",
        "  #print(error.shape)\n",
        "  #adjustments\n",
        "  adjustments = error * sigmoid_derivate(activated_output)\n",
        "  #print(adjustments.shape)\n",
        "  weights=weights + np.dot(inputs.T,adjustments)\n",
        "  #print(weights.shape)\n",
        " \n",
        "format_output = [np.round(x,2) for x in activated_output]\n",
        "print(\"activated_output\")\n",
        "print(format_output)\n",
        "print(\"weights\")\n",
        "print(weights)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "activated_output\n",
            "[array([0.]), array([1.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([1.]), array([0.]), array([1.]), array([0.]), array([1.]), array([1.]), array([1.]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.05]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.02]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([1.]), array([1.]), array([1.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([1.]), array([1.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([1.]), array([1.]), array([1.]), array([0.]), array([0.98]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.01]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([1.]), array([1.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.05]), array([1.]), array([1.]), array([1.]), array([1.]), array([0.]), array([1.]), array([0.]), array([1.]), array([1.]), array([1.]), array([0.]), array([1.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([0.]), array([1.]), array([1.]), array([1.]), array([1.]), array([0.]), array([0.01]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([1.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.01]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([1.]), array([1.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.])]\n",
            "weights\n",
            "[[ 46.54250575]\n",
            " [-13.56287743]\n",
            " [  5.83670792]\n",
            " [ -8.08651269]\n",
            " [ -6.89001017]\n",
            " [ -3.37871629]\n",
            " [ -1.89051734]\n",
            " [ -9.73980875]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6QR4oAW1xdyu"
      },
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
        "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
        "- Try and implement your own backpropagation algorithm.\n",
        "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
      ]
    }
  ]
}