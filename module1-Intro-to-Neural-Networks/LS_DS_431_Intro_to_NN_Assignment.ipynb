{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "# Intro to Neural Networks Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:\n",
    "Input layer contains a set of cells. Each cell represents one feature, and contains the value present in that feature for this observation.\n",
    "### Hidden Layer:\n",
    "Hidden layers are all the layers to the right of the input layer. These are called hidden, because we cannot directly influence them.\n",
    "### Output Layer:\n",
    "An output layer contains as many neurons as you expect to solve your particular machine-learning problem. The output layer is activated by an activation function specific to the problem. For example for regression, the activation function is just the identity function (you don't want to manipulate the values of your output). For binary classfication, you might use the sigmoid function to restrict values between 0 and 1. These values are then the probability of predicting the primary class. You can use ceil(output) to get the binary classification you need.\n",
    "### Neuron:\n",
    "In machine learning, a neuron is a function that takes multiple inputs and outputs one value. If the neuron is in the first hidden layer, it's inputs are all the feature values of the input layer + the bias value. If the neuron is in a deeper hidden layer, it's inputs are all the neurons to it's left + the bias value.\n",
    "### Weight:\n",
    "The weight is a number that is multiplied by the current neuron's value + bias to give the output of the neuron. Each neuron then essentially follows this equation: output = activation_function(sum(Weight * earlier_layer_value) + bias)\n",
    "### Activation Function (also called Transfer Function):\n",
    "An activation function takes a value and returns another value. It has to be differentiable, since we will differentiate it during the backpropagation step. You may choose different activation functions for different layers.\n",
    "An activation function is applied after a neuron has weighed all input values, added them together, and then added a bias.\n",
    "\n",
    "Common activation functions are the Sigmoid, tanh, step, and relu.\n",
    "### Node Map:\n",
    "A node map is a diagram that shows how our nodes in the neural network are arranged, and the connections between them. It helps us understand which kind of neural network we're looking at. Each type of map will have different properties, and we will get a better fit if we select the right kind.\n",
    "### Perceptron:\n",
    "The first and simplest kind of neural network that we could talk about is the perceptron. A perceptron is just a single node or neuron of a neural network with nothing else. It can take any number of inputs and spit out an output. What a neuron does is it takes each of the input values, multplies each of them by a weight, sums all of these products up, adds a bias, and then passes the sum through what is called an \"activation function\" the result of which is the final value.\n",
    "### Bias:\n",
    "Bias is a value added to shift the sum(weight * earlier_layer_values). Bias is also a variable, so we can control the y-intercept (ex. in a 1-feature regression application with a neural net)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "Input layer (contains values of each feature)\n",
    "\n",
    "     |\n",
    "     v\n",
    "     \n",
    "Hidden layer 1 (contains values calculated using: activation_function(sum(input_layer * weight) + bias)\n",
    "\n",
    "     |\n",
    "     v\n",
    "\n",
    "Hidden layer 2 (contains values calculated using: activation_function(sum(input_layer * weight) + bias)\n",
    "\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "Output layer (contains values calculated using: activation_function(sum(input_layer * weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  0  0  1\n",
       "1  1  0  1\n",
       "2  0  1  1\n",
       "3  1  1  0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0, 0], \n",
    "              [1, 0],\n",
    "              [0, 1], \n",
    "              [1, 1]])\n",
    "y = np.array([[1], \n",
    "              [1], \n",
    "              [1], \n",
    "              [0]])\n",
    "df = pd.DataFrame(data=np.concatenate([X, y], axis=1)) # , \n",
    "                  # columns=['x1', 'x2', 'y'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Explanation for why this is is given here:\n",
    "# https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs  = X\n",
    "weights = 2 * np.random.random((2, 1)) - 1\n",
    "correct_outputs = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized weights after training: \n",
      "[[0.000000000000]\n",
      " [-0.000000000000]]\n",
      "activated_output: [[0.500]\n",
      " [0.500]\n",
      " [0.500]\n",
      " [0.500]]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(10000):\n",
    "  \n",
    "  # Weighted sum of inputs and weights\n",
    "  weighted_sum = np.dot(inputs, weights)\n",
    "  \n",
    "  # Activate with sigmoid function\n",
    "  activated_output = sigmoid(weighted_sum)\n",
    "  \n",
    "  # Calculate Error\n",
    "  error = correct_outputs - activated_output\n",
    "  \n",
    "  # Calculate weight adjustments with sigmoid_derivative\n",
    "  adjustments = error * sigmoid_derivative(activated_output)\n",
    "  \n",
    "  # Update weights\n",
    "  weights += np.dot(inputs.T, adjustments)\n",
    "  \n",
    "print('optimized weights after training: ')\n",
    "print(weights)\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "print('activated_output:', activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset like: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "- [Titanic](https://raw.githubusercontent.com/ryanleeallred/datasets/master/titanic.csv)\n",
    "- [A two-class version of the Iris dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/Iris.csv)\n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized weights after training: \n",
      "[[-0.690]\n",
      " [-0.690]]\n",
      "activated_output: [[0.731]\n",
      " [0.577]\n",
      " [0.577]\n",
      " [0.406]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.731],\n",
       "       [0.577],\n",
       "       [0.577],\n",
       "       [0.406]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, inputs, weights, bias, outputs):\n",
    "        self.inputs = inputs\n",
    "        self.weights = weights\n",
    "        self.outputs = outputs\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Explanation for why this is is given here:\n",
    "    # https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e\n",
    "    def sigmoid_derivative(x):\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    \n",
    "    def iterate(self, num_iters):\n",
    "        for iteration in range(num_iters):\n",
    "\n",
    "          # Weighted sum of inputs and weights\n",
    "          weighted_sum = np.dot(self.inputs, \n",
    "                                self.weights) + bias\n",
    "\n",
    "          # Activate with sigmoid function\n",
    "          activated_output = sigmoid(weighted_sum)\n",
    "\n",
    "          # Calculate Error\n",
    "          error = self.outputs - activated_output\n",
    "\n",
    "          # Calculate weight adjustments with sigmoid_derivative\n",
    "          adjustments = error * sigmoid_derivative(activated_output)\n",
    "\n",
    "          # Update weights\n",
    "          self.weights += np.dot(self.inputs.T, adjustments)\n",
    "\n",
    "        print('optimized weights after training: ')\n",
    "        print(weights)\n",
    "\n",
    "        np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "        print('activated_output:', activated_output)\n",
    "\n",
    "        return activated_output\n",
    "    \n",
    "inputs  = X\n",
    "weights = 2 * np.random.random((2, 1)) - 1\n",
    "bias    = 1\n",
    "correct_outputs = y\n",
    "perceptron = Perceptron(inputs, weights, bias, correct_outputs)\n",
    "perceptron.iterate(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
