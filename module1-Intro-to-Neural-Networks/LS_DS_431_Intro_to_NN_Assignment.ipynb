{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "# Intro to Neural Networks Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:\n",
    "> This is the first layer of a \"Neural Network.\" It's composed of a range of numbers or \"neurons\"/\"nodes\" (imputs - from 1 to n), where each number is that neuron's value or \"Activation\".\n",
    "### Hidden Layer:\n",
    "> Determines how Neural Network's process will be handled to produce \"output layer.\" Number of layers and number of neurons/nodes in each layer varies.\n",
    "### Output Layer:\n",
    "> Last layer in the \"Neural Network.\" Just like the imput layer, this has a range of neurons/nodes (0 to n) that returns resulting values from neural network (ie., prediction(s)/classifications).\n",
    "### Neuron:\n",
    "> Units that compose each layer. Neurons in layers past input layer are determined by the sum of the weighted activations from neurons connnected to it. This weighted sum is usually passed through a function that would restrict the resulting value into a desired value range (ie., maybe  using sigmoid function that squishes values to range of 0 and 1)\n",
    "### Weight:\n",
    "> One of the two parameters included in the connection of one neuron and the neuron in next layer. Weights are numbers that multiplied to the activation value of the neuron assigned . Initially, weights are assigned arbitrarilly but then adjusted usually through \"back propagation\" method after error is calculated at the end of each round/\"Epoc\".\n",
    "### Activation Function: \n",
    "> Activations in one layer determine the activations in the next layer. A measure of how positive the relevant weighted sum is (activation = function * (w1.a1 + w2.a2 +...+wn.an)\n",
    "### Node Map:\n",
    "### Perceptron:\n",
    "> Simplest kind of neural network. A single neuron/node that can take any number of inputs and return a single output. It takes the weighted sum of all inputs and passes it through the activation function before returning an output.\n",
    "### Bias:\n",
    "> Number (either positive or negative) added to the total weighted activation sum before passing through squisification function. The \"bias\" tells you how high or low the weighted sum needs to be before it starts to give meaningfully active. This is done for each of the neurons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "#### Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "import numpy as np\n",
    "#np.random.seed(1)\n",
    "\n",
    "inputs = np.array([[0, 0, 1],\n",
    "                   [1, 0, 1],\n",
    "                   [0, 1, 1],\n",
    "                   [1, 1, 0]])\n",
    "\n",
    "correct_outputs = ([[1],\n",
    "                   [1],\n",
    "                   [1],\n",
    "                   [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights for 3 inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.31602553],\n",
       "       [-0.03893788],\n",
       "       [-0.31142186]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights with values between -1 and 1\n",
    "weights = 2 * np.random.random((3,1)) - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculated weighted sum:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL STEPS ABOVE PUT TOGETHER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Weights After Training: \n",
      "[[-2.65703498]\n",
      " [-2.67003806]\n",
      " [ 8.28132436]]\n",
      "Output After Training: \n",
      "[[0.99974649]\n",
      " [0.99640035]\n",
      " [0.99635338]\n",
      " [0.00483956]]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(1000):\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    \n",
    "    error = correct_outputs - activated_output\n",
    "    \n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "    \n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "    \n",
    "print(f'Optimized Weights After Training: \\n{weights}')\n",
    "print(f'Output After Training: \\n{activated_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset like: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "- [Titanic](https://raw.githubusercontent.com/ryanleeallred/datasets/master/titanic.csv)\n",
    "- [A two-class version of the Iris dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/Iris.csv)\n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tutorial from `machinelearningmastery.com`\n",
    "https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Making Predictions\n",
    "The first step is to develop a function that can make predictions.\n",
    "\n",
    "This will be needed both in the evaluation of candidate weights values in stochastic gradient descent, and after the model is finalized and we wish to start making predictions on the data or new data.\n",
    "\n",
    "Below is a function named `predict()` that predicts an output value for a row given a set of weights.\n",
    "\n",
    "The first weight is always the bias as it is standalone and not responsible for a specific input value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two inputs values (X1 and X2) and three weight values (bias, w1 and w2). The activation equation we have modeled for this problem is:\n",
    "\n",
    "`activation = (w1 * X1) + (w2 * X2) + bias`\n",
    "\n",
    "`activation = (0.206 * X1) + (-0.234 * X2) + -0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected->0, Predicted->0.0\n",
      "Expected->0, Predicted->0.0\n",
      "Expected->0, Predicted->0.0\n",
      "Expected->0, Predicted->0.0\n",
      "Expected->0, Predicted->0.0\n",
      "Expected->1, Predicted->1.0\n",
      "Expected->1, Predicted->1.0\n",
      "Expected->1, Predicted->1.0\n",
      "Expected->1, Predicted->1.0\n",
      "Expected->1, Predicted->1.0\n"
     ]
    }
   ],
   "source": [
    "# Make prediction with weights\n",
    "def predict(row, weights):\n",
    "    activation = weights[0]\n",
    "    for i in range(len(row)-1):\n",
    "        activation += weights[i + 1] * row[i]\n",
    "    return 1.0 if activation >= 0.0 else 0.0\n",
    "\n",
    "# We can contrive a small dataset to test our prediction function:\n",
    "# test predictions\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "           [1.465489372,2.362125076,0],\n",
    "           [3.396561688,4.400293529,0],\n",
    "           [1.38807019,1.850220317,0],\n",
    "           [ 3.06407232,3.005305973,0],\n",
    "           [7.627531214,2.759262235,1],\n",
    "           [5.332441248,2.088626775,1],\n",
    "           [6.922596716,1.77106367,1],\n",
    "           [8.675418651,-0.242068655,1],\n",
    "           [7.673756466,3.508563011,1]]\n",
    "\n",
    "weights = [-0.1, 0.20653640140000007, -0.23418117710000003]\n",
    "\n",
    "for row in dataset:\n",
    "    prediction = predict(row, weights)\n",
    "    print(f\"Expected->{row[-1]}, Predicted->{prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training Network Weights\n",
    "We can estimate the weight values for our training data using \"stochastic gradient descent.\"\n",
    "\n",
    "Stochastic gradient descent requeres two parameters:\n",
    "- **Learning Rate**: Uset to limit the amount each weight is corrected each time it is updatated.\n",
    "- **Epochs**: The number of times to run through the training data while updating the weight.\n",
    "\n",
    "These, along with the training data will be the arguments to the function.\n",
    "\n",
    "There are 3 loops we need to perform in the function:\n",
    "1. Loop over each epoch.\n",
    "2. Loop over each row in the training data for an epoch.\n",
    "3. Loop over each weight and update it for a row in an epoch.\n",
    "\n",
    "We update each weight fore each row in the training data, each epoch.\n",
    "\n",
    "Weights are updated based on the error the model made. The error is calculated as the difference between the expected output value and the prediction made with tthe candidate weights.\n",
    "\n",
    "There is one weight for each input attribute, and these are updated in a consistent way, for example:\n",
    "\n",
    "`w(t+1) = w(t) + learning_rate * (expected(t) - predicted(t)) *x(t)`\n",
    "\n",
    "The bias is updated in a similar way, except without an input as it is not associated with a specific input value:\n",
    "\n",
    "`bias(t+1) = bias(t) + learning_rate * (expected(t) - predicted(t))`\n",
    "\n",
    "Below is included a function named **`train_weights()`** that calculates weight values for the dataset using stochastic gradient descent.\n",
    "\n",
    "You can see that we keep track of the \"sum of squared error\" (a positive value) each epoch so that we can pring out a nice message each outer loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.1, error=2.0\n",
      ">epoch=1, lrate=0.1, error=1.0\n",
      ">epoch=2, lrate=0.1, error=0.0\n",
      ">epoch=3, lrate=0.1, error=0.0\n",
      ">epoch=4, lrate=0.1, error=0.0\n",
      "[-0.1, 0.20653640140000007, -0.23418117710000003]\n"
     ]
    }
   ],
   "source": [
    "# Make prediction with weights\n",
    "def predict(row, weights):\n",
    "    activation = weights[0]\n",
    "    for i in range(len(row)-1):\n",
    "        activation += weights[i + 1] * row[i]\n",
    "    return 1.0 if activation >= 0.0 else 0.0\n",
    "\n",
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(train, l_rate, n_epoch):\n",
    "    weights = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0.0\n",
    "        for row in train:\n",
    "            prediction = predict(row, weights)\n",
    "            error = row[-1] - prediction\n",
    "            sum_error += error**2\n",
    "            weights[0] = weights[0] + l_rate * error\n",
    "            for i in range(len(row)-1):\n",
    "                weights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "        print(f'>epoch={epoch}, lrate={l_rate}, error={sum_error}')\n",
    "    return weights\n",
    "\n",
    "# Calculate weights\n",
    "dataset = [[2.7810836, 2.550537003, 0],\n",
    "            [1.465489372, 2.362125076, 0],\n",
    "            [3.396561688, 4.400293529, 0],\n",
    "            [1.38807019, 1.850220317, 0],\n",
    "            [3.06407232, 3.005305973, 0],\n",
    "            [7.627531214, 2.759262235, 1],\n",
    "            [5.332441248, 2.088626775, 1],\n",
    "            [6.922596716, 1.77106367, 1],\n",
    "            [8.675418651, -0.242068655, 1],\n",
    "            [7.673756466, 3.508563011, 1]]\n",
    "l_rate = 0.1\n",
    "n_epoch = 5\n",
    "weights = train_weights(dataset, l_rate, n_epoch)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the \"Pima Diabetes\" Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "import pandas as pd\n",
    "\n",
    "# Pima Indians Diabetes dataset\n",
    "url = 'https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv'\n",
    "\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving df as csv\n",
    "# df.to_csv(\"diabetes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use **k-fold cross validation** to estimate the performance of the learned model on unseen data. This means that we will construct and evaluate K models and estimate the performance as the mean model error. Classification accuracy will be used to evaluate each model. These behaviors are provided in the **cross_validation_split()**, **accuracy_metric()** and **evaluate_algorith()** helper functions.\n",
    "\n",
    "We will use the **`predict()`** and **`train_weights()`** functions created above in the example above to train the model and new **`perceptron()`** function to tie them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [50.390625, 70.703125, 60.546875]\n",
      "Mean Accuracy: 60.547%\n"
     ]
    }
   ],
   "source": [
    "# Perceptron Algorithm on the Pima Diabetes Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        next(file)\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column])\n",
    "        \n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    " \n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    " \n",
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "\tactivation = weights[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tactivation += weights[i + 1] * row[i]\n",
    "\treturn 1.0 if activation >= 0.0 else 0.0\n",
    " \n",
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(train, l_rate, n_epoch):\n",
    "\tweights = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tfor row in train:\n",
    "\t\t\tprediction = predict(row, weights)\n",
    "\t\t\terror = row[-1] - prediction\n",
    "\t\t\tweights[0] = weights[0] + l_rate * error\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "\treturn weights\n",
    " \n",
    "# Perceptron Algorithm With Stochastic Gradient Descent\n",
    "def perceptron(train, test, l_rate, n_epoch):\n",
    "\tpredictions = list()\n",
    "\tweights = train_weights(train, l_rate, n_epoch)\n",
    "\tfor row in test:\n",
    "\t\tprediction = predict(row, weights)\n",
    "\t\tpredictions.append(prediction)\n",
    "\treturn(predictions)\n",
    " \n",
    "# Test the Perceptron algorithm on the sonar dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'diabetes.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert string class to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# evaluate algorithm\n",
    "n_folds = 3\n",
    "l_rate = 0.01\n",
    "n_epoch = 500\n",
    "scores = evaluate_algorithm(dataset, perceptron, n_folds, l_rate, n_epoch)\n",
    "\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
