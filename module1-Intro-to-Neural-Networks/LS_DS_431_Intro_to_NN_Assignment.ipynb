{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "# Intro to Neural Networks Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:  \n",
    "this is the left-most layer of neurons of a NN, in which the data space of the network is defined.  \n",
    "\n",
    "### Hidden Layer:  \n",
    "any layer between the input layer and the output layer; receives output from an earlier layer, applies some function to that information for each neuron in the layer, and pushes the new output to the next layer in the NN.  \n",
    "  \n",
    "### Output Layer:  \n",
    "this is the right-most layer of neurons of a NN, which uses input of nearest hidden layer to return probabilities for the outcomes of interest.\n",
    "  \n",
    "### Neuron:  \n",
    "a container for a floating-point number that ranges between 0.0 and 1.0; columns of neurons form layers. \n",
    "  \n",
    "### Weight:  \n",
    "a numeric value multiplied by the respective inputs into a given layer (except for output layer?); weighting relates input data to output data.     \n",
    "  \n",
    "### Activation Function:  \n",
    "a function that is used to reshape input data into the range of 0, 1, thus creating a unique \"activation\"; applied to each neuron of a given layer; ReLU (for rectified linear unit) is currently a popular activation function, $f(x)=\\max(0, x)$.\n",
    "\n",
    "### Node Map:  \n",
    "a diagram of the topology of a given neural network; defining various kinds of cells (neurons), it shows how those cells combine in layers, and how those layers assemble themselves relative to one another, to form the NN.  \n",
    "  \n",
    "### Perceptron:  \n",
    "the simplest kind of NN, it has no hidden layers; has an input layer that is weighted then passed through an activation function to return the output layer.  \n",
    "  \n",
    "### Bias:  \n",
    "an integer used in combination with weight to optimize a given layer's explanation of patterns in data of that layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "Every layer of a NN receives inputs. Every layer except the output layer has a weight applied to (multiplied by) the value of each neuron in that layer; each of those neuronal products then summed; and a layer-specific bias then added to each sum of products. Each of these new float values is then passed into the layer-specific activation function, which generates new values. These new values become the inputs for the next layer in the NN. It's key to note that each neuron in a given layer passes input to each neuron in the immediately adjacent layer.  \n",
    "  \n",
    "The flow of information begins at the input layer and ends at the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "# Define x1 and x2\n",
    "\n",
    "x1 = [0, 1, 0, 1]\n",
    "x2 = [0, 0, 1, 1]\n",
    "\n",
    "\n",
    "# Write logic for NAND gate, which will return 'actual' output\n",
    "def nand_perceptron(list1, list2):\n",
    "    output = []\n",
    "    for tup in [tuple([val1, val2]) for val1, val2 in zip(list1, list2)]:\n",
    "        if tup == (1, 1):\n",
    "            output.append(0)\n",
    "        else:\n",
    "            output.append(1)\n",
    "    return output\n",
    "\n",
    "\n",
    "# Compare actual and expected\n",
    "\n",
    "expected = [1, 1, 1, 0]\n",
    "actual = nand_perceptron(x1, x2)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs are: [[0. 0. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "correct_outputs are: [[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]] \n",
      "\n",
      "optimized weights after training: \n",
      "[[-11.83986516]\n",
      " [-11.83986516]\n",
      " [ 17.8086623 ]]\n",
      "\n",
      "outputs after training:\n",
      "[[0.99999998]\n",
      " [0.99744895]\n",
      " [0.99744895]\n",
      " [0.00281221]]\n"
     ]
    }
   ],
   "source": [
    "# Okay, let me do this more accurately with an assist from RA/LSDS, DR/LSDS01\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs = np.array(list(zip(x1, x2, np.ones(4))))  # zips x1 and x2, plus adds in a column of ones for bias\n",
    "print('inputs are:', inputs)\n",
    "correct_outputs = np.array([[1], [1], [1], [0]])  # key logic here: only if all inputs are True, does output\n",
    "                                                  # return False\n",
    "print('correct_outputs are:', correct_outputs, '\\n')\n",
    "weights = 2 * np.random.random((3,1)) - 1\n",
    "\n",
    "\n",
    "def sigmoid(x):  # activation function that returns output in range(0, 1)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "for iteration in range(10000):\n",
    "    # Weighted sum of inputs and weights\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "\n",
    "    # Activate with sigmoid function\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "\n",
    "    # Calculate Error\n",
    "    error = correct_outputs - activated_output\n",
    "\n",
    "    # Calculate weight adjustments with sigmoid_derivative\n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "\n",
    "    # Update weights\n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "  \n",
    "print('optimized weights after training: ')\n",
    "print(weights)\n",
    "\n",
    "print(\"\\noutputs after training:\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset like: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "- [Titanic](https://raw.githubusercontent.com/ryanleeallred/datasets/master/titanic.csv)\n",
    "- [A two-class version of the Iris dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/Iris.csv)\n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, (768, 9))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sum(), df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()  # target is clearly 'Outcome', a class variable for ultimate diabetes onset (1) or not (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    268\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Outcome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    163\n",
       "0     85\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Outcome[(df.BMI > 30) & (df.Glucose > 120)].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
