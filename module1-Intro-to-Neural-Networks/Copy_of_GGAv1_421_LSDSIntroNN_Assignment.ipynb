{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of GGAv1 421 LSDSIntroNN_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dVfaLrjLvxvQ"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "# Neural Networks\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 1*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wxtoY12mwmih"
      },
      "source": [
        "## Define the Following:\n",
        "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
        "\n",
        "### Input Layer:\n",
        "### Hidden Layer:\n",
        "### Output Layer:\n",
        "### Neuron:\n",
        "### Weight:\n",
        "### Activation Function:\n",
        "### Node Map:\n",
        "### Perceptron:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk8C0WnrfGmm",
        "colab_type": "text"
      },
      "source": [
        "Input Layer: layer taking the raw input\n",
        "\n",
        "Hidden Layer: a layer not showing output\n",
        "\n",
        "Output Layer: layer that put out the final output\n",
        "\n",
        "Neuron: perceptron/neuron/node a unit of weight/bias/target evaluation\n",
        "\n",
        "Weight: the iteration-relative result of how incorrect the last input was, emphasis \n",
        "\n",
        "Activation Function: e.g. reLU, \"squishification\" function for getting a binary output\n",
        "\n",
        "Node Map: diagram of NN architecture (also archetcture map)\n",
        "\n",
        "Perceptron: \"a single neuron/node in a network\" and/or \"is is the simplest of neural networks\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW9aiaQTlh3M",
        "colab_type": "text"
      },
      "source": [
        "\\begin{align}\n",
        " y = sigmoid(\\sum(weight_{1}input_{1} + weight_{2}input_{2} + weight_{3}input_{3}) + bias)\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NXuy9WcWzxa4"
      },
      "source": [
        "## Inputs -> Outputs\n",
        "\n",
        "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm1gMFuXliI-",
        "colab_type": "text"
      },
      "source": [
        "Here we will use a 3 layer model of a neural network:\n",
        "\n",
        "1. input layer\n",
        "2. hidden layer\n",
        "3. output layer\n",
        "\n",
        "Data goes into the NN through the input layer.\n",
        "\n",
        "The raw input Data goes into the NN through the input layer.\n",
        "(if all the data goes through the whole network once, that is an epoch, if only a sample goes through that is a batch, here we will assume all the data goes through)\n",
        "\n",
        "Each node/neuron/perceptron in each layer of the network carries out a similar routine of data processing:\n",
        "\n",
        "Framework:\n",
        "The overall framework is similar to the general data analysis 'pipeline':\n",
        "\n",
        "There is one y and there are several Xs which can be graphed.\n",
        "\n",
        "y is the desired target for prediction\n",
        "\n",
        "X's are categories (columns) of data\n",
        "\n",
        "comparisons are generally made between standardized or normalized or log normalized data.\n",
        "\n",
        "deviations from means or predictions are 'error' calculated by convensional measures such as MSE (mean squared error) or ASE (absolute squared error)\n",
        "\n",
        "Process:\n",
        "overally, an iterative process analogous to a biological neuron that either fires or doesn't fire, and has an error-measure function to suppress the firing of unhelpful signals.\n",
        "1. Each datum is given a weight, if randomly to start with.\n",
        "2. The sum of all these datum*weight products is summed up. \n",
        "3. bias (which acts like y-intercept in a graph) is added\n",
        "4. the result of these neuron processes then goes through a function (activation function - a usually normalizing process that can be then interpreted as 'fire or not fire')\n",
        "5. errors are calculated comparing the activated outputs to the y target (e.g. Mean Square Error)\n",
        "6. the error is used to adjust the weights so that good outcomes have higher weights and bad outcomes have lower weights) (note, this takes the place of step 2)\n",
        "7. the process repeates a desired number of times until the weights are satisfactory\n",
        "8. data can be output through that set of weights\n",
        "\n",
        "Hidden layers do additional processing that is not directly seen. The output layer does the final processing before output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6sWR43PTwhSk"
      },
      "source": [
        "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
        "\n",
        "| x1 | x2 | y |\n",
        "|----|----|---|\n",
        "| 0  | 0  | 1 |\n",
        "| 1  | 0  | 1 |\n",
        "| 0  | 1  | 1 |\n",
        "| 1  | 1  | 0 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx-8v4afJ9cW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data = { 'x1': [0,1,0,1],\n",
        "         'x2': [0,0,1,1],\n",
        "         'y':  [1,1,1,0]\n",
        "       }\n",
        "\n",
        "df = pd.DataFrame.from_dict(data).astype('int')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sgh7VFGwnXGH",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrN3JzV6J9cd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxtgQ_M-5mly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Basic NN (artificial) Neural Network in Python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8enuBHby5tHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Notes:\n",
        "# Data-Set(X) & Target(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRYlrtxm5tKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "# X\n",
        "# numpy array\n",
        "inputs = np.array([\n",
        "                   [0,0],\n",
        "                   [1,0],\n",
        "                   [0,1],\n",
        "                   [1,1]\n",
        "\n",
        "])\n",
        "\n",
        "# Y\n",
        "# list of corrected outputs\n",
        "correct_outputs = [[1],\n",
        "                   [1],\n",
        "                   [1],\n",
        "                   [0]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gRDCyUZ5tOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inspecting/testing\n",
        "# inputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmpcx5oju1Rt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#np.random.random((2, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7UCzU-p5tQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inspecting/testing\n",
        "# correct_outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BXtmF6m1Ry2E",
        "colab": {}
      },
      "source": [
        "# this is the activiation function to be used in the NN\n",
        "# creating a sigmoid function\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "  #return float(1 / (1 + np.exp(-x)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYUXwV0ot99j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inspecting/testing\n",
        "# sigmoid(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnc5yRjI-E1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = 2 * np.random.random((2, 1)) -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ex5b896fWrMQ",
        "colab": {}
      },
      "source": [
        "weighted_sum = np.dot(inputs, weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0FNWf1G57Q3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inspecting/testing\n",
        "# weighted_sum"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XfYIsOsO5553",
        "colab": {}
      },
      "source": [
        "# this is the probability that an answer will be zero or one\n",
        "activated_output = sigmoid(weighted_sum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R9nmneLR5558",
        "colab": {}
      },
      "source": [
        "# error ?\n",
        "# bias ?\n",
        "error = corrected_output = activated_output\n",
        "# error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EEr9JbT3555_",
        "colab": {}
      },
      "source": [
        "# back propagation: adjusting weights to minimize error\n",
        "# gradient descent: use the derivative to find the minimum of a function ()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B5F_qChJ556C",
        "colab": {}
      },
      "source": [
        "def sigmoid_derivative(x):\n",
        "  sx = sigmoid(x)\n",
        "  return sx * (1 - sx)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XlWUzzeM556E",
        "colab": {}
      },
      "source": [
        "# these are the changes that will be made to the weights\n",
        "adjustments = error * sigmoid_derivative(activated_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "llmNHRH5W4BA",
        "colab": {}
      },
      "source": [
        "# this is the probability that an answer will be zero or one\n",
        "activated_output = sigmoid(weighted_sum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8VdOpPxz54bp",
        "colab": {}
      },
      "source": [
        "# back propagation: adjusting weights to minimize error\n",
        "# gradient descent: use the derivative to find the minimum of a function ()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X_m0Yb7d54bs",
        "colab": {}
      },
      "source": [
        "def sigmoid_derivative(x):\n",
        "  sx = sigmoid(x)\n",
        "  return sx * (1 - sx)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bipupo5L54bv",
        "colab": {}
      },
      "source": [
        "adjustments = error * sigmoid_derivative(activated_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kdV6Nk8Lo5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# back propagation: adjusting weights to minimize error\n",
        "# gradient descent: use the derivative to find the minimum of a function ()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PERka4p1kif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_derivative(x):# inspecting/testing\n",
        "  sx = sigmoid(x)\n",
        "  return sx * (1 - sx)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6iJlT7m1ZIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adjustments = error * sigmoid_derivative(activated_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4XoNiOJ5tVJ",
        "colab_type": "code",
        "outputId": "da731ff3-10bf-4047-a1d7-45286188122e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Steps we've already done: \n",
        "# 1. Randomly Initialized Weights already. Those are in memory as `weights`\n",
        "# 2. We've already got input data & correct_outputs\n",
        "\n",
        "\n",
        "# Update our weights 10,000 times - (fingers crossed that this process reduces error)\n",
        "for iteration in range(10000):\n",
        "    \n",
        "    # Weighted sum of inputs / weights\n",
        "    weighted_sum = np.dot(inputs, weights)\n",
        "    \n",
        "    # Activate / activation function\n",
        "    activated_output = sigmoid(weighted_sum)\n",
        "    \n",
        "    # Cac error\n",
        "    error = correct_outputs - activated_output\n",
        "    \n",
        "    adjustments = error * sigmoid_derivative(activated_output)\n",
        "    \n",
        "    # Update the Weights\n",
        "    weights += np.dot(inputs.T, adjustments)\n",
        "    \n",
        "print(\"Weights after training\")\n",
        "print(weights)\n",
        "\n",
        "print(\"Output after training\")\n",
        "print(activated_output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights after training\n",
            "[[-3.19189120e-16]\n",
            " [ 1.52655666e-16]]\n",
            "Output after training\n",
            "[[0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1XbAYVp5tX5",
        "colab_type": "code",
        "outputId": "a0f42042-3f51-4824-e880-720715811468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "correct_outputs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1], [1], [1], [0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xf7sdqVs0s4x"
      },
      "source": [
        "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
        "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
        "\n",
        "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxCcyseXwArW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x2cmwv2J9cg",
        "colab_type": "code",
        "outputId": "3a4db42d-f082-4a88-a918-6a7991588638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
        "diabetes.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu4yqsSBJ9cj",
        "colab_type": "text"
      },
      "source": [
        "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXe2aauRJ9ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
        "\n",
        "feats = list(diabetes)[:-1]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8XVTmkcwbtN",
        "colab_type": "code",
        "outputId": "492820af-3386-4221-9414-1ecdbea28eab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "feats"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Pregnancies',\n",
              " 'Glucose',\n",
              " 'BloodPressure',\n",
              " 'SkinThickness',\n",
              " 'Insulin',\n",
              " 'BMI',\n",
              " 'DiabetesPedigreeFunction',\n",
              " 'Age']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TJpC_slwbwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ?\n",
        "#X = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-W0tiX1F1hh2",
        "colab": {}
      },
      "source": [
        "#version with function\n",
        "\n",
        "class Perceptron(object):\n",
        "    \n",
        "  def __init__(self, niter = 10):\n",
        "    self.niter = niter\n",
        "    \n",
        "  def __sigmoid(self, x):\n",
        "    return None\n",
        "    \n",
        "  def __sigmoid_derivative(self, x):\n",
        "    return None\n",
        "\n",
        "  def fit(self, X, y):\n",
        "#   \"\"\"Fit training data\n",
        "#   X : Training vectors, X.shape : [#samples, #features]\n",
        "#   y : Target values, y.shape : [#samples]\n",
        "#   \"\"\"\n",
        "\n",
        "    # Does this assign random starting weights?...starting at zero, not random?\n",
        "    # weights\n",
        "    self.weight = np.zeros(1 + X.shape[1])\n",
        "\n",
        "    # calculates what the errors are\n",
        "    # Number of misclassifications\n",
        "    self.errors = []  # Number of misclassifications\n",
        "\n",
        "    # adjusts the weights based on the errors\n",
        "    for i in range(self.niter):\n",
        "      err = 0\n",
        "      for xi, target in zip(X, y):\n",
        "        delta_w = self.rate * (target - self.predict(xi))\n",
        "        self.weight[1:] += delta_w * xi\n",
        "        self.weight[0] += delta_w\n",
        "        err += int(delta_w != 0.0)\n",
        "      self.errors.append(err)\n",
        "    return self\n",
        "\n",
        "  def net_input(self, X):\n",
        "    \"\"\"Calculate net input\"\"\"\n",
        "    return np.dot(X, self.weight[1:]) + self.weight[0]\n",
        "\n",
        "  # makes a prediction based on input\n",
        "  def predict(self, X):\n",
        "    \"\"\"Return class label after unit step\"\"\"\n",
        "    return np.where(self.net_input(X) >= 0.0, 1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeLOnHoAyMe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#version with separate items\n",
        "\n",
        "class Perceptron(object):\n",
        "    \n",
        "  def __init__(self, niter = 10):\n",
        "    self.niter = niter\n",
        "    \n",
        "  def __sigmoid(self, x):\n",
        "    return None\n",
        "    \n",
        "  def __sigmoid_derivative(self, x):\n",
        "    return None\n",
        "\n",
        "  def fit(self, X, y):\n",
        "#   \"\"\"Fit training data\n",
        "#   X : Training vectors, X.shape : [#samples, #features]\n",
        "#   y : Target values, y.shape : [#samples]\n",
        "#   \"\"\"\n",
        "\n",
        "    # Does this assign random starting weights?...starting at zero, not random?\n",
        "    # weights\n",
        "    self.weight = np.zeros(1 + X.shape[1])\n",
        "\n",
        "    # calculates what the errors are\n",
        "    # Number of misclassifications\n",
        "    self.errors = []  # Number of misclassifications\n",
        "\n",
        "    # adjusts the weights based on the errors\n",
        "    for i in range(self.niter):\n",
        "      # Weighted sum of inputs / weights\n",
        "      weighted_sum = np.dot(inputs, weights)\n",
        "      \n",
        "      # Activate / activation function\n",
        "      activated_output = sigmoid(weighted_sum)\n",
        "      \n",
        "      # Calculating error\n",
        "      error = correct_outputs - activated_output\n",
        "      \n",
        "      # changes made to weights\n",
        "      adjustments = error * sigmoid_derivative(activated_output)\n",
        "      \n",
        "      # Update the Weights\n",
        "      weights += np.dot(inputs.T, adjustments)\n",
        "    return self\n",
        "\n",
        "  def net_input(self, X):\n",
        "    \"\"\"Calculate net input\"\"\"\n",
        "    return np.dot(X, self.weight[1:]) + self.weight[0]\n",
        "\n",
        "  # makes a prediction based on input\n",
        "  def predict(self, X):\n",
        "    \"\"\"Return class label after unit step\"\"\"\n",
        "    return np.where(self.net_input(X) >= 0.0, 1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrjaPWlx5anB",
        "colab_type": "code",
        "outputId": "66fa3bf1-a415-4bcf-b937-a213773c903a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "pn = Perceptron(0.1, 10)\n",
        "pn.fit(X, y)\n",
        "plt.plot(range(1, len(pn.errors) + 1), pn.errors, marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Number of misclassifications')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-02792181884b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of misclassifications'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() takes from 1 to 2 positional arguments but 3 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Naqxq9IVv60Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    for i in range(self.niter):\n",
        "      # Weighted sum of inputs / weights\n",
        "      weighted_sum = np.dot(inputs, weights)\n",
        "      \n",
        "      # Activate / activation function\n",
        "      activated_output = sigmoid(weighted_sum)\n",
        "      \n",
        "      # Calculating error\n",
        "      error = correct_outputs - activated_output\n",
        "      \n",
        "      # changes made to weights\n",
        "      adjustments = error * sigmoid_derivative(activated_output)\n",
        "      \n",
        "      # Update the Weights\n",
        "      weights += np.dot(inputs.T, adjustments)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "    \"\"\"Return class label after unit step\"\"\"\n",
        "        return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruqQoYHP5ISp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
        "  # setup marker generator and color map\n",
        "  markers = ('s', 'x', 'o', '^', 'v')\n",
        "  colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "  cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "  # plot the decision surface\n",
        "  x1_min, x1_max = X[:,  0].min() - 1, X[:, 0].max() + 1\n",
        "  x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "  xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "  np.arange(x2_min, x2_max, resolution))\n",
        "  Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "  Z = Z.reshape(xx1.shape)\n",
        "  plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
        "  plt.xlim(xx1.min(), xx1.max())\n",
        "  plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "  # plot class samples\n",
        "  for idx, cl in enumerate(np.unique(y)):\n",
        "    plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
        "    alpha=0.8, c=cmap(idx),\n",
        "    marker=markers[idx], label=cl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzoGU3md5J3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_decision_regions(X, y, classifier=pn)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6QR4oAW1xdyu"
      },
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
        "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
        "- Try and implement your own backpropagation algorithm.\n",
        "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
      ]
    }
  ]
}