{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:\n",
    "\n",
    "The input layer of a neural network takes the input signals, it doesn't apply any operations or have weights and biases. The input layer for image recognition might be all the pixels in the image.\n",
    "\n",
    "For a dataset in a dataframe, the input layer might be all the feature columns.\n",
    "\n",
    "### Hidden Layer:\n",
    "\n",
    "The hidden layer, or layers, of a neural network, will take inputs from the previous layer (either input or another hidden layer) and apply weights to all the inputs as well as a bias term and apply an activation function (some sort of sigmoid or other activation function). If the inputs trigger the activation function, then the hidden layer nueron will send a signal to the next layer.\n",
    "\n",
    "\n",
    "### Output Layer:\n",
    "\n",
    "The output layer is the last layer of the network and recieves signals from the last hidden layer. Depending on the signals it recieves it will modify the output signal that we see.\n",
    "\n",
    "### Neuron:\n",
    "\n",
    "A single node, the basic unit of neural network. It recieves inputs, applies a weight to them and a bias. It then applies an activation function, which if it activates, sends a signal to the next next layer of the network.\n",
    "\n",
    "### Weight:\n",
    "\n",
    "Weight represents the strength of the connection between two neurons/nodes. They can be positive, negative or near zero.\n",
    "\n",
    "### Activation Function:\n",
    "\n",
    "This is the function, usually sigmoid, tanh, or relu that introduce the non-linearity of neural networks. They squash the inputs into a smaller range like a sigmoid and so the function either activates and sends a signal to the next layer, or it doesn't activate and sends no signal.\n",
    "\n",
    "### Node Map:\n",
    "\n",
    "A visual representation of the neural net, there are several different NN structures that are suited for different tasks: LSTM for natural language, or Deep Convoluted NN for image recognition.\n",
    "\n",
    "### Perceptron:\n",
    "\n",
    "A single node neural network, it takes several inputs, weighs them, applies an activation function and a bias and then send out an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "#### Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(2001)\n",
    "\n",
    "inputs = np.array([\n",
    "    [0,1,0, 1],\n",
    "    [0,0,1,1]\n",
    "])\n",
    "\n",
    "correct_outputs = [[1,1,1,0]]\n",
    "\n",
    "def tanh(x):\n",
    "    nom = 2\n",
    "    denom = (1 + np.exp(-2 * x))\n",
    "    return (nom / denom) - 1\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    return 1 - (tanh(x) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.71298746, -0.27716047],\n",
       "       [-0.27310629, -0.95130961]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(inputs, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.61254677, -0.270275  ],\n",
       "       [-0.26651287, -0.74037537]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.71298746, -0.27716047],\n",
       "       [-0.27310629, -0.95130961]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,4) and (1,4) not aligned: 4 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-bd511a2abb2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#weigh the input variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mweighted_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mweighted_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,4) and (1,4) not aligned: 4 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "weights = 2 * np.random.random((1,4)) - 1\n",
    "\n",
    "for epoch in range(10000):\n",
    "    #weigh the input variables\n",
    "    weighted_input = np.dot(inputs, weights)\n",
    "    weighted_input\n",
    "\n",
    "    #activate the function on the weighted input\n",
    "    activated_output = tanh(weighted_input)\n",
    "    activated_output\n",
    "\n",
    "    #calculate the error term as the difference between correct and activated output\n",
    "    error = correct_outputs - activated_output\n",
    "    error\n",
    "\n",
    "    #gradient adjust back propogate the error\n",
    "    adjustments = error * tanh_deriv(activated_output)\n",
    "    adjustments\n",
    "\n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "    weights\n",
    "\n",
    "print('Weights after training ', weights)\n",
    "\n",
    "print('Activated output after training ', activated_output)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64351347, 0.65772245],\n",
       "       [0.06606537, 0.64535942],\n",
       "       [0.12120544, 0.50294212],\n",
       "       [0.39197015, 0.126939  ]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random((4,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting that this function did not work with tanh, I'll try sigmoid\n",
    "\n",
    "def perceptron(activation, deriv, inputs, correct_outputs, weights, epochs):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #weigh the input variables\n",
    "        weighted_input = np.dot(inputs, weights)\n",
    "\n",
    "        #activate the function on the weighted input\n",
    "        activated_output = activation(weighted_input)\n",
    "\n",
    "        #calculate the error term as the difference between correct and activated output\n",
    "        error = correct_outputs - activated_output\n",
    "        \n",
    "\n",
    "        #gradient adjust back propogate the error\n",
    "        adjustments = error * deriv(activated_output)\n",
    "        weights += np.dot(inputs.T, adjustments)\n",
    "        \n",
    "\n",
    "    print('Weights after training ', weights)\n",
    "\n",
    "    print('Activated output after training ', activated_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training  [[ 1.66533454e-16]\n",
      " [-3.05311332e-16]]\n",
      "Activated output after training  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([\n",
    "    [0,0],\n",
    "    [1,0],\n",
    "    [0,1],\n",
    "    [1,1]\n",
    "])\n",
    "\n",
    "correct_outputs = [[1], [1], [1], [0]]\n",
    "\n",
    "weights = 2 * np.random.random((2,1)) - 1\n",
    "\n",
    "perceptron(sigmoid, sigmoid_derivative, inputs, correct_outputs, weights, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training\n",
      "[[-16.45541379]\n",
      " [-16.45541379]\n",
      " [ 25.53131611]]\n",
      "Output after training\n",
      "[[1.00000000e+00]\n",
      " [9.99745484e-01]\n",
      " [9.99745484e-01]\n",
      " [2.80263095e-04]]\n"
     ]
    }
   ],
   "source": [
    "# neither of the above implementations works so I'll try the dummy columns\n",
    "import numpy as np\n",
    "np.random.seed(812)\n",
    "\n",
    "# adding a row of dummy ones\n",
    "\n",
    "inputs = np.array([\n",
    "    [0,0, 1],\n",
    "    [1,0, 1],\n",
    "    [0,1, 1],\n",
    "    [1,1, 1]\n",
    "])\n",
    "\n",
    "correct_outputs = [[1], [1], [1], [0]]\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1 - sx)\n",
    "\n",
    "weights = 2 * np.random.random((3,1)) - 1  #this creates the potential for negative random numbers, so we get range(-1,1)\n",
    "weights\n",
    "\n",
    "bias = -0.8\n",
    "\n",
    "for iteration in range(100000):\n",
    "    # Weighted sum of inputs/weights\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "    # Activate!\n",
    "    activated_output = sigmoid(weighted_sum + bias) \n",
    "    \n",
    "    # Cac error\n",
    "    error = correct_outputs - activated_output\n",
    "    \n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "    \n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "    \n",
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that worked, so I needed bias in order to `activate` the sigmoid function? maybe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset like: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "- [Titanic](https://raw.githubusercontent.com/ryanleeallred/datasets/master/titanic.csv)\n",
    "- [A two-class version of the Iris dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/Iris.csv)\n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv'\n",
    "pima_df = pd.read_csv(url)\n",
    "\n",
    "pima_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want activations that correspond to negative weights to be lower\n",
    "# and activations that correspond to positive weights to be higher\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Set up Architecture of neural network\n",
    "        self.input = 8\n",
    "        self.hiddenNodes = 9\n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        # Initial Weights\n",
    "        # 8x9 matrix for first layer\n",
    "        self.weights1 = np.random.randn(self.input, self.hiddenNodes)\n",
    "        #9x1 matrix for hidden to output layer\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forawrd\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #weighted sum of inputs and hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "\n",
    "        #Activations of weighted sums\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "\n",
    "        #Weighted sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "\n",
    "        # Final Activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "\n",
    "        return (self.activated_output)\n",
    "    \n",
    "    def get_attributes(self):\n",
    "        \n",
    "        attributes = ['weights1', 'hidden_sum', 'activated_hidden', 'weights2', 'output']\n",
    "\n",
    "        [print(i + '\\n', getattr(nn,i), '\\n' + '---'*3) for i in dir(nn) if i in attributes]\n",
    "\n",
    "    def backward(self, X, y, o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        self.o_error = y - o #error in output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o) # apply derivative of sigmoid to error\n",
    "\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T) # z2 error: how much our hidden layer weights were off\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)\n",
    "\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta) #adjust second set (hidden => output) weights\n",
    "        self.weights1 += X.T.dot(self.z2_delta) #Adjust first set (input => hidden) weights\n",
    "\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Outcome'\n",
    "\n",
    "X = pima_df.drop(columns=target).values\n",
    "y = np.array(pima_df[target]).reshape(pima_df.shape[0], 1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(691, 1)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 weights: \n",
      " [[-0.95536998 -0.26147293 -0.5255482  -1.27932348 -1.15666451 -0.82438149\n",
      "   0.85881406 -0.17868153  0.55579413]\n",
      " [-1.2205535  -1.10577648  0.24260815  0.63241576  1.11306683 -0.99875325\n",
      "   0.13751903 -0.02939866  0.4040244 ]\n",
      " [-1.43998803  0.3988736  -0.16112681  1.08606078  1.10026901  0.70566204\n",
      "   0.24548742 -2.01646116 -0.6149177 ]\n",
      " [-0.73106283 -0.58544964  0.01084408 -0.4755477   0.33709497  1.5317098\n",
      "   3.32809322  0.40969134  0.14847584]\n",
      " [-1.41704162 -0.77984809 -0.91179833 -1.51700024 -0.78250644  0.05435741\n",
      "  -0.1479062  -0.69677054 -0.79682132]\n",
      " [ 0.0144473   0.51020183  0.42874783  0.46821144  0.75036515 -1.20297078\n",
      "   0.28298656 -0.4748899  -0.02505139]\n",
      " [-0.36871357 -2.59725252  0.98548421  0.89970768  1.16001943 -2.13608208\n",
      "  -0.67313108 -0.07811872 -0.81917962]\n",
      " [-0.36132764 -1.1560178   0.33528418 -0.05034614 -0.01697218 -1.91639276\n",
      "   2.23954675  0.01640627 -0.27541553]]\n",
      "Layer 2 weights: \n",
      " [[ 0.52777853]\n",
      " [ 1.55659807]\n",
      " [-0.50422319]\n",
      " [ 1.20872874]\n",
      " [ 0.08290514]\n",
      " [ 0.6616254 ]\n",
      " [ 0.32657562]\n",
      " [ 1.04850656]\n",
      " [-1.46419506]]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "print(\"Layer 1 weights: \\n\", nn.weights1)\n",
    "print(\"Layer 2 weights: \\n\", nn.weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()\n",
    "\n",
    "for i in range(1000):\n",
    "    nn.train(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.35064935064935066\n"
     ]
    }
   ],
   "source": [
    "print('Loss: ', str(np.mean(np.square(y_test - nn.feed_forward(X_test)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
