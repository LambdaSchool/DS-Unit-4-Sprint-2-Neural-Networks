{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:\n",
    "\n",
    "The input layer is the layer or the neural network that git the values to the neural network.  It will pass its value, multiplied with a weight to the next layer of the neural network, which is the hidden layer.  There is usually the same number of nodes in the input layer as their are features in the in data.  The input layer is the layer that gets the values from the data set and then passes them into the neural network. \n",
    "\n",
    "### Hidden Layer:\n",
    "\n",
    "The hidden layer are the layers of the neural network which is not accessable from the outside of the neural network.  They recieve the values multiplied by the weights from either a previous hidden layer or from the input layer.  They will use an activation function to adjust the signal either and then send the signal to another hidden layer or to the output layer.\n",
    "\n",
    "### Output Layer:\n",
    "\n",
    "The output layer recieves input from the hidden layer and then will create the output.  It at times will also have a function act upon it to change the values such that the endresult is what is needed for they type of model. For example if it is a classification it will give a value that will mean a certain type of classification.\n",
    "\n",
    "### Neuron:\n",
    "\n",
    "A neuron is node with in a neural network. You can think of a nueron as each node in the hidden layers and the output layers, and the input layers.  \n",
    "\n",
    "### Weight:\n",
    "\n",
    "The weight is multiplied by the values passed to a hidden layer node.  Weights that are higher, will cause the value that is passed to the hidden layer to have a larger effect or (conncection) to the hidden node.  Values that are passed in that are multiplied with a small weight will have less of an effect on what the hidden layers will pass on to the next node. \n",
    "\n",
    "### Activation Function:\n",
    "\n",
    "The activation function is a function that causes that value that the output node to have to be outputted in an expected format.  One of the common types of activation functions, for example is the sigmoid function.  This function will cause that all the values will be bounded between 1 and 0.\n",
    "\n",
    "### Node Map:\n",
    "\n",
    "A node map is a visual representation of the topology or the flow of the neural network.  It shows how each node will interact with the nodes around it.  \n",
    "\n",
    "### Perceptron:\n",
    "\n",
    "A perceptron is a single layered neural network. It will have inputs and then multiply them by the weights and then sum them up and then pass the sum into an activation function (possibly the sigmoid function) which will output a value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "The flow of a neural network is thus:\n",
    "\n",
    "The inputs (the values from the data) are passed into neural network to the input layers.  There is usually one input node per feature of the data.  From the input layer the value from each node is sent to all the nodes of the next layer, which is a hidden layer.   Each node of the hidden layer will recieve a value from all the input nodes.  When the value is passed to the hidden layer, it is multiplied by a weight value and then summed up with the other values at that node that have also been multiplied by their own specific weight.  This summed up value is then added to the bias.  At each layer all the nodes will then pass their information into an activation function.  The activation function at this point is used to help determine if and what is passed on to the next layer of nodes.   This complete value is then passed on to the next layer or the neural network, which may be another hidden layer and the same process is then repeated, or it is passed to the output layer where the values are put into a transormation function.  This function will make it so the values outputed are the type expected, such as possibly bounding the values between 1 and 0.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   x1  x2  y\n0   0   0  1\n1   1   0  1\n2   0   1  1\n3   1   1  0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   x1  x2\n0   0   0\n1   1   0\n2   0   1\n3   1   1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1</th>\n      <th>x2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "train = df[[\"x1\", \"x2\"]]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a sigmoid function\n",
    "def sigmoid_function(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating derivative of the sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid_function(x)\n",
    "    return (sx * (1-sx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the weights that will be used at the start\n",
    "def create_weights(numImputs):\n",
    "    np.random.seed(42)\n",
    "    weights = 2 * np.random.random((numImputs,1)) -1\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[-0.25091976],\n       [ 0.90142861]])"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# Looking at the weights\n",
    "weights = create_weights(2)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing the dot product to get the weight sum\n",
    "def weighted_sum(weights, input):\n",
    "    weighted_sum = np.dot(input, weights)\n",
    "    return weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.        ],\n       [-0.25091976],\n       [ 0.90142861],\n       [ 0.65050885]])"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "weights_sum = weighted_sum(weights, train)\n",
    "weights_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.5       ],\n       [0.43759713],\n       [0.71124299],\n       [0.65712512]])"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# trying out the weighted_sum method\n",
    "activated_output = sigmoid_function(weights_sum)\n",
    "activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.711242993921282"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "sigmoid_function(0.90142861)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the series into a multidimensional array\n",
    "def make_y_vals_correct_shape():\n",
    "    y = df[\"y\"]\n",
    "    y = np.array(y)\n",
    "    return  np.reshape(y, (y.shape[0], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = make_y_vals_correct_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1],\n       [1],\n       [1],\n       [0]])"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.5       ],\n       [ 0.56240287],\n       [ 0.28875701],\n       [-0.65712512]])"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "# calculating the difference between the correct and the \n",
    "# error\n",
    "\n",
    "error = y - activated_output\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.125     ],\n       [ 0.13841065],\n       [ 0.05930387],\n       [-0.14805798]])"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "# now doing the adjustment of the error\n",
    "# calculating the amount to adjust the weights\n",
    "amount_to_adjust = error*sigmoid_derivative(weights_sum)\n",
    "amount_to_adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.29832161],\n       [0.84243177]])"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# now we are updating the weigts by adding the adjust amout to the weight\n",
    "weights += np.dot(train.T, amount_to_adjust)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that will do perceptron code it will iterate through the steps until it has set the weights and the bias\n",
    "# to be able to predict with 99% accuracy.\n",
    "def adjust_weights():\n",
    "\n",
    "    # creating a set of weights to start with:\n",
    "    weights = create_weights(2)\n",
    "    y = make_y_vals_correct_shape()\n",
    "    learning_rate = .1\n",
    "    numEpochs = 15000\n",
    "    # Will now the for loop for the iterations to adjust the weights\n",
    "    for i in range(numEpochs):\n",
    "        # Getting the activated output\n",
    "        sums_of_weights = weighted_sum(weights, train) + 7\n",
    "        activated_output = sigmoid_function(sums_of_weights)\n",
    "        # will then the errror of the amounts\n",
    "        error = y - activated_output\n",
    "        # will then get the amount needed to adjust the weights\n",
    "        amount_to_adjust = error*sigmoid_derivative(weights_sum) # * learning_rate\n",
    "        # will now adjust the weights\n",
    "        weights += np.dot(train.T, amount_to_adjust)\n",
    "    \n",
    "    # Will print out the weights and then the outputs\n",
    "    print(f\"The weights after training with {numEpochs} epochs is:\\n\")\n",
    "    print(weights, \"\\n\")\n",
    "    # Printing out the activated outputs\n",
    "    print(f\"The outputs after training for {numEpochs} is:\\n\")\n",
    "    print(activated_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The weights after training with 15000 epochs is:\n\n[[-4.56835883]\n [-4.76686051]] \n\nThe outputs after training for 15000 is:\n\n[[0.99908895]\n [0.9192085 ]\n [0.90318623]\n [0.08824781]]\n"
    }
   ],
   "source": [
    "# running the function above:\n",
    "adjust_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n0            6      148             72             35        0  33.6   \n1            1       85             66             29        0  26.6   \n2            8      183             64              0        0  23.3   \n3            1       89             66             23       94  28.1   \n4            0      137             40             35      168  43.1   \n\n   DiabetesPedigreeFunction  Age  Outcome  \n0                     0.627   50        1  \n1                     0.351   31        0  \n2                     0.672   32        1  \n3                     0.167   21        0  \n4                     2.288   33        1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregnancies</th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n      <th>Outcome</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>148</td>\n      <td>72</td>\n      <td>35</td>\n      <td>0</td>\n      <td>33.6</td>\n      <td>0.627</td>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>85</td>\n      <td>66</td>\n      <td>29</td>\n      <td>0</td>\n      <td>26.6</td>\n      <td>0.351</td>\n      <td>31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>183</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.3</td>\n      <td>0.672</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>89</td>\n      <td>66</td>\n      <td>23</td>\n      <td>94</td>\n      <td>28.1</td>\n      <td>0.167</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>137</td>\n      <td>40</td>\n      <td>35</td>\n      <td>168</td>\n      <td>43.1</td>\n      <td>2.288</td>\n      <td>33</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "X = diabetes[feats]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dtype('int64')"
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "diabetes[\"Outcome\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will be doing minmax scaling\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.35294118, 0.74371859, 0.59016393, ..., 0.50074516, 0.23441503,\n        0.48333333],\n       [0.05882353, 0.42713568, 0.54098361, ..., 0.39642325, 0.11656704,\n        0.16666667],\n       [0.47058824, 0.91959799, 0.52459016, ..., 0.34724292, 0.25362938,\n        0.18333333],\n       ...,\n       [0.29411765, 0.6080402 , 0.59016393, ..., 0.390462  , 0.07130658,\n        0.15      ],\n       [0.05882353, 0.63316583, 0.49180328, ..., 0.4485842 , 0.11571307,\n        0.43333333],\n       [0.05882353, 0.46733668, 0.57377049, ..., 0.45305514, 0.10119556,\n        0.03333333]])"
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n0            6      148             72             35        0  33.6   \n1            1       85             66             29        0  26.6   \n\n   DiabetesPedigreeFunction  Age  \n0                     0.627   50  \n1                     0.351   31  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregnancies</th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>148</td>\n      <td>72</td>\n      <td>35</td>\n      <td>0</td>\n      <td>33.6</td>\n      <td>0.627</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>85</td>\n      <td>66</td>\n      <td>29</td>\n      <td>0</td>\n      <td>26.6</td>\n      <td>0.351</td>\n      <td>31</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, niter = 10):\n",
    "        self.niter = niter\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "        \n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        sx = self.__sigmoid(x)\n",
    "        return (sx * (1-sx))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "   # \"\"\"Fit training data\n",
    "   # X : Training vectors, X.shape : [#samples, #features]\n",
    "   # y : Target values, y.shape : [#samples]\n",
    "   # \"\"\"\n",
    "        # Will be scaling the data\n",
    "        self.scaler = MinMaxScaler()\n",
    "        scaled_data = self.scaler.fit_transform(X)\n",
    "        num_Cols = len(X.columns.values.tolist())\n",
    "        # Randomly Initialize Weights\n",
    "        weights = 2 * np.random.random((num_Cols,1) ) - 1\n",
    "\n",
    "        for i in range(self.niter):\n",
    "            # Weighted sum of inputs / weights\n",
    "            weight_sum = weighted_sum(weights, X)\n",
    "            # Activate!\n",
    "            activated_output = self.__sigmoid(X)\n",
    "            \n",
    "            \n",
    "            # Cac error\n",
    "            error = y - activated_output\n",
    "\n",
    "            # Update the Weights\n",
    "            # Getting the amount to adjust\n",
    "            amount_to_adjust = error*self.__sigmoid_derivative(weight_sum)\n",
    "            weights = weights + np.dot(X.T, amount_to_adjust)\n",
    "        #breakpoint()\n",
    "        print(\"The acitvated amount of the first train data after the iterations is: \", activated_output[:1], \"\\n\")\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        # Will be doing the transform of the data that is to \n",
    "        # be predicted\n",
    "        scaled_data = self.scaler.transform(X)\n",
    "    #\"\"\"Return class label after unit step\"\"\"\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to run the class\n",
    "p = Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The acitvated amount of the first train data after the iterations is:     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  \\\n0     0.997527      1.0            1.0            1.0      0.5  1.0   \n\n   DiabetesPedigreeFunction  Age  \n0                  0.651809  1.0   \n\n"
    }
   ],
   "source": [
    "p.fit(X, diabetes[\"Outcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing a predidiction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('lam4-nnf-s2': conda)",
   "language": "python",
   "name": "python37064bitlam4nnfs2condaa223e28e4e5343f49ecebf6cc0bb1d2c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}