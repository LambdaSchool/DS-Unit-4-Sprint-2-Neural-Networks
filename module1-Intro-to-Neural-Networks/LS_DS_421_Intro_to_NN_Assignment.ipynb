{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:\n",
    "The input layer is composed of artificial input neurons. This input brings the initial data into the system for further processing by subsequent layers of artificial neurons. The input layer is the very beginning of the workflow for the artificial neural network.\n",
    "\n",
    "### Hidden Layer:\n",
    "A hidden layer is a layer in between input layers and output layers, where artificial neurons take in a set of weighted inputs, the bias and then produce an output through an activation function. All the computation is done in this layer.\n",
    "\n",
    "### Output Layer:\n",
    "The output layer produces the result for given inputs, after being processed in the hidden layer.\n",
    "\n",
    "### Neuron:\n",
    " a neuron is a mathematical function that model the functioning of a biological neuron. Typically, a neuron compute the weighted average of its input, and this sum is passed through a nonlinear function, often called activation function, such as the sigmoid.\n",
    "\n",
    "### Weight:\n",
    "A weight represent the strength of the connection between units. This weight brings down the importance of the input value.\n",
    "\n",
    "### Activation Function:\n",
    "the activation function is responsible for transforming the summed weighted input from the node into the activation of the node (or output) for that input.\n",
    "\n",
    "### Node Map:\n",
    "Node map is a visual representation of the structure of a neural networks layers.\n",
    "\n",
    "### Perceptron:\n",
    "A neural network with a single layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "#### The data is fed into the neural network through nodes in the input layer. These input nodes are transformed in the hidden layers by computing the dot product between these inputs and the weights and then combining with a bias value. The results are fed through an activation function, being finally presented in the output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "inputs = np.array([\n",
    "    [0,0],\n",
    "    [1,0],\n",
    "    [0,1],\n",
    "    [1,1]\n",
    "])\n",
    "\n",
    "correct_outputs = [[1], [1], [1], [0]]\n",
    "\n",
    "X=inputs\n",
    "y=correct_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/ (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "# class Perceptron(object):\n",
    "#     \"\"\"\n",
    "#     Perceptron classifier.\n",
    "    \n",
    "#     Parameters\n",
    "#     -----------\n",
    "#     eta: float\n",
    "#         learning rate (between 0.0 and 1.0)\n",
    "#     n_iter: int\n",
    "#         Passes (epochs) over the training set.\n",
    "    \n",
    "#     Attributes\n",
    "#     -----------\n",
    "#     weight: ld-array\n",
    "#         Weights after fitting.\n",
    "#     errors:list\n",
    "#         Number of misclassification in every epoch.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, eta = 0.01, n_iter = 10):\n",
    "#         self.eta = eta\n",
    "#         self.n_iter = n_iter\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         \"\"\"\n",
    "#         Fit method for training data.\n",
    "        \n",
    "#         Parameters\n",
    "#         ---------\n",
    "#         X: {array-like}, shape = [n_samples, n_features]   # rows by columns\n",
    "#             Training vectors, where 'n_samples' is the number of samples\n",
    "#             and 'n_features' is the number of features.\n",
    "#         y: {array-like}, shape = [n_samples]\n",
    "#             Target values.\n",
    "            \n",
    "#         Returns\n",
    "#         ---------\n",
    "#         self:object\n",
    "        \n",
    "#         \"\"\"\n",
    "\n",
    "#         # weights\n",
    "#         self.weight = np.zeros(1 + X.shape[1])\n",
    "        \n",
    "#         # number of misclassifications\n",
    "#         self.errors = []\n",
    "        \n",
    "#         for _ in range(self.n_iter):   #loop through my data\n",
    "#             errors = 0\n",
    "#             for xi, target in zip(X,y):   #bind together X with y with zip\n",
    "#                 update = self.eta * (target - self.predict(xi))\n",
    "#                 self.weight[1:] += update * xi\n",
    "#                 self.weight[0] += update\n",
    "#                 errors += int(update != 0.0)   #only calculate the error if the value is not exactly the correct value, otherwise, error = 0\n",
    "#             self.errors.append(errors)\n",
    "#         return self\n",
    "    \n",
    "#     def net_input(self, X):\n",
    "#         \"\"\"\n",
    "#         Calculate the net input.\n",
    "#         \"\"\"\n",
    "#         return np.dot(X, self.weight[1:]) + self.weight[0]\n",
    "    \n",
    "#     def predict(self,X):\n",
    "#         \"\"\"\n",
    "#         Return class label after unit step.\n",
    "#         \"\"\"\n",
    "#         return np.where(self.net_input(X) >= 2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.25091976],\n",
       "       [ 0.90142861]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # initializing random weights\n",
    "# weights = 2 * np.random.random((2, 1)) - 1\n",
    "\n",
    "# weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [-0.25091976],\n",
       "       [ 0.90142861],\n",
       "       [ 0.65050885]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted_sum = np.dot(X, weights)\n",
    "\n",
    "# weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [-0.25091976],\n",
       "       [ 0.90142861],\n",
       "       [ 0.65050885]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted_sum.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Perceptron(object):\n",
    "#     \"\"\"\n",
    "#     Perceptron classifier.\n",
    "    \n",
    "#     Parameters\n",
    "#     -----------\n",
    "#     eta: float\n",
    "#         learning rate (between 0.0 and 1.0)\n",
    "#     n_iter: int\n",
    "#         Passes (epochs) over the training set.\n",
    "    \n",
    "#     Attributes\n",
    "#     -----------\n",
    "#     weight: ld-array\n",
    "#         Weights after fitting.\n",
    "#     errors:list\n",
    "#         Number of misclassification in every epoch.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, eta = 0.01, n_iter = 10):\n",
    "#         self.eta = eta\n",
    "#         self.n_iter = n_iter\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         \"\"\"Fit training data\n",
    "#         X : Training vectors, X.shape : [#samples, #features]\n",
    "#         y : Target values, y.shape : [#samples]\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Randomly Initialize Weights\n",
    "#         self.weights = np.zeros(1 + X.shape[1])\n",
    "#         self.errors = []\n",
    "        \n",
    "#         for i in range(self.n_iter):\n",
    "#           err = 0\n",
    "#           for xi, target in zip(X, y):\n",
    "#             delta_w = self.eta * (target - self.predict(xi))\n",
    "#             self.weights[1:] += delta_w * xi\n",
    "#             self.weights[0] += delta_w\n",
    "#             err += int(delta_w != 0.0)\n",
    "#           self.errors.append(err)\n",
    "#         return self\n",
    "\n",
    "#     def net_input(self, X):\n",
    "#         \"\"\"Calculate net input\"\"\"\n",
    "#         return np.dot(X, self.weights[1:]) + self.weights[0]\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         \"\"\"Return class label after unit step\"\"\"\n",
    "#         return np.where(self.net_input(X) >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Perceptron at 0x2c4b99cae80>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nand_class = Perceptron()\n",
    "\n",
    "# nand_class.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nand_class.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: overflow encountered in multiply\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Lenovo\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: overflow encountered in multiply\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training\n",
      "[[nan]\n",
      " [nan]]\n",
      "Output after training\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "Error\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "# # Update our weights 10,000 times\n",
    "# for iteration in range(10000):\n",
    "    \n",
    "#     # Weighted sum of inputs / weights\n",
    "#     weighted_sum = np.dot(X, weights)\n",
    "    \n",
    "#     # Activate!\n",
    "#     activated_output = weighted_sum\n",
    "    \n",
    "#     # Cac error\n",
    "#     error = correct_outputs - activated_output\n",
    "    \n",
    "#     adjustments = error * activated_output\n",
    "    \n",
    "#     # Update the Weights\n",
    "#     adjustments = error * activated_output\n",
    "#     weights += np.dot(X.T, adjustments)\n",
    "    \n",
    "# print(\"Weights after training\")\n",
    "# print(weights)\n",
    "\n",
    "# print(\"Output after training\")\n",
    "# print(activated_output)\n",
    "\n",
    "# print(\"Error\")\n",
    "# print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "X = diabetes[['Pregnancies', 'BloodPressure', 'Age']].values\n",
    "y = np.array(diabetes['Outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6, 72, 50],\n",
       "       [ 1, 66, 31],\n",
       "       [ 8, 64, 32],\n",
       "       ...,\n",
       "       [ 5, 72, 30],\n",
       "       [ 1, 60, 47],\n",
       "       [ 1, 70, 23]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron(object):\n",
    "    \"\"\"\n",
    "    Perceptron classifier.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    eta: float\n",
    "        learning rate (between 0.0 and 1.0)\n",
    "    n_iter: int\n",
    "        Passes (epochs) over the training set.\n",
    "    \n",
    "    Attributes\n",
    "    -----------\n",
    "    weight: ld-array\n",
    "        Weights after fitting.\n",
    "    errors:list\n",
    "        Number of misclassification in every epoch.\n",
    "    \"\"\"\n",
    "    def __init__(self, eta = 0.01, n_iter = 10):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data\n",
    "        X : Training vectors, X.shape : [#samples, #features]\n",
    "        y : Target values, y.shape : [#samples]\n",
    "        \"\"\"\n",
    "\n",
    "        # Randomly Initialize Weights\n",
    "        self.weights = np.zeros(1 + X.shape[1])\n",
    "        self.errors = []\n",
    "        \n",
    "        for _ in range(self.n_iter):\n",
    "          err = 0\n",
    "          for xi, target in zip(X, y):\n",
    "            delta_w = self.eta * (target - self.predict(xi))\n",
    "            self.weights[1:] += delta_w * xi\n",
    "            self.weights[0] += delta_w\n",
    "            err += int(delta_w != 0.0)\n",
    "          self.errors.append(err)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.weights[1:]) + self.weights[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Perceptron(n_iter = 100)\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[1, 66, 21]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65234375"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(model.predict(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = scaler.fit_transform(diabetes[feats].values)\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Perceptron(object):\n",
    "#     \"\"\"\n",
    "#     Perceptron classifier.\n",
    "    \n",
    "#     Parameters\n",
    "#     -----------\n",
    "#     eta: float\n",
    "#         learning rate (between 0.0 and 1.0)\n",
    "#     n_iter: int\n",
    "#         Passes (epochs) over the training set.\n",
    "    \n",
    "#     Attributes\n",
    "#     -----------\n",
    "#     weight: ld-array\n",
    "#         Weights after fitting.\n",
    "#     errors:list\n",
    "#         Number of misclassification in every epoch.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, eta = 0.01, n_iter = 10):\n",
    "#         self.eta = eta\n",
    "#         self.n_iter = n_iter\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         \"\"\"Fit training data\n",
    "#         X : Training vectors, X.shape : [#samples, #features]\n",
    "#         y : Target values, y.shape : [#samples]\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Randomly Initialize Weights\n",
    "#         self.weights = np.zeros(1 + X.shape[1])\n",
    "#         self.errors = []\n",
    "        \n",
    "#         for _ in range(self.n_iter):\n",
    "#           err = 0\n",
    "#           for xi, target in zip(X, y):\n",
    "#             delta_w = self.eta * (target - self.predict(xi))\n",
    "#             self.weights[1:] += delta_w * xi\n",
    "#             self.weights[0] += delta_w\n",
    "#             err += int(delta_w != 0.0)\n",
    "#           self.errors.append(err)\n",
    "#         return self\n",
    "\n",
    "#     def net_input(self, X):\n",
    "#         \"\"\"Calculate net input\"\"\"\n",
    "#         return np.dot(X, self.weights[1:]) + self.weights[0]\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         \"\"\"Return class label after unit step\"\"\"\n",
    "#         return np.where(self.net_input(X) >= 0.0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perceptron = Perceptron()\n",
    "\n",
    "# perceptron.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = perceptron.predict(X)\n",
    "# diabetes['preds'] = preds\n",
    "# diabetes['correct'] = diabetes['Outcome'] == diabetes['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diabetes.correct.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
