{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "An initial linear combination of inputs and added non-linearity form a layer in the Neural Networks.\n",
    "\n",
    "There are 3 main types of layers in NN\n",
    "\n",
    "### Input Layer: \n",
    "Input layer is what receives inputs. these inputs can be loaded from an  external source such as web service or csv file. Input layers takes in the inputs, performs the calculations via its neurons and then output is transmitted onto the subsequent layers. Sometimes it is also called visible layers because it is the only layer that our data intracts with directly.\n",
    "\n",
    "The number of neurons in an input layer is dependent on the shape of your training data.\n",
    "Traditionally,\n",
    "\n",
    "    Number of Neurons = Number of Training data Features + 1\n",
    "\n",
    "One additional node is to capture the bias term.\n",
    "\n",
    "The neurons simply calculate the weighted sum of inputs and weights, add the bias and execute an activation function.\n",
    "\n",
    "### Hidden Layer:\n",
    "Hidden layers resides in-between inputs layers and output layers,where artificial neurons take in a set of weighted inputs and produce an output through an activation function.The hidden layers' job is to transform the inputs into something that the output layer can use.hidden layer in artificial neural networks a layer of neurons, whose output is connected to the inputs of other neurons and therefore is not visible as a network output.There could be zero or more hidden layers in a neural network.\n",
    "\n",
    "### Output Layer:\n",
    "The final layer is called the Output Layer.The output layer takes inputs from the  previous layer before it, performs the calculations via its neurons and then the output is computed. The purpose of the output layer is to output a vector of values that is in a format that is suitable for the type of problem that we're trying to address.\n",
    "\n",
    "### Neuron:\n",
    "Neural network is a set of neurons organized in layers. Each neuron is a mathematical operation that takes it’s input, multiplies it by it’s weights, subsequently, a bias (constant) is added to the weighted sum and then passes the weighted sum through the activation function to the other neurons.\n",
    "\n",
    "### Weight:\n",
    "In Neural network, some inputs are provided to an artificial neuron, and with each input a weight is associated.The weight shows the effectivenessof  a particular input(or importance of the feature). More the weight of input, more it will have impact on network.Weight increases the steepness of activation function. This means weight decide how fast the activation function will trigger whereas bias is used to delay the triggering of the activation function.\n",
    "\n",
    "Bias is like the intercept added in a linear equation. It is an additional parameter in the Neural Network which is used to adjust the output along with the weighted sum of the inputs to the neuron. Therefore Bias is a constant which helps the model in a way that it can fit best for the given data.\n",
    "### Activation Function:\n",
    "Activation function decides, whether a neuron should be activated or not by calculating weighted sum and further adding bias with it. The purpose of the activation function is to introduce non-linearity into the output of a neuron.A neural network without an activation function is essentially just a linear regression model.  The activation function does the non-linear transformation to the input making it capable to learn and perform more complex tasks.It is used to determine the output of neural network like yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. \n",
    "Activation functions (non-linearities) are needed so we can break\n",
    "the linearity and represent more complicated relationships.Moreover, activation functions are required in order to stack layers.\n",
    "Activation functions transform inputs into outputs of a different kind.\n",
    "Activation functions are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated (“fired”) or not, based on whether each neuron’s input is relevant for the model’s prediction. \n",
    "For example- . Ourbrain is a kind of an ‘activation function’. It tells us whether it is cold\n",
    "enough for us to put on a jacket.\n",
    "\n",
    "\n",
    "### Node Map:\n",
    "### Perceptron:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?\n",
    "Neural network is a set of neurons organized in layers. Each neuron is a mathematical operation that takes it’s input, multiplies it by it’s weights, subsequently, a bias (constant) is added to the weighted sum and then passes the weighted sum through the activation function to the other neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "#### Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   0   0  1\n",
       "1   1   0  1\n",
       "2   0   1  1\n",
       "3   1   1  0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "Y = df['y'].values\n",
    "Y = df['y'].to_numpy().reshape(-1, 1)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:, [0,1]].values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "\n",
    "    def __init__(self, input_size=2, output_size=1, lr=0.01, niter=10):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.lr = lr\n",
    "        self.niter = niter\n",
    "\n",
    "        # Initialize weights once\n",
    "        self.W = self.__init_weights(type_='custom')\n",
    "        self.b = self.__init_bias()\n",
    "        self.loss = []\n",
    "\n",
    "    def __init_weights(self, type_='custom'):\n",
    "        if type_ == 'uniform':\n",
    "            W = np.random.random(self.input_size, self.output_size)\n",
    "        elif type_ == 'normal':\n",
    "            W = np.random.randn(self.input_size, self.output_size)\n",
    "        elif type_ == 'custom':\n",
    "            W = 2 * np.random.random((self.input_size, self.output_size)) - 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'Parameter passed is wrong type, can only be \"uniform\", \"normal\", or \"custom\"')\n",
    "        return W\n",
    "\n",
    "    def __init_bias(self):\n",
    "        return np.full((1, self.output_size), 0.1)\n",
    "\n",
    "    def __sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    def __sigmoid_derivative(self, X):\n",
    "        sx = self.__sigmoid(X)\n",
    "        return sx * (1-sx)\n",
    "    def __feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Computes Z values, which passing through sigmoid gives prediction probability\n",
    "        \"\"\"\n",
    "        # Weighted sum of inputs / weights + bias\n",
    "        Z = (X @ self.W) + self.b\n",
    "        return Z\n",
    "\n",
    "    def __backprop(self, dO, X):\n",
    "        \"\"\"\n",
    "        Calculate weight adjustments\n",
    "        \"\"\"\n",
    "        dW = np.dot(X.T, dO)\n",
    "        db = np.sum(dO)\n",
    "        return dW, db\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data\n",
    "        X : Training vectors, X.shape : [#samples, #features]\n",
    "        y : Target values, y.shape : [#samples]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            y.shape[1]\n",
    "        except IndexError:\n",
    "            y = y.reshape(-1, 1)\n",
    "        \n",
    "        for i in range(self.niter):\n",
    "            # Forward Prop\n",
    "            Z = self.__feed_forward(X)\n",
    "            y_hat = self.__sigmoid(Z)\n",
    "\n",
    "            # Calculate cost/error MAE\n",
    "            E = (y - y_hat)\n",
    "            self.loss.append(np.sum(E.T))\n",
    "\n",
    "            # Calculate adjustments/gradient\n",
    "            dO = E * self.__sigmoid_derivative(y_hat)\n",
    "            dW, db = self.__backprop(dO, X)\n",
    "\n",
    "            # Update the Weights with new gradient\n",
    "            self.W += self.lr * dW\n",
    "            self.b += self.lr * db\n",
    "            \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return prediction probabilites\"\"\"\n",
    "        return self.__sigmoid(self.__feed_forward(X))\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Return class labels\"\"\"\n",
    "        pred_probas = self.predict_proba(X)\n",
    "        return (pred_probas > threshold).astype('int32')\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(np.arange(1, len(self.loss)+1), np.array(self.loss),\n",
    "                 marker='o', color='steelblue')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('MAE Error')\n",
    "        plt.show()\n",
    "\n",
    "    def accuracy_score(self, y_true, y_pred):\n",
    "        return (y_true == y_pred).sum()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nand = Perceptron(input_size=2, output_size=1, lr=0.1, niter=1000)\n",
    "nand.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training\n",
      "[[-2.92082557]\n",
      " [-2.91348555]]\n",
      "Predict X1: 0, X2: 0, y = [[1]]\n",
      "Predict X1: 0, X2: 1, y = [[1]]\n",
      "Predict X1: 1, X2: 0, y = [[1]]\n",
      "Predict X1: 1, X2: 1, y = [[0]]\n"
     ]
    }
   ],
   "source": [
    "print('Weights after training')\n",
    "print(nand.W)\n",
    "\n",
    "\n",
    "test_X = np.array\n",
    "\n",
    "print(f'Predict X1: 0, X2: 0, y = {nand.predict(np.array([0, 0]))}')\n",
    "print(f'Predict X1: 0, X2: 1, y = {nand.predict(np.array([0, 1]))}')\n",
    "print(f'Predict X1: 1, X2: 0, y = {nand.predict(np.array([1, 0]))}')\n",
    "print(f'Predict X1: 1, X2: 1, y = {nand.predict(np.array([1, 1]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nand.plot_loss();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "X = diabetes[feats]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pregnancies',\n",
       " 'Glucose',\n",
       " 'BloodPressure',\n",
       " 'SkinThickness',\n",
       " 'Insulin',\n",
       " 'BMI',\n",
       " 'DiabetesPedigreeFunction',\n",
       " 'Age']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, niter = 10):\n",
    "        self.niter = niter\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return None\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "    \"\"\"Fit training data\n",
    "    X : Training vectors, X.shape : [#samples, #features]\n",
    "    y : Target values, y.shape : [#samples]\n",
    "    \"\"\"\n",
    "\n",
    "        # Randomly Initialize Weights\n",
    "        weights = ...\n",
    "\n",
    "        for i in range(self.niter):\n",
    "            # Weighted sum of inputs / weights\n",
    "\n",
    "            # Activate!\n",
    "\n",
    "            # Cac error\n",
    "\n",
    "            # Update the Weights\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "    \"\"\"Return class label after unit step\"\"\"\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
