{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer: This is what takes in the data that is being fed into the model. \n",
    "### Hidden Layer: This is where the neural network takes the input and processes it into the output given weights and biases.\n",
    "### Output Layer: This is what comes out of the model, either a classification or a prediction value or set of values. \n",
    "### Neuron: This takes the inputs, multiplies them by the weights, and does the sum. This is basically what does the work for the hidden layers. \n",
    "### Weight: This is a measure of how important a feature is. The heavier the weight, the more important it is. \n",
    "### Activation Function: This defines the output of the neuron given a particular input. This is what is used to do the calculations. \n",
    "### Node Map: A diagram illustrating the different layers of a neural network. \n",
    "### Perceptron: A single-layer neural network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "The flow of a neural network stars with both the input and the output when training. The network sees both the input data and the output classification or value and creates a network based on layers which interpret the data using the following: Weights, which give more or less importance to the features, biases, which are values that are created to account for values in the data that don't adhere exactly to a linear model (think of a sigmoid graph), and activation functions, which are the method used to construct the model, which can be linear or non-linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(4, 3)\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   x1  x2  y\n0   0   0  1\n1   1   0  1\n2   0   1  1\n3   1   1  0"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return 1/(1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 1, 0])"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_output = np.array(df['y'])\n",
    "correct_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "weights:\n [[0.37454012 0.95071431 0.73199394 0.59865848]\n [0.15601864 0.15599452 0.05808361 0.86617615]]\n\n(4, 2) (2, 4) \n\nweighted_sum:\n [[0.         0.         0.         0.        ]\n [0.37454012 0.95071431 0.73199394 0.59865848]\n [0.15601864 0.15599452 0.05808361 0.86617615]\n [0.53055876 1.10670883 0.79007755 1.46483463]]\n\nactivated_outputs:\n [[0.5        0.5        0.5        0.5       ]\n [0.59255557 0.72125881 0.67524268 0.64534933]\n [0.53892573 0.53891974 0.51451682 0.70394941]\n [0.62961342 0.75151503 0.68784798 0.81227101]]\n\nerror:\n [[ 0.5         0.5         0.5        -0.5       ]\n [ 0.40744443  0.27874119  0.32475732 -0.64534933]\n [ 0.46107427  0.46108026  0.48548318 -0.70394941]\n [ 0.37038658  0.24848497  0.31215202 -0.81227101]]\n\nadjustments:\n [[ 1.32436064  1.32436064  1.32436064 -1.32436064]\n [ 1.14435026  0.85211767  0.96274622 -1.87580654]\n [ 1.25143136  1.25144289  1.29761402 -2.12713912]\n [ 1.06555954  0.77532524  0.93315649 -2.64233285]]\n\nweights:\n [[ 2.58444992  2.57815722  2.62789665 -3.9194809 ]\n [ 2.47300954  2.18276266  2.28885412 -3.90329582]]\n\n"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "weights = np.random.random((2,4))\n",
    "print(f'weights:\\n {weights}\\n')\n",
    "print(df.iloc[::, [0,1]].shape, weights.shape, '\\n')\n",
    "\n",
    "inputs = df.iloc[::, [0,1]]\n",
    "weighted_sum = np.dot(inputs, weights)\n",
    "print(f'weighted_sum:\\n {weighted_sum}\\n')\n",
    "\n",
    "activated_outputs = sigmoid(weighted_sum)\n",
    "print(f'activated_outputs:\\n {activated_outputs}\\n')\n",
    "\n",
    "error = correct_output - activated_outputs\n",
    "print(f'error:\\n {error}\\n')\n",
    "\n",
    "adjustments = error * sigmoid_derivative(activated_outputs)\n",
    "print(f'adjustments:\\n {adjustments}\\n')\n",
    "\n",
    "weights = weights + np.dot(inputs.T, adjustments)\n",
    "print(f'weights:\\n {weights}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Weights after training\n[[ 11.21706391  11.21706801  11.21707498 -10.5981444 ]\n [ 11.21704343  11.21700392  11.21701622 -10.598124  ]]\nOutput after training\n[[5.00000000e-01 5.00000000e-01 5.00000000e-01 5.00000000e-01]\n [9.99986557e-01 9.99986557e-01 9.99986557e-01 2.49629101e-05]\n [9.99986556e-01 9.99986556e-01 9.99986556e-01 2.49634191e-05]\n [1.00000000e+00 1.00000000e+00 1.00000000e+00 6.23190699e-10]]\n"
    }
   ],
   "source": [
    "\n",
    "for iteration in range(10000):\n",
    "    \n",
    "    # Weighted sum of inputs / weights\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "    # Activate!\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    \n",
    "    # Cac error\n",
    "    error = correct_output - activated_output\n",
    "    \n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "    \n",
    "    # Update the Weights\n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "    \n",
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregnancies</th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n      <th>Outcome</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>148</td>\n      <td>72</td>\n      <td>35</td>\n      <td>0</td>\n      <td>33.60000</td>\n      <td>0.62700</td>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>85</td>\n      <td>66</td>\n      <td>29</td>\n      <td>0</td>\n      <td>26.60000</td>\n      <td>0.35100</td>\n      <td>31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>183</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.30000</td>\n      <td>0.67200</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>89</td>\n      <td>66</td>\n      <td>23</td>\n      <td>94</td>\n      <td>28.10000</td>\n      <td>0.16700</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>137</td>\n      <td>40</td>\n      <td>35</td>\n      <td>168</td>\n      <td>43.10000</td>\n      <td>2.28800</td>\n      <td>33</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin      BMI  \\\n0            6      148             72             35        0 33.60000   \n1            1       85             66             29        0 26.60000   \n2            8      183             64              0        0 23.30000   \n3            1       89             66             23       94 28.10000   \n4            0      137             40             35      168 43.10000   \n\n   DiabetesPedigreeFunction  Age  Outcome  \n0                   0.62700   50        1  \n1                   0.35100   31        0  \n2                   0.67200   32        1  \n3                   0.16700   21        0  \n4                   2.28800   33        1  "
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregnancies</th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n      <th>Outcome</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.35294</td>\n      <td>0.74372</td>\n      <td>0.59016</td>\n      <td>0.35354</td>\n      <td>0.00000</td>\n      <td>0.50075</td>\n      <td>0.23442</td>\n      <td>0.48333</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.05882</td>\n      <td>0.42714</td>\n      <td>0.54098</td>\n      <td>0.29293</td>\n      <td>0.00000</td>\n      <td>0.39642</td>\n      <td>0.11657</td>\n      <td>0.16667</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.47059</td>\n      <td>0.91960</td>\n      <td>0.52459</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.34724</td>\n      <td>0.25363</td>\n      <td>0.18333</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.05882</td>\n      <td>0.44724</td>\n      <td>0.54098</td>\n      <td>0.23232</td>\n      <td>0.11111</td>\n      <td>0.41878</td>\n      <td>0.03800</td>\n      <td>0.00000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00000</td>\n      <td>0.68844</td>\n      <td>0.32787</td>\n      <td>0.35354</td>\n      <td>0.19858</td>\n      <td>0.64232</td>\n      <td>0.94364</td>\n      <td>0.20000</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin     BMI  \\\n0      0.35294  0.74372        0.59016        0.35354  0.00000 0.50075   \n1      0.05882  0.42714        0.54098        0.29293  0.00000 0.39642   \n2      0.47059  0.91960        0.52459        0.00000  0.00000 0.34724   \n3      0.05882  0.44724        0.54098        0.23232  0.11111 0.41878   \n4      0.00000  0.68844        0.32787        0.35354  0.19858 0.64232   \n\n   DiabetesPedigreeFunction     Age  Outcome  \n0                   0.23442 0.48333        1  \n1                   0.11657 0.16667        0  \n2                   0.25363 0.18333        1  \n3                   0.03800 0.00000        0  \n4                   0.94364 0.20000        1  "
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "# Setting X and y for classification\n",
    "X = diabetes[feats]\n",
    "y = diabetes['Outcome']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "diabetes[feats] = scaler.fit_transform(X)\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-4-057d68540610>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-057d68540610>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    self.niter = niter\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, niter = 10):\n",
    "    self.niter = niter\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return None\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "    \"\"\"Fit training data\n",
    "    X : Training vectors, X.shape : [#samples, #features]\n",
    "    y : Target values, y.shape : [#samples]\n",
    "    \"\"\"\n",
    "\n",
    "    # Randomly Initialize Weights\n",
    "    self.weight = np.zeros(1 + X.shape[1])\n",
    "\n",
    "    # Number of misclassifications\n",
    "    self.errors = []  # Number of misclassifications\n",
    "\n",
    "    for i in range(self.niter):\n",
    "      err = 0\n",
    "      for xi, target in zip(X, y):\n",
    "        delta_w = self.rate * (target - self.predict(xi))\n",
    "        self.weight[1:] += delta_w * xi\n",
    "        self.weight[0] += delta_w\n",
    "        err += int(delta_w != 0.0)\n",
    "      self.errors.append(err)\n",
    "    return self\n",
    "\n",
    "  def net_input(self, X):\n",
    "    \"\"\"Calculate net input\"\"\"\n",
    "    return np.dot(X, self.weight[1:]) + self.weight[0]\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"Return class label after unit step\"\"\"\n",
    "    return np.where(self.net_input(X) >= 0.0, 1, -1)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "    \"\"\"Return class label after unit step\"\"\"\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python (NN)",
   "language": "python",
   "name": "python-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}