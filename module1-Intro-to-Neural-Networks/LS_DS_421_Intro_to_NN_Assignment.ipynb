{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "## *LSDS Unit 4 Sprint 2 Assignment 1*\n",
    "\n",
    "## Tobias Reaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Define the Following:\n",
    "\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "### Input Layer\n",
    "\n",
    "The input layer is the \"first line of defense\" against an input. It is the first layer to receive the input, hence the nameâ€”though it is sometimes called the visible layer because it is the only part of the network that is exposed to / interacts with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "### Hidden Layer\n",
    "\n",
    "The intermediate layer or layers between the input and the output, if the architecture of the network indeed has an output layer / cells. It is \"hidden\" because it is not generally viewed by the external observer - nor is it really cared about in most circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "### Output Layer\n",
    "\n",
    "As with the above, this layer is also aptly-named, as it is the final layer of neurons which end up providing the final output of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "### Neuron\n",
    "\n",
    "An individual cell within a neural network; the most fundamental unit of such a network, out of which the rest of the properties of the network derive. It is a single node with an activation function that determines, given an input, to fire or not, or how much to fire and where."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "### Weight\n",
    "\n",
    "Another word for the bias term that can be added to the network and which affects the entirety of a single layer of neurons in the same way. This allows for the output of said layer to be...well...biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "### Activation Function\n",
    "\n",
    "The determinant inside each neuron of how much \"signal\" or information is passed through to the next layer. Typically, the neurons within the same layer all have the same activation function. To relate it to biology, this would be what determines whether a neuron fires or not.\n",
    "\n",
    "The activation function is the specification that allows each neuron to aggregate all of the inputs it receives and to produce a single output.\n",
    "\n",
    "Common activation functions such as sigmoid and tanh map all numbers to a limited range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "### Node Map:\n",
    "\n",
    "A visual representation of the internal architecture of a neural network; i.e. a map of the nodes (neurons) in the network and how they are connected to and share information with one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "### Perceptron\n",
    "\n",
    "A perceptron is a neural network architecture which is the simplest of the feedforward methods and one of the simplest and most basic overall. Perceptrons are also known as linear binary classifiers, as they are usually used to classify data into two classes.\n",
    "\n",
    "It is an artificial neuron with a single layer and a unit step activation function. A perceptron connects an arbitrary number of inputs with a single output node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "---\n",
    "\n",
    "## Inputs -> Outputs\n",
    "\n",
    "> Explain the flow of information through a neural network from inputs to outputs.  \n",
    "> Be sure to include: inputs, weights, bias, and activation functions.  \n",
    "> How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "The input layer is the first layer of neurons to interact with the data, and indeed the only layer to interact with it directly. However, what each neuron / perceptron does with the input is generally the same, and specifically similar within each layer.\n",
    "\n",
    "The input is given to the first layer, and each of the neurons that make up that layer multiply the input by a weight (bias), sums the resulting products and passes them into an activation function. The activation function provides the output from each neuron either to the next layer or as the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "> _From Cassidy_\n",
    "\n",
    "The activation of the nodes in the input layer is proportional to the data itself. Each feature in the dataset corresponds to a particular node in the activation layer. Starting after the input, all the neurons in the Nth layer are connected to all the neurons in the N+1th layer.  Each connection (between one node and the next) has an associated weight, which determines how important that connection is.  Higher weights mean that the activation of the N+1th neuron will respond more strongly to activation in the Nth. Each neuron in the hidden layer also has a bias, which gets added in the summation of activations from its predecessor neurons. That way each neuron has a baseline activity regardless of the activation of its predecessors. Each node sums up its inputs and passes them through an activation function, which can map the sum of outputs to a particular range (eg (0-1)) or otherwise transform it in a useful way. \n",
    "\n",
    "The overall flow of information looks like this.  An input neuron activates based on the value of a feature in the dataset.  The activity of that neuron influences all the neurons in the next layer, proportionally to the weights connecting them to the first. Each neuron in the 2nd layer decides on its own output value by considering the contributions from everything in the first layer plus a bias term, all passed through the activation function.  The process then repeats for the next layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Imports ====== #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import dovpanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   0   0  1\n",
       "1   1   0  1\n",
       "2   0   1  1\n",
       "3   1   1  0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "# Define the training data and the target vector\n",
    "\n",
    "inputs = df.drop(columns=[\"y\"])\n",
    "\n",
    "correct_outputs = df.drop(columns=[\"x1\", \"x2\"])\n",
    "\n",
    "print(inputs.shape)\n",
    "print(correct_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BXtmF6m1Ry2E"
   },
   "outputs": [],
   "source": [
    "# Sigmoid activation function and its derivative for updating weights\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of sigmoid function, for updating weights.\"\"\"\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1 - sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12803007],\n",
       "       [-0.03577113]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize random weights for the two inputs\n",
    "weights = 2 * np.random.random((2, 1)) - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [ 0.12803007],\n",
       "       [-0.03577113],\n",
       "       [ 0.09225894]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weighted sum of inputs and weights\n",
    "weighted_sum = np.dot(inputs, weights)\n",
    "weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       ],\n",
       "       [0.53196387],\n",
       "       [0.49105817],\n",
       "       [0.52304839]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the activated values for the end of first training epoch\n",
    "# These are the predictions for the first round\n",
    "activate_output = sigmoid(weighted_sum)\n",
    "activate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.468036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.508942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.523048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          y\n",
       "0  0.500000\n",
       "1  0.468036\n",
       "2  0.508942\n",
       "3 -0.523048"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate error by taking difference of output and correct output\n",
    "err = correct_outputs - activate_output\n",
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23500371],\n",
       "       [0.23311533],\n",
       "       [0.23551451],\n",
       "       [0.23365174]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient descent - adjust the weights to reduce the error on the next epoch\n",
    "adjusted = err = sigmoid_derivative(activate_output)\n",
    "adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== FuncZone ====== #\n",
    "# Put it into a function\n",
    "\n",
    "def perceptronator(X, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the process 5,000 times\n",
    "for _ in range(10000):\n",
    "    \n",
    "    # Weighted sum of inputs and weights\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "    # === Activate: Engage === #\n",
    "    # Get the activated values for the end of first training epoch\n",
    "    activated_out = sigmoid(weighted_sum)\n",
    "    \n",
    "    # Calculate the error\n",
    "    error = correct_outputs - activated_out\n",
    "    \n",
    "    # Calc the adjustments to be made\n",
    "    adjust = error + sigmoid_derivative(activated_out)\n",
    "    \n",
    "    # Update the weights\n",
    "    weights += np.dot(inputs.T, adjust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training\n",
      "[[0.6451489]\n",
      " [0.6451489]]\n",
      "Output after training\n",
      "[[0.5       ]\n",
      " [0.65591645]\n",
      " [0.65591645]\n",
      " [0.78419759]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BXtmF6m1Ry2E"
   },
   "outputs": [],
   "source": [
    "# Sigmoid activation function and its derivative for updating weights\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"Derivative of sigmoid function, for updating weights.\"\"\"\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01593195],\n",
       "       [-0.958399  ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize random weights for the two inputs\n",
    "weights = 2 * np.random.random((2, 1)) - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the process 5,000 times\n",
    "for _ in range(5000):\n",
    "    \n",
    "    # Weighted sum of inputs and weights\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "    # === Activate: Engage === #\n",
    "    # Get the activated values for the end of first training epoch\n",
    "    activated_out = sigmoid(weighted_sum)\n",
    "    \n",
    "    # Calculate the error\n",
    "    error = correct_outputs - activated_out\n",
    "    \n",
    "    # Calc the adjustments to be made\n",
    "    adjust = error + sigmoid_derivative(activated_out)\n",
    "    \n",
    "    # Update the weights\n",
    "    weights += np.dot(inputs.T, adjust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training\n",
      "[[0.6451489]\n",
      " [0.6451489]]\n",
      "Output after training\n",
      "[[0.5       ]\n",
      " [0.65591645]\n",
      " [0.65591645]\n",
      " [0.78419759]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "\n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Test the class' functionality out on the NAND gate defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, n_iter: int = 10):\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"Derivative of sigmoid function, for updating weights.\"\"\"\n",
    "        sx = sigmoid(x)\n",
    "        return sx * (1 - sx)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit on the training data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.arrays\n",
    "            Training vectors, X.shape : [#samples, #features]\n",
    "        y : np.array\n",
    "            Target values, y.shape : [#samples]\n",
    "        \"\"\"\n",
    "        # Set the initial random weight values\n",
    "        weights = np.zeros(1 + X.shape[1])\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            # Weighted sum of inputs and weights\n",
    "            weighted_sum = np.dot(X, weights)\n",
    "            # === Activate === #\n",
    "            activated_out = sigmoid(weighted_sum)\n",
    "            # Calculate the error\n",
    "            error = correct_outputs - activated_out\n",
    "            # Calc the adjustments to be made\n",
    "            adjust = error + sigmoid_derivative(activated_out)\n",
    "            # Update the weights\n",
    "            weights += np.dot(X.T, adjust)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step.\n",
    "        Default step function.\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the dataset as numpy arrays\n",
    "\n",
    "inputs = np.array([[0,0],\n",
    "                   [1,0],\n",
    "                   [0,1],\n",
    "                   [1,1]])\n",
    "\n",
    "correct_outputs = np.array([[1],\n",
    "                           [1],\n",
    "                           [1],\n",
    "                           [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,2) and (3,) not aligned: 2 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-5f825816666a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Instantiate and train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-a4687fc8e543>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# Weighted sum of inputs and weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mweighted_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;31m# === Activate === #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mactivated_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4,2) and (3,) not aligned: 2 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Instantiate and train\n",
    "pn = Perceptron()\n",
    "pn.fit(inputs, correct_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, time to apply it to the diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "X = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Diabetus, Part 2\n",
    "\n",
    "*From Cassidy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "# Copied from https://medium.com/@thomascountz/19-line-line-by-line-python-perceptron-b6f113b161f3\n",
    "# This class uses ReLU activation function and adjusts the weights by \n",
    "# multiplying times the error (label-prediction) and the learning rate. \n",
    "class Perceptron(object):\n",
    "\n",
    "    def __init__(self, no_of_inputs, threshold=100, learning_rate=0.01):\n",
    "        self.threshold = threshold\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.zeros(no_of_inputs + 1)\n",
    "           \n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        if summation > 0:\n",
    "            activation = 1\n",
    "        else:\n",
    "            activation = 0            \n",
    "        return activation\n",
    "\n",
    "    def train(self, training_inputs, labels):\n",
    "        for _ in range(self.threshold):\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                self.weights[1:] += self.learning_rate * (label - prediction) * inputs\n",
    "                self.weights[0] += self.learning_rate * (label - prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K0y5y6l70zS-"
   },
   "source": [
    "First, we'll test it on the same NAND gate defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zE_VYHW60zS_"
   },
   "outputs": [],
   "source": [
    "inputs = np.array([[0,0],\n",
    "                   [1,0],\n",
    "                   [0,1],\n",
    "                   [1,1]])\n",
    "\n",
    "correct_outputs = np.array([[1],\n",
    "                           [1],\n",
    "                           [1],\n",
    "                           [0]])\n",
    "\n",
    "pn = Perceptron(no_of_inputs=2, threshold=100, learning_rate=0.01)\n",
    "pn.train(inputs, correct_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZGNkLWyo0zTA",
    "outputId": "5817c334-ab5b-417e-cbd1-c74cf0055657"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03, -0.01, -0.02])"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FarMWO880zTC",
    "outputId": "4d265cc3-5d58-42c2-9c81-7d1c5068efc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron's predictions for NAND gate\n",
      "0 0 -> 1\n",
      "1 0 -> 1\n",
      "0 1 -> 1\n",
      "1 1 -> 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Perceptron's predictions for NAND gate\")\n",
    "for row in inputs:\n",
    "    print(f'{row[0]} {row[1]} -> {pn.predict(row)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BX15_mVf0zTD"
   },
   "source": [
    "Now for something more complicated, let's try the Pima Indians Diatabetes database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEafwFTV0zTE",
    "outputId": "0bae683d-1474-49ee-d751-41b0b5ea443e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "url = 'https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MHSG6lxl0zTF"
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns='Outcome').values\n",
    "y = df.Outcome.values\n",
    "no_of_inputs = pima_inputs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "92f5_Qzh0zTH",
    "outputId": "2bb04645-0706-40af-94fb-77e91d99dab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [-2.9      8.97     1.14    -2.85    -1.9      1.63     0.635    0.56346\n",
      " -1.2    ]\n",
      "Accuracy: 0.5885416666666666\n"
     ]
    }
   ],
   "source": [
    "# Accuracy after 10 iterations\n",
    "pn = Perceptron(no_of_inputs=no_of_inputs, threshold=10, learning_rate=0.01)\n",
    "pn.train(X, y)\n",
    "y_pred = [pn.predict(row) for row in X]\n",
    "print(f'weights: {pn.weights}')\n",
    "print(f'Accuracy: {accuracy_score(y, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-qyFjLK-0zTI",
    "outputId": "aa1c4c2f-00a4-4773-e0e8-c3ce341f0c25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [-28.28     15.3       0.97     -3.48     -2.74      1.59     -0.127\n",
      "   6.89463  -2.25   ]\n",
      "Accuracy: 0.6536458333333334\n"
     ]
    }
   ],
   "source": [
    "# Accuracy after 100 iterations\n",
    "pn = Perceptron(no_of_inputs=no_of_inputs, threshold=100, learning_rate=0.01)\n",
    "pn.train(X, y)\n",
    "y_pred = [pn.predict(row) for row in X]\n",
    "print(f'weights: {pn.weights}')\n",
    "print(f'Accuracy: {accuracy_score(y, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FrGlVGtI0zTK",
    "outputId": "04140955-5920-41d4-ced5-05382a36fc06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [-237.44      13.58       2.61      -3.4       -2.16       2.52\n",
      "    2.666     46.19301   -1.95   ]\n",
      "Accuracy: 0.6171875\n"
     ]
    }
   ],
   "source": [
    "# Accuracy after 1000 iterations\n",
    "pn = Perceptron(no_of_inputs=no_of_inputs, threshold=1000, learning_rate=0.01)\n",
    "pn.train(X, y)\n",
    "y_pred = [pn.predict(row) for row in X]\n",
    "print(f'weights: {pn.weights}')\n",
    "print(f'Accuracy: {accuracy_score(y, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CoTznnDf0zTN",
    "outputId": "d55f9a14-782b-4635-8824-5ab465a0d902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [-847.85      12.63       5.72      -3.94      -1.88       1.59\n",
      "    8.516    124.50169    1.21   ]\n",
      "Accuracy: 0.6822916666666666\n"
     ]
    }
   ],
   "source": [
    "# Accuracy after 10000 iterations\n",
    "pn = Perceptron(no_of_inputs=no_of_inputs, threshold=10000, learning_rate=0.01)\n",
    "pn.train(X, y)\n",
    "y_pred = [pn.predict(row) for row in X]\n",
    "print(f'weights: {pn.weights}')\n",
    "print(f'Accuracy: {accuracy_score(y, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
