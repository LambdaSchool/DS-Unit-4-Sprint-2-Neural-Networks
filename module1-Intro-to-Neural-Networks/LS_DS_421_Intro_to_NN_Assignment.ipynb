{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:\n",
    "- the layer that takes the input neurons/ brings the data into the system\n",
    "### Hidden Layer:\n",
    "- the \"black box\" of the NN, in between the input and output layers, doesn't look outside\n",
    "### Output Layer:\n",
    "- the final layer that outputs the numbers/makes inferences\n",
    "### Neuron:\n",
    "- a perceptron that takes a weighted sum of inputs, adds bias, as passes through an activation function to determine whether it passes info to the next stage; where output = activation_function(WX + b)\n",
    "### Weight:\n",
    "- a matrix that transforms neuron inputs into another shape\n",
    "### Activation Function:\n",
    "- transforms a matrix into a desirable shape for output. (sigmoid, tanh, step, relu)\n",
    "### Node Map:\n",
    "- a graph of the neural network - where different nodes represent different operations\n",
    "### Perceptron:\n",
    "- a single layer of a neuron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "`inputs --> weights(x) --> bias(+) --> (activation function) --> output`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   0   0  1\n",
       "1   1   0  1\n",
       "2   0   1  1\n",
       "3   1   1  0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sigmoid & derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training\n",
      "[[ 0.64368429]\n",
      " [-0.7410635 ]]\n",
      "Output after training\n",
      "[[0.5       ]\n",
      " [0.66363649]\n",
      " [0.31044607]\n",
      " [0.47041145]]\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([\n",
    "    [0,0],\n",
    "    [1,0],\n",
    "    [0,1],\n",
    "    [1,1]\n",
    "])\n",
    "\n",
    "correct_outputs = [[1], [1], [1], [0]]\n",
    "\n",
    "for iteration in range(1000):\n",
    "    \n",
    "    weights = 2 * np.random.random((2,1)) - 1\n",
    "    \n",
    "    # Weighted sum of inputs / weights\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "    # Activate!\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    \n",
    "    # Cac error\n",
    "    error = correct_outputs - activated_output\n",
    "    \n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "    \n",
    "    # Update the Weights\n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "    \n",
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Perceptron class that takes 2 inputs and spits out 1 output to solve all this stuff for us\n",
    "class Perceptron():\n",
    "    def __init__(self, inputs=2, output=1, rate=0.01, n_iter=10000):\n",
    "        self.input = inputs\n",
    "        self.output = output\n",
    "        self.rate = rate\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "        # Initialize random weights\n",
    "        self.weight = np.random.randn(inputs, output)\n",
    "        \n",
    "        # Initialize bias\n",
    "        self.bias = np.zeros(output)\n",
    "        \n",
    "        # Initialize loss function (empty list for empty perceptron)\n",
    "        self.loss = []\n",
    "        pass\n",
    "    \n",
    "    # Define sigmoid function\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Sigmoid function\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # Sigmoid derivative function\n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"\n",
    "        Sigmoid derivative\n",
    "        \"\"\"\n",
    "        sx = self.sigmoid(x)\n",
    "        return sx * (1-sx)\n",
    "    \n",
    "    # Define neural network affine function\n",
    "    def affine_func(self, x, w, b):\n",
    "        '''\n",
    "        y = Wx + b\n",
    "        '''\n",
    "        scores = x.dot(w) + b\n",
    "        cache = (x, w, b)\n",
    "        return scores, cache\n",
    "    \n",
    "    # Define back-propagation\n",
    "    def back_prop(self, d_out, cache):\n",
    "        '''\n",
    "        dY\n",
    "        '''\n",
    "        x, w, b = cache\n",
    "        dx = d_out.dot(w.T)\n",
    "        dw = x.reshape(-1, 1).dot(d_out.reshape(-1, 1))\n",
    "        db = np.sum(d_out, axis=0)\n",
    "        \n",
    "        return dx, dw, db\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        '''\n",
    "        Fit the perceptron\n",
    "        '''\n",
    "        for i in range(self.n_iter):            \n",
    "            for j in range(x.shape[0]):\n",
    "                # forward prop\n",
    "                scores, cache = self.affine_func(x[j], self.weight, self.bias)\n",
    "                \n",
    "                # scoring\n",
    "                out = self.sigmoid(scores)\n",
    "                loss = y[j] - out.reshape(-1,)\n",
    "                self.loss.append(loss)\n",
    "                \n",
    "                # back prop\n",
    "                dout = loss * self.sigmoid_derivative(out)\n",
    "                _, dw, db = self.back_prop(dout.reshape(-1,), cache)\n",
    "        \n",
    "                # gradient update\n",
    "                self.weight += dw\n",
    "                self.bias += db\n",
    "        pass\n",
    "        \n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        Make predictions\n",
    "        '''\n",
    "        return self.sigmoid(self.affine_func(x, self.weight, self.bias)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['x1', 'x2']].to_numpy()\n",
    "y = df['y'].to_numpy()\n",
    "\n",
    "nand = Perceptron(inputs=2, output=1)\n",
    "nand.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-11.84503314],\n",
       "        [-11.84565959]]), array([17.81572187]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nand.weight, nand.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [0.99999998]\n",
      "[0 1] [0.99745242]\n",
      "[1 0] [0.99745402]\n",
      "[1 1] [0.00280101]\n"
     ]
    }
   ],
   "source": [
    "test1 = np.array([0, 0])\n",
    "test2 = np.array([0, 1])\n",
    "test3 = np.array([1, 0])\n",
    "test4 = np.array([1, 1])\n",
    "print(\"{}\".format(test1), nand.predict(test1))\n",
    "print(\"{}\".format(test2), nand.predict(test2))\n",
    "print(\"{}\".format(test3), nand.predict(test3))\n",
    "print(\"{}\".format(test4), nand.predict(test4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03355237, 0.82762513, 0.40262844, ..., 0.18789327, 0.00350622,\n",
       "        0.27960308],\n",
       "       [0.008424  , 0.71604034, 0.55598426, ..., 0.22407851, 0.00295683,\n",
       "        0.26114412],\n",
       "       [0.04039768, 0.92409698, 0.32318146, ..., 0.11765825, 0.00339341,\n",
       "        0.16159073],\n",
       "       ...,\n",
       "       [0.02691539, 0.65135243, 0.38758161, ..., 0.14103664, 0.00131885,\n",
       "        0.16149234],\n",
       "       [0.00665306, 0.83828547, 0.39918356, ..., 0.20025708, 0.00232192,\n",
       "        0.31269379],\n",
       "       [0.00791454, 0.73605211, 0.55401772, ..., 0.24060198, 0.00249308,\n",
       "        0.18203439]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "X = diabetes[feats]\n",
    "y = diabetes['Outcome']\n",
    "transformer = Normalizer().fit(X)\n",
    "\n",
    "X = transformer.transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "nn = Perceptron(inputs=8, output=1, n_iter=1000)\n",
    "nn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(nn.loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.to_numpy()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5018994123772049"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean((nn.predict(X) - y) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare against linear regression accuracy\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08841098782251833"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
