{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:\n",
    "- The input layer is the beginning of the flow of a neural network. This is the data that is going to be processed by the subsequent layers. The dimensionality of the data is what determines how many nodes make up the input layer.\n",
    "\n",
    "### Hidden Layer:\n",
    "- This is the layer between the input and output layers. The hidden layer takes in the data from the input layer, performs a function on it, then transforms it into outputs.\n",
    "\n",
    "### Output Layer:\n",
    "- The output layer is the results of the transformed data. The number of nodes in the output layer is determined by the number of classes in the data.\n",
    "\n",
    "### Neuron:\n",
    "- Neurons are the individual units that make up a neural network. A neuron is esseantially a function that takes in data, activates/transforms it, and outputs the data.\n",
    "![image](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/47_blog_image_2.png)\n",
    "\n",
    "### Weight:\n",
    "- A weight determines how much significance an input will have on an output. Each input has a weight value associated with it, representing the strength of the connection.\n",
    "\n",
    "### Activation Function:\n",
    "- Also called the transfer function, the activation function determines if the neuron will be activated or not. The purpose of this is to introduce non-linearlity to a model, making it capable of learning and performing more complex tasks.\n",
    "\n",
    "### Node Map:\n",
    "- A node map is a visual diagram of a neural network, showing the path from inputs to outputs. \n",
    "\n",
    "### Perceptron:\n",
    "- Perceptron is a single layer of a neural network. It takes each input, multiplies it by the weight, sums the products, and passes it through the activation function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "\n",
    "![image](https://miro.medium.com/max/1170/1*lk7bbg9gbYKpDYSHABaDOg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs = np.array([[1,0,0],[1,1,0],[1,0,1],[1,1,1]])\n",
    "correct_outputs = [[1], [1], [1], [0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid activation function and derivative\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22123647],\n",
       "       [-0.73027631],\n",
       "       [ 0.1360908 ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize random weights for inputs\n",
    "\n",
    "weights = 2 * np.random.random((3,1)) - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training\n",
      "[[10.87158056]\n",
      " [-7.210552  ]\n",
      " [-7.21055151]]\n",
      "Output after training\n",
      "[[0.99998095]\n",
      " [0.97491392]\n",
      " [0.97491394]\n",
      " [0.02796291]]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(1000):\n",
    "    \n",
    "    # Weighted sum of inputs/weights\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "    # Activate\n",
    "    activated_outputs = sigmoid(weighted_sum)\n",
    "    \n",
    "    # Calculate error\n",
    "    error = correct_outputs - activated_outputs\n",
    "    \n",
    "    # Adjsutments\n",
    "    adjustments = error * sigmoid_derivative(activated_outputs)\n",
    "    \n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "    \n",
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "    \n",
    "print(\"Output after training\")\n",
    "print(activated_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x1 = [0, 1, 0, 1]\n",
    "x2 = [0, 0, 1, 1]\n",
    "\n",
    "x = [0, 1, 0]\n",
    "y = [0, 0, 1]\n",
    "\n",
    "plt.scatter(x, y, color='red', marker='o', label='y = one')\n",
    "plt.scatter(1,1, color='blue', marker='x', label='y = zero')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend(loc='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset like: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "- [Titanic](https://raw.githubusercontent.com/ryanleeallred/datasets/master/titanic.csv)\n",
    "- [A two-class version of the Iris dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/Iris.csv)\n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses Aboard</th>\n",
       "      <th>Parents/Children Aboard</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Owen Harris Braund</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. John Bradley (Florence Briggs Thayer) Cum...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Miss. Laina Heikkinen</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. Jacques Heath (Lily May Peel) Futrelle</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. William Henry Allen</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass                                               Name  \\\n",
       "0         0       3                             Mr. Owen Harris Braund   \n",
       "1         1       1  Mrs. John Bradley (Florence Briggs Thayer) Cum...   \n",
       "2         1       3                              Miss. Laina Heikkinen   \n",
       "3         1       1        Mrs. Jacques Heath (Lily May Peel) Futrelle   \n",
       "4         0       3                            Mr. William Henry Allen   \n",
       "\n",
       "      Sex   Age  Siblings/Spouses Aboard  Parents/Children Aboard     Fare  \n",
       "0    male  22.0                        1                        0   7.2500  \n",
       "1  female  38.0                        1                        0  71.2833  \n",
       "2  female  26.0                        0                        0   7.9250  \n",
       "3  female  35.0                        1                        0  53.1000  \n",
       "4    male  35.0                        0                        0   8.0500  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df[\"Survived\"].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22.    ,  3.    ,  7.25  ],\n",
       "       [38.    ,  1.    , 71.2833],\n",
       "       [26.    ,  3.    ,  7.925 ],\n",
       "       ...,\n",
       "       [ 7.    ,  3.    , 23.45  ],\n",
       "       [26.    ,  1.    , 30.    ],\n",
       "       [32.    ,  3.    ,  7.75  ]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[[\"Age\", \"Pclass\", \"Fare\"]].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "  def __init__(self, rate = 0.01, niter = 10):\n",
    "    self.rate = rate\n",
    "    self.niter = niter\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    \"\"\"Fit training data\n",
    "    X : Training vectors, X.shape : [#samples, #features]\n",
    "    y : Target values, y.shape : [#samples]\n",
    "    \"\"\"\n",
    "\n",
    "    # weights\n",
    "    self.weight = np.zeros(1 + X.shape[1])\n",
    "\n",
    "    # Number of misclassifications\n",
    "    self.errors = []  # Number of misclassifications\n",
    "\n",
    "    for i in range(self.niter):\n",
    "      err = 0\n",
    "      for xi, target in zip(X, y):\n",
    "        delta_w = self.rate * (target - self.predict(xi))\n",
    "        self.weight[1:] += delta_w * xi\n",
    "        self.weight[0] += delta_w\n",
    "        err += int(delta_w != 0.0)\n",
    "      self.errors.append(err)\n",
    "    return self\n",
    "\n",
    "  def net_input(self, X):\n",
    "    \"\"\"Calculate net input\"\"\"\n",
    "    return np.dot(X, self.weight[1:]) + self.weight[0]\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"Return class label after unit step\"\"\"\n",
    "    return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Penalty 0.1 is not supported. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36m_get_penalty_type\u001b[0;34m(self, penalty)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPENALTY_TYPES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '0.1'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-4338b7cc27eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Did not survive\"\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Survived\"\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, penalty, alpha, fit_intercept, max_iter, tol, shuffle, verbose, eta0, n_jobs, random_state, early_stopping, validation_fraction, n_iter_no_change, class_weight, warm_start, n_iter)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mn_iter_no_change\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter_no_change\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             n_iter=n_iter)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loss, penalty, alpha, l1_ratio, fit_intercept, max_iter, tol, shuffle, verbose, epsilon, n_jobs, random_state, learning_rate, eta0, power_t, early_stopping, validation_fraction, n_iter_no_change, class_weight, warm_start, average, n_iter)\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0mvalidation_fraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_fraction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mn_iter_no_change\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter_no_change\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m             average=average, n_iter=n_iter)\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loss, penalty, alpha, C, l1_ratio, fit_intercept, max_iter, tol, shuffle, verbose, epsilon, random_state, learning_rate, eta0, power_t, early_stopping, validation_fraction, n_iter_no_change, warm_start, average, n_iter)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# current tests expect init to do parameter validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# but we are not allowed to set attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_max_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36m_validate_params\u001b[0;34m(self, set_max_iter, for_partial_fit)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# raises ValueError if not registered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_penalty_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_learning_rate_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36m_get_penalty_type\u001b[0;34m(self, penalty)\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mPENALTY_TYPES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Penalty %s is not supported. \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Penalty 0.1 is not supported. "
     ]
    }
   ],
   "source": [
    "pn = Perceptron(0.1, 10)\n",
    "pn.fit(X_train, y_train)\n",
    "results = pn.predict(X_test)\n",
    "print(\"Did not survive\"(results == -1).sum())\n",
    "print(\"Survived\"(results == 1).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljohnson/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "clf = Perceptron()\n",
    "clf.fit(X_train, y_train)\n",
    "resutls = clf.predict(X_test)\n",
    "print((results == 0).sum())\n",
    "print((results == 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
