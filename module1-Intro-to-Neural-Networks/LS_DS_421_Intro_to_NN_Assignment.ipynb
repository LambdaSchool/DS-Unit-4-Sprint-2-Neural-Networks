{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "## *LSDS Unit 4 Sprint 2 Assignment 1*\n",
    "\n",
    "## Tobias Reaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Define the Following:\n",
    "\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "### Input Layer\n",
    "\n",
    "The input layer is the \"first line of defense\" against an input. It is the first layer to receive the input, hence the nameâ€”though it is sometimes called the visible layer because it is the only part of the network that is exposed to / interacts with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "### Hidden Layer\n",
    "\n",
    "The intermediate layer or layers between the input and the output, if the architecture of the network indeed has an output layer / cells. It is \"hidden\" because it is not generally viewed by the external observer - nor is it really cared about in most circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "### Output Layer\n",
    "\n",
    "As with the above, this layer is also aptly-named, as it is the final layer of neurons which end up providing the final output of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "### Neuron\n",
    "\n",
    "An individual cell within a neural network; the most fundamental unit of such a network, out of which the rest of the properties of the network derive. It is a single node with an activation function that determines, given an input, to fire or not, or how much to fire and where."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "### Weight\n",
    "\n",
    "Another word for the bias term that can be added to the network and which affects the entirety of a single layer of neurons in the same way. This allows for the output of said layer to be...well...biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "### Activation Function\n",
    "\n",
    "The determinant inside each neuron of how much \"signal\" or information is passed through to the next layer. Typically, the neurons within the same layer all have the same activation function. To relate it to biology, this would be what determines whether a neuron fires or not.\n",
    "\n",
    "The activation function is the specification that allows each neuron to aggregate all of the inputs it receives and to produce a single output.\n",
    "\n",
    "Common activation functions such as sigmoid and tanh map all numbers to a limited range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "### Node Map:\n",
    "\n",
    "A visual representation of the internal architecture of a neural network; i.e. a map of the nodes (neurons) in the network and how they are connected to and share information with one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "### Perceptron\n",
    "\n",
    "A perceptron is a neural network architecture which is the simplest of the feedforward methods and one of the simplest and most basic overall. Perceptrons are also known as linear binary classifiers, as they are usually used to classify data into two classes.\n",
    "\n",
    "It is an artificial neuron with a single layer and a unit step activation function. A perceptron connects an arbitrary number of inputs with a single output node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "---\n",
    "\n",
    "## Inputs -> Outputs\n",
    "\n",
    "> Explain the flow of information through a neural network from inputs to outputs.  \n",
    "> Be sure to include: inputs, weights, bias, and activation functions.  \n",
    "> How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "The input layer is the first layer of neurons to interact with the data, and indeed the only layer to interact with it directly. However, what each neuron / perceptron does with the input is generally the same, and specifically similar within each layer.\n",
    "\n",
    "The input is given to the first layer, and each of the neurons that make up that layer multiply the input by a weight (bias), sums the resulting products and passes them into an activation function. The activation function provides the output from each neuron either to the next layer or as the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "> _From Cassidy_\n",
    "\n",
    "The activation of the nodes in the input layer is proportional to the data itself. Each feature in the dataset corresponds to a particular node in the activation layer. Starting after the input, all the neurons in the Nth layer are connected to all the neurons in the N+1th layer.  Each connection (between one node and the next) has an associated weight, which determines how important that connection is.  Higher weights mean that the activation of the N+1th neuron will respond more strongly to activation in the Nth. Each neuron in the hidden layer also has a bias, which gets added in the summation of activations from its predecessor neurons. That way each neuron has a baseline activity regardless of the activation of its predecessors. Each node sums up its inputs and passes them through an activation function, which can map the sum of outputs to a particular range (eg (0-1)) or otherwise transform it in a useful way. \n",
    "\n",
    "The overall flow of information looks like this.  An input neuron activates based on the value of a feature in the dataset.  The activity of that neuron influences all the neurons in the next layer, proportionally to the weights connecting them to the first. Each neuron in the 2nd layer decides on its own output value by considering the contributions from everything in the first layer plus a bias term, all passed through the activation function.  The process then repeats for the next layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Imports ====== #\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Set random seed === #\n",
    "np.random.seed(92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Set up the data === #\n",
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [1, 1],\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "    [1],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Steptron, Mach 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Ax_1Ewsu59o"
   },
   "outputs": [],
   "source": [
    "class Steptron:\n",
    "    def __init__(self, eta: float = 0.01, epochs: int = 10):\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit Perceptron to training dataset.\"\"\"\n",
    "        # === Initialize weights === #\n",
    "        self.weight = np.zeros(1 + X.shape[1])\n",
    "        self.errors = []  # Number of misclassifications\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            err = 0\n",
    "            print(f\"=== Epoch {i + 1} ===\")\n",
    "            for xi, target in zip(X, y):\n",
    "                # === Visual indiciation of results === #\n",
    "                print(f\"Expected: {target} | Predicted: {self.predict(xi)}\")\n",
    "                \n",
    "                delta_w = self.eta * (target - self.predict(xi))\n",
    "                self.weight[1:] += delta_w * xi\n",
    "                self.weight[0] += delta_w\n",
    "                err += int(delta_w != 0.0)\n",
    "\n",
    "            self.errors.append(err)\n",
    "\n",
    "        return \"Gonna need a montage!\"\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input.\"\"\"\n",
    "        return np.dot(X, self.weight[1:]) + self.weight[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return target label after unit step.\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1 ===\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [0] | Predicted: 1\n",
      "=== Epoch 2 ===\n",
      "Expected: [1] | Predicted: 0\n",
      "Expected: [1] | Predicted: 0\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [0] | Predicted: 1\n",
      "=== Epoch 3 ===\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [1] | Predicted: 0\n",
      "Expected: [1] | Predicted: 0\n",
      "Expected: [0] | Predicted: 1\n",
      "=== Epoch 4 ===\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [1] | Predicted: 0\n",
      "Expected: [0] | Predicted: 1\n",
      "=== Epoch 5 ===\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [1] | Predicted: 0\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [0] | Predicted: 0\n",
      "=== Epoch 6 ===\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [0] | Predicted: 0\n",
      "=== Epoch 7 ===\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [0] | Predicted: 0\n",
      "=== Epoch 8 ===\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [0] | Predicted: 0\n",
      "=== Epoch 9 ===\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [0] | Predicted: 0\n",
      "=== Epoch 10 ===\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [1] | Predicted: 1\n",
      "Expected: [0] | Predicted: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Gonna need a montage!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Instantiate and fit the model === #\n",
    "sn = Steptron()\n",
    "sn.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "\n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "X = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Diabetus, Part 2\n",
    "\n",
    "*From Cassidy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "# Copied from https://medium.com/@thomascountz/19-line-line-by-line-python-perceptron-b6f113b161f3\n",
    "# This class uses ReLU activation function and adjusts the weights by \n",
    "# multiplying times the error (label-prediction) and the learning rate. \n",
    "class Perceptron(object):\n",
    "\n",
    "    def __init__(self, no_of_inputs, threshold=100, learning_rate=0.01):\n",
    "        self.threshold = threshold\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.zeros(no_of_inputs + 1)\n",
    "           \n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        if summation > 0:\n",
    "            activation = 1\n",
    "        else:\n",
    "            activation = 0            \n",
    "        return activation\n",
    "\n",
    "    def train(self, training_inputs, labels):\n",
    "        for _ in range(self.threshold):\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                self.weights[1:] += self.learning_rate * (label - prediction) * inputs\n",
    "                self.weights[0] += self.learning_rate * (label - prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K0y5y6l70zS-"
   },
   "source": [
    "First, we'll test it on the same NAND gate defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zE_VYHW60zS_"
   },
   "outputs": [],
   "source": [
    "inputs = np.array([[0,0],\n",
    "                   [1,0],\n",
    "                   [0,1],\n",
    "                   [1,1]])\n",
    "\n",
    "correct_outputs = np.array([[1],\n",
    "                           [1],\n",
    "                           [1],\n",
    "                           [0]])\n",
    "\n",
    "pn = Perceptron(no_of_inputs=2, threshold=100, learning_rate=0.01)\n",
    "pn.train(inputs, correct_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZGNkLWyo0zTA",
    "outputId": "5817c334-ab5b-417e-cbd1-c74cf0055657"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03, -0.01, -0.02])"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FarMWO880zTC",
    "outputId": "4d265cc3-5d58-42c2-9c81-7d1c5068efc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron's predictions for NAND gate\n",
      "0 0 -> 1\n",
      "1 0 -> 1\n",
      "0 1 -> 1\n",
      "1 1 -> 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Perceptron's predictions for NAND gate\")\n",
    "for row in inputs:\n",
    "    print(f'{row[0]} {row[1]} -> {pn.predict(row)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BX15_mVf0zTD"
   },
   "source": [
    "Now for something more complicated, let's try the Pima Indians Diatabetes database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEafwFTV0zTE",
    "outputId": "0bae683d-1474-49ee-d751-41b0b5ea443e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "url = 'https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MHSG6lxl0zTF"
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns='Outcome').values\n",
    "y = df.Outcome.values\n",
    "no_of_inputs = pima_inputs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "92f5_Qzh0zTH",
    "outputId": "2bb04645-0706-40af-94fb-77e91d99dab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [-2.9      8.97     1.14    -2.85    -1.9      1.63     0.635    0.56346\n",
      " -1.2    ]\n",
      "Accuracy: 0.5885416666666666\n"
     ]
    }
   ],
   "source": [
    "# Accuracy after 10 iterations\n",
    "pn = Perceptron(no_of_inputs=no_of_inputs, threshold=10, learning_rate=0.01)\n",
    "pn.train(X, y)\n",
    "y_pred = [pn.predict(row) for row in X]\n",
    "print(f'weights: {pn.weights}')\n",
    "print(f'Accuracy: {accuracy_score(y, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-qyFjLK-0zTI",
    "outputId": "aa1c4c2f-00a4-4773-e0e8-c3ce341f0c25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [-28.28     15.3       0.97     -3.48     -2.74      1.59     -0.127\n",
      "   6.89463  -2.25   ]\n",
      "Accuracy: 0.6536458333333334\n"
     ]
    }
   ],
   "source": [
    "# Accuracy after 100 iterations\n",
    "pn = Perceptron(no_of_inputs=no_of_inputs, threshold=100, learning_rate=0.01)\n",
    "pn.train(X, y)\n",
    "y_pred = [pn.predict(row) for row in X]\n",
    "print(f'weights: {pn.weights}')\n",
    "print(f'Accuracy: {accuracy_score(y, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FrGlVGtI0zTK",
    "outputId": "04140955-5920-41d4-ced5-05382a36fc06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [-237.44      13.58       2.61      -3.4       -2.16       2.52\n",
      "    2.666     46.19301   -1.95   ]\n",
      "Accuracy: 0.6171875\n"
     ]
    }
   ],
   "source": [
    "# Accuracy after 1000 iterations\n",
    "pn = Perceptron(no_of_inputs=no_of_inputs, threshold=1000, learning_rate=0.01)\n",
    "pn.train(X, y)\n",
    "y_pred = [pn.predict(row) for row in X]\n",
    "print(f'weights: {pn.weights}')\n",
    "print(f'Accuracy: {accuracy_score(y, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CoTznnDf0zTN",
    "outputId": "d55f9a14-782b-4635-8824-5ab465a0d902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [-847.85      12.63       5.72      -3.94      -1.88       1.59\n",
      "    8.516    124.50169    1.21   ]\n",
      "Accuracy: 0.6822916666666666\n"
     ]
    }
   ],
   "source": [
    "# Accuracy after 10000 iterations\n",
    "pn = Perceptron(no_of_inputs=no_of_inputs, threshold=10000, learning_rate=0.01)\n",
    "pn.train(X, y)\n",
    "y_pred = [pn.predict(row) for row in X]\n",
    "print(f'weights: {pn.weights}')\n",
    "print(f'Accuracy: {accuracy_score(y, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
