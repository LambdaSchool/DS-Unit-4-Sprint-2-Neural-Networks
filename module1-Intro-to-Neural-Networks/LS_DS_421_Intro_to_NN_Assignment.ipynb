{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_421_Intro_to_NN_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coopwilliams/DS-Unit-4-Sprint-2-Neural-Networks/blob/master/module1-Intro-to-Neural-Networks/LS_DS_421_Intro_to_NN_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dVfaLrjLvxvQ"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "# Neural Networks\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 1*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wxtoY12mwmih"
      },
      "source": [
        "## Define the Following:\n",
        "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
        "\n",
        "### Input Layer:\n",
        "A layer of neurons that process the input to a neural network. These neurons compute an activation function on the inputs and pass on the result.\n",
        "### Hidden Layer:\n",
        "A layer of neurons that applies a non-linear function to their inputs.\n",
        "### Output Layer:\n",
        "A layer of neurons that linearly scales its input to produce the \"answer\" returned by the neural network.\n",
        "### Neuron:\n",
        "A node in a neural network, inspired by real-life neural function in the brain. Each node has weights, biases, and usually an activation function. These weights can be modified through backpropagation.\n",
        "### Weight:\n",
        "A scalar that is multiplied by the input to a neuron.\n",
        "### Activation Function:\n",
        "A function that normalizes the input to a neuron.\n",
        "### Node Map:\n",
        "A diagram of a neural network architecture.\n",
        "### Perceptron:\n",
        "A simple neural network for binary classication.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NXuy9WcWzxa4"
      },
      "source": [
        "## Inputs -> Outputs\n",
        "\n",
        "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PlSwIJMC0A8F"
      },
      "source": [
        "#### Your Answer Here\n",
        "Scalars are inputted to a neural network through the input cells. The activation functions in the input cells compute a linear transformation on those scalars. The result is passed on to many (if not all of) the cells in the first hidden layer. Each hidden layer in its turn computes a non-linear transformation on its inputs and passes on the result. Often this transformation is as simple as multiplying each input by a weight and summing all the products. The last hidden layer passes its results to the output layer, which scales its inputs to produce the final output of the neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6sWR43PTwhSk"
      },
      "source": [
        "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
        "\n",
        "| x1 | x2 | y |\n",
        "|----|----|---|\n",
        "| 0  | 0  | 1 |\n",
        "| 1  | 0  | 1 |\n",
        "| 0  | 1  | 1 |\n",
        "| 1  | 1  | 0 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otEhdPPBfHKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data = { 'x1': [0,1,0,1],\n",
        "         'x2': [0,0,1,1],\n",
        "         'y':  [1,1,1,0]\n",
        "       }\n",
        "\n",
        "df = pd.DataFrame.from_dict(data).astype('float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sgh7VFGwnXGH",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    sx = sigmoid(x)\n",
        "    return sx * (1-sx)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puzYO7ZxfHK1",
        "colab_type": "code",
        "outputId": "1015b84c-682e-4a7e-bca3-b0b9498bce4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "x1    float64\n",
              "x2    float64\n",
              "y     float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkyCQi_tfHK5",
        "colab_type": "code",
        "outputId": "f281ecfb-c789-4153-917a-717316e22d4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "X = df.loc[:,'x1':'x2'].to_numpy()\n",
        "y = df.loc[:, 'y'].to_numpy().reshape(4, 1)\n",
        "# input, targets\n",
        "X, y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 1.]]), array([[1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjOwDEptfHK9",
        "colab_type": "code",
        "outputId": "b9f4c951-dacd-45ee-effb-a78e11a0564f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "weights = 2 * np.random.random((2,1)) - 1\n",
        "weights"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.49221345],\n",
              "       [-0.41017347]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SU7Jd6nfHLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bias = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-67bd1MfHLX",
        "colab_type": "code",
        "outputId": "b82a4caf-d229-45e1-b56c-cf828723b2eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(10):\n",
        "    \n",
        "    weighted_sum = np.dot(X, weights) + bias\n",
        "    print(\"-------\\n\",weighted_sum)\n",
        "    activated_output = sigmoid(weighted_sum)\n",
        "    print(activated_output)\n",
        "    error = y - activated_output\n",
        "    print(\"err\", error)\n",
        "    adjustments = error * sigmoid_derivative(activated_output)\n",
        "    print(X.T, adjustments)\n",
        "    weights += np.dot(X.T, adjustments)\n",
        "    bias += error\n",
        "\n",
        "print(\"Weights\")\n",
        "print(weights)\n",
        "print(\"output\")\n",
        "print(activated_output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------\n",
            " [[ 0.        ]\n",
            " [ 0.49221345]\n",
            " [-0.41017347]\n",
            " [ 0.08203998]]\n",
            "[[0.5       ]\n",
            " [0.62062773]\n",
            " [0.39887053]\n",
            " [0.5204985 ]]\n",
            "err [[ 0.5       ]\n",
            " [ 0.37937227]\n",
            " [ 0.60112947]\n",
            " [-0.5204985 ]]\n",
            "[[0. 1. 0. 1.]\n",
            " [0. 0. 1. 1.]] [[ 0.11750186]\n",
            " [ 0.08626602]\n",
            " [ 0.14445997]\n",
            " [-0.12169452]]\n",
            "-------\n",
            " [[ 0.5       ]\n",
            " [ 0.83615723]\n",
            " [ 0.21372145]\n",
            " [-0.45112157]]\n",
            "[[0.62245933]\n",
            " [0.69765527]\n",
            " [0.55322791]\n",
            " [0.38909414]]\n",
            "err [[ 0.37754067]\n",
            " [ 0.30234473]\n",
            " [ 0.44677209]\n",
            " [-0.38909414]]\n",
            "[[0. 1. 0. 1.]\n",
            " [0. 0. 1. 1.]] [[ 0.08580219]\n",
            " [ 0.06708653]\n",
            " [ 0.10356457]\n",
            " [-0.09368281]]\n",
            "-------\n",
            " [[ 0.87754067]\n",
            " [ 1.11190568]\n",
            " [ 0.6703753 ]\n",
            " [-0.85693023]]\n",
            "[[0.70631233]\n",
            " [0.75248422]\n",
            " [0.66158719]\n",
            " [0.29798111]]\n",
            "err [[ 0.29368767]\n",
            " [ 0.24751578]\n",
            " [ 0.33841281]\n",
            " [-0.29798111]]\n",
            "[[0. 1. 0. 1.]\n",
            " [0. 0. 1. 1.]] [[ 0.06497565]\n",
            " [ 0.05388439]\n",
            " [ 0.07598128]\n",
            " [-0.07286578]]\n",
            "-------\n",
            " [[ 1.17122834]\n",
            " [ 1.34044006]\n",
            " [ 1.0119036 ]\n",
            " [-1.17077725]]\n",
            "[[0.76336697]\n",
            " [0.7925623 ]\n",
            " [0.73339252]\n",
            " [0.23671452]]\n",
            "err [[ 0.23663303]\n",
            " [ 0.2074377 ]\n",
            " [ 0.26660748]\n",
            " [-0.23671452]]\n",
            "[[0. 1. 0. 1.]\n",
            " [0. 0. 1. 1.]] [[ 0.05131276]\n",
            " [ 0.04449798]\n",
            " [ 0.05843568]\n",
            " [-0.05835731]]\n",
            "-------\n",
            " [[ 1.40786137]\n",
            " [ 1.53401843]\n",
            " [ 1.27858945]\n",
            " [-1.42127273]]\n",
            "[[0.80342841]\n",
            " [0.8225935 ]\n",
            " [0.78220957]\n",
            " [0.19446214]]\n",
            "err [[ 0.19657159]\n",
            " [ 0.1774065 ]\n",
            " [ 0.21779043]\n",
            " [-0.19446214]]\n",
            "[[0. 1. 0. 1.]\n",
            " [0. 0. 1. 1.]] [[ 0.04199373]\n",
            " [ 0.0376205 ]\n",
            " [ 0.04690027]\n",
            " [-0.04815881]]\n",
            "-------\n",
            " [[ 1.60443296]\n",
            " [ 1.70088662]\n",
            " [ 1.49512134]\n",
            " [-1.62753171]]\n",
            "[[0.83263704]\n",
            " [0.8456505 ]\n",
            " [0.81684571]\n",
            " [0.16416877]]\n",
            "err [[ 0.16736296]\n",
            " [ 0.1543495 ]\n",
            " [ 0.18315429]\n",
            " [-0.16416877]]\n",
            "[[0. 1. 0. 1.]\n",
            " [0. 0. 1. 1.]] [[ 0.03535134]\n",
            " [ 0.03243474]\n",
            " [ 0.03892616]\n",
            " [-0.04076689]]\n",
            "-------\n",
            " [[ 1.77179592]\n",
            " [ 1.84690397]\n",
            " [ 1.67643489]\n",
            " [-1.80187337]]\n",
            "[[0.85468087]\n",
            " [0.86376318]\n",
            " [0.84243187]\n",
            " [0.14162317]]\n",
            "err [[ 0.14531913]\n",
            " [ 0.13623682]\n",
            " [ 0.15756813]\n",
            " [-0.14162317]]\n",
            "[[0. 1. 0. 1.]\n",
            " [0. 0. 1. 1.]] [[ 0.03042668]\n",
            " [ 0.02842031]\n",
            " [ 0.03315361]\n",
            " [-0.03522885]]\n",
            "-------\n",
            " [[ 1.91711505]\n",
            " [ 1.97633225]\n",
            " [ 1.83192777]\n",
            " [-1.95238033]]\n",
            "[[0.87181638]\n",
            " [0.87828963]\n",
            " [0.86199122]\n",
            " [0.12429404]]\n",
            "err [[ 0.12818362]\n",
            " [ 0.12171037]\n",
            " [ 0.13800878]\n",
            " [-0.12429404]]\n",
            "[[0. 1. 0. 1.]\n",
            " [0. 0. 1. 1.]] [[ 0.0266525 ]\n",
            " [ 0.02523921]\n",
            " [ 0.0288107 ]\n",
            " [-0.0309538 ]]\n",
            "-------\n",
            " [[ 2.04529867]\n",
            " [ 2.09232802]\n",
            " [ 1.96779345]\n",
            " [-2.08453206]]\n",
            "[[0.88547171]\n",
            " [0.89015526]\n",
            " [0.87737391]\n",
            " [0.11060934]]\n",
            "err [[ 0.11452829]\n",
            " [ 0.10984474]\n",
            " [ 0.12262609]\n",
            " [-0.11060934]]\n",
            "[[0. 1. 0. 1.]\n",
            " [0. 0. 1. 1.]] [[ 0.02367927]\n",
            " [ 0.02266663]\n",
            " [ 0.02543872]\n",
            " [-0.02756793]]\n",
            "-------\n",
            " [[ 2.15982696]\n",
            " [ 2.19727146]\n",
            " [ 2.08829033]\n",
            " [-2.20217191]]\n",
            "[[0.89658351]\n",
            " [0.90000422]\n",
            " [0.88975984]\n",
            " [0.09955562]]\n",
            "err [[ 0.10341649]\n",
            " [ 0.09999578]\n",
            " [ 0.11024016]\n",
            " [-0.09955562]]\n",
            "[[0. 1. 0. 1.]\n",
            " [0. 0. 1. 1.]] [[ 0.02128273]\n",
            " [ 0.02054913]\n",
            " [ 0.02275198]\n",
            " [-0.02482734]]\n",
            "Weights\n",
            "[[ 0.35677483]\n",
            " [-0.38585458]]\n",
            "output\n",
            "[[0.89658351]\n",
            " [0.90000422]\n",
            " [0.88975984]\n",
            " [0.09955562]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2kYSKVifHLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xf7sdqVs0s4x"
      },
      "source": [
        "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
        "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
        "\n",
        "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgizodYPfHLf",
        "colab_type": "code",
        "outputId": "b3600fe6-1b96-4193-8b74-ade71372c956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
        "diabetes.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlh0UDe-fHLk",
        "colab_type": "text"
      },
      "source": [
        "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlJ--duLfHLl",
        "colab_type": "code",
        "outputId": "eb4c34a0-448d-4f85-ebc9-3f1fd818a859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
        "\n",
        "feats = list(diabetes)[:-1]\n",
        "\n",
        "minmax = Normalizer()\n",
        "\n",
        "X = minmax.fit_transform(diabetes[feats])\n",
        "y = diabetes['Outcome'].to_numpy()\n",
        "train = pd.DataFrame(data=X, columns=feats)\n",
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.033552</td>\n",
              "      <td>0.827625</td>\n",
              "      <td>0.402628</td>\n",
              "      <td>0.195722</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.187893</td>\n",
              "      <td>0.003506</td>\n",
              "      <td>0.279603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.008424</td>\n",
              "      <td>0.716040</td>\n",
              "      <td>0.555984</td>\n",
              "      <td>0.244296</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.224079</td>\n",
              "      <td>0.002957</td>\n",
              "      <td>0.261144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.040398</td>\n",
              "      <td>0.924097</td>\n",
              "      <td>0.323181</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.117658</td>\n",
              "      <td>0.003393</td>\n",
              "      <td>0.161591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.006612</td>\n",
              "      <td>0.588467</td>\n",
              "      <td>0.436392</td>\n",
              "      <td>0.152076</td>\n",
              "      <td>0.621527</td>\n",
              "      <td>0.185797</td>\n",
              "      <td>0.001104</td>\n",
              "      <td>0.138852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.596386</td>\n",
              "      <td>0.174127</td>\n",
              "      <td>0.152361</td>\n",
              "      <td>0.731335</td>\n",
              "      <td>0.187622</td>\n",
              "      <td>0.009960</td>\n",
              "      <td>0.143655</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies   Glucose  ...  DiabetesPedigreeFunction       Age\n",
              "0     0.033552  0.827625  ...                  0.003506  0.279603\n",
              "1     0.008424  0.716040  ...                  0.002957  0.261144\n",
              "2     0.040398  0.924097  ...                  0.003393  0.161591\n",
              "3     0.006612  0.588467  ...                  0.001104  0.138852\n",
              "4     0.000000  0.596386  ...                  0.009960  0.143655\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXDaDUUiEJZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv670V7_OxhJ",
        "colab_type": "code",
        "outputId": "6eee54dc-f6b0-4a18-eff5-754cb8dbb5de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(384, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZl88rTxcfuJ",
        "colab_type": "code",
        "outputId": "83323a27-1f6a-403a-d69e-7217e8f38faf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(384,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-t7WhA0O90e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = y_train.reshape(384, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JXFYvbPfHLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class Perceptron():\n",
        "#     def __init__(self, rate=0.5, n_iter=10):\n",
        "#         self.rate = rate\n",
        "#         self.n_iter = n_iter\n",
        "#     def fit(self, X, y):\n",
        "#         self.weights = np.zeros(1 + X.shape[1])\n",
        "#         self.errors = []\n",
        "#         for i in range(self.n_iter):\n",
        "#             err = 0\n",
        "#             for xi, target in zip(X, y):\n",
        "#                 print(\"-------xt\", xi, target)\n",
        "#                 print(\"delta is\", (target - self.predict(xi)))\n",
        "#                 delta = self.rate * (target - self.predict(xi))\n",
        "#                 self.weights[1:] += delta * xi\n",
        "#                 self.weights[0] += delta\n",
        "#                 err += int(delta != 0.0)\n",
        "#                 if delta != 0.0:\n",
        "#                     print(delta, self.weights)\n",
        "#             self.errors.append(err)\n",
        "#             print(\"errs\", self.errors)\n",
        "#             print(\"-----------\")\n",
        "#         return self\n",
        "#     def net_input(self, X):\n",
        "#         return np.dot(X, self.weights[1:]) + self.weights[0]\n",
        "#     def predict(self, X):\n",
        "#         return np.where(self.net_input(X) >= 0.0, 1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-W0tiX1F1hh2",
        "colab": {}
      },
      "source": [
        "##### Update this Class #####\n",
        "\n",
        "class Perceptron(object):\n",
        "    \n",
        "    def __init__(self, niter = 10):\n",
        "      self.niter = niter\n",
        "      self.weights = 2 * np.random.random((8, 1)) - 1\n",
        "      self.bias = 0\n",
        "    def __sigmoid(x):\n",
        "      return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def __sigmoid_derivative(x):\n",
        "      sx = sigmoid(x)\n",
        "      return sx * (1-sx)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "      \"\"\"Fit training data\n",
        "      X : Training vectors, X.shape : [#samples, #features]\n",
        "      y : Target values, y.shape : [#samples]\n",
        "      \"\"\"\n",
        "\n",
        "      # Randomly Initialize Weights\n",
        "      \n",
        "      for i in range(self.niter):\n",
        "        weighted_sum = np.dot(X, self.weights) + self.bias\n",
        "        # print(\"-------\\nWeighted\",weighted_sum.shape)\n",
        "        activated_output = sigmoid(weighted_sum)\n",
        "        # print(\"activated\", activated_output.shape)\n",
        "        error = y - activated_output\n",
        "        # print(\"err\", y.shape, error.shape)\n",
        "        adjustments = error * sigmoid_derivative(activated_output)\n",
        "        # print(\"err\", error.shape)\n",
        "        # print(\"adjustments\", X.shape, adjustments.shape)\n",
        "        # print(error.sum())\n",
        "        self.weights += np.dot(X.T, adjustments)\n",
        "        self.bias += error\n",
        "\n",
        "    def predict(self, X):\n",
        "      \"\"\"Return class label after unit step\"\"\"\n",
        "      # print(X.shape, self.weights.shape, self.bias.shape)\n",
        "      netted = sigmoid(np.dot(X, self.weights) + self.bias)\n",
        "      # print(netted)\n",
        "      # return np.where(netted >= 0.5, 1, 0)\n",
        "      return netted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lraUpXiLDkQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "percy = Perceptron(niter=20000)\n",
        "percy.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8TEtzgfKC-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test = y_test.reshape(384, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPkQi0ptMBiB",
        "colab_type": "code",
        "outputId": "d1c01e8e-8d4e-48f2-83dc-1ebe5afe8b5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(384, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3a4gwLZPEDO",
        "colab_type": "code",
        "outputId": "c14a8fe9-9a3b-4742-a66e-a2a0b95160ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(384, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDrIjuu5MwDW",
        "colab_type": "code",
        "outputId": "626533fe-dbd1-4b1f-eadf-f2ad5f9ae2a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "percy.weights"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  2.6098826 ],\n",
              "       [  3.86426758],\n",
              "       [-12.1227976 ],\n",
              "       [  0.78962576],\n",
              "       [ -0.50126405],\n",
              "       [ -1.20380815],\n",
              "       [ -0.05986509],\n",
              "       [ -0.37123798]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LYHjL0CKKUX",
        "colab_type": "code",
        "outputId": "a84abd3f-c944-4e72-e353-80dee8513c0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred = percy.predict(X_test)\n",
        "# sum(np.where(y_pred != y_test, 1, 0))\n",
        "sum(y_pred - y_test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([24.23566798])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6QR4oAW1xdyu"
      },
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
        "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
        "- Try and implement your own backpropagation algorithm.\n",
        "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmAZSVxeN3CU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}