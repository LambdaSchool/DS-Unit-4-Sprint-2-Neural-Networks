{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:\n",
    "The input layer, or visible layer, is the only neuron layer that interacts with and receives input from the dataset. There can be only one input layer in the network but it can have many nodes.\n",
    "### Hidden Layer:\n",
    "The hidden layers can only be accessed by first going through the input layer; that is, they never see the raw original data but only the data after the input layer has passed it along. There can be multiple hidden layers.\n",
    "### Output Layer:\n",
    "The output layer is the final layer in the network. Just like the input layer, there can be only one output layer. In a regression or binary classification model, there will be only one output node, while in a multiclass classification there would be multiple output nodes, one for each class.\n",
    "### Neuron:\n",
    "The neurons are the nodes that make up the neural network. They are called neurons because the idea of a neural network is to use math to represent the brain, and the brain is made up of cells called neurons. In the brain, a neuron sends a charge along to the next neuron if a certain threshold of energy is met; in a neural network, the nodes receive inputs and pass information to the next node if the input reaches a specified threshold.\n",
    "Each neuron \n",
    "### Weight:\n",
    "Weight represents the relationship between two nodes. It is the same as the gradient, or steepness, of a linear regression. \n",
    "\n",
    "### Activation Function:\n",
    "The activation function modifies the output value so that it's in a format that fits the needs of our model. For example, a classification problem would need to produce a probability, which is rounded up to 1 or down to 0.\n",
    "### Node Map:\n",
    "A node map is a high level visual representation of the layers of a neural network.\n",
    "### Perceptron:\n",
    "A perceptron is the simplest type of neural network, which is just a single node. It takes in multiple inputs and gives an output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "#### \n",
    "Information enters the neural network in the form of an input node and has a bias value added to it (similarly to the y-intercept value in a linear regression). This total is multiplied by a weight. If the neural network has additional hidden layers, they will take in this new value and return a calculated value. Once the data travels through all of the layers, it comes to the output. If an activation function is needed to adjust the results of the calculations, it is run here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "correct_outputs = np.array([[1],[1],[1],[0],[0]])\n",
    "inputs = np.array([\n",
    "    [0,0], [1,0], [0,1], [1,1], [1,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1 - sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.45015828],\n",
       "       [ 0.54457178]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = 2 * np.random.random((2,1)) - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(10000):\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    error = correct_outputs - activated_output\n",
    "    adjustments = error * sigmoid_derivative(weighted_sum)\n",
    "    weights += np.dot(inputs.T, adjustments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training\n",
      "[[-0.36490336]\n",
      " [-0.36490336]]\n",
      "Output after training\n",
      "[[0.5       ]\n",
      " [0.40977312]\n",
      " [0.40977312]\n",
      " [0.32523714]\n",
      " [0.32523714]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "X = diabetes[feats].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-113-7dd8dc9e220f>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-113-7dd8dc9e220f>\"\u001b[1;36m, line \u001b[1;32m18\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m       \n^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, niter = 10):\n",
    "        self.niter = niter\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return None\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "    \"\"\"Fit training data\n",
    "    X : Training vectors, X.shape : [#samples, #features]\n",
    "    y : Target values, y.shape : [#samples]\n",
    "    \"\"\"\n",
    "\n",
    "        # Randomly Initialize Weights\n",
    "        self.weight = 2 * np.random.random(X.shape[1],1) - 1\n",
    "\n",
    "        for i in range(self.niter):\n",
    "            # Weighted sum of inputs / weights\n",
    "            weighted_sum = np.dot(inputs, weights)\n",
    "            # Activate!\n",
    "            activated_outputs = sigmoid(weighted_sum)\n",
    "            # Cac error\n",
    "            error = correct_outputs - activated_outputs\n",
    "            # Update the Weights\n",
    "            adjustments = error * sigmoid_derivative(weighted_sum)\n",
    "            weights += np.dot(inputs.T, adjustments)\n",
    "\n",
    "    def predict(self, X):\n",
    "    \"\"\"Return class label after unit step\"\"\"\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
