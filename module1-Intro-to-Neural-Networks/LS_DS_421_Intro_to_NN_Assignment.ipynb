{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:\n",
    "\n",
    "* aka x = dependent variable\n",
    "* Each input is a feature or category from the dataframe\n",
    "* The more inputs we have the bigger the input layer (more nodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layer:\n",
    "\n",
    "* A layer within the NN that allows for various models to be held. We don't directly interact with the hidden layer.\n",
    "* The more complicated a Hidden Layer, the more time and resource intensive the NN will be. \n",
    "* Hidden Layers can be thought of as a series of models we pass the data through for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer:\n",
    "\n",
    "* The final layer of the Neural Network, where we receive our predictions\n",
    "* Illustrated as a scientific number currently\n",
    "* At this step the NN is feeding the finalized predictions from the models in the wires/hidden layers\n",
    "    Then the NN finds the best of the best and releases those, feeding them back to the input layer for comparison and update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron:\n",
    "\n",
    "* Also known as a node, basically an encapsulated feature, with a minimum threshold for firing a signal to the next node. If the signal is weaker, less precedence. If the threshold is met and exceeded, that node will be considered a more superior node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight:\n",
    "\n",
    "* Based on the accuracy of the current model on the node/category. If more accurate, or it reaches the threshold for firing a signal, the node's weight is likely to be higher, whereas a node that never sends a signal would be considered negative (or, in simple terms, something for the NN to ignore.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function:\n",
    "\n",
    "* The function used to calculate the point of activation for the node to fire to the next layer of nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Map:\n",
    "\n",
    "* Showcases the layout of the neural network. Different node maps are called different things, depending on presence of inputs, outputs, hidden layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron:\n",
    "\n",
    "* The most basic NN model. Requires two inputs, has one output, no showcased hidden layers. Weights are the wires connecting the perceptron model, bias is assumed to be accounted for in most node maps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "Each node is a category in a datafram. That node is then passed through the activation function. If the prediction and subsequent comparison of the target within the node is accurate, the node gets a higher weight. This is done using the MAE/MSE model in our example. Whichever metric you use, the better the prediction is to the target, the more likely that node is going to fire to the next layer of modes. After applying the weight, we then apply the bias (the average mean of the category encapsulated as a node in this case), and move on to the next layer. In the Perceptron model, the inputs move through the activation function/weight/bias, and then are output as the amount the prediction is closest to the actual target. (scientific notation). We then pass that information back to the input mode in order to improve the weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [[0 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]] output [[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(143)\n",
    "data= { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')\n",
    "\n",
    "\n",
    "inputs = np.array(df[['x1', 'x2']].values)\n",
    "correct_output = np.array(df['y'].apply(lambda x:[x]).tolist())\n",
    "print(\"input\", inputs, \"output\", correct_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Weights after training: \n",
      " [[14.91340903]\n",
      " [-9.97695756]\n",
      " [-9.97695756]] \n",
      " \n",
      " Output after training: \n",
      " [[0.99999967]\n",
      " [0.99287044]\n",
      " [0.99287044]\n",
      " [0.00642952]]\n"
     ]
    }
   ],
   "source": [
    "#SIGMOID ACTIVATION FUNCTION#\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/ (1 + np.exp(-x))\n",
    "\n",
    "#SIGMOID WEIGHT UPDATE#\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)\n",
    "\n",
    "\n",
    "#Initialize Random Weights for our inputs\n",
    "weights = 2 * np.random.random((3,1)) - 1\n",
    "\n",
    "for iteration in range(10000):\n",
    "    #Weighted Sum\n",
    "    weighted_sum = np.dot(inputs, weights[1:]) + weights[0]\n",
    "\n",
    "    #Results of 1 Training Epoch \n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    #True Values are [1,1,1,0]\n",
    "\n",
    "    #Calculate Error\n",
    "    error = correct_output - activated_output\n",
    "\n",
    "    #Gradient Descent/Backprop\n",
    "    adjusted = error * sigmoid_derivative(activated_output)\n",
    "\n",
    "    #Epoch/Formalization\n",
    "    weights[1:] += np.dot(inputs.T, adjusted)\n",
    "    weights[0] += adjusted.mean()\n",
    "\n",
    "\n",
    "print(\n",
    "      '\\n \\n Weights after training: \\n', weights, \n",
    "      '\\n \\n Output after training: \\n', activated_output\n",
    "      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()\n",
    "\n",
    "\n",
    "#Borrowed from Chris for analysis since I bombed at this section\n",
    "import seaborn as sns \n",
    "feats = list(diabetes[:-1])\n",
    "for a in feats:\n",
    "    for b in feats:\n",
    "        sns.relplot(x=a, y=b, hue='Outcome', data=diabetes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97518461, 0.22139326],\n",
       "       [0.95436003, 0.29865855],\n",
       "       [0.99199174, 0.12630277],\n",
       "       ...,\n",
       "       [0.97735099, 0.21162476],\n",
       "       [0.97263201, 0.23235098],\n",
       "       [0.95050693, 0.31070334]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = ['Glucose', 'BMI']\n",
    "normal = Normalizer()\n",
    "X2 = normal.transform(diabetes[feats])\n",
    "\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, niter = 10):\n",
    "    self.niter = niter\n",
    "    self.inputs = 2\n",
    "    self.outputNodes = 1\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "        return \n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        sx = self.__sigmoid(x)\n",
    "        return sx * (1 - sx)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "    \"\"\"Fit training data\n",
    "    X : Training vectors, X.shape : [#samples, #features]\n",
    "    y : Target values, y.shape : [#samples]\n",
    "    \"\"\"\n",
    "\n",
    "    # Randomly Initialize Weights\n",
    "    weights = ...\n",
    "\n",
    "    for i in range(self.niter):\n",
    "        # Weighted sum of inputs / weights\n",
    "\n",
    "        # Activate!\n",
    "\n",
    "        # Cac error\n",
    "\n",
    "        # Update the Weights\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "    \"\"\"Return class label after unit step\"\"\"\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
