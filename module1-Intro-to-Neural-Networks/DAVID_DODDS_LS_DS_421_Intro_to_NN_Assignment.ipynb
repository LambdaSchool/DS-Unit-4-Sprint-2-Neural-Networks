{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer: the first layer of nodes in a neural network, recieving the normalized or transformed version of the original data, which enters the neural network as a vector. Each node corresponds to a feature of the observation. The input layer does itself have weights, biases, and an activation function, but it only outputs activation values to the next layer. It does not recieve activation values. It only recieves the vectorized raw data.\n",
    "\n",
    "### Hidden Layer: a layer of nodes which does not recieve inputs from the original data. It only recieves activation values from previous nodes of the neural network. However, nodes in a hidden layer also do not present a final output value from the neural network. They recieve activation values, and must also pass activation values on to other nodes in subsequent layers. Hidden layer nodes are therefore intermediating nodes. They also have weights, biases, and an activation function.\n",
    "\n",
    "### Output Layer: a layer of nodes which does not recive input activation values from the original data, but from previous nodes in the neural network; but unlike a hidden layer, it's output activation values are the final outputs of the neural network. These output activation values are interpreted as probablilities, each node either representing a class within the target, a target itself, or some medley of the two. Nodes in the output layer also have wights, biases, and activation functions, but their output activation values are the final interpretation of the neural network upon that data observation. No further nodes recieve activation values from the output layer.\n",
    "\n",
    "### Neuron: also known as a node, this is a function within a neural network which recieves input values either from the raw original data, or from one or more previous nodes. It applies weight coefficients and a bias intercept to those input values, and the sum of those weights and biases applied to each prior input are run through an activation function to relativize the output of the node compared to all other outputs in its layer. This final relativized output is the activation value, which is then input to one or many subsequent nodes, or is interpreted as an output value of the neural network in the case of the output layer.\n",
    "\n",
    "### Weight: a coefficient multiplied to an input from a previous node, or the activation value from a previous node. This weight value is uniqe to the function of that node, and sometimes to the node from which it recieves the input.\n",
    "\n",
    "### Activation Function: a function which relates the nodes that are members of the same layer. Each node being a function that presents to the activation function its weighted sum, the activation function then returns to the node an activation value that is calculated relative to all of the other nodes encapsulated by that activation function. It is that activation value that is then passed from one node to the next layer, or as an output.\n",
    "\n",
    "### Node Map: a visual depiction of the node architecture, flow of activation values and layer membership between the nodes of a neural network. The nodes can be color coded based on layer membership, input properties, output properties, or function structure, but generally nodes in the same layer are aligned and parallel to other layers, and data moves from left to right. The leftmost layer is usually the input layer, and the rightmost layer is usually the output layer.\n",
    "\n",
    "### Perceptron: The simplest architecture possible for a neural network. A perceptron is a neural network with no hidden layer. Data flows directly into the input layer, and then directly into the final output layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "## 1] The raw data is vectorized, usually into a float value from between 1 and 0.\n",
    "## 2] A single observation is fed into the input layer as a vector, usually with each utilized feature being passed to a single input node.\n",
    "## 3] Input nodes take the input value, multipy it by a weight, add a bias to that product, and pass that weighted sum into the activation function for the input layer.\n",
    "## 4] The acivation function of the input layer uses a compression-type function to compress sometimes radically varying input values to within 1 and 0, relative to the range of other weighted sums it recieves from nodes of that layer. It then returns to the respective node its \"squishified\", or [1,0] compressed weighted sum (the activation value).\n",
    "## 5] The node then passes its activation value on to other nodes in the next layer, as an input to that node's weight/bias function.\n",
    "## 6] If the next node in the network, recieving that prior activation value, is a hidden layer node, steps 3-5 are repeated for that node and that layer, and it's activation value is passed onto one or many nodes in the subsequent layer.\n",
    "## 7] If the next node in the network, recieving that prior activation value, is an output layer node, steps 3-5 are repeated for that node and that layer, but it's activation value is not passed into another node. It is output to us, the consumer of that data, assigned to a target or target class, as a probability that the observation it came from represents a result in that target or that target class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   0   0  1\n",
       "1   1   0  1\n",
       "2   0   1  1\n",
       "3   1   1  0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "      <th>bises</th>\n",
       "      <th>biases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y  bises  biases\n",
       "0   0   0  1      1       1\n",
       "1   1   0  1      1       1\n",
       "2   0   1  1      1       1\n",
       "3   1   1  0      1       1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['biases'] = [1]*4\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [1, 0, 1],\n",
       "       [0, 1, 1],\n",
       "       [1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.asarray(df[['x1','x2','biases']])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ_outp = np.asarray(df[['y']])\n",
    "targ_outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.718281828459045"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sig_dirv(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9933071490757153"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.73838908],\n",
       "       [ 0.06171138],\n",
       "       [-0.53454334]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = 2*np.random.random((3,1))-1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.53454334],\n",
       "       [ 0.20384574],\n",
       "       [-0.47283196],\n",
       "       [ 0.26555712]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_sum = np.dot(inputs, weights)\n",
    "weight_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36945785],\n",
       "       [0.5507857 ],\n",
       "       [0.38394618],\n",
       "       [0.56600186]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actv_vals = sigmoid(weight_sum)\n",
    "actv_vals\n",
    "\n",
    "#AND HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.63054215],\n",
       "       [ 0.4492143 ],\n",
       "       [ 0.61605382],\n",
       "       [-0.56600186]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = targ_outp - actv_vals\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14689031],\n",
       "       [ 0.11114497],\n",
       "       [ 0.14571614],\n",
       "       [-0.13903482]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = error * sig_dirv(weight_sum)\n",
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.71049923],\n",
       "       [ 0.0683927 ],\n",
       "       [-0.26982675]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights += np.dot(inputs.T, adj)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training \n",
      " [[-10.66174175]\n",
      " [-10.66174175]\n",
      " [ 16.07735254]]\n",
      "\n",
      " Output after training \n",
      " [[0.9999999 ]\n",
      " [0.99557305]\n",
      " [0.99557305]\n",
      " [0.00524028]]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(100000):\n",
    "    weight_sum = np.dot(inputs, weights)\n",
    "    actv_vals = sigmoid(weight_sum)\n",
    "    error = targ_outp - actv_vals\n",
    "    adj = error * sig_dirv(weight_sum)\n",
    "    weights += np.dot(inputs.T, adj)\n",
    "\n",
    "print('Weights after training \\n', weights)\n",
    "print('\\n Output after training \\n', actv_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+000],\n",
       "       [1.00000000e+000],\n",
       "       [1.00000000e+000],\n",
       "       [1.00000000e+000],\n",
       "       [1.00000000e+000],\n",
       "       [1.00000000e+000],\n",
       "       [1.00000000e+000],\n",
       "       [1.00000000e+000],\n",
       "       [9.75035066e-040],\n",
       "       [1.00000000e+000],\n",
       "       [1.00000000e+000],\n",
       "       [1.00000000e+000],\n",
       "       [1.00000000e+000],\n",
       "       [8.27284880e-118],\n",
       "       [1.00000000e+000]])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "inputs = diabetes[feats]\n",
    "\n",
    "inputs['biases'] = [1]*len(inputs)\n",
    "X = np.array(inputs)\n",
    "y = np.array(diabetes['Outcome'])\n",
    "np.random.seed(8)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sig_dirv(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)\n",
    "\n",
    "weights = 2*np.random.random((len(X[0]),1))-1\n",
    "weight_sum = np.dot(X, weights)\n",
    "actv_vals = sigmoid(weight_sum)\n",
    "actv_vals[:15]\n",
    "\n",
    "# LEFT OFF HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>biases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  biases  \n",
       "0                     0.627   50       1  \n",
       "1                     0.351   31       1  \n",
       "2                     0.672   32       1  \n",
       "3                     0.167   21       1  \n",
       "4                     2.288   33       1  "
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  bias  \n",
       "0                     0.627   50     1  \n",
       "1                     0.351   31     1  \n",
       "2                     0.672   32     1  \n",
       "3                     0.167   21     1  \n",
       "4                     2.288   33     1  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs['bias'] = [1] * len(inputs)\n",
    "# inputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = np.array(diabetes[['Outcome']])\n",
    "\n",
    "y = np.array(diabetes['Outcome'])\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(inputs)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "normer = Normalizer().fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03355237, 0.82762513, 0.40262844, 0.19572216, 0.        ,\n",
       "        0.18789327, 0.00350622, 0.27960308],\n",
       "       [0.008424  , 0.71604034, 0.55598426, 0.24429612, 0.        ,\n",
       "        0.22407851, 0.00295683, 0.26114412],\n",
       "       [0.04039768, 0.92409698, 0.32318146, 0.        , 0.        ,\n",
       "        0.11765825, 0.00339341, 0.16159073],\n",
       "       [0.00661199, 0.58846737, 0.43639153, 0.15207584, 0.62152733,\n",
       "        0.185797  , 0.0011042 , 0.13885185],\n",
       "       [0.        , 0.5963863 , 0.17412739, 0.15236146, 0.73133502,\n",
       "        0.18762226, 0.00996009, 0.14365509]])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0 = normer.transform(X)\n",
    "X0[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "# class Perceptron(object):\n",
    "    \n",
    "#     def __init__(self, rate = 0.01, niter = 10):\n",
    "#         self.rate = rate\n",
    "#         self.niter = niter\n",
    "    \n",
    "#     def __sigmoid(self, x):\n",
    "#         return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "#     def __sigmoid_derivative(self, x):\n",
    "#         sx = sigmoid(x)\n",
    "#         return sx * (1-sx)\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         # Randomly Initialize Weights\n",
    "#         self.weight = np.zeros(1+X.shape[1])\n",
    "# #         print(weight.shape)\n",
    "#         self.bias = np.ones((X.shape[0],1))\n",
    "#     #adds a bias value of 1 to each observation, so now tey have 9 values\n",
    "#         X = np.concatenate((X,self.bias), axis=1)\n",
    "        \n",
    "#         # Number of misclassifications\n",
    "#         self.errors = []\n",
    "\n",
    "#         for i in range(self.niter):\n",
    "#             err = 0\n",
    "#             # Weighted sum of inputs / weights\n",
    "#             for xi, target in zip(X,y):\n",
    "#                 delta_w = self.rate * (target - self.predict(xi))\n",
    "#             # Activate!\n",
    "#                 self.weight += delta_w * xi\n",
    "# #                 self.bias[0] += delta_w\n",
    "#             # Cac error\n",
    "#                 err += int(delta_w)\n",
    "#             # Update the Weights\n",
    "#             self.errors.append(err)\n",
    "#         return self\n",
    "    \n",
    "#     def net_input(self, X):\n",
    "#         return np.dot(X, self.weight) #+ self.bias[0]\n",
    "\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         \"\"\"Return class label after unit step\"\"\"\n",
    "#         return np.where(self.net_input(X) >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn = Perceptron(8,100,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.train(X0,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, len(pn.errors)+1), pn.errors, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of Misclassifications')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 1)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-298-2c486d845625>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-292-d8fd4d74b673>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0msummation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0msummation\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m           \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "pred = pn.predict(X0.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020833333333333332"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
