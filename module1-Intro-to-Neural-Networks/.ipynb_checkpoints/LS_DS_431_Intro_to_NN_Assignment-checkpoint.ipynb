{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "# Intro to Neural Networks Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "\n",
    "\n",
    "### Input Layer:\n",
    "    Input layers hold data that models train on. Each neuron in the input layer(s) acts as unique attributes of the data being processed or could be more complex, \n",
    "    such as a previous layer's neurons. The network uses the input/s, passes them through multiple hidden layers and creates outputs that predict the combined input \n",
    "    layer.\n",
    "\n",
    "### Hidden Layer:\n",
    "    You can think of a hidden layer as a miniature function that has unknown coefficients. We want to learn those coefficients.\n",
    "    There are usually multiple hidden layers, where each neuron recieves input from all the previous layer's neurons sends the output to each neuron in the \n",
    "    next layer. Convultional networks differ though, they send the output to only some of the neurons in the proceeding layer.\n",
    "### Output Layer:\n",
    "    Recieves input from the previous or final hidden layer, it can also apply an activation function, and ultimately it returns an output that represents your\n",
    "    modeled networks predictions.\n",
    "### Neuron:\n",
    "    The neuron is essentially applying an activation function (given the sum of weights of the input layers) that passes them to the \n",
    "    output layer in the form of an output nodule. \n",
    "### Weight:\n",
    "    Weights are pretty obvious, it's the importance of the input. \n",
    "    They do get somewhat complex though. Each input is multipled by the weight, but this weight is determined by the synapse that connects the input to the current \n",
    "    neuron. If there are 5 inputs or linked nuerons from the previous layer, we can expect 5 distinct weights, one for ecah connecting synapse.\n",
    "    \n",
    "    If we have only one input, the network is literally just linear regression, with the difference being the network processes the weighted input with an \n",
    "    activation function.\n",
    "    \n",
    "    Something to think about is applied bias terms, which are additional constant values that we attach to our neurons. Essentially they add to the weighted inputs \n",
    "    before the activation function is applied to the summed weights. \n",
    "    \n",
    "    Ultimately, the higher the attached weight, the more importance that is given to that input, or combination of inputs, previous neurons in the chained layers.\n",
    "### Activation Function:\n",
    "    Activation functions are the powerhouses of neural networks. They're what allows us to model complex non-linear relationships. \n",
    "    Our activation functions modify our input with non-linear functions, which while it may sound simple from a high-level overview, \n",
    "    can really become quite complex. Some of the most popular activation functions are relu, sigmoid. \n",
    "    \n",
    "    Typically activation functions are non-linear. This is contrasted nicely with linear regression which limits a predicted equation to a straight line. \n",
    "    Neural networks use gradient descent to continually improve themselves, and so our activation functions are tpically continuously differentiable.\n",
    "    Activation functions also typically sqaush daya into narrow ranges that make the model more effecient and stable.\n",
    "    \n",
    "   #### How does aggregation work for the activation function\n",
    "   Note this this is for fully connected networks only. CNN/etc are different.\n",
    "   ![aggregation](https://cdn-images-1.medium.com/max/900/1*4MVN69gdM72BtTtY75tntg.png)\n",
    "       \n",
    "       If we break down this picture into parts we have\n",
    "       b = bias\n",
    "       x = input to neuron\n",
    "       w = weights\n",
    "       n = number of inputs from incoming layer\n",
    "       i = counter from 0 to n\n",
    "       \n",
    "       Initially, the only neurons that will even have values are the input neurons on \n",
    "       the input layer.\n",
    "       \n",
    "           1. We multiply every incoming nueron by it's corresponding weight.\n",
    "           2. We sum total all the values.\n",
    "           3. We add the bias to the neuron in question.\n",
    "       \n",
    "       We take all the weights and begin to throw them in matrices for processing.\n",
    "           1. Neural nets make weight matrices from N-to-M\n",
    "           2. A M-by-1 matrix is made from the biases\n",
    "           3. We can view inputs as a N-by-1 matrix.\n",
    "           4. We tranpose the weight matrix to M-by-N\n",
    "           5. Find the dot product of the transposed weights and inputs. \n",
    "              Meaning we multiply M-by-N and N-by-1, which gives us M-by-1\n",
    "           6. We add the output of this to the bias matrix.\n",
    "           7. Now we can run an activation function on each value in the vector.\n",
    "   \n",
    "    \n",
    "### Loss functions\n",
    "    We alos need to think about our loss, or cost function. The loss function is simply a metric that tells us how good the model is at making predictions given \n",
    "    a particular set of parameters. Loss functions have their own curve, and their own derivatives. The slope of a loss function's curve tells us how to change the\n",
    "    paramaters, or how the model should change it's paramaters to become more effecient. When the model makes a prediction, it uses the cost function to update the\n",
    "    params. Popular cost functions are MSE and cross-entropy loss\n",
    "    \n",
    "### Perceptron:\n",
    "    A perceptron is merely a neural network with a single layer. \n",
    "    More formally it is a form of supervised learning for binary classification.\n",
    "    We call this derived classifcation a threshold function, as it maps the input \n",
    "    x to f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset like: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "- [Titanic](https://raw.githubusercontent.com/ryanleeallred/datasets/master/titanic.csv)\n",
    "- [A two-class version of the Iris dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/Iris.csv)\n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Perceptron(object):\n",
    "    \"\"\"Implements a perceptron network\"\"\"\n",
    "    def __init__(self, input_size, learning_rate=1, epochs=100):\n",
    "        self.weight = np.zeros(input_size + 1)\n",
    "        # add one for bias\n",
    "        self.epochs = epochs\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def activation_fn(self, x):\n",
    "        \"\"\"\"\"\"\n",
    "        return 1 if x >= 0 else 0\n",
    "\n",
    "    def predict(self, x):\n",
    "        # see notes above, we agg by taking transposed dot product of weights and \n",
    "        # the 'incoming' input. For initialzation, this is the original data.\n",
    "        # since this is a single layer, it won't grow more complex.\n",
    "        z = self.W.T.dot(x)\n",
    "        a = self.activation_fn(z)\n",
    "        return a\n",
    "\n",
    "    def fit(self, X, labels):\n",
    "        for _ in range(self.epochs):\n",
    "            for i in range(labels.shape[0]):\n",
    "                x = np.insert(X[i], 0, 1)\n",
    "                y = self.predict(x)\n",
    "                e = d[i] - y\n",
    "                self.W = self.W + self.lr * e * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "658.222px",
    "left": "1365.34px",
    "top": "50.2222px",
    "width": "341.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
