{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "\n",
    "# <u>**Intro to Neural Networks**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "\n",
    "\n",
    "<img src=\"https://static1.squarespace.com/static/5800c6211b631b49b4d63657/t/5a6caf4753450a17187dd1d3/1517075821859/fullyconnected_525.gif\" alt=\"gif\" title=\"gif\"/>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*V1mNUbnpA7thNIUCHJNpuA.png\" alt=\"abstraction\" title=\"abstraction\"/>\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "## <u>**Neuron**</u>:\n",
    "### The basic unit of computation in a neural network is the neuron, often called a node or unit. It receives input from some other nodes, or from an external source and computes an output. Each input has an associated weight (w), which is assigned on the basis of its relative importance to other inputs. The node applies a function f (defined below) to the weighted sum of its inputs as shown in the Figure below:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-09-at-3-42-21-am.png?w=568&h=303\" alt=\"Image of Neuron\" title=\"An Image of a Neuron\"/>\n",
    "\n",
    "\n",
    "\n",
    "> ### **Operations at one neuron of a neural network**:\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*H2GFATdntBwfKcR4Kuc5mw.png\" alt=\"op\" title=\"ops\"/>\n",
    "\n",
    "\n",
    "> ### **Connections:**\n",
    "> ### Information flows through the network across connections. A connection always has a weight value associated with it. Goal of the training is to update this weight value to decrease the loss(error). \n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*5zQUUvTbSTJyGQH_qdB2hA.png\" alt=\"conn\" title=\"conns\"/>\n",
    "\n",
    "\n",
    "\n",
    "> ### <u>**Weights:**</u>\n",
    "> ### A weight represents the strength of the connection between units. If the weight from node 1 to node 2 has greater magnitude, it means that neuron 1 has greater influence over neuron 2. A weight brings down the importance of the input value. Weights near zero means changing this input will not change the output. Negative weights mean increasing this input will decrease the output. A weight decides how much influence the input will have on the output.\n",
    "> ### <u>**Bias(Offset):**</u>\n",
    "> ### A bias unit is an \"extra\" neuron added to each pre-output layer that stores the value of a constant (typically 1 or -1). Bias units aren't connected to any previous layer and in this sense don't represent a true \"activity\".\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*0NKtEk20-qnaLkwOa8DlnA.png\" alt=\"conn\" title=\"conns\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "## <u>**Activation Function**</u>: \n",
    "### An activation function decides, whether a neuron should be activated or not by calculating weighted sum and further adding bias with it. The purpose of the activation function is to introduce non-linearity into the output of a neuron.\n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-d131b1b1ffb1ae9d842e135f05635f1c\" alt=\"act\" title=\"acts\" height=\"899.2\" width=\"1199.2\"/>\n",
    "\n",
    "___\n",
    "## <u>**Layers**</u>: \n",
    "### A node layer is a row of neurons that turn on or off as the input is fed through the net. Each layer’s output is simultaneously the subsequent layer’s input, starting from an initial input layer receiving your data.\n",
    "\n",
    "\n",
    "> ### <u>**Input Layer**</u>: First layer - this is where information from the outside world enters the neural network. Nodes in this layer pass information to the hidden layer.\n",
    "> ### <u>**Hidden Layer**</u>: Nodes in the hidden layer have no direct connection to the outside world - they perform computations and transfer information from the input nodes to the output nodes.\n",
    "> ### <u>**Output Layer**</u>: This is the layer where information exits the neural network (ideally structured that it is interpretable by humans)\n",
    "\n",
    "\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2017/07/Screen-Shot-2017-07-26-at-1.44.58-PM.png\" alt=\"A diagram of a Neural Network\" title=\"Neural Network Diagram\" height=\"408.24\" width=\"715.05\"  />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "## <u>**Perceptron**</u>: \n",
    "### The first and simplest kind of neural network that we could talk about is the perceptron. A perceptron is just a single node or neuron of a neural network with nothing else. It can take any number of inputs and spit out an output. What a neuron does is it takes each of the input values, multplies each of them by a weight, sums all of these products up, and then passes the sum through an activation function the result of which is the final value.\n",
    "\n",
    "![Figure 2.1](http://www.ryanleeallred.com/wp-content/uploads/2019/04/Screen-Shot-2019-04-01-at-2.34.58-AM.png)\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "#### Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset like: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "- [Titanic](https://raw.githubusercontent.com/ryanleeallred/datasets/master/titanic.csv)\n",
    "- [A two-class version of the Iris dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/Iris.csv)\n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
