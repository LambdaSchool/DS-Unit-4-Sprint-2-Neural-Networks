{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "# Intro to Neural Networks Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer: \n",
    "The input layers is the layer that receives the input from our dataset. (also known as the visible layer), it's the only layer exposed to the data and interacts directly with the data.\n",
    "### Hidden Layer:\n",
    "The hidden layer can only be accessed through the input layer and this is where the functions are performed.\n",
    "### Output Layer:\n",
    "The output layer is the final layer where we receive some result that is modified by an activation function with the purpose of transforming said result into a format that makes sense for the context of our problem.\n",
    "### Neuron:\n",
    "A neuron can visualize a single node within our neural network where on the left to represent the dendrites, we have our input layer, the center being the axon reprented by the weights and bias, and the end of the neuron where the axon terminal would be, would be represented by the output layer.\n",
    "### Weight:\n",
    "The weight can be thought of as a transformation of the input layer before being exposed to the transformation by the activation function.  The weights may sometimes be followed by some bias to represent a constant value. \n",
    "### Activation Function:\n",
    "The activation function is found in each node that decides how much signal is passed onto the next layer. This is especially important in neural networks because it's telling the input it needs to pass this test through the activation function to make it to the next step.  \n",
    "### Node Map:\n",
    "A node map can be thought of as a visual diagram of the architecture or 'topology' of our neural network. It's like a flow chart that shows the path from inputs to outputs. \n",
    "### Perceptron:\n",
    "The first and simplest kind of neural network that is represented by a signle node or neuron of a neural network with nothing else. \n",
    "It can take any number of inputs and spit out an output. What a neuron does is it takes each of the input values, multiplies each of them by a weight, sums all of these products up, and then passes the sum through an activation function and the result of which is hte final value. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "#### Your Answer Here\n",
    "\n",
    "We first have inputs that can be represented by the data from our dataset that will pass through our neural network and at the end of our neural network we'll have outputs that will be labeled representations of whatever we are trying to predict. In the process of our neural networks we'll have assigned some weights and bias to help the neural networks assume certain features of what the output should be. Before we get an output, the output layer is checked with an activation function that will determine whether or not the weighted inputs that are passed through the neural network carry enough signal to pass onto the final layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "inputs = np.array([\n",
    "    [0,0,1],\n",
    "    [1,0,1],\n",
    "    [0,1,1],\n",
    "    [1,1,0]\n",
    "])\n",
    "\n",
    "correct_outputs = [[0],[1],[1],[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/ (1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.16595599],\n",
       "       [ 0.44064899],\n",
       "       [-0.99977125]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = 2 * np.random.random((3,1)) - 1\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.99977125],\n",
       "       [-1.16572724],\n",
       "       [-0.55912226],\n",
       "       [ 0.274693  ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_sum = np.dot(inputs,weights)\n",
    "weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2689864 ],\n",
       "       [0.23762817],\n",
       "       [0.36375058],\n",
       "       [0.56824466]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activated_outputs = sigmoid(weighted_sum)\n",
    "activated_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2689864 ],\n",
       "       [ 0.76237183],\n",
       "       [ 0.63624942],\n",
       "       [-0.56824466]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = correct_outputs - activated_outputs\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06604473],\n",
       "       [ 0.18792752],\n",
       "       [ 0.15391469],\n",
       "       [-0.13118329]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjustments = error * sigmoid_derivative(activated_outputs)\n",
    "adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10921176],\n",
       "       [ 0.46338038],\n",
       "       [-0.72397378]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#update weights\n",
    "weights += np.dot(inputs.T, adjustments)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training\n",
      "[[-0.22491407]\n",
      " [-0.22491407]\n",
      " [ 0.82736567]]\n",
      "Ouptut after training\n",
      "[[0.69579763]\n",
      " [0.64621699]\n",
      " [0.64621699]\n",
      " [0.38940163]]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(10000):\n",
    "    weighted_sum = np.dot(inputs,weights)\n",
    "    \n",
    "    # activate!\n",
    "    activated_outputs = sigmoid(weighted_sum)\n",
    "    \n",
    "    # calculate the error\n",
    "    error = correct_outputs - activated_outputs\n",
    "    \n",
    "    # calculate weight wadjustments - watch linked videos for more intuition!\n",
    "    adjustments = error * sigmoid(activated_outputs)\n",
    "    \n",
    "    # Update weights\n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "\n",
    "print('Weights after training')\n",
    "print(weights)\n",
    "\n",
    "print('Ouptut after training')\n",
    "print(activated_outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset like: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "- [Titanic](https://raw.githubusercontent.com/ryanleeallred/datasets/master/titanic.csv)\n",
    "- [A two-class version of the Iris dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/Iris.csv)\n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
